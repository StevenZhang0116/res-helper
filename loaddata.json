{"path": ["../paper/1-s2.0-S0959438800001537-main.pdf", "../paper/PIIS0896627309002062.pdf", "../paper/2012.05208.pdf", "../paper/NIPS-2011-policy-gradient-coagent-networks-Paper.pdf", "../paper/carlson16.pdf", "../paper/PhysRevLett.59.2229.pdf", "../paper/make-03-00032-v2.pdf", "../paper/1-s2.0-S0896627315010405-main.pdf", "../paper/nrn1406.pdf", "../paper/2107.04084.pdf", "../paper/*nrn3136.pdf", "../paper/fnhum-06-00359.pdf", "../paper/09-AOS692.pdf", "../paper/s10827-009-0179-x.pdf", "../paper/1703.00810.pdf", "../paper/nature12983.pdf", "../paper/1-s2.0-S0896627310007579-main.pdf", "../paper/2030801.pdf", "../paper/s41467-020-17236-y.pdf", "../paper/6-3-406.pdf", "../paper/nature11911.pdf", "../paper/1710.10044.pdf", "../paper/2111.06377.pdf", "../paper/nn.4486.pdf", "../paper/nature14855.pdf", "../paper/1810.07411.pdf", "../paper/science.279.5355.1351.pdf", "../paper/Parallel_Reinforcement_Learning_with_Linear_Functi.pdf", "../paper/1908.01867.pdf", "../paper/nature07140.pdf", "../paper/SussilloArXive16.pdf", "../paper/2112.full.pdf", "../paper/1804.00222.pdf", "../paper/hafner19a.pdf", "../paper/johansen-et-al-2014-hebbian-and-neuromodulatory-mechanisms-interact-to-trigger-associative-memory-formation.pdf", "../paper/1909.12475.pdf", "../paper/wan13.pdf", "../paper/1702.08591.pdf", "../paper/2102.05815.pdf", "../paper/489-A16.pdf", "../paper/graybiel-2008-habits-rituals-and-the-evaluative-brain.pdf", "../paper/nrn2286.pdf", "../paper/1811.03600.pdf", "../paper/1803.01206.pdf", "../paper/NIPS-1997-generalized-prioritized-sweeping-Paper.pdf", "../paper/1-s2.0-S0896627317307791-mainext.pdf", "../paper/1611.01232.pdf", "../paper/srep00417.pdf", "../paper/1301.3583.pdf", "../paper/1602.03253.pdf", "../paper/1-s2.0-S009286741730990X-main.pdf", "../paper/nature13294.pdf", "../paper/nature10776.pdf", "../paper/1-s2.0-S0896627321007285-main.pdf", "../paper/2101.03288.pdf", "../paper/agarwal14a.pdf", "../paper/deconvolutionalnetworks.pdf", "../paper/2307.09218.pdf", "../paper/2106.04554.pdf", "../paper/s10994-011-5235-x.pdf", "../paper/NeurIPS-2019-putting-an-end-to-end-to-end-gradient-isolated-learning-of-representations-Paper.pdf", "../paper/1602.02830.pdf", "../paper/1936.full.pdf", "../paper/41593_2010_BFnn2501_MOESM20_ESM.pdf", "../paper/1406.2661.pdf", "../paper/1604.07379.pdf", "../paper/1708.02596.pdf", "../paper/1-s2.0-S0896627319300108-main.pdf", "../paper/fncir-09-00085.pdf", "../paper/science.pdf", "../paper/annurev-neuro-120320-082744.pdf", "../paper/2111.08005.pdf", "../paper/guo22d.pdf", "../paper/1905.00414.pdf", "../paper/nn.4242.pdf", "../paper/0700.pdf", "../paper/2023.10.31.564922v1.full.pdf", "../paper/Eur J of Neuroscience - 2020 - Rubin.pdf", "../paper/1-s2.0-S016622361100110X-main.pdf", "../paper/1-s2.0-S0031938405004002-main.pdf", "../paper/fnsyn-02-00029.pdf", "../paper/1801.08116.pdf", "../paper/2108.01368.pdf", "../paper/PIIS0896627317304142.pdf", "../paper/1-s2.0-S0092867419311705-main.pdf", "../paper/NIPS-2011-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent-Paper.pdf", "../paper/NeurIPS-2022-biologically-plausible-backpropagation-through-arbitrary-timespans-via-local-neuromodulators-Supplemental-Conference.pdf", "../paper/440396v1.full.pdf", "../paper/nokland19a.pdf", "../paper/1705.08292.pdf", "../paper/1809.10635.pdf", "../paper/1801.04062.pdf", "../paper/PhysRevX.4.021039.pdf", "../paper/1-s2.0-S089662731500625X-main.pdf", "../paper/guideTR.pdf", "../paper/annurev-vision-082114-035447.pdf", "../paper/1905.12616.pdf", "../paper/annurev-neuro-092619-094115.pdf", "../paper/NIPS-2017-cortical-microcircuits-as-gated-recurrent-neural-networks-Paper.pdf", "../paper/1707.06203.pdf", "../paper/1809.02386.pdf", "../paper/nature23020.pdf", "../paper/mishkin-iclr2016.pdf", "../paper/1810.01075.pdf", "../paper/NIPS-2017-attention-is-all-you-need-Paper.pdf", "../paper/1-s2.0-S0896627312008215-main.pdf", "../paper/smith2003.pdf", "../paper/s41467-023-40141-z.pdf", "../paper/1603.07285v1.pdf", "../paper/1-s2.0-S0022249621000857-main.pdf", "../paper/annurev-neuro-072116-031109.pdf", "../paper/fncom-08-00038.pdf", "../paper/1507.01053v1.pdf", "../paper/wacongne-et-al-2011-evidence-for-a-hierarchy-of-predictions-and-prediction-errors-in-human-cortex.pdf", "../paper/1211.5063.pdf", "../paper/s41586-020-03051-4.pdf", "../paper/s41467-023-36583-0.pdf", "../paper/1705.02436.pdf", "../paper/2106.02736.pdf", "../paper/nips_deep_workshop_2010.pdf", "../paper/1802.01933.pdf", "../paper/nature04676.pdf", "../paper/1706.05806.pdf", "../paper/1-s2.0-S0166223607000483-main.pdf", "../paper/2005.02181.pdf", "../paper/1-s2.0-S1053811910000698-main.pdf", "../paper/s41467-018-05873-3.pdf", "../paper/1611.01353.pdf", "../paper/document.pdf", "../paper/1810.02032.pdf", "../paper/s41562-018-0466-5.pdf", "../paper/elife-13665-v2.pdf", "../paper/1-s2.0-S0893608006001481-main.pdf", "../paper/2203.00573.pdf", "../paper/Rahnev&Denison(2018)BBS.pdf", "../paper/cunningham15a.pdf", "../paper/berkes2011a.pdf", "../paper/1803.07728.pdf", "../paper/nn.2590.pdf", "../paper/elife-66551-v1.pdf", "../paper/2006.07232.pdf", "../paper/1207.0057.pdf", "../paper/nn.4650.pdf", "../paper/1710.11198.pdf", "../paper/NIPS-2004-reducing-spike-train-variability-a-computational-theory-of-spike-timing-dependent-plasticity-Paper.pdf", "../paper/1506.02025.pdf", "../paper/TR_Dimensionality_Reduction_Review_2009.pdf", "../paper/Zavatone-Veth_2022_J._Stat._Mech._2022_114008.pdf", "../paper/WRAP-constraining-levels-justification-Sanborn-2017.pdf", "../paper/NIPS-1999-policy-gradient-methods-for-reinforcement-learning-with-function-approximation-Paper.pdf", "../paper/nrn1607.pdf", "../paper/1612.03214.pdf", "../paper/rstb.2019.0761.pdf", "../paper/1603.05642.pdf", "../paper/1902.04760.pdf", "../paper/cortes_vapnik95.pdf", "../paper/gk7812.pdf", "../paper/NIPS-2017-dynamic-routing-between-capsules-Paper.pdf", "../paper/rstb.2008.0306.pdf", "../paper/1904.01569.pdf", "../paper/bromberg-martin-et-al-2010-a-pallidus-habenula-dopamine-pathway-signals-inferred-stimulus-values.pdf", "../paper/1809.06848.pdf", "../paper/science.273.5283.1868.pdf", "../paper/2023.06.27.546656v2.full.pdf", "../paper/CABN.8.4.429.pdf", "../paper/nihms530595.pdf", "../paper/ardid.jn2007.pdf", "../paper/1802.09766.pdf", "../paper/1703.04730.pdf", "../paper/Cowles-MarkovChainMonte-1996.pdf", "../paper/1803.10122.pdf", "../paper/PIIS0896627302008206.pdf", "../paper/ullman-et-al-2016-atoms-of-recognition-in-human-and-computer-vision.pdf", "../paper/ncomms11393.pdf", "../paper/building-machines-that-learn-and-think-like-people.pdf", "../paper/3685.full.pdf", "../paper/2022.03.28.485868v2.full.pdf", "../paper/rieecml05.pdf", "../paper/PhysRevLett.130.237401.pdf", "../paper/brainsci-09-00300.pdf", "../paper/NIPS-2016-matrix-completion-has-no-spurious-local-minimum-Paper.pdf", "../paper/1403.6382.pdf", "../paper/2022.05.09.491042v3.full.pdf", "../paper/smdae_techreport.pdf", "../paper/dawcourvday08.pdf", "../paper/murdoch-et-al-2019-definitions-methods-and-applications-in-interpretable-machine-learning.pdf", "../paper/erhan09a.pdf", "../paper/nn.2396.pdf", "../paper/PIIS0896627312007039.pdf", "../paper/file121231231.pdf", "../paper/*1-s2.0-0893608094901090-main.pdf", "../paper/1711.02391.pdf", "../paper/nn.3711.pdf", "../paper/s00422-012-0512-8.pdf", "../paper/nature03721.pdf", "../paper/nn.4042.pdf", "../paper/1812.05905.pdf", "../paper/nn.4056.pdf", "../paper/1-s2.0-S0896627316001021-main.pdf", "../paper/1602.05179v2.pdf", "../paper/s41586-023-06377-x.pdf", "../paper/Recurrent_nets_that_time_and_count.pdf", "../paper/PehlevanSenguptaChklovskii18.pdf", "../paper/NIPS-2010-double-q-learning-Paper.pdf", "../paper/2006.13332.pdf", "../paper/BHR2014.pdf", "../paper/*ijcv06-mvu.pdf", "../paper/PIIS089662731400734X.pdf", "../paper/2020102120200969969.pdf", "../paper/Gerstner96.pdf", "../paper/s41586-021-03819-2.pdf", "../paper/1010.3467.pdf", "../paper/miconi18a.pdf", "../paper/1-s2.0-S0959438818302009-main.pdf", "../paper/nature13282.pdf", "../paper/physrev.00023.2014.pdf", "../paper/954_ffjord_free_form_continuous_dy.pdf", "../paper/1-s2.0-016622369390081V-main.pdf", "../paper/download (1).pdf", "../paper/file (7).pdf", "../paper/A_model_of_saliency-based_visual_attention_for_rapid_scene_analysis.pdf", "../paper/1708.04133.pdf", "../paper/1611.01224.pdf", "../paper/1512.04455.pdf", "../paper/7255.full.pdf", "../paper/Delving_Deep_into_Rectifiers_Surpassing_Human-Level_Performance_on_ImageNet_Classification.pdf", "../paper/1312.6026.pdf", "../paper/Analysis_of_the_back-propagation_algorithm_with_momentum.pdf", "../paper/lm.php.pdf", "../paper/835_towards_deep_learning_models_r.pdf", "../paper/PIIS0960982221010526.pdf", "../paper/1-s2.0-S0896627311009305-main.pdf", "../paper/1804.11271.pdf", "../paper/PSD-01.pdf", "../paper/srep32672.pdf", "../paper/1-s2.0-S1053811914005199-main.pdf", "../paper/1909.09902.pdf", "../paper/science.aad3647.pdf", "../paper/1-s2.0-S0896627318309577-main.pdf", "../paper/GreBouSmoSch05.pdf", "../paper/PIIS0896627309005479.pdf", "../paper/Paper 29.pdf", "../paper/27_brain_inspired_predictive_codi.pdf", "../paper/MLSys-2019-parmac-distributed-optimisation-of-nested-functions-with-application-to-learning-binary-autoencoders-Paper.pdf", "../paper/pnas.1903070116.pdf", "../paper/NeurIPS-2018-parameters-as-interacting-particles-long-time-convergence-and-asymptotic-error-scaling-of-neural-networks-Paper.pdf", "../paper/551.full.pdf", "../paper/fnbot-17-1127642.pdf", "../paper/1-s2.0-S027826261530035X-main.pdf", "../paper/PIIS009286741930220X.pdf", "../paper/s41583-021-00473-5.pdf", "../paper/PIIS0092867418305129.pdf", "../paper/1809.03702.pdf", "../paper/science.1236425.pdf", "../paper/2003.01513.pdf", "../paper/1-s2.0-S0959438817302428-main.pdf", "../paper/s41583-018-0049-5.pdf", "../paper/1801.01290.pdf", "../paper/jn.01095.2002.pdf", "../paper/nn.3658.pdf", "../paper/PhysRevLett.97.048104.pdf", "../paper/NIPS-2016-composing-graphical-models-with-neural-networks-for-structured-representations-and-fast-inference-Paper.pdf", "../paper/PIIS089662730300761X.pdf", "../paper/NIPS-2001-reinforcement-learning-with-long-short-term-memory-Paper.pdf", "../paper/science.aav7893.pdf", "../paper/pnas.93.23.13339.pdf", "../paper/1812.07040.pdf", "../paper/1-s2.0-S0896627301005426-main.pdf", "../paper/2201.08025.pdf", "../paper/1306.1091.pdf", "../paper/9659.full.pdf", "../paper/2209.13233.pdf", "../paper/s41928-022-00913-9.pdf", "../paper/sprekeler-naud-2018-sparse-bursts-optimize-information-transmission-in-a-multiplexed-neural-code.pdf", "../paper/gershman-daw-2017-reinforcement-learning-and-episodic-memory-in-humans-and-animals-an-integrative-framework.pdf", "../paper/nature03687.pdf", "../paper/elife-18073-v2.pdf", "../paper/2201.13415.pdf", "../paper/1-s2.0-S0893608021000903-main.pdf", "../paper/doe-nc.pdf", "../paper/NeurIPS-2020-the-interplay-between-randomness-and-structure-during-learning-in-rnns-Paper.pdf", "../paper/nn1100_1178.pdf", "../paper/annurev-neuro-071714-034002.pdf", "../paper/NN00new.pdf", "../paper/annurev-neuro-080317-061948.pdf", "../paper/1607.06450.pdf", "../paper/1003.0358.pdf", "../paper/maron19a.pdf", "../paper/1-s2.0-S0896627319300534-main.pdf", "../paper/*pnas.1403112111.pdf", "../paper/nn.3106.pdf", "../paper/NIPS-1988-learning-sequential-structure-in-simple-recurrent-networks-Paper.pdf", "../paper/1319.full.pdf", "../paper/1611.09913.pdf", "../paper/nature04671.pdf", "../paper/PIIS0896627304007123.pdf", "../paper/*nature12160.pdf", "../paper/PIIS0896627301003014.pdf", "../paper/neco.2010.05-09-1010.pdf", "../paper/cogscibm.pdf", "../paper/nature18933.pdf", "../paper/1810.06721.pdf", "../paper/1905.11481.pdf", "../paper/pnas.1320116110.pdf", "../paper/Murphy (Miller), Balanced Amplification_ A New Mechanism of Selective Amplification of Neural Activity Patterns.pdf", "../paper/1610.03483.pdf", "../paper/Beery_Recognition_in_Terra_ECCV_2018_paper.pdf", "../paper/1809.08848.pdf", "../paper/s41586-019-0919-7.pdf", "../paper/acs.chemrev.6b00163.pdf", "../paper/1-s2.0-S1364661309002617-main.pdf", "../paper/s42256-019-0048-x.pdf", "../paper/1-s2.0-S0893608001000685-main.pdf", "../paper/2001.07092.pdf", "../paper/1706.01905.pdf", "../paper/1205.0411.pdf", "../paper/PhysRevLett.69.3717.pdf", "../paper/1411.1792.pdf", "../paper/2451.full.pdf", "../paper/1706.05587.pdf", "../paper/NIPS-2006-greedy-layer-wise-training-of-deep-networks-Paper.pdf", "../paper/1995-williams.pdf", "../paper/1707.07907.pdf", "../paper/1806.05451.pdf", "../paper/ml95-stable-dp.pdf", "../paper/1512.04860.pdf", "../paper/s41586-023-06221-2.pdf", "../paper/PIIS0896627303001697.pdf", "../paper/1901.08584.pdf", "../paper/NIPS-2013-annealing-between-distributions-by-averaging-moments-Paper.pdf", "../paper/2210.14215.pdf", "../paper/nn1817.pdf", "../paper/file (6).pdf", "../paper/9587.full.pdf", "../paper/annurev-neuro-070815-013851.pdf", "../paper/Angrist-IdentificationCausalEffects-1996.pdf", "../paper/1-s2.0-S0896627306004715-main.pdf", "../paper/1506.00019.pdf", "../paper/nature12601.pdf", "../paper/1-s2.0-S1364661320301066-main.pdf", "../paper/1-s2.0-S0959438821000933-main.pdf", "../paper/1608.05859.pdf", "../paper/nature12600.pdf", "../paper/3397_single_layers_of_attention_suf.pdf", "../paper/Mignacco_2021_Mach._Learn.__Sci._Technol._2_035029.pdf", "../paper/document copy.pdf", "../paper/1-s2.0-S0959438813000330-main.pdf", "../paper/18999.full.pdf", "../paper/koutnik2014sab.pdf", "../paper/nature11321.pdf", "../paper/PhysRevX.8.041029.pdf", "../paper/Bidirectional_Long_Short-Term_Memory_Networks_for_Predicting_the_Subcellular_Localization_of_Eukaryotic_Proteins.pdf", "../paper/1802.10031.pdf", "../paper/1-s2.0-S0896627317304713-main.pdf", "../paper/1301.3584.pdf", "../paper/1511.05222.pdf", "../paper/1-s2.0-S0959438814000373-main.pdf", "../paper/2006.15222.pdf", "../paper/1807.01251.pdf", "../paper/41467_2016_BFncomms13276_MOESM954_ESM.pdf", "../paper/Simard.pdf", "../paper/1-s2.0-S0079742108605368-main.pdf", "../paper/JN_Kloosterman_Chen14.pdf", "../paper/1-s2.0-S0959438813002158-main.pdf", "../paper/nn.4244.pdf", "../paper/nature08499.pdf", "../paper/1706.04698.pdf", "../paper/1905.06922.pdf", "../paper/science.1225266.pdf", "../paper/1-s2.0-S0959438816000118-main.pdf", "../paper/1612.02734.pdf", "../paper/s41586-019-1924-6.pdf", "../paper/Cook_Software_Validation.pdf", "../paper/michaels-et-al-2020-a-goal-driven-modular-neural-network-predicts-parietofrontal-neural-dynamics-during-grasping.pdf", "../paper/nn2066.pdf", "../paper/nn.3477.pdf", "../paper/Cook-ValidationSoftwareBayesian-2006.pdf", "../paper/1-s2.0-S0959438821001276-main.pdf", "../paper/nn1209.pdf", "../paper/2308.11809.pdf", "../paper/1805.10451.pdf", "../paper/nn.3311.pdf", "../paper/spruston-remy-2007-dendritic-spikes-induce-single-burst-long-term-potentiation.pdf", "../paper/Mnih_Volodymyr_PhD_Thesis.pdf", "../paper/jozefowicz15.pdf", "../paper/383076a0.pdf", "../paper/*NIPS-2003-locality-preserving-projections-Paper.pdf", "../paper/LudvigBellemarePearson2011_RLPrimer.pdf", "../paper/nature11057.pdf", "../paper/1412.0233.pdf", "../paper/s41586-020-2907-3.pdf", "../paper/Probability_of_error_for_optimal_codes_in_a_Gaussian_channel.pdf", "../paper/1-s2.0-S0092867407012056-main.pdf", "../paper/nature25457.pdf", "../paper/1-s2.0-S0896627312008471-main.pdf", "../paper/science.1195870.pdf", "../paper/schulman15.pdf", "../paper/nature11527.pdf", "../paper/2254.full.pdf", "../paper/NeurIPS-2018-slayer-spike-layer-error-reassignment-in-time-Paper.pdf", "../paper/Model-Free_reinforcement_learning_with_continuous_action_in_practice.pdf", "../paper/s41586-021-03771-1.pdf", "../paper/1903.06070.pdf", "../paper/444.full.pdf", "../paper/2102.09351.pdf", "../paper/1611.02648.pdf", "../paper/ncomms4675.pdf", "../paper/pnas.1109359109.pdf", "../paper/1910.01619.pdf", "../paper/A_Comprehensive_Review_of_Stability_Analysis_of_Continuous-Time_Recurrent_Neural_Networks.pdf", "../paper/1-s2.0-S0165017307001774-main.pdf", "../paper/1511.05432.pdf", "../paper/balduzzi17b.pdf", "../paper/1511.07543.pdf", "../paper/2022.11.14.516537v1.full.pdf", "../paper/1-s2.0-036402139090002E-main.pdf", "../paper/Baldi_Hornik-89.pdf", "../paper/1702.03006.pdf", "../paper/*1312.6120.pdf", "../paper/ncomms13804.pdf", "../paper/fnsyn-04-00002.pdf", "../paper/1709.04396.pdf", "../paper/514_unbiased_online_recurrent_opti.pdf", "../paper/science.1169405.pdf", "../paper/pnas.1820226116.pdf", "../paper/Myung-2003.pdf", "../paper/1-s2.0-S0896627303002551-main.pdf", "../paper/nn0900_895.pdf", "../paper/2002.04745.pdf", "../paper/jn.2000.83.1.588.pdf", "../paper/1-s2.0-S1074742704000735-main.pdf", "../paper/Formal_Theory_of_Creativity_Fun_and_Intrinsic_Motivation_19902010.pdf", "../paper/1811.03567.pdf", "../paper/1-s2.0-S0092867411001279-main.pdf", "../paper/RumelhartBackprop.pdf", "../paper/s41593-019-0417-0.pdf", "../paper/PIIS0896627319307901.pdf", "../paper/1-s2.0-S089662730900083X-main.pdf", "../paper/1803.01834.pdf", "../paper/1-s2.0-030645229500436M-main.pdf", "../paper/2007.05520.pdf", "../paper/ma18a.pdf", "../paper/2022.03.07.483196v1.full.pdf", "../paper/HSIC-CV.pdf", "../paper/35016072.pdf", "../paper/science.aaa6090.pdf", "../paper/1901.02860.pdf", "../paper/418939v2.full.pdf", "../paper/fncom-13-00097.pdf", "../paper/1-s2.0-089360809090019H-main.pdf", "../paper/2105.14602.pdf", "../paper/1708.02182.pdf", "../paper/annurev.neuro.29.051605.113038.pdf", "../paper/1-s2.0-S0896627319301230-main.pdf", "../paper/NeurIPS-2019-untangling-in-invariant-speech-recognition-Paper.pdf", "../paper/elife-16534-v1.pdf", "../paper/PIIS0896627317304634.pdf", "../paper/fnins-11-00324.pdf", "../paper/science.aaa4056.pdf", "../paper/2481.full.pdf", "../paper/pnas.2014196118.pdf", "../paper/summerside-et-al-2018-vigor-of-reaching-movements-reward-discounts-the-cost-of-effort.pdf", "../paper/1407.7906.pdf", "../paper/s41593-022-01224-0.pdf", "../paper/1-s2.0-S0896627309000038-main.pdf", "../paper/*PIIS0896627312008173.pdf", "../paper/Tsai Science 2009.pdf", "../paper/1-s2.0-S0896627309001287-mainext.pdf", "../paper/1610.01644.pdf", "../paper/NeurIPS-2021-neural-population-geometry-reveals-the-role-of-stochasticity-in-robust-perception-Paper.pdf", "../paper/2302.11529.pdf", "../paper/41467_2021_21696_MOESM1_ESM.pdf", "../paper/nature20101.pdf", "../paper/Richardson-DietSelectionOptimization-1986.pdf", "../paper/3157096.3157348.pdf", "../paper/elife-66039-v4.pdf", "../paper/2305.08746.pdf", "../paper/nature14273.pdf", "../paper/Eur J of Neuroscience - 2005 - Yin - Blockade of NMDA receptors in the dorsomedial striatum prevents action outcome.pdf", "../paper/ncfast.pdf", "../paper/2209.07484.pdf", "../paper/2103.03386.pdf", "../paper/science.274.5286.427.pdf", "../paper/ESNTutorialRev.pdf", "../paper/s41583-023-00756-z.pdf", "../paper/1512.06293.pdf", "../paper/s41593-020-0699-2.pdf", "../paper/1-s2.0-S0959438821000027-main.pdf", "../paper/nn.3064.pdf", "../paper/s41593-023-01485-3.pdf", "../paper/1607.04331.pdf", "../paper/1-s2.0-S0959438821001112-main.pdf", "../paper/1312.6034.pdf", "../paper/18-188.pdf", "../paper/1-s2.0-S2352154616301371-main.pdf", "../paper/s41592-022-01675-0.pdf", "../paper/1-s2.0-S1053811922004797-main.pdf", "../paper/s41593-017-0028-6.pdf", "../paper/nrn.2016.150.pdf", "../paper/1611.01578.pdf", "../paper/1711.07205.pdf", "../paper/1909.05858.pdf", "../paper/Input_feature_selection_by_mutual_information_based_on_Parzen_window.pdf", "../paper/Hippocampus - 2015 - Buzs ki - Hippocampal sharp wave\u2010ripple  A cognitive biomarker for episodic memory and planning.pdf", "../paper/hazan19a.pdf", "../paper/nn.4247.pdf", "../paper/nn0801_819.pdf", "../paper/s41593-018-0095-3.pdf", "../paper/GokcenNatCompSci2022.pdf", "../paper/s42256-022-00556-7.pdf", "../paper/neco.2008.11-07-654.pdf", "../paper/1411.1784.pdf", "../paper/1-s2.0-S0959438818301065-main.pdf", "../paper/ben-yishai-et-al-1995-theory-of-orientation-tuning-in-visual-cortex.pdf", "../paper/pnas.201821594.pdf", "../paper/1606.00915.pdf", "../paper/Chap6_PDP86.pdf", "../paper/840_nervenet_learning_structured_p.pdf", "../paper/1-s2.0-S0896627318304185-main.pdf", "../paper/1409.3215.pdf", "../paper/download.pdf", "../paper/1606.07129.pdf", "../paper/NIPS-2013-learning-stochastic-feedforward-neural-networks-Paper.pdf", "../paper/1-s2.0-S2211124718303735-main.pdf", "../paper/2022.08.15.503870v1.full.pdf", "../paper/nn1222.pdf", "../paper/nature07150.pdf", "../paper/NIPS-1999-actor-critic-algorithms-Paper.pdf", "../paper/5950-Article Text-9175-1-10-20200513.pdf", "../paper/1709.01953.pdf", "../paper/s00211-014-0673-6.pdf", "../paper/jones-love-BBS.pdf", "../paper/1-s2.0-S0896627316309576-main.pdf", "../paper/PIIS0896627311001255.pdf", "../paper/1981-20731-001.pdf", "../paper/1901.09049.pdf", "../paper/PIIS0960982213009202.pdf", "../paper/pin.pdf", "../paper/1703.10622.pdf", "../paper/9353.full.pdf", "../paper/1707.02921.pdf", "../paper/NeurIPS_time_permutation_invariant_representation.pdf", "../paper/*Lafon06.pdf", "../paper/ravanbakhsh17a.pdf", "../paper/nonconvergence.pdf", "../paper/19-562.pdf", "../paper/Multi-column_deep_neural_networks_for_image_classification.pdf", "../paper/s41586-019-1346-5.pdf", "../paper/089976698300017115.pdf", "../paper/2020.06.15.148114v2.full.pdf", "../paper/shah_PsychRL_2012.pdf", "../paper/science.1104171.pdf", "../paper/nature10835.pdf", "../paper/pnas.1316181111.pdf", "../paper/1702.08608.pdf", "../paper/2301.03246.pdf", "../paper/PIIS0896627319304945.pdf", "../paper/2010.00525.pdf", "../paper/1611.00712.pdf", "../paper/1906.00586.pdf", "../paper/Lakshminarayan_Chinta_Venkateswararao_PhDThesis.pdf", "../paper/s41586-021-03652-7.pdf", "../paper/1-s2.0-S1471489216301333-main.pdf", "../paper/s41467-021-21696-1.pdf", "../paper/pnas.1820296116.pdf", "../paper/CHL90.pdf", "../paper/1911.00890.pdf", "../paper/2006.12878.pdf", "../paper/2020.11.02.365072v1.full.pdf", "../paper/1-s2.0-S0143417913000784-mainext.pdf", "../paper/1-s2.0-S1053811913005065-main.pdf", "../paper/NIPS-2015-learning-continuous-control-policies-by-stochastic-value-gradients-Paper.pdf", "../paper/srivastava14a.pdf", "../paper/pnas.201403112si.pdf", "../paper/mmc2.pdf", "../paper/NeurIPS-2022-biologically-plausible-backpropagation-through-arbitrary-timespans-via-local-neuromodulators-Paper-Conference.pdf", "../paper/jphysiol00723-0461.pdf", "../paper/q_learn.pdf", "../paper/NIPS-1993-credit-assignment-through-time-alternatives-to-backpropagation-Paper.pdf", "../paper/1506.06579.pdf", "../paper/1503.03585.pdf", "../paper/1605.07146.pdf", "../paper/1806.01242.pdf", "../paper/564476v2.full.pdf", "../paper/nature18942.pdf", "../paper/science.1159775.pdf", "../paper/Wang_Learning_to_Detect_CVPR_2017_paper.pdf", "../paper/11573.full.pdf", "../paper/1-s2.0-S0896627311001966-main.pdf", "../paper/s41593-019-0460-x.pdf", "../paper/pnas.0508601103.pdf", "../paper/tr00-004.pdf", "../paper/nbt1406.pdf", "../paper/1410.3916.pdf", "../paper/2205.10868.pdf", "../paper/2005.11362.pdf", "../paper/The_use_of_fast_Fourier_transform_for_the_estimation_of_power_spectra_A_method_based_on_time_averaging_over_short_modified_periodograms.pdf", "../paper/s41593-021-00857-x.pdf", "../paper/nn.3413.pdf", "../paper/18531.full.pdf", "../paper/file copy 5.pdf", "../paper/1-s2.0-S0959438812001316-main.pdf", "../paper/*s41583-020-0277-3.pdf", "../paper/file copy 12.pdf", "../paper/1703.00522.pdf", "../paper/PIIS0896627314007843.pdf", "../paper/NIPS-2015-learning-both-weights-and-connections-for-efficient-neural-network-Paper.pdf", "../paper/s11263-008-0178-9.pdf", "../paper/1-s2.0-S0896627314000191-main.pdf", "../paper/baity-jesi18a.pdf", "../paper/iconip98_pre.pdf", "../paper/1602.03032.pdf", "../paper/saxe22a.pdf", "../paper/1-s2.0-S0092867415009733-main.pdf", "../paper/8492_no_free_lunch_from_deep_learni.pdf", "../paper/J063-97-bvr-td.pdf", "../paper/1-s2.0-S1364661304002153-main.pdf", "../paper/1-s2.0-S0079612305490111-main.pdf", "../paper/annurev-neuro-062111-150509.pdf", "../paper/elife-43299-v3.pdf", "../paper/1-s2.0-S016622361100124X-main.pdf", "../paper/PIIS0092867415011964.pdf", "../paper/nn.2889.pdf", "../paper/1905.04610.pdf", "../paper/1203.3497.pdf", "../paper/3251.full.pdf", "../paper/larochelle11a.pdf", "../paper/2307.13586.pdf", "../paper/2201.01666.pdf", "../paper/1803.01271.pdf", "../paper/1905.11786.pdf", "../paper/1511.09468.pdf", "../paper/jaderberg17a.pdf", "../paper/1609.04836.pdf", "../paper/nn.3981.pdf", "../paper/el-ghaoui-et-al-2021-implicit-deep-learning.pdf", "../paper/1-s2.0-S0896627314003602-main.pdf", "../paper/out.pdf", "../paper/1906.00443.pdf", "../paper/nature13235.pdf", "../paper/14-335.pdf", "../paper/12477.full.pdf", "../paper/NeurIPS-2020-supervised-contrastive-learning-Paper.pdf", "../paper/s41467-017-01827-3.pdf", "../paper/PIIS0896627312009920.pdf", "../paper/s42256-022-00568-3.pdf", "../paper/s41593-021-00840-6.pdf", "../paper/2103.14662.pdf", "../paper/1707.04926.pdf", "../paper/1703.01365.pdf", "../paper/nips02-localglobal-in-press.pdf", "../paper/s41586-019-1816-9.pdf", "../paper/file (1).pdf", "../paper/nrn.2016.53.pdf", "../paper/1-s2.0-S0959438817300612-main.pdf", "../paper/1911.02116.pdf", "../paper/atanasov_etal_iclr_2022.pdf", "../paper/elife-20899-v2.pdf", "../paper/file1111.pdf", "../paper/nature01341.pdf", "../paper/CollinsFrank2013PsychReview.pdf", "../paper/1712.01815.pdf", "../paper/P10-1040.pdf", "../paper/2005.04168.pdf", "../paper/1507.06527.pdf", "../paper/yeung-et-al-2004-synaptic-homeostasis-and-input-selectivity-follow-from-a-calcium-dependent-plasticity-model.pdf", "../paper/node_perturbation.pdf", "../paper/1-s2.0-S0893608021003403-main.pdf", "../paper/elife-48198-v1.pdf", "../paper/gal17a.pdf", "../paper/PIIS0896627316305116.pdf", "../paper/A Learning Algorithm for Continually Running Fully recurrent neural networks.pdf", "../paper/TINS10.pdf", "../paper/Convolutional adaptive denoising autoencoders for hierarchical feature extraction.pdf", "../paper/9565.full.pdf", "../paper/1-s2.0-S0079612306650346-main.pdf", "../paper/1-s2.0-S030645221730547X-main.pdf", "../paper/9424.full.pdf", "../paper/NIPS-2005-off-policy-learning-with-options-and-recognizers-Paper.pdf", "../paper/file copy 13.pdf", "../paper/nn.4433.pdf", "../paper/file copy 4.pdf", "../paper/1411.4077.pdf", "../paper/jn.1998.79.2.1017.pdf", "../paper/1207.4708.pdf", "../paper/nature08577.pdf", "../paper/s00415-011-6299-z.pdf", "../paper/1611.05397.pdf", "../paper/file2222.pdf", "../paper/2007.06700.pdf", "../paper/bhs348.pdf", "../paper/11597.full.pdf", "../paper/Box-SamplingBayesInference-1980.pdf", "../paper/1802.01561.pdf", "../paper/PIIS0896627300806339.pdf", "../paper/2109.12894.pdf", "../paper/1606.03498.pdf", "../paper/1808.03305.pdf", "../paper/A Computational Model of Birdsong Learning by Auditory Experience and Auditory Feedback 1998-2926.pdf", "../paper/1-s2.0-S2405896321005462-main.pdf", "../paper/gutmann12a.pdf", "../paper/1-s2.0-S0893608002002289-main.pdf", "../paper/BF03194840.pdf", "../paper/13402.full.pdf", "../paper/elife-36275-v1.pdf", "../paper/NeurIPS-2020-a-theoretical-framework-for-target-propagation-Paper.pdf", "../paper/nihms766520.pdf", "../paper/1808.04873.pdf", "../paper/1-s2.0-S0092867414002906-main.pdf", "../paper/s41593-021-00873-x.pdf", "../paper/PIIS0896627312008586.pdf", "../paper/4052.full.pdf", "../paper/s41467-023-37180-x.pdf", "../paper/10356_a_path_towards_autonomous_mach.pdf", "../paper/mmc3.pdf", "../paper/s42254-021-00314-5.pdf", "../paper/alain14a.pdf", "../paper/1-s2.0-S0896627321005018-main.pdf", "../paper/1-s2.0-S0166432809005099-main.pdf", "../paper/NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "../paper/1-s2.0-S0896627313005576-main.pdf", "../paper/1611.02163.pdf", "../paper/1812.11118.pdf", "../paper/tramer20a.pdf", "../paper/1810.05728.pdf", "../paper/s41593-019-0555-4.pdf", "../paper/1610.04161.pdf", "../paper/nn1905.pdf", "../paper/Han_Direct_Feedback_Alignment_Based_Convolutional_Neural_Network_Training_for_Low-Power_ICCVW_2019_paper.pdf", "../paper/1503.02101.pdf", "../paper/1511.02476.pdf", "../paper/1510.02777.pdf", "../paper/1-s2.0-S0893608017302903-main.pdf", "../paper/1808.09372.pdf", "../paper/1803.02155.pdf", "../paper/1504.00941.pdf", "../paper/SMPBSSW-09.pdf", "../paper/elife-79908-v1.pdf", "../paper/petreska-etal-2011-nips-preprint.pdf", "../paper/Suh_2011_Science.pdf", "../paper/1994_Koch_Meinhardt_Pattern_Formation.pdf", "../paper/2006.10739.pdf", "../paper/1-s2.0-S0896627319300716-main.pdf", "../paper/gk8001.pdf", "../paper/nature17643.pdf", "../paper/1410.6460.pdf", "../paper/RLAlgsInMDPs.pdf", "../paper/Synthesis_and_stabilization_of_complex_behaviors_through_online_trajectory_optimization.pdf", "../paper/nature11554.pdf", "../paper/jeb188912.pdf", "../paper/thesis.pdf", "../paper/2206.11228.pdf", "../paper/annurev-neuro-100219-105424.pdf", "../paper/35020214.pdf", "../paper/zenke17a.pdf", "../paper/PIIS2211124714008638.pdf", "../paper/1-s2.0-S0960982213011299-main.pdf", "../paper/1-s2.0-S0166223604003352-main.pdf", "../paper/2308.16898.pdf", "../paper/1906.02691.pdf", "../paper/NeurIPS-2019-updates-of-equilibrium-prop-match-gradients-of-backprop-through-time-in-an-rnn-with-static-input-Paper.pdf", "../paper/nature00854.pdf", "../paper/file.pdf", "../paper/Variable_Binding_for_Sparse_Distributed_Representations_Theory_and_Applications.pdf", "../paper/Reinforcement learning  Computational theory and biological mechanisms.pdf", "../paper/elife-09685-v1.pdf", "../paper/pnas.79.8.2554.pdf", "../paper/1804.08150.pdf", "../paper/file copy 11.pdf", "../paper/1810.04805.pdf", "../paper/1-s2.0-S1074742798938436-main.pdf", "../paper/1-s2.0-S1053811909007186-main.pdf", "../paper/lu-et-al-2014-spike-timing-dependent-bdnf-secretion-and-synaptic-plasticity.pdf", "../paper/nature05051.pdf", "../paper/Maximum likelihood estimation of cascade point-process neural encoding models.pdf", "../paper/2105.13120.pdf", "../paper/file copy 6.pdf", "../paper/annurev.neuro.23.1.649.pdf", "../paper/070704277.pdf", "../paper/10.1.1.441.7873.pdf", "../paper/s41467-017-01109-y.pdf", "../paper/2310.01770.pdf", "../paper/NeurIPS-2020-a-biologically-plausible-neural-network-for-slow-feature-analysis-Paper.pdf", "../paper/1314.full.pdf", "../paper/1-s2.0-S0896627312003844-main.pdf", "../paper/jn.00429.2011.pdf", "../paper/1809.04184.pdf", "../paper/1905.00313.pdf", "../paper/1602.04951.pdf", "../paper/1-s2.0-S0893608000000265-main.pdf", "../paper/2305.20050.pdf", "../paper/764_associated_learning_an_alterna.pdf", "../paper/s41593-018-0147-8.pdf", "../paper/1-s2.0-S0959438817300910-am.pdf", "../paper/nn1859.pdf", "../paper/2102.11343.pdf", "../paper/NeurIPS-2020-learning-identifiable-and-interpretable-latent-models-of-high-dimensional-neural-activity-using-pi-vae-Paper.pdf", "../paper/1801.01423.pdf", "../paper/fncom-15-640235.pdf", "../paper/1705.08026.pdf", "../paper/1177728716.pdf", "../paper/2006.13436.pdf", "../paper/NIPS-2002-distance-metric-learning-with-application-to-clustering-with-side-information-Paper.pdf", "../paper/NeurIPS-2020-complex-dynamics-in-simple-neural-networks-understanding-gradient-flow-in-phase-retrieval-Paper.pdf", "../paper/1506.02640.pdf", "../paper/1712.09913.pdf", "../paper/1802.08195.pdf", "../paper/nn1643.pdf", "../paper/1908.01580.pdf", "../paper/1-s2.0-S0092867419303915-main.pdf", "../paper/nature11347.pdf", "../paper/PIIS0896627318300072.pdf", "../paper/portilla99-reprint.pdf", "../paper/icml-2008-denoising-autoencoders.pdf", "../paper/fileasdad12.pdf", "../paper/nn.2479.pdf", "../paper/1705.08741.pdf", "../paper/neco_a_01193.pdf", "../paper/pnas01060-0383.pdf", "../paper/NeurIPS-2022-learning-single-index-models-with-shallow-neural-networks-Paper-Conference.pdf", "../paper/williams92simple.pdf", "../paper/1-s2.0-S0092867420312289-main.pdf", "../paper/nrn3476.pdf", "../paper/nature10918.pdf", "../paper/1607.02533.pdf", "../paper/1609.01596.pdf", "../paper/science.1117593.pdf", "../paper/BF00961879.pdf", "../paper/1904.09751.pdf", "../paper/10099428.pdf", "../paper/2202.12837.pdf", "../paper/2022.12.30.522267v1.full.pdf", "../paper/1-s2.0-S0896627318308328-main.pdf", "../paper/1-s2.0-S0896627312008021-main.pdf", "../paper/NIPS-2014-weighted-importance-sampling-for-off-policy-learning-with-linear-function-approximation-Paper.pdf", "../paper/1-s2.0-S0010027716300907-main.pdf", "../paper/PhysRevLett.116.038701.pdf", "../paper/2301.08028.pdf", "../paper/journal.pone.0252345.pdf", "../paper/arjovsky16.pdf", "../paper/nn1253.pdf", "../paper/file copy 7.pdf", "../paper/TD_Policy_Eval_04.pdf", "../paper/8368.full.pdf", "../paper/s41467-020-14578-5.pdf", "../paper/srep28073.pdf", "../paper/file copy 10.pdf", "../paper/GershmanBlei2012.pdf", "../paper/2111.09794.pdf", "../paper/nature15257.pdf", "../paper/science.290.5500.2323.pdf", "../paper/nature05078.pdf", "../paper/nn.3405.pdf", "../paper/fncom-13-00018.pdf", "../paper/annurev.neuro.28.061604.135722.pdf", "../paper/1756-6606-6-10.pdf", "../paper/mazumder10a.pdf", "../paper/1-s2.0-S0896627301005244-main.pdf", "../paper/paper4.pdf", "../paper/026_Reinforcement_EngelEtAl.pdf", "../paper/journal.pcbi.1011509.pdf", "../paper/applsci-12-08805-v3.pdf", "../paper/nature08010.pdf", "../paper/s41593-018-0321-z.pdf", "../paper/BF03333113.pdf", "../paper/2311.06928.pdf", "../paper/keller2023.pdf", "../paper/elife-69094-v3.pdf", "../paper/NeurIPS-2021-gradient-starvation-a-learning-proclivity-in-neural-networks-Paper.pdf", "../paper/1-s2.0-S0959438823001058-main.pdf", "../paper/PIIS0896627308010921.pdf", "../paper/s41562-017-0180-8.pdf", "../paper/PIIS0896627301005827.pdf", "../paper/1-s2.0-S0896627312009592-main.pdf", "../paper/nature10844.pdf", "../paper/nn.4197.pdf", "../paper/1-s2.0-S0893608002000564-main.pdf", "../paper/neural-network-approximation.pdf", "../paper/1-s2.0-S0959438811002236-main.pdf", "../paper/1-s2.0-S0896627315004134-main.pdf", "../paper/crick1989.pdf", "../paper/1611.05141.pdf", "../paper/2010.12632.pdf", "../paper/annrev.pdf", "../paper/2301.07733.pdf", "../paper/7817.full.pdf", "../paper/tnn-94-gradient.pdf", "../paper/science.7770778.pdf", "../paper/PIIS0896627311001036.pdf", "../paper/1810.02054.pdf", "../paper/pnas.1420068112.pdf", "../paper/9050.full.pdf", "../paper/s41592-020-01018-x.pdf", "../paper/9803008.pdf", "../paper/PhysRevLett.128.180201-accepted.pdf", "../paper/6923.full.pdf", "../paper/1812.06488.pdf", "../paper/elife-42870-v2.pdf", "../paper/mmc4.pdf", "../paper/1312.5602.pdf", "../paper/1902.02405.pdf", "../paper/nn.4385.pdf", "../paper/fnins-12-00774.pdf", "../paper/1810.01993.pdf", "../paper/2021.12.21.473757.full.pdf", "../paper/1911.10688.pdf", "../paper/1906.04554.pdf", "../paper/elife-08998-v3.pdf", "../paper/file copy 14.pdf", "../paper/13522.full.pdf", "../paper/file copy 3.pdf", "../paper/1511.04599.pdf", "../paper/1112.6209.pdf", "../paper/1805.10369.pdf", "../paper/511_the_effectiveness_of_layer_by_.pdf", "../paper/1-s2.0-S0166223612002032-main.pdf", "../paper/24_eligibility_traces_provide_a_d.pdf", "../paper/d93b.pdf", "../paper/He_Deep_Residual_Learning_CVPR_2016_paper.pdf", "../paper/1-s2.0-S0304394017302975-main.pdf", "../paper/ijcv04.pdf", "../paper/1703.03777.pdf", "../paper/1-s2.0-S0301008296000421-main.pdf", "../paper/10.21105.joss.01003.pdf", "../paper/0899766054615699.pdf", "../paper/1-s2.0-S0149763416301336-main.pdf", "../paper/seo-et-al-2008-cortical-mechanisms-for-reinforcement-learning-in-competitive-games.pdf", "../paper/glimcher-2011-understanding-dopamine-and-reinforcement-learning-the-dopamine-reward-prediction-error-hypothesis.pdf", "../paper/nrn1932.pdf", "../paper/PIIS0896627322000058.pdf", "../paper/15497.full.pdf", "../paper/backprop.pdf", "../paper/1-s2.0-S0022249615000759-main.pdf", "../paper/MolanoMazonShaoDuqueYangOstojicRocha__22.pdf", "../paper/1-s2.0-S089360800200045X-main.pdf", "../paper/1-s2.0-S266638992200160X-main.pdf", "../paper/nrn2022.pdf", "../paper/2009.05359.pdf", "../paper/2023_Farrell_current_opinion.pdf", "../paper/nrn.2016.40.pdf", "../paper/s41422-020-00448-8.pdf", "../paper/Santiago_Cadena_Diverse_feature_visualizations_ECCV_2018_paper.pdf", "../paper/1801.00062.pdf", "../paper/nrn2787.pdf", "../paper/1406.3269.pdf", "../paper/*nn.3450.pdf", "../paper/1707.06887.pdf", "../paper/RationalUseOfCognitiveResources.pdf", "../paper/2302.00487.pdf", "../paper/WIRES Cognitive Science - 2011 - Huang - Predictive coding.pdf", "../paper/neco.1990.2.4.490.pdf", "../paper/nrn2963.pdf", "../paper/2021.05.17.444526.full.pdf", "../paper/nature04766.pdf", "../paper/file (12).pdf", "../paper/nn.3776.pdf", "../paper/1704.06440.pdf", "../paper/1805.08651.pdf", "../paper/1603.01912.pdf", "../paper/1907.05600.pdf", "../paper/1603.09382.pdf", "../paper/1-s2.0-S0950705120302896-main.pdf", "../paper/1-s2.0-S0896627302009674-main.pdf", "../paper/farries-fairhall-2007-reinforcement-learning-with-modulated-spike-timing-dependent-synaptic-plasticity.pdf", "../paper/PIIS0092867421010606.pdf", "../paper/jn.00371.2011.pdf", "../paper/s41593-019-0414-3.pdf", "../paper/13326.full.pdf", "../paper/s10107-015-0893-2.pdf", "../paper/nature04968.pdf", "../paper/1-s2.0-S0092867421003731-main.pdf", "../paper/NIPS-2017-a-simple-neural-network-module-for-relational-reasoning-Paper.pdf", "../paper/1702.08720.pdf", "../paper/Gershman12.pdf", "../paper/annurev.neuro.23.1.473.pdf", "../paper/science.1155564.pdf", "../paper/NIPS-2003-learning-curves-for-stochastic-gradient-descent-in-linear-feedforward-networks-Paper.pdf", "../paper/1802.03774.pdf", "../paper/MartensProvost_Explaining.pdf", "../paper/2006.07123.pdf", "../paper/Wilson.nn03.batch.pdf", "../paper/1-s2.0-0167278983902981-main.pdf", "../paper/nature03236.pdf", "../paper/annurev.neuro.24.1.167.pdf", "../paper/1605.08104.pdf", "../paper/2010.11931.pdf", "../paper/nature05860.pdf", "../paper/2303.13506.pdf", "../paper/1701.01724.pdf", "../paper/1910.01526.pdf", "../paper/1703.01161.pdf", "../paper/930_bayesian_deep_convolutional_ne.pdf", "../paper/sharpee-et-al-2013-trade-off-between-curvature-tuning-and-position-invariance-in-visual-area-v4.pdf", "../paper/simplified_ppo_objective.pdf", "../paper/statistical-learning-of-temporal-community-structure-in-the-hippocampus.pdf", "../paper/palmer-et-al-2015-predictive-information-in-a-sensory-population.pdf", "../paper/ims.pdf", "../paper/file copy 2.pdf", "../paper/PIIS0896627318302502.pdf", "../paper/1412.6806.pdf", "../paper/science.aal4835.pdf", "../paper/NIPS-2011-variational-learning-for-recurrent-spiking-networks-Paper.pdf", "../paper/2008.08186.pdf", "../paper/elife-47463-v2.pdf", "../paper/file copy 15.pdf", "../paper/PIIS0896627320301380.pdf", "../paper/1712.00310.pdf", "../paper/241_deep_variational_information_b.pdf", "../paper/214262v2.full.pdf", "../paper/Azabou2023.pdf", "../paper/neco_a_00883.pdf", "../paper/PIIS089662730080700X.pdf", "../paper/1803.09574.pdf", "../paper/J Exper Analysis Behavior - 2013 - Lau - DYNAMIC RESPONSE\u2010BY\u2010RESPONSE MODELS OF MATCHING BEHAVIOR IN RHESUS MONKEYS.pdf", "../paper/PIIS0896627320307054.pdf", "../paper/1805.08296.pdf", "../paper/Backpropagation_through_time_what_it_does_and_how_to_do_it.pdf", "../paper/mmc5.pdf", "../paper/Deep_HessianFree.pdf", "../paper/NeurIPS-2021-improved-variance-aware-confidence-sets-for-linear-bandits-and-linear-mixture-mdp-Paper.pdf", "../paper/1606.04695.pdf", "../paper/NIPS-2017-neural-system-identification-for-large-populations-separating-what-and-where-Paper.pdf", "../paper/1802.09477.pdf", "../paper/pnas.1506407112.pdf", "../paper/NivMathPsych09.pdf", "../paper/s41583-020-0262-x.pdf", "../paper/s41467-019-11786-6.pdf", "../paper/1703.03400.pdf", "../paper/science.1179850.pdf", "../paper/zheng22c.pdf", "../paper/NIPS-2014-extracting-latent-structure-from-multiple-interacting-neural-populations-Paper.pdf", "../paper/clune-et-al-2013-the-evolutionary-origins-of-modularity.pdf", "../paper/1-s2.0-S106352030300023X-main.pdf", "../paper/Cold Spring Harb Perspect Biol-2012-Lu\u0308scher-a005710.pdf", "../paper/1511.03034.pdf", "../paper/1-s2.0-S016622360001657X-main.pdf", "../paper/s41586-021-04223-6.pdf", "../paper/338947v3.full.pdf", "../paper/annurev-conmatphys-031119-050745.pdf", "../paper/elife-55592-v1.pdf", "../paper/PIIS1364661321001480.pdf", "../paper/sriperumbudur10a.pdf", "../paper/Manuscript - Gidon et al., 2020.pdf", "../paper/2110.10470.pdf", "../paper/NIPS-2016-memory-efficient-backpropagation-through-time-Paper.pdf", "../paper/12366.full.pdf", "../paper/1812.07965.pdf", "../paper/BF00114731.pdf", "../paper/PIIS0896627304005768.pdf", "../paper/1906.10720.pdf", "../paper/PIIS0896627318304367.pdf", "../paper/NIPS-2001-a-natural-policy-gradient-Paper.pdf", "../paper/Attention_as_an_Organ_System.pdf", "../paper/Liu_Modprop_2022.pdf", "../paper/NIPS-2014-expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-continuous-or-discrete-weights-Paper.pdf", "../paper/NIPS-2011-ica-with-reconstruction-cost-for-efficient-overcomplete-feature-learning-Paper.pdf", "../paper/1-s2.0-S0896627300810727-main.pdf", "../paper/1511.05653.pdf", "../paper/fncom-07-00194.pdf", "../paper/RupamAutostep.pdf", "../paper/18686.pdf", "../paper/1810.06966.pdf", "../paper/1712.08969.pdf", "../paper/NIPS-2001-on-spectral-clustering-analysis-and-an-algorithm-Paper.pdf", "../paper/neco.1995.7.6.1129.pdf", "../paper/1704.04651.pdf", "../paper/bhl152.pdf", "../paper/1-s2.0-S0896627321006772-main.pdf", "../paper/2101.09258.pdf", "../paper/NIPS-2013-spectral-methods-for-neural-characterization-using-generalized-quadratic-models-Paper.pdf", "../paper/1301559.pdf", "../paper/TGbbs.pdf", "../paper/1909.13719.pdf", "../paper/science.1210280.pdf", "../paper/PIIS0896627309002372.pdf", "../paper/1603.08511.pdf", "../paper/fncom-11-00048.pdf", "../paper/s41583-020-0275-5.pdf", "../paper/1811.01501.pdf", "../paper/pnas.1031596100.pdf", "../paper/nn.2640.pdf", "../paper/1906.00889.pdf", "../paper/nrn1931.pdf", "../paper/1507.08240.pdf", "../paper/zeilerECCV2014.pdf", "../paper/1206.3881.pdf", "../paper/1-s2.0-S0896627314001123-main.pdf", "../paper/1-s2.0-S0896627315007655-main.pdf", "../paper/s41583-023-00740-7.pdf", "../paper/carter19a.pdf", "../paper/1024.full.pdf", "../paper/NIPS-2016-linear-dynamical-neural-population-models-through-nonlinear-embeddings-Paper.pdf", "../paper/Neuroscience-Inspired_Online_Unsupervised_Learning_Algorithms_Artificial_Neural_Networks.pdf", "../paper/1904.05391.pdf", "../paper/1701.00160.pdf", "../paper/rstb.2013.0288.pdf", "../paper/belilovsky20a.pdf", "../paper/1608.02164.pdf", "../paper/bhab191.pdf", "../paper/annurev-neuro-070815-013831.pdf", "../paper/s41551-020-0591-0.pdf", "../paper/1-s2.0-S0896627312003777-main.pdf", "../paper/jn.00803.2004.pdf", "../paper/1608.05151.pdf", "../paper/pnas.2102157118.pdf", "../paper/wang12.pdf", "../paper/AFST_2000_6_9_2_245_0.pdf", "../paper/science.abj5861.pdf", "../paper/Biologically_Motivated_Algorithms_for_Propagating_.pdf", "../paper/aivodji19a.pdf", "../paper/1211.4246.pdf", "../paper/s41586-021-04329-x.pdf", "../paper/9217-Article Text-12745-1-2-20201228.pdf", "../paper/nature12112.pdf", "../paper/OReillyFrank06_pbwm-neural-comp-2006.pdf", "../paper/1-s2.0-S0896627305008925-main.pdf", "../paper/psych.pdf", "../paper/Rho_Neural_Residual_Flow_Fields_for_Efficient_Video_Representations_ACCV_2022_paper.pdf", "../paper/1701.06538.pdf", "../paper/annurev-neuro-070815-013824.pdf", "../paper/2310.20587.pdf", "../paper/NeurIPS-2020-dynamical-mean-field-theory-for-stochastic-gradient-descent-in-gaussian-mixture-classification-Paper.pdf", "../paper/cadieu-et-al-2007-a-model-of-v4-shape-selectivity-and-invariance.pdf", "../paper/weinberger09a.pdf", "../paper/1902.09630.pdf", "../paper/neco.2007.19.2.442.pdf", "../paper/NIPS-2017-resurrecting-the-sigmoid-in-deep-learning-through-dynamical-isometry-theory-and-practice-Paper.pdf", "../paper/1669.full.pdf", "../paper/nature07709.pdf", "../paper/nn.3563.pdf", "../paper/empca.pdf", "../paper/13457.full.pdf", "../paper/s41467-022-32646-w.pdf", "../paper/pnas.0903214106.pdf", "../paper/elife-06063-v1.pdf", "../paper/annurev-neuro-090919-022842.pdf", "../paper/nrn2578.pdf", "../paper/10464.full.pdf", "../paper/WuNC06.pdf", "../paper/s1064827502419154.pdf", "../paper/1177011136.pdf", "../paper/1-s2.0-S0959438818301569-main.pdf", "../paper/1502.03509.pdf", "../paper/Cerrone_End-To-End_Learned_Random_Walker_for_Seeded_Image_Segmentation_CVPR_2019_paper.pdf", "../paper/387278a0.pdf", "../paper/1707.01495.pdf", "../paper/2002.09405.pdf", "../paper/1471-2202-9-S1-O13.pdf", "../paper/journal.pone.0247014.pdf", "../paper/1-s2.0-S0168010212001800-main.pdf", "../paper/bengio14.pdf", "../paper/nature19325.pdf", "../paper/file copy 16.pdf", "../paper/science.290.5500.2319.pdf", "../paper/1406.1078.pdf", "../paper/1-s2.0-S0896627305001170-main.pdf", "../paper/1507.04296.pdf", "../paper/OReillyFrank06_pbwm.pdf", "../paper/1207.0580.pdf", "../paper/12176.full.pdf", "../paper/science.1150769.pdf", "../paper/BF00992698.pdf", "../paper/refinetti21a.pdf", "../paper/pnas.2111821118.pdf", "../paper/PIIS0896627318308560.pdf", "../paper/pnas.0505220103.pdf", "../paper/mmc6.pdf", "../paper/090771806.pdf", "../paper/s41583-023-00693-x.pdf", "../paper/shrikumar17a.pdf", "../paper/s41467-018-06560-z.pdf", "../paper/1-s2.0-0165017395000143-main.pdf", "../paper/1610.06940.pdf", "../paper/NeurIPS-2018-gradient-descent-for-spiking-neural-networks-Paper.pdf", "../paper/2309.06402v1.pdf", "../paper/1803.07770.pdf", "../paper/neco.2006.18.7.1577.pdf", "../paper/1089.full.pdf", "../paper/1604.00289.pdf", "../paper/nn.4191.pdf", "../paper/nn.3826.pdf", "../paper/Dasgupta17.pdf", "../paper/fncom-09-00149.pdf", "../paper/bengio+lecun_chapter2007.pdf", "../paper/1-s2.0-B9780444626042000228-main.pdf", "../paper/ncomms16091.pdf", "../paper/2107.13586.pdf", "../paper/nihms-1754234.pdf", "../paper/jocn_a_00365.pdf", "../paper/1-s2.0-S0893608009001026-main.pdf", "../paper/2023.07.18.549575v2.full.pdf", "../paper/2112.00980.pdf", "../paper/yang21f.pdf", "../paper/NeurIPS-2020-gaussian-gated-linear-networks-Paper.pdf", "../paper/1704.05796.pdf", "../paper/Valente2022nc_LDSvsRNN.pdf", "../paper/089976601750541787.pdf", "../paper/1412.3555.pdf", "../paper/218_towards_nonlinear_disentanglem.pdf", "../paper/16601.full.pdf", "../paper/573_learning_independent_features_.pdf", "../paper/2106.09620.pdf", "../paper/1811.04918.pdf", "../paper/2022.05.17.492325.full.pdf", "../paper/NIPS-2004-spike-timing-dependent-plasticity-and-mutual-information-maximization-for-a-spiking-neuron-model-Paper.pdf", "../paper/91979-v1.pdf", "../paper/WickensReynoldsHyland2003.pdf", "../paper/PIIS0960982213009913.pdf", "../paper/9673.full.pdf", "../paper/gutmann10a.pdf", "../paper/jn.1999.82.5.2676.pdf", "../paper/1-s2.0-S009286742031165X-main.pdf", "../paper/1601.00670.pdf", "../paper/1511.06342.pdf", "../paper/1412.7525.pdf", "../paper/*852_on_the_information_bottleneck_.pdf", "../paper/NIPS-2016-exponential-expressivity-in-deep-neural-networks-through-transient-chaos-Paper.pdf", "../paper/418939v1.full.pdf", "../paper/1-s2.0-S0364021387800253-main.pdf", "../paper/[Volume 1] David E. Rumelhart, James L. McClelland, PDP Research Group - Parallel Distributed Processing_ Explorations in the Microstructure of Cognition_ Foundations (1986, The MIT Press) - libgen.li.pdf", "../paper/1801.01944.pdf", "../paper/mmc2 (2).pdf", "../paper/The Journal of Physiology - 2013 - Grillner.pdf", "../paper/*s41586-021-04268-7.pdf", "../paper/Mainen-ReliabilitySpikeTiming-1995.pdf", "../paper/pritzel17a.pdf", "../paper/21m1446769.pdf", "../paper/annurev-neuro-061010-113641.pdf", "../paper/32.full.pdf", "../paper/webb20a.pdf", "../paper/1705.07874.pdf", "../paper/2210.02157.pdf", "../paper/12-BEJSP17.pdf", "../paper/7376.full.pdf", "../paper/Gaussian_Mixture_Variational_Autoencoder_for_Semi-Supervised_Topic_Modeling.pdf", "../paper/buzsa\u0301ki-et-al-2007-hippocampal-place-cell-assemblies-are-speed-controlled-oscillators.pdf", "../paper/file (3).pdf", "../paper/1611.02779.pdf", "../paper/NeurIPS-2020-simulating-a-primary-visual-cortex-at-the-front-of-cnns-improves-robustness-to-image-perturbations-Paper.pdf", "../paper/083824v2.full.pdf", "../paper/1-s2.0-S0896627314001524-main.pdf", "../paper/Okorokova_2020_J._Neural_Eng._17_046035.pdf", "../paper/1509.02971.pdf", "../paper/1308.3432.pdf", "../paper/1-s2.0-S0896627314002517-main.pdf", "../paper/nn.2466.pdf", "../paper/elife-23763-v4.pdf", "../paper/jn.90941.2008.pdf", "../paper/2005.14165.pdf", "../paper/1606.05340.pdf", "../paper/Embedded_Data_Representations.pdf", "../paper/445715a.pdf", "../paper/NeurIPS-2020-discovering-symbolic-models-from-deep-learning-with-inductive-biases-Paper.pdf", "../paper/woods-et-al-2002-shape-perception-reduces-activity-in-human-primary-visual-cortex.pdf", "../paper/s10462-012-9338-y.pdf", "../paper/1-s2.0-S0896627317301034-main.pdf", "../paper/6235.full.pdf", "../paper/1704.00764.pdf", "../paper/file copy.pdf", "../paper/1412.6558.pdf", "../paper/2006.04182.pdf", "../paper/nrn2762.pdf", "../paper/1808.06670.pdf", "../paper/Meyer-TwoMomentDecisionModels-1987.pdf", "../paper/1-s2.0-S0893608005800036-main.pdf", "../paper/NIPS-2016-deep-learning-models-of-the-retinal-response-to-natural-scenes-Paper.pdf", "../paper/deep_Qlearning_report.pdf", "../paper/759567.pdf", "../paper/The_Role_of_Constraints_in_Hebbian_Learning.pdf", "../paper/WaiJor08_FTML.pdf", "../paper/nature11601.pdf", "../paper/Nature Neuroscience_13_3_2010.pdf", "../paper/22.pdf", "../paper/1810.11393.pdf", "../paper/Eur J of Neuroscience - 2004 - Schoenbaum - Cocaine\u2010experienced rats exhibit learning deficits in a task sensitive to.pdf", "../paper/PhysRevX.11.021064.pdf", "../paper/1767_fantastic_generalization_measu.pdf", "../paper/1901.11084.pdf", "../paper/dt.pdf", "../paper/3236386.3241340.pdf", "../paper/1-s2.0-S089662731500210X-main.pdf", "../paper/1505.04597.pdf", "../paper/2209.09174.pdf", "../paper/barcodes.pdf", "../paper/nrn1055.pdf", "../paper/1-s2.0-0165027084900074-main.pdf", "../paper/2103.11511.pdf", "../paper/2106.01862.pdf", "../paper/1704.08847.pdf", "../paper/belilovsky19a.pdf", "../paper/zhong17a.pdf", "../paper/s41593-021-01007-z.pdf", "../paper/PIIS089662731730185X.pdf", "../paper/s41467-022-34938-7.pdf", "../paper/fulltext.pdf", "../paper/978-1-4419-8853-9.pdf", "../paper/2001.02811.pdf", "../paper/nn.3433.pdf", "../paper/1905.13210.pdf", "../paper/etd.pdf", "../paper/1961189.1961200.pdf", "../paper/fnins-08-00349.pdf", "../paper/2303.04129.pdf", "../paper/nature07842.pdf", "../paper/murray-et-al-2016-stable-population-coding-for-working-memory-coexists-with-heterogeneous-neural-dynamics-in-prefrontal.pdf", "../paper/1-s2.0-S0165027016000418-main.pdf", "../paper/nature07467.pdf", "../paper/1807.03748.pdf", "../paper/SchillerSteil2004-ATW.pdf", "../paper/pnas.1105933108.pdf", "../paper/1801.04381.pdf", "../paper/1606.04671.pdf", "../paper/s41593-018-0314-y.pdf", "../paper/12717.full.pdf", "../paper/1-s2.0-S0896627311005435-main.pdf", "../paper/Berkay2016DLLimitationEuroSP.pdf", "../paper/PIIS0896627300000301.pdf", "../paper/72136.pdf", "../paper/2203.09517.pdf", "../paper/**16m1080173.pdf", "../paper/NeurIPS-2019-data-dependence-of-plateau-phenomenon-in-learning-with-neural-network-statistical-mechanical-analysis-Paper.pdf", "../paper/NIPS-2012-large-scale-distributed-deep-networks-Paper.pdf", "../paper/29_aisg_iclr2019.pdf", "../paper/fnana-11-00071.pdf", "../paper/Deep_Cascade_Learning.pdf", "../paper/science.1249098.pdf", "../paper/ncomms6768.pdf", "../paper/Dissociable_Roles_of_Ventral_and_Dorsal_Striatum_i.pdf", "../paper/1508.01983.pdf", "../paper/NIPS-2017-convergent-block-coordinate-descent-for-training-tikhonov-regularized-deep-neural-networks-Paper.pdf", "../paper/1611.07004.pdf", "../paper/1609.04747.pdf", "../paper/1-s2.0-S0896627321007790-main (1).pdf", "../paper/Scaling_Equilibrium_Propagation_to_Deep_ConvNets_b.pdf", "../paper/nn0400_391.pdf", "../paper/7546.full.pdf", "../paper/annurev-neuro-071714-033919.pdf", "../paper/1812.07956.pdf", "../paper/s41593-019-0480-6.pdf", "../paper/s41583-019-0133-5.pdf", "../paper/NeurIPS-2020-a-simple-normative-network-approximates-local-non-hebbian-learning-in-the-cortex-Paper.pdf", "../paper/NC110201.pdf", "../paper/BF00992696.pdf", "../paper/1-s2.0-S0896627319310955-main.pdf", "../paper/NIPS-2016-adaptive-optimal-training-of-animal-behavior-Paper.pdf", "../paper/1608.04644.pdf", "../paper/2209.11895.pdf", "../paper/2211.15661.pdf", "../paper/file (1) copy 2.pdf", "../paper/1210.4856.pdf", "../paper/1-s2.0-S089662731931044X-main.pdf", "../paper/0899766042321814.pdf", "../paper/1811.11682.pdf", "../paper/1705.11146.pdf", "../paper/ewrl2011_submission_11.pdf", "../paper/1804.08617.pdf", "../paper/linderman17a.pdf", "../paper/pnas.2201968119.pdf", "../paper/1805.01532.pdf", "../paper/Tsividis14.pdf", "../paper/1-s2.0-S0896627312001729-main.pdf", "../paper/1806.08734.pdf", "../paper/NIPS-2011-inferring-spike-timing-dependent-plasticity-from-spike-train-data-Paper.pdf", "../paper/NIPS-2012-neurally-plausible-reinforcement-learning-of-working-memory-tasks-Paper.pdf", "../paper/2010.08262.pdf", "../paper/Eur J of Neuroscience - 2003 - Mongillo.pdf", "../paper/elife-21886-v2.pdf", "../paper/1412.6980.pdf", "../paper/cvpr03a.pdf", "../paper/1203.1513.pdf", "../paper/2004.07780.pdf", "../paper/s43588-021-00030-1.pdf", "../paper/1508.04582.pdf", "../paper/jn.00095.2007.pdf", "../paper/Annals of the New York Academy of Sciences - 2007 - KOBAYASHI - Reward Prediction Error Computation in the Pedunculopontine.pdf", "../paper/121_pub_IEEE.pdf", "../paper/1810.09284.pdf", "../paper/Self-Supervised_Visual_Feature_Learning_With_Deep_Neural_Networks_A_Survey.pdf", "../paper/1-s2.0-S0896627313011276-main.pdf", "../paper/pnas.2013663117.pdf", "../paper/2007.11471.pdf", "../paper/rowland19a.pdf", "../paper/2004.05154.pdf", "../paper/ENEURO.0443-17.2018.full.pdf", "../paper/1801.05894.pdf", "../paper/1807.03819.pdf", "../paper/Thesis-augmented.pdf", "../paper/1202.3890.pdf", "../paper/A_direct_adaptive_method_for_faster_backpropagation_learning_the_RPROP_algorithm.pdf", "../paper/1412.6572.pdf", "../paper/NeurIPS-2018-which-neural-net-architectures-give-rise-to-exploding-and-vanishing-gradients-Paper.pdf", "../paper/1907.10599.pdf", "../paper/fnins-15-633674.pdf", "../paper/2310.19308.pdf", "../paper/girard-et-al-2001-feedforward-and-feedback-connections-between-areas-v1-and-v2-of-the-monkey-have-similar-rapid.pdf", "../paper/1503.00680.pdf", "../paper/1-s2.0-S0166223608000180-main.pdf", "../paper/fncom-04-00024.pdf", "../paper/elife-10989-v2.pdf", "../paper/1712.09926.pdf", "../paper/1502.01852.pdf", "../paper/s11263-014-0788-3.pdf", "../paper/s41593-019-0392-5.pdf", "../paper/1-s2.0-S136466130800137X-main.pdf", "../paper/2305.11252.pdf", "../paper/1-s2.0-S1364661318301669-main.pdf", "../paper/nature14236.pdf", "../paper/nn736.pdf", "../paper/file (11).pdf", "../paper/Deep_Neural_Network_Approximation_Theory.pdf", "../paper/15747.full.pdf", "../paper/1-s2.0-S089662730300535X-main.pdf", "../paper/Cold Spring Harb Perspect Biol-2012-Mayford-a005751.pdf", "../paper/fncir-12-00053.pdf", "../paper/8455.full.pdf", "../paper/1308.0850.pdf", "../paper/1904.12191.pdf", "../paper/s41583-021-00502-3.pdf", "../paper/overview13.pdf", "../paper/s43588-021-00184-y.pdf", "../paper/1-s2.0-S0042698908004380-main.pdf", "../paper/elife-03476-v2.pdf", "../paper/1806.07572.pdf", "../paper/ncomms13276.pdf", "../paper/2021.11.08.467806.full.pdf", "../paper/35861.pdf", "../paper/pricai10.pdf", "../paper/RC61.full.pdf", "../paper/nn0201_184.pdf", "../paper/science.aab4113.pdf", "../paper/s10827-021-00780-x.pdf", "../paper/ncomms14531.pdf", "../paper/2106.12612.pdf", "../paper/12978.full.pdf", "../paper/2010.11929.pdf", "../paper/nrn.2017.111.pdf", "../paper/wan21a.pdf", "../paper/ncomms12815.pdf", "../paper/s41593-018-0135-z.pdf", "../paper/2302.00111.pdf", "../paper/lewi-nc08.pdf", "../paper/1611.03673.pdf", "../paper/652.pdf", "../paper/pnas.0808113105.pdf", "../paper/083857v3.full.pdf", "../paper/1610.02136.pdf", "../paper/henaff16.pdf", "../paper/pascanu13.pdf", "../paper/wang20k.pdf", "../paper/1510.06096.pdf", "../paper/1903.01599.pdf", "../paper/455_icmlpaper.pdf", "../paper/Yagishita.DA.2014.Science.pdf", "../paper/1-s2.0-S095943881930039X-main.pdf", "../paper/41593_2017_BFnn4650_MOESM1_ESM.pdf", "../paper/s41593-021-00980-9.pdf", "../paper/file (1) copy 3.pdf", "../paper/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf", "../paper/1-s2.0-S0896627302010929-main.pdf", "../paper/2111.00034.pdf", "../paper/1802.03426.pdf", "../paper/1702.06463.pdf", "../paper/nrn1888.pdf", "../paper/1409.5185.pdf", "../paper/cohen-et-al-2020-the-limits-of-color-awareness-during-active-real-world-vision.pdf", "../paper/2102.09672.pdf", "../paper/PIIS009286742031254X.pdf", "../paper/nn.4613.pdf", "../paper/1806.01261.pdf", "../paper/elife-10094-v2.pdf", "../paper/free_energy.pdf", "../paper/1002.full.pdf", "../paper/1-s2.0-S002200009791504X-main.pdf", "../paper/2007.00810.pdf", "../paper/Eur J of Neuroscience - 2018 - Shindou.pdf", "../paper/friston-et-al-2009-predictive-coding-under-the-free-energy-principle.pdf", "../paper/PhysRevX.5.021028.pdf", "../paper/1703.00887.pdf", "../paper/blei03a.pdf", "../paper/2101.03419.pdf", "../paper/nn1931.pdf", "../paper/s00422-009-0341-6.pdf", "../paper/1511.09249.pdf", "../paper/Sutton-Precup-Singh-AIJ99.pdf", "../paper/1609.08144.pdf", "../paper/1-s2.0-S0165380698000169-main.pdf", "../paper/10295-Article Text-13823-1-2-20201228.pdf", "../paper/1312.6114.pdf", "../paper/s41593-018-0152-y.pdf", "../paper/greensmith04a.pdf", "../paper/nrn3241.pdf", "../paper/nn.4617.pdf", "../paper/elife-28295-v3.pdf", "../paper/NIPS-2016-optimal-architectures-in-a-solvable-model-of-deep-networks-Paper.pdf", "../paper/765_recurrent_experience_replay_in.pdf", "../paper/1206.6461.pdf", "../paper/35044563.pdf", "../paper/1806.05759.pdf", "../paper/NeurIPS-2018-assessing-the-scalability-of-biologically-motivated-deep-learning-algorithms-and-architectures-Paper.pdf", "../paper/ncomms12554.pdf", "../paper/science.aav9436.pdf", "../paper/1807.04587.pdf", "../paper/2023.06.27.546656v1.full.pdf", "../paper/NeurIPS-2019-limitations-of-lazy-training-of-two-layers-neural-network-Paper.pdf", "../paper/1-s2.0-S0042698911001544-main.pdf", "../paper/neco_a_01571.pdf", "../paper/1-s2.0-S2352154615001229-main.pdf", "../paper/2201.11903.pdf", "../paper/nn.4401.pdf", "../paper/BdSLT.pdf", "../paper/2023.02.28.530327.full.pdf", "../paper/783_deep_neural_networks_as_gaussi.pdf", "../paper/neco.2007.19.3.706.pdf", "../paper/1-s2.0-S0893608014002135-main.pdf", "../paper/1-s2.0-S0896627314006461-main.pdf", "../paper/1606.04460.pdf", "../paper/1-s2.0-S0896627315008260-main.pdf", "../paper/1410.5401.pdf", "../paper/Self-organization_in_a_perceptual_network.pdf", "../paper/3870.full.pdf", "../paper/fncom-08-00135.pdf", "../paper/41586_2023_6031_MOESM1_ESM.pdf", "../paper/1711.05136.pdf", "../paper/nrn.2016.9.pdf", "../paper/1701.08734.pdf", "../paper/1-s2.0-S0028393213000675-main.pdf", "../paper/nrn.2018.6.pdf", "../paper/1704.03732.pdf", "../paper/BF00198477.pdf", "../paper/parisien.eliasmith.2007.negative weights.penult.neurcomp.pdf", "../paper/elife-27756-v2.pdf", "../paper/1511.06581.pdf", "../paper/1-s2.0-S0896627312008197-main.pdf", "../paper/Markram-1997.pdf", "../paper/hm95.pdf", "../paper/214262.full.pdf", "../paper/1605.08454.pdf", "../paper/agivision2011.pdf", "../paper/drew-abbott-2006-extending-the-effects-of-spike-timing-dependent-plasticity-to-behavioral-timescales.pdf", "../paper/2006.11239.pdf", "../paper/1-s2.0-S0896627318303878-main.pdf", "../paper/jjnh91.pdf", "../paper/sciadv.adg3256.pdf", "../paper/1906.06766.pdf", "../paper/pdp8.pdf", "../paper/nature14540.pdf", "../paper/nmeth.4399.pdf", "../paper/file (10).pdf", "../paper/nn.3024.pdf", "../paper/nature09263.pdf", "../paper/1604.06778.pdf", "../paper/1703.07914.pdf", "../paper/1612.05596.pdf", "../paper/vanseijen13.pdf", "../paper/1503.00690.pdf", "../paper/nature03015.pdf", "../paper/s41583-022-00620-6.pdf", "../paper/neco_a_01160.pdf", "../paper/1409.4842.pdf", "../paper/16494.full.pdf", "../paper/1611.05763.pdf", "../paper/1-s2.0-S1074742714001543-main.pdf", "../paper/2000-15248-005.pdf", "../paper/1-s2.0-S0166223604001900-main.pdf", "../paper/neco.1992.4.3.415.pdf", "../paper/089976603321780272.pdf", "../paper/Independent_rate_and_temporal_coding_in_hippocampa.pdf", "../paper/bishop-gtm-ncomp-98.pdf", "../paper/*1-s2.0-S0959438818300990-main.pdf", "../paper/bengio03a.pdf", "../paper/s41586-019-1767-1.pdf", "../paper/Nonlinear_backpropagation_doing_backpropagation_without_derivatives_of_the_activation_function.pdf", "../paper/opt_comp.pdf", "../paper/323533a0.pdf", "../paper/NeurIPS-2018-understanding-batch-normalization-Paper.pdf", "../paper/nature04587.pdf", "../paper/PhysRevX.8.031003.pdf", "../paper/9475.full.pdf", "../paper/fnins-12-00608.pdf", "../paper/carreira-perpinan14.pdf", "../paper/*manifold-perception.pdf", "../paper/41593_2019_460_MOESM1_ESM.pdf", "../paper/NIPS-2008-gaussian-process-factor-analysis-for-low-dimensional-single-trial-analysis-of-neural-population-activity-Paper.pdf", "../paper/1811.02017.pdf", "../paper/2020.06.12.148775v3.full.pdf", "../paper/PIIS1364661319300129.pdf", "../paper/1506.05254.pdf", "../paper/1-s2.0-S0028390806002401-main.pdf", "../paper/neco.1996.8.5.895.pdf", "../paper/PIIS0960982215002067.pdf", "../paper/Eur J of Neuroscience - 2020 - Rubin - The credit assignment problem in cortico\u2010basal ganglia\u2010thalamic networks  A review .pdf", "../paper/Barto1983.pdf", "../paper/1804.02464.pdf", "../paper/PIIS0896627321009478.pdf", "../paper/shouval-et-al-2002-a-unified-model-of-nmda-receptor-dependent-bidirectional-synaptic-plasticity.pdf", "../paper/s42979-021-00815-1.pdf", "../paper/1412.6614.pdf", "../paper/41583_2020_277_MOESM1_ESM.pdf", "../paper/1-s2.0-S1364661315001023-main.pdf", "../paper/PIIS0166223615001551.pdf", "../paper/s41586-020-2802-y.pdf", "../paper/1609.09059.pdf", "../paper/neco.1997.9.7.1493.pdf", "../paper/8400.full.pdf", "../paper/nature16961.pdf", "../paper/hyvarinen05a.pdf", "../paper/*1902.06162.pdf", "../paper/neco05.pdf", "../paper/jn.00024.2007.pdf", "../paper/1602.05908.pdf", "../paper/*NIPS-2002-stochastic-neighbor-embedding-Paper.pdf", "../paper/1-s2.0-S0896627310002321-main.pdf", "../paper/jn.00454.2022.pdf", "../paper/nn.2501.pdf", "../paper/1406.2751.pdf", "../paper/PIIS0896627315010375.pdf", "../paper/1406.2989.pdf", "../paper/1-s2.0-S0896627303005075-main.pdf", "../paper/torcs.pdf", "../paper/1-s2.0-0885064X88900210-main.pdf", "../paper/1812.11103.pdf", "../paper/pnas.2018422118.pdf", "../paper/elife-22901-v1.pdf", "../paper/2302.05326.pdf", "../paper/pnas.201611835si.pdf", "../paper/1611.00035.pdf", "../paper/2302.03025.pdf", "../paper/elife-38105-v2.pdf", "../paper/1-s2.0-S0896627318304744-main.pdf", "../paper/yang21c.pdf", "../paper/LakshmananNC2015.pdf", "../paper/2103.00020.pdf", "../paper/1905.07088.pdf", "../paper/1706.10295.pdf", "../paper/1-s2.0-S1364661307001593-main.pdf", "../paper/1901.10995.pdf", "../paper/nrn1327.pdf", "../paper/NIPS-2015-deep-learning-with-elastic-averaging-sgd-Paper.pdf", "../paper/1406.6247.pdf", "../paper/1502.04681.pdf", "../paper/NeurIPS-2019-a-unified-theory-for-the-origin-of-grid-cells-through-the-lens-of-pattern-formation-Paper.pdf", "../paper/DayanNiv2008.pdf", "../paper/1605.02026.pdf", "../paper/2306.04532.pdf", "../paper/1-1-1.pdf", "../paper/1-s2.0-S089360800580125X-main.pdf", "../paper/1803.10760.pdf", "../paper/1703.01988.pdf", "../paper/file (1) copy 4.pdf", "../paper/download copy.pdf", "../paper/2210.03310.pdf", "../paper/croce20b.pdf", "../paper/1610.06545.pdf", "../paper/1705.08439.pdf", "../paper/*s41586-018-0102-6.pdf", "../paper/bergstra12a.pdf", "../paper/2006.10246.pdf", "../paper/PIIS0092867408012981.pdf", "../paper/tr-final.pdf", "../paper/s41467-019-08931-6.pdf", "../paper/file copy 9.pdf", "../paper/1803.01686.pdf", "../paper/nature14153.pdf", "../paper/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf", "../paper/s13040-017-0155-3.pdf", "../paper/PIIS0896627321006826.pdf", "../paper/lleintro.pdf", "../paper/bordelon20a.pdf", "../paper/1-s2.0-S0896627309002876-main.pdf", "../paper/Functional_network_reorganization_in_motor_cortex_.pdf", "../paper/NIPS-2014-inferring-synaptic-conductances-from-spike-trains-with-a-biophysically-inspired-point-process-model-Paper.pdf", "../paper/s42256-020-0216-z.pdf", "../paper/NeurIPS-2019-intrinsic-dimension-of-data-representations-in-deep-neural-networks-Paper.pdf", "../paper/rstb.2016.0260.pdf", "../paper/nrn2558.pdf", "../paper/1311.2901.pdf", "../paper/Ghahramani 2015 Nature.pdf", "../paper/fncir-07-00037.pdf", "../paper/12368.full.pdf", "../paper/1505.04544.pdf", "../paper/1205.4839.pdf", "../paper/LeNgiCoaLahProNg11.pdf", "../paper/1905.11946.pdf", "../paper/nocedal80.pdf", "../paper/1305.6663.pdf", "../paper/1606.01651.pdf", "../paper/kass1995.pdf", "../paper/1-s2.0-S0893608003002466-main.pdf", "../paper/science.274.5293.1724.pdf", "../paper/s41593-019-0381-8.pdf", "../paper/1-s2.0-S0301008213000129-main.pdf", "../paper/385533a0.pdf", "../paper/nature01614.pdf", "../paper/Voxel-Based Morphometry.pdf", "../paper/science.1255514.pdf", "../paper/337129a0.pdf", "../paper/elife-49547-v2.pdf", "../paper/1805.09545.pdf", "../paper/NIPS-2011-empirical-models-of-spiking-in-neural-populations-Paper.pdf", "../paper/1-s2.0-S0896627316307231-mainext.pdf", "../paper/elife-04250-v1.pdf", "../paper/tsitsiklis02a.pdf", "../paper/1-s2.0-S0149763418303609-main.pdf", "../paper/file (2).pdf", "../paper/1412.5068.pdf", "../paper/PIIS0896627318309930.pdf", "../paper/1-s2.0-S0896627308008362-main.pdf", "../paper/1909.11304.pdf", "../paper/allen-zhu19a.pdf", "../paper/2012.04030.pdf", "../paper/Spectral_Clustering_on_Multiple_Manifolds.pdf", "../paper/1512.08571.pdf", "../paper/deng2014large.pdf", "../paper/wang-2010-neurophysiological-and-computational-principles-of-cortical-rhythms-in-cognition.pdf", "../paper/nn.2648.pdf", "../paper/273.full_.pdf", "../paper/PIIS1364661320302199.pdf", "../paper/s41586-019-1261-9.pdf", "../paper/jn.01171.2003.pdf", "../paper/s43586-020-00001-2.pdf", "../paper/NeurIPS-2020-learning-to-learn-with-feedback-and-local-plasticity-Paper.pdf", "../paper/annurev-statistics-022513-115657.pdf", "../paper/NIPS-2013-distributed-representations-of-words-and-phrases-and-their-compositionality-Paper.pdf", "../paper/PhysRevLett.126.180604-accepted.pdf", "../paper/1123_learning_an_embedding_space_fo.pdf", "../paper/OReilly96_generec_nc.pdf", "../paper/P-2237-30457224.pdf", "../paper/1902.06720.pdf", "../paper/braun2022exact.pdf", "../paper/nrn.2015.26.pdf", "../paper/file copy 8.pdf", "../paper/370140a0.pdf", "../paper/1-s2.0-S1364661316300432-main.pdf", "../paper/jn.1998.80.1.1.pdf", "../paper/1707.03300.pdf", "../paper/neco.2007.19.6.1468.pdf", "../paper/1806.07366.pdf", "../paper/nn.4403.pdf", "../paper/FFA13.pdf", "../paper/1801.10130.pdf", "../paper/Distributed_MinMax_Learning_Scheme_for_Neural_Networks_With_Applications_to_High-Dimensional_Classification.pdf", "../paper/1902.09229.pdf", "../paper/1511.06434.pdf", "../paper/NeurIPS-2018-manifold-tiling-localized-receptive-fields-are-optimal-in-similarity-preserving-neural-networks-Paper.pdf", "../paper/s41592-021-01330-0.pdf", "../paper/1-s2.0-S0896627315003645-main.pdf", "../paper/fnins-10-00106.pdf", "../paper/2011.09468.pdf", "../paper/gu20a.pdf", "../paper/elife-21492-v2.pdf", "../paper/Using_goal_driven_deep_learning_models_t.pdf", "../paper/1801.00690.pdf", "../paper/nn.2264.pdf", "../paper/gillett-et-al-2020-characteristics-of-sequential-activity-in-networks-with-temporally-asymmetric-hebbian-learning.pdf", "../paper/2305.13245.pdf", "../paper/PIIS009286742031388X.pdf", "../paper/1508.00330.pdf", "../paper/1909.08053.pdf", "../paper/s41593-018-0309-8.pdf", "../paper/2310.13018.pdf", "../paper/1412.4564.pdf", "../paper/NeurIPS-2018-learning-a-latent-manifold-of-odor-representations-from-neural-responses-in-piriform-cortex-Paper.pdf", "../paper/1-s2.0-S0022249608001181-main.pdf", "../paper/Deep_Neural_Networks_for_Acoustic_Modeling_in_Speech_Recognition_The_Shared_Views_of_Four_Research_Groups.pdf", "../paper/8443.full.pdf", "../paper/1-s2.0-S002224961100071X-main.pdf", "../paper/nn.3807.pdf", "../paper/1507.07580.pdf", "../paper/1811.10766.pdf", "../paper/67258.pdf", "../paper/1-s2.0-S009286741730538X-main.pdf", "../paper/PIIS2667237521001600.pdf", "../paper/PVF.pdf", "../paper/PhysRevLett.65.1683.pdf", "../paper/109_explaining_the_learning_dynami.pdf", "../paper/nrn1427.pdf", "../paper/10005.full.pdf", "../paper/0912.3995.pdf", "../paper/GershmanBleiNiv2010.pdf", "../paper/1502.04156.pdf", "../paper/2508_detecting_modularity_in_deep_n.pdf", "../paper/1-s2.0-S0959438817300454-main.pdf", "../paper/s41593-019-0517-x.pdf", "../paper/fnins-14-00119.pdf", "../paper/s42256-022-00498-0.pdf", "../paper/Adaptive_control_of_Markov_chains_with_average_cost.pdf", "../paper/GrosmarkBuzsaki_Science_2016_Combined.pdf", "../paper/s41467-017-02717-4.pdf", "../paper/2206.00364.pdf", "../paper/PIIS089662731400405X.pdf", "../paper/1605.07110.pdf", "../paper/ENEURO.0427-20.2020.full.pdf", "../paper/1803.09082.pdf", "../paper/89_M_JPA.pdf", "../paper/1711.05769.pdf", "../paper/1708.05144.pdf", "../paper/4693.full.pdf", "../paper/41467_2020_14578_MOESM1_ESM.pdf", "../paper/science.aaa5542.pdf", "../paper/s41593-019-0520-2.pdf", "../paper/A_generalized_iterative_LQG_method_for_locally-optimal_feedback_control_of_constrained_nonlinear_stochastic_systems.pdf", "../paper/file (1) copy.pdf", "../paper/2011.08088.pdf", "../paper/sdm97.pdf", "../paper/1602.07389.pdf", "../paper/seijen14.pdf", "../paper/ruvolo13.pdf", "../paper/bengio12a.pdf", "../paper/dct06.pdf", "../paper/s10994-012-5278-7.pdf", "../paper/b101281.pdf", "../paper/2023.07.09.548255.full.pdf", "../paper/2008.02217.pdf", "../paper/strehl09a.pdf", "../paper/canatara_phd_dissertation.pdf", "../paper/1702.05043.pdf", "../paper/1802.02611.pdf", "../paper/science.1188224.pdf", "../paper/1-s2.0-S0959438820301367-main.pdf", "../paper/NIPS-1999-spike-based-learning-rules-and-stabilization-of-persistent-neural-activity-Paper.pdf", "../paper/neco.1989.1.2.270.pdf", "../paper/1176325622.pdf", "../paper/1703.00548.pdf", "../paper/1503.05571.pdf", "../paper/PIIS0960982221016821.pdf", "../paper/122344.122377.pdf", "../paper/Back_propagation_through_adjoints_for_the_identification_of_nonlinear_dynamic_systems_using_recurrent_neural_models.pdf", "../paper/science.1172377.pdf", "../paper/IEEE_transactions_on_pattern_analysis_20(12).pdf", "../paper/1710.02298.pdf", "../paper/nature06445.pdf", "../paper/2007.15139.pdf", "../paper/350.full.pdf", "../paper/PIIS0896627301001787.pdf", "../paper/1-s2.0-S095943880900124X-main.pdf", "../paper/he-embedding2.pdf", "../paper/2109.03879.pdf", "../paper/1511.02543.pdf", "../paper/nn.2388.pdf", "../paper/070703983.pdf", "../paper/2002.05709.pdf", "../paper/1-s2.0-S0896627309006953-main.pdf", "../paper/file (9).pdf", "../paper/duchi11a.pdf", "../paper/nn.2439.pdf", "../paper/NeurIPS-2021-credit-assignment-through-broadcasting-a-global-error-vector-Paper.pdf", "../paper/085886v1.full.pdf", "../paper/Saxe.13.HierCat.pdf", "../paper/1409.0473.pdf", "../paper/Kalman60.pdf", "../paper/nn.4061.pdf", "../paper/J of Comparative Neurology - 1 July 1989 - Rockland.pdf", "../paper/krotov-hopfield-2019-unsupervised-learning-by-competing-hidden-units.pdf", "../paper/s41586-020-2350-5.pdf", "../paper/Unified Segmentation.pdf", "../paper/NeurIPS-2018-modular-networks-learning-to-decompose-neural-computation-Paper.pdf", "../paper/Listen_attend_and_spell_A_neural_network_for_large_vocabulary_conversational_speech_recognition.pdf", "../paper/1606.05336.pdf", "../paper/s41586-021-03950-0.pdf", "../paper/Ras04.pdf", "../paper/agarwal-etal.pdf", "../paper/TodorovIROS12.pdf", "../paper/nn.4049.pdf", "../paper/s41593-019-0502-4.pdf", "../paper/s10107-006-0706-8.pdf", "../paper/1-s2.0-S0167691104000398-main.pdf", "../paper/GoodmanEtAl2015-Chapter.pdf", "../paper/bernstein18a.pdf", "../paper/1-s2.0-S0896627314011350-main.pdf", "../paper/PIIS0960982216305656.pdf", "../paper/1703.01327.pdf", "../paper/1411.5908.pdf", "../paper/fncom-10-00131.pdf", "../paper/annurev-neuro-072116-031538.pdf", "../paper/p087c.pdf", "../paper/NIPS-2014-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization-Paper.pdf", "../paper/BayesOptLoop.pdf", "../paper/NeurIPS-2019-bipartite-expander-hopfield-networks-as-self-decoding-high-capacity-error-correcting-codes-Paper.pdf", "../paper/9870.full.pdf", "../paper/1511.05946.pdf", "../paper/s41593-019-0377-4.pdf", "../paper/1511.05952.pdf", "../paper/1-s2.0-S0959438823001411-main.pdf", "../paper/2301.05217.pdf", "../paper/elife-51005-v3.pdf", "../paper/pnas.81.10.3088.pdf", "../paper/1-s2.0-S0959438821001227-main.pdf", "../paper/NIPS-2009-measuring-invariances-in-deep-networks-Paper.pdf", "../paper/Moser-Annreview+2008.pdf", "../paper/1802.03268.pdf", "../paper/1-s2.0-S0306452214008707-main.pdf", "../paper/jn.00097.2009.pdf", "../paper/annualrev_10.pdf", "../paper/1806.01822.pdf", "../paper/nature09159.pdf", "../paper/elife-47889-v2.pdf", "../paper/nn1560.pdf", "../paper/LakeEtAl2015Science.pdf", "../paper/1412.6856.pdf", "../paper/NeurIPS-2020-can-the-brain-do-backpropagation-exact-implementation-of-backpropagation-in-predictive-coding-networks-Paper.pdf", "../paper/camastra2016.pdf", "../paper/NIPS-2014-a-framework-for-studying-synaptic-plasticity-with-neural-spike-train-data-Paper.pdf", "../paper/Batty, Multilayer recurrent network models of primate retinal ganglion cll responses.pdf", "../paper/1112_learning_to_solve_the_credit_a.pdf", "../paper/1605.06743.pdf", "../paper/NeurIPS-2020-neural-path-features-and-neural-path-kernel-understanding-the-role-of-gates-in-deep-learning-Paper.pdf", "../paper/science.3749885.pdf", "../paper/s41593-022-01088-4.pdf", "../paper/rprop.pdf", "../paper/1-s2.0-S0959438818300485-main.pdf", "../paper/s41586-023-06415-8.pdf", "../paper/Gollisch_2008_Latencies.pdf", "../paper/2011.13456.pdf", "../paper/1-s2.0-S0896627317304178-main.pdf", "../paper/1512.00567.pdf", "../paper/NeurIPS-2019-adversarial-robustness-through-local-linearization-Paper.pdf", "../paper/978-3-642-35289-8.pdf", "../paper/1702.08360.pdf", "../paper/Gerstner(2015)_three_factor_EM.pdf", "../paper/2006.02427.pdf", "../paper/s41583-022-00642-0.pdf", "../paper/fnhum-08-00825.pdf", "../paper/1-s2.0-S0896627320303664-main.pdf", "../paper/PIIS0896627310002874.pdf", "../paper/2305.06160.pdf", "../paper/2311.04830.pdf", "../paper/PhysRevLett.61.259.pdf", "../paper/Plumbley02-nnica_accepted_notice.pdf", "../paper/nature03689.pdf", "../paper/2103.12593v1.pdf", "../paper/neco_a_00409.pdf", "../paper/nn.2599.pdf", "../paper/ranzato-cvpr-07.pdf", "../paper/Behavioral_time_scale_synaptic_plasticity_underlie.pdf", "../paper/1-s2.0-S0959438815000768-main.pdf", "../paper/1802.06070.pdf", "../paper/BleiLafferty2009.pdf", "../paper/2107.12808.pdf", "../paper/1-s2.0-S0896627315004766-main.pdf", "../paper/1-s2.0-S0959438814001044-main.pdf", "../paper/2106.02073.pdf", "../paper/nature14446.pdf", "../paper/1803.00933.pdf", "../paper/2206.01079.pdf", "../paper/1312.6199.pdf", "../paper/1912.10189.pdf", "../paper/BTT.pdf", "../paper/A_1013776130161.pdf", "../paper/nn.2957.pdf", "../paper/564476.full.pdf", "../paper/white-modayil-sutton-2014.pdf", "../paper/1411.6191.pdf", "../paper/2002.04010.pdf", "../paper/1-s2.0-S030645220200026X-main.pdf", "../paper/1684.full.pdf", "../paper/PIIS0896627318307414.pdf", "../paper/zimmermann21a.pdf", "../paper/s41583-020-00395-8.pdf", "../paper/nature13665.pdf", "../paper/2210.08340.pdf", "../paper/2310.02207.pdf", "../paper/Representation_Learning_A_Review_and_New_Perspectives.pdf", "../paper/1-s2.0-S0896627317306955-main.pdf", "../paper/1605.08803.pdf", "../paper/1408.5093.pdf", "../paper/2101.04704.pdf", "../paper/fncom-11-00024.pdf", "../paper/NeurIPS-2019-real-time-reinforcement-learning-Paper.pdf", "../paper/*nature12742.pdf", "../paper/nn.3532.pdf", "../paper/1511.06295.pdf", "../paper/The Journal of Physiology - 2023 - Richards - The study of plasticity has always been about gradients.pdf", "../paper/pnas.0802631105.pdf", "../paper/A_1011204814320.pdf", "../paper/PhysRevResearch.3.013176.pdf", "../paper/PIIS0092867409011040.pdf", "../paper/elife-25784-v1.pdf", "../paper/file (5).pdf", "../paper/*vandermaaten08a.pdf", "../paper/Martin-Brualla_NeRF_in_the_Wild_Neural_Radiance_Fields_for_Unconstrained_Photo_CVPR_2021_paper.pdf", "../paper/2011.12428.pdf", "../paper/nn.3917.pdf", "../paper/1806.00900.pdf", "../paper/691_kernel_rnn_learning_kernl_.pdf", "../paper/lecun1.pdf", "../paper/fnhum-05-00039.pdf", "../paper/2202.08384.pdf", "../paper/1-s2.0-S0896627310009384-main.pdf", "../paper/1-s2.0-S2352154621000024-main.pdf", "../paper/CABN.4.4.483.pdf", "../paper/Eligibility Traces for Off-Policy Policy Evaluation.pdf", "../paper/1811.03804.pdf", "../paper/5314.full.pdf", "../paper/nn.4062.pdf", "../paper/glorot10a.pdf", "../paper/pnas.1604850113.pdf", "../paper/9833.full.pdf", "../paper/nn1826.pdf", "../paper/1909.08156.pdf", "../paper/1803.00885.pdf", "../paper/qin_etal_neco_2021.pdf", "../paper/8672-Article Text-12200-1-2-20201228.pdf", "../paper/1809.04474.pdf", "../paper/1-s2.0-S0896627321010357-main.pdf", "../paper/hyva\u0308rinen-2013-independent-component-analysis-recent-advances.pdf", "../paper/1401.4082.pdf", "../paper/1802.07569.pdf", "../paper/2007.02686.pdf", "../paper/castellani-et-al-2001-a-biophysical-model-of-bidirectional-synaptic-plasticity-dependence-on-ampa-and-nmda-receptors.pdf", "../paper/2203.06056.pdf", "../paper/PIIS0960982222001166.pdf", "../paper/nn.2163.pdf", "../paper/2107.04474.pdf", "../paper/nature11649.pdf", "../paper/2023.01.16.523429.full.pdf", "../paper/science.1166466.pdf", "../paper/11791-Article Text-15319-1-2-20201228.pdf", "../paper/MACnc92b.pdf", "../paper/neco_a_00998.pdf", "../paper/BF02289451.pdf", "../paper/1910.02054.pdf", "../paper/s41583-019-0197-2.pdf", "../paper/s10994-015-5528-6.pdf", "../paper/1312.6211.pdf", "../paper/1910.07104.pdf", "../paper/1-s2.0-S0959438817300429-main.pdf", "../paper/koh17a.pdf", "../paper/5609.full.pdf", "../paper/nrn.2017.85.pdf", "../paper/1-s2.0-S0896627318300655-main.pdf", "../paper/physrev.00030.2005.pdf", "../paper/nature13664.pdf", "../paper/Deep_learning_and_the_information_bottleneck_principle.pdf", "../paper/NeurIPS-2021-relative-flatness-and-generalization-Supplemental.pdf", "../paper/Deep Learning with Dynamic Spiking Neurons and FixedFeedback Weights.pdf", "../paper/Science-2012-Eliasmith-Spaun.pdf", "../paper/s41467-022-34452-w.pdf", "../paper/2435.full.pdf", "../paper/s41593-019-0550-9.pdf", "../paper/2104.14294.pdf", "../paper/1511.04508.pdf", "../paper/1-s2.0-S0893608002000473-main.pdf", "../paper/1509.03005.pdf", "../paper/1904.11955.pdf", "../paper/2310.06114.pdf", "../paper/PIIS0896627316310406.pdf", "../paper/1-s2.0-S0959438820300817-main.pdf", "../paper/s41467-022-28552-w.pdf", "../paper/nature06725.pdf", "../paper/nn.3137.pdf", "../paper/6266.full.pdf", "../paper/NIPS-2016-bayesian-latent-structure-discovery-from-multi-neuron-recordings-Paper.pdf", "../paper/1-s2.0-S0924809905800150-main.pdf", "../paper/nn.3645.pdf", "../paper/NIPS-2016-probing-the-compositionality-of-intuitive-functions-Paper.pdf", "../paper/pnas.1905544116.pdf", "../paper/978-3-319-12637-1.pdf", "../paper/1-s2.0-S0893608019300784-main.pdf", "../paper/1909.04630.pdf", "../paper/nn.3862.pdf", "../paper/s41593-020-0671-1.pdf", "../paper/NeurIPS-2021-biological-learning-in-key-value-memory-networks-Paper.pdf", "../paper/osdi22-zheng-lianmin.pdf", "../paper/2110.05038.pdf", "../paper/1707.02286.pdf", "../paper/1510.05067.pdf", "../paper/2069_are_neural_nets_modular_inspec.pdf", "../paper/nn0199_79.pdf", "../paper/1-s2.0-S0010945217303258-main.pdf", "../paper/mumford-carlsson et al.pdf", "../paper/1709.06560.pdf", "../paper/jcb_200806149.pdf", "../paper/2203.10036.pdf", "../paper/3697.full.pdf", "../paper/089976602760407955.pdf", "../paper/1606.03813.pdf", "../paper/neco.1994.6.2.296.pdf", "../paper/1-s2.0-S0893608020303117-main.pdf", "../paper/jn.00364.2007.pdf", "../paper/s42256-023-00650-4.pdf", "../paper/2205.14135.pdf", "../paper/1910.04958.pdf", "../paper/NeurIPS-2018-how-to-start-training-the-effect-of-initialization-and-architecture-Paper.pdf", "../paper/Eur J of Neuroscience - 2001 - Hall - Involvement of the central nucleus of the amygdala and nucleus accumbens core in.pdf", "../paper/1907.08549.pdf", "../paper/s41539-019-0048-y.pdf", "../paper/KakadeLangford-icml2002.pdf", "../paper/1-s2.0-S0959438816302641-main.pdf", "../paper/2010.14765.pdf", "../paper/NeurIPS-2020-biological-credit-assignment-through-dynamic-inversion-of-feedforward-networks-Paper.pdf", "../paper/1-s2.0-S0042698997001697-main.pdf", "../paper/3351.full.pdf", "../paper/PIIS1550413118305151.pdf", "../paper/41586_2012_BFnature11129_MOESM225_ESM.pdf", "../paper/798553v2.full.pdf", "../paper/*saul03a.pdf", "../paper/s41593-020-00733-0.pdf", "../paper/1711.00165.pdf", "../paper/blundell15.pdf", "../paper/1-s2.0-S2352154618302092-main.pdf", "../paper/24_line_attractor_dynamics_in_rec.pdf", "../paper/1511.04524.pdf", "../paper/2006.09011.pdf", "../paper/2210.06591.pdf", "../paper/2105.10446.pdf", "../paper/NeurIPS-2022-learning-dynamics-of-deep-linear-networks-with-multiple-pathways-Paper-Conference.pdf", "../paper/nrn3785.pdf", "../paper/s41592-020-01008-z.pdf", "../paper/1908.05575.pdf", "../paper/2203.03466.pdf", "../paper/s41586-023-06031-6.pdf", "../paper/elife-17086-v1.pdf", "../paper/annurev.neuro.22.1.241.pdf", "../paper/s41598-018-22160-9.pdf", "../paper/1-s2.0-0306452294905363-main.pdf", "../paper/1-s2.0-S1053811913006599-main.pdf", "../paper/35087601.pdf", "../paper/elife-05558-v2.pdf", "../paper/Varieties_of_learning_automata_an_overview.pdf", "../paper/s41586-018-0654-5.pdf", "../paper/Magnetic Resonance in Med - 2005 - Buxton - Dynamics of blood flow and oxygenation changes during brain activation  The.pdf", "../paper/1703.03906.pdf", "../paper/2005.05541.pdf", "../paper/07.rah.rec.nips.pdf", "../paper/s41593-019-0364-9.pdf", "../paper/ncomms13239.pdf", "../paper/WISPmanhattenrule2015.pdf", "../paper/9781680835397-summary.pdf", "../paper/fnins-15-629892.pdf", "../paper/science.aah6066.pdf", "../paper/1802.05408.pdf", "../paper/journal.pcbi.1010255.pdf", "../paper/Niv_2007_tonic_dopamine.pdf", "../paper/1-s2.0-S0042698997001211-main.pdf", "../paper/2208.02813.pdf", "../paper/2106.01345.pdf", "../paper/hebbdot.pdf", "../paper/asru_2013.pdf", "../paper/s41467-020-19788-5.pdf", "../paper/ncomms13749.pdf", "../paper/PIIS0896627317305093.pdf", "../paper/das-et-al-2006-computational-prediction-of-methylation-status-in-human-genomic-sequences.pdf", "../paper/1504.00641.pdf", "../paper/nature14251.pdf", "../paper/mniha16.pdf", "../paper/1635.full.pdf", "../paper/whalen-et-al-2020-delta-oscillations-are-a-robust-biomarker-of-dopamine-depletion-severity-and-motor-dysfunction-in.pdf", "../paper/532_icmlpaper.pdf", "../paper/1-s2.0-S0896627313011276-mmc1.pdf", "../paper/Surrogate_Gradient_Learning_in_Spiking_Neural_Networks_Bringing_the_Power_of_Gradient-Based_Optimization_to_Spiking_Neural_Networks.pdf", "../paper/1634.full.pdf", "../paper/jn.01311.2006.pdf", "../paper/8145.full.pdf", "../paper/1907.02893.pdf", "../paper/*science.aao0284.pdf", "../paper/s41592-019-0598-1.pdf", "../paper/*pnas.2015509117.pdf", "../paper/2307.08691.pdf", "../paper/s42256-021-00430-y.pdf", "../paper/2111.11215.pdf", "../paper/s44159-022-00115-2.pdf", "../paper/1-s2.0-S0959438815001889-main.pdf", "../paper/1910.02509.pdf", "../paper/NeurIPS-2018-long-short-term-memory-and-learning-to-learn-in-networks-of-spiking-neurons-Paper.pdf", "../paper/NeurIPS-2021-reverse-engineering-learned-optimizers-reveals-known-and-novel-mechanisms-Paper.pdf", "../paper/2010.02502.pdf", "../paper/neco.2006.18.6.1318.pdf", "../paper/mei-et-al-2018-a-mean-field-view-of-the-landscape-of-two-layer-neural-networks.pdf", "../paper/nn963.pdf", "../paper/1-s2.0-S0042698901000736-main.pdf", "../paper/PhysRevE.69.066138.pdf", "../paper/blais-et-al-1999-the-role-of-presynaptic-activity-in-monocular-deprivation-comparison-of-homosynaptic-and.pdf", "../paper/8812.full.pdf", "../paper/1506.02438.pdf", "../paper/1806.06144.pdf", "../paper/bhx339.pdf", "../paper/DeiRas11.pdf", "../paper/1390156.1390177.pdf", "../paper/hyvarinen19a.pdf", "../paper/2111.02080.pdf", "../paper/NeurIPS-2022-beyond-accuracy-generalization-properties-of-bio-plausible-temporal-credit-assignment-rules-Paper-Conference.pdf", "../paper/nature02618.pdf", "../paper/2561_score_based_generative_modelin.pdf", "../paper/1803.03635.pdf", "../paper/1902.01007.pdf", "../paper/pnas.1802705116.pdf", "../paper/coalson-et-al-2018-the-impact-of-traditional-neuroimaging-methods-on-the-spatial-localization-of-cortical-areas.pdf", "../paper/2984875.pdf", "../paper/2102.11107.pdf", "../paper/5066.full.pdf", "../paper/Eur J of Neuroscience - 2012 - Collins - How much of reinforcement learning is working memory  not reinforcement learning .pdf", "../paper/nn.4339.pdf", "../paper/NIPS-2007-kernel-measures-of-conditional-dependence-Paper.pdf", "../paper/bhaa023.pdf", "../paper/10532_stochastic_solutions_for_linea.pdf", "../paper/Hippocampus - 2007 - Burgess - An oscillatory interference model of grid cell firing.pdf", "../paper/mmc2 copy.pdf", "../paper/Manifold_clustering.pdf", "../paper/fnsyn-02-00146.pdf", "../paper/PIIS1364661319300610.pdf", "../paper/pnas.1611835114.pdf", "../paper/PIIS0896627313000937.pdf", "../paper/schaul15.pdf", "../paper/jn.00697.2004.pdf", "../paper/41593_2018_147_MOESM1_ESM.pdf", "../paper/2911.pdf", "../paper/1910.03561.pdf", "../paper/Normalized_cuts_and_image_segmentation.pdf", "../paper/SuperSpike_Supervised_Learning_in_Multilayer_Spiki.pdf", "../paper/1300015.pdf", "../paper/1803.09522.pdf", "../paper/Predicting_spike_timing_of_neocortical_pyramidal_neurons.pdf", "../paper/2108.00131.pdf", "../paper/fncom-10-00094.pdf", "../paper/annurev-neuro-070918-050421.pdf", "../paper/1-s2.0-0959438894901015-main.pdf", "../paper/s41592-018-0109-9.pdf", "../paper/NIPS-2007-the-tradeoffs-of-large-scale-learning-Paper.pdf", "../paper/science.1123513.pdf", "../paper/1310.5438.pdf", "../paper/2004.08013.pdf", "../paper/1-s2.0-S1053811914009094-main.pdf", "../paper/PINN_RPK_2019_1.pdf", "../paper/1-s2.0-S1053811913005053-main.pdf", "../paper/fncom-09-00120.pdf", "../paper/applsci-12-10228-v2.pdf", "../paper/436.full.pdf", "../paper/1-s2.0-S0896627307006265-mainext.pdf", "../paper/2006.10782.pdf", "../paper/s41467-021-23103-1.pdf", "../paper/*Laplacian.pdf", "../paper/nn.3865.pdf", "../paper/friston-2013-life-as-we-know-it.pdf", "../paper/2111.05464.pdf", "../paper/KDD-96.final.frame.pdf", "../paper/2106.09685.pdf", "../paper/variational-intro.pdf", "../paper/1411.4555.pdf", "../paper/1404.7828.pdf", "../paper/1676_adversarial_score_matching_and.pdf", "../paper/2311.10869.pdf", "../paper/NIPS-2016-the-forget-me-not-process-Paper.pdf", "../paper/3646.full.pdf", "../paper/089976699300016827.pdf", "../paper/erhan10a.pdf", "../paper/srep22057.pdf", "../paper/2003.08934.pdf", "../paper/Variational Inference  A Review for Statisticians.pdf", "../paper/6028.full.pdf", "../paper/annurev-neuro-080317-061956.pdf", "../paper/1-s2.0-S1053811913003108-main.pdf", "../paper/science.298.5594.824.pdf", "../paper/2209.14988.pdf", "../paper/2022.03.17.484712v1.full.pdf", "../paper/PIIS089662730900899X.pdf", "../paper/Human Brain Mapping - 2021 - Gao - Nonlinear manifold learning in functional magnetic resonance imaging uncovers a.pdf", "../paper/1609.05158.pdf", "../paper/1808.03357.pdf", "../paper/duvenaud13.pdf", "../paper/s41467-019-08350-7.pdf", "../paper/1904.00687.pdf", "../paper/s43588-022-00390-2.pdf", "../paper/science.1238406.pdf", "../paper/1-s2.0-S095943880600122X-main.pdf", "../paper/salakhutdinov09a.pdf", "../paper/1-s2.0-S0031320313000678-main.pdf", "../paper/nrn3962.pdf", "../paper/1-s2.0-S0896627305003624-main.pdf", "../paper/1804.06893.pdf", "../paper/1608.08782.pdf", "../paper/1507.07680.pdf", "../paper/cshperspect-SYP-a005710.pdf", "../paper/1-s2.0-S089662731200092X-main.pdf", "../paper/1611.02247.pdf", "../paper/1912.02164.pdf", "../paper/1-s2.0-S095943881500183X-main.pdf", "../paper/nature24636.pdf", "../paper/nrn.2017.149.pdf", "../paper/nn.2831.pdf", "../paper/pnas.1319438111.pdf", "../paper/NIPS-2010-discriminative-clustering-by-regularized-information-maximization-Paper.pdf", "../paper/1602.04915.pdf", "../paper/0004057.pdf", "../paper/s41586-020-03171-x.pdf", "../paper/netn_a_00018.pdf", "../paper/Scalarized_multi-objective_reinforcement_learning_Novel_design_techniques.pdf", "../paper/s41583-022-00570-z.pdf", "../paper/2205.05198.pdf", "../paper/1703.08475.pdf", "../paper/Response_acquisition_by_humans_with_delayed_reinfo.pdf", "../paper/1990-27437-001.pdf", "../paper/1910.07467.pdf", "../paper/1502.05477.pdf", "../paper/wangf16.pdf", "../paper/1906.04688.pdf", "../paper/2022.12.07.519455v1.full.pdf", "../paper/1-s2.0-S0959438814000166-main.pdf", "../paper/NeurIPS-2018-neural-tangent-kernel-convergence-and-generalization-in-neural-networks-Paper.pdf", "../paper/PIIS2211124718317960.pdf", "../paper/1907.02649.pdf", "../paper/1509.05936.pdf", "../paper/pnas.2111821118.sapp.pdf", "../paper/PIIS0896627319307317.pdf", "../paper/2455_unbiased_contrastive_divergenc.pdf", "../paper/NeurIPS-2019-a-geometric-perspective-on-optimal-representations-for-reinforcement-learning-Paper.pdf", "../paper/1806.09077.pdf", "../paper/file (8).pdf", "../paper/s41586-022-04915-7.pdf", "../paper/1705.03122.pdf", "../paper/jov-8-8-11.pdf", "../paper/1907.06374.pdf", "../paper/1606.08165.pdf", "../paper/0912.3832.pdf", "../paper/science.1254126.pdf", "../paper/2106.13031.pdf", "../paper/1604.06057.pdf", "../paper/NeurIPS-2021-credit-assignment-through-broadcasting-a-global-error-vector-Supplemental.pdf", "../paper/7476.full.pdf", "../paper/1911.04252.pdf", "../paper/1-s2.0-S1364661313001277-main.pdf", "../paper/2102.12627.pdf", "../paper/ward-lupyan-2013-language-can-boost-otherwise-unseen-objects-into-visual-awareness.pdf", "../paper/ito-2001-cerebellar-long-term-depression-characterization-signal-transduction-and-functional-roles.pdf", "../paper/WelTeh2011a.pdf", "../paper/Gradient-based_learning_applied_to_document_recognition.pdf", "../paper/2310.08513.pdf", "../paper/1803.00101.pdf", "../paper/NeurIPS-2018-how-does-batch-normalization-help-optimization-Paper.pdf", "../paper/1811.01768.pdf", "../paper/BF00337259.pdf", "../paper/8360.full.pdf", "../paper/PIIS1364661312001957.pdf", "../paper/nature11129.pdf", "../paper/2006.10350.pdf", "../paper/fnsyn-02-00025.pdf", "../paper/1605.07262.pdf", "../paper/1-s2.0-S0896627308009434-main.pdf", "../paper/neco_a_00949.pdf", "../paper/1705.10412.pdf", "../paper/1-s2.0-S0896627311010051-main.pdf", "../paper/chen20j.pdf", "../paper/2307.06324.pdf", "../paper/21243-Article Text-25256-1-2-20220628.pdf", "../paper/huang18b.pdf", "../paper/2010.03409.pdf", "../paper/16-505.pdf", "../paper/annurev.physiol.64.092501.114547.pdf", "../paper/1902.03149.pdf", "../paper/fnint-07-00025.pdf", "../paper/vincent10a.pdf", "../paper/1-s2.0-S0896627318305439-main.pdf", "../paper/1703.03864.pdf", "../paper/PIIS089662731631039X.pdf", "../paper/jn.90833.2008.pdf", "../paper/aan3846_bittner_sm.pdf", "../paper/science.1211095.pdf", "../paper/189_deep_rewiring_training_very_sp.pdf", "../paper/2108.01210.pdf", "../paper/2206.00823.pdf", "../paper/nn.3496.pdf", "../paper/nihms-1010474.pdf", "../paper/joachims_98a.pdf", "../paper/1705.07224.pdf", "../paper/bcm.pdf", "../paper/nn.3643.pdf", "../paper/41467_2020_17236_MOESM1_ESM.pdf", "../paper/1651_implementing_inductive_bias_fo.pdf", "../paper/koutnik2014gecco.pdf", "../paper/41586_2013_BFnature12160_MOESM50_ESM.pdf", "../paper/sutskever13.pdf", "../paper/s42256-021-00376-1.pdf", "../paper/014_2008.pdf", "../paper/1502.03167.pdf", "../paper/2203.00555.pdf", "../paper/annurev.neuro.30.051606.094225.pdf", "../paper/nature19818.pdf", "../paper/nn1954.pdf", "../paper/Deep_reinforcement_learning_with_successor_features_for_navigation_across_similar_environments.pdf", "../paper/2002.07720.pdf", "../paper/A_deep_learning_framework_for_neuroscience_vFinal_RC.pdf", "../paper/jn.91050.2008.pdf", "../paper/1-s2.0-S0893608005801261-main.pdf", "../paper/1805.10842.pdf", "../paper/nature06910.pdf", "../paper/3305890.3306024.pdf", "../paper/nrn1379.pdf", "../paper/1502.02251.pdf"], "abstract": ["732\n\ncomplementary roles of basal ganglia and cerebellum in\nlearning and motor control\nkenji doya\n\nthe classical notion that the basal ganglia and the cerebellum\nare dedicated to motor control has been challenged by the\naccumulation of evidence revealing their involvement in non-\nmotor, cognitive functions. from a computational viewpoint, it\nhas been suggested that the cerebellum, the basal ganglia,\nand the cerebral cortex are specialized for different types of\nlearning: namely, supervised learning, reinforcement learning\nand unsupervised learning, respectively. this idea of learning-\noriented specialization is helpful in understanding the\ncomplementary roles of the basal ganglia and the cerebellum\nin motor control and cognitive functions.\n\naddresses\ninformation sciences division, atr international and crest, japan\nscience and technology corporation, 2-2-2 hikaridai, seika, soraku,\nkyoto 619-0288, japan; e-mail: doya@isd.atr.co.jp\n\ncurrent opinion in neurobiology 2000, 10:732\u2013739\n\n0959", "neuron\n\narticle\n\npathway interactions and synaptic plasticity in the\ndendritic tuft regions of ca1 pyramidal neurons\n\nhiroto takahashi1 and jeffrey c. magee1,*\n1howard hughes medical institute, janelia farm research campus, 19700 helix drive, ashburn, va 20147, usa\n*correspondence: mageej@janelia.hhmi.org\ndoi 10.1016/j.neuron.2009.03.007\n\nsummary\n\ninput comparison is thought\nto occur in many\nneuronal circuits, including the hippocampus, where\nfunctionally important\ninteractions between the\nschaffer collateral and perforant pathways have\nbeen hypothesized. we investigated this idea using\nmultisite, whole-cell recordings and ca2+ imaging\nand found that properly timed, repetitive stimulation\nof both pathways results in the generation of large\nplateau potentials in distal dendrites of ca1 pyra-\nmidal neurons. these dendritic plateau potentials\nproduce widespread ca2+ in\ufb02ux, large after-depolar-\nizations, burst \ufb01ring output, and long-term potentia-\ntion of perforant path synapses. plateau d", "on the binding problem in arti\ufb01cial neural networks\n\nklaus gre\ufb00\u2217\ngoogle research, brain team\ntucholskystra\u00dfe 2, 10116 berlin, germany\nsjoerd van steenkiste\nj\u00fcrgen schmidhuber\nistituto dalle molle di studi sull\u2019intelligenza arti\ufb01ciale (idsia)\nuniversit\u00e0 della svizzera italiana (usi)\nscuola universitaria professionale della svizzera italiana (supsi)\nvia la santa 1, 6962 viganello, switzerland\n\nklausg@google.com\n\nsjoerd@idsia.ch\n\njuergen@idsia.ch\n\nabstract\n\ncontemporary neural networks still fall short of human-level generalization, which extends\nfar beyond our direct experiences. in this paper, we argue that the underlying cause for\nthis shortcoming is their inability to dynamically and \ufb02exibly bind information that is\ndistributed throughout the network. this binding problem a\ufb00ects their capacity to acquire\na compositional understanding of the world in terms of symbol-like entities (like objects),\nwhich is crucial for generalizing in predictable and systematic ways. to address this issue", "policy gradient coagent networks\n\nphilip s. thomas\n\ndepartment of computer science\n\nuniversity of massachusetts amherst\n\namherst, ma 01002\n\npthomas@cs.umass.edu\n\nabstract\n\nwe present a novel class of actor-critic algorithms for actors consisting of sets\nof interacting modules. we present, analyze theoretically, and empirically eval-\nuate an update rule for each module, which requires only local information: the\nmodule\u2019s input, output, and the td error broadcast by a critic. such updates are\nnecessary when computation of compatible features becomes prohibitively dif\ufb01-\ncult and are also desirable to increase the biological plausibility of reinforcement\nlearning methods.\n\n1\n\nintroduction\n\nmethods for solving sequential decision problems with delayed reward, where the problems are for-\nmulated as markov decision processes (mdps), have been compared to the learning mechanisms of\nanimal brains [3, 4, 9, 10, 13, 20, 22]. these comparisons stem from similarities between activa-\ntion of dopamin", "partition functions from\n\nrao-blackwellized tempered sampling\n\ndavid e. carlson\u22171,2\npatrick stinson\u22172\nari pakman\u22171,2\nliam paninski1,2\n1 department of statistics\n2 grossman center for the statistics of mind\ncolumbia university, new york, ny, 10027\n\nabstract\n\npartition functions of probability distributions\nare important quantities for model evaluation and\ncomparisons. we present a new method to com-\npute partition functions of complex and multi-\nmodal distributions. such distributions are of-\nten sampled using simulated tempering, which\naugments the target space with an auxiliary in-\nverse temperature variable. our method exploits\nthe multinomial probability law of the inverse\ntemperatures, and provides estimates of the par-\ntition function in terms of a simple quotient of\nrao-blackwellized marginal inverse temperature\nprobability estimates, which are updated while\nsampling. we show that the method has interest-\ning connections with several alternative popular\nmethods, and offers some s", "volume 59, number 19\n\nphysical review letters\n\n9 november 1987\n\ngeneralization\n\nof back-propagation\n\nto recurrent neural networks\n\napplied physics laboratory,\n\nfernando j. pineda\njohns hopkins university, laurel, maryland 20707\n(received 10 june 1987)\n\nan adaptive neural network with asymmetric\n\nhopfield network with graded neurons\nhinton,\nto the master/slave\n\nand williams\n\nto modify adaptively\n\nconnections\nand uses a recurrent\n\nthis network is related to the\nof the 6 rule of rumelhart,\nthe synaptic weights. the new network bears a resemblance\n\nis introduced.\ngeneralization\n\nnetwork of lapedes and farber, but\n\nit is architecturally\n\nsimpler.\n\npacs numbers:\n\n87.30.6y\n\nin\n\nsystem,\n\nin which\n\nsequential\n\nalth. ough\n\nperforming\n\nsimulations\n\ninstructions\n\na discrete\n\nthe present\n\nor continuous\n\nthe traditional\n\n(neurodynamics)\n\ndoes not exist,\nthree salient\n\nthe neural network approach\n\nis a paradigm for com-\nparadigm of a finite-\nputation\nin a\nstate machine\ndiscrete state space is replace", "article\nclassi\ufb01cation of explainable arti\ufb01cial intelligence methods\nthrough their output formats\n\ngiulia vilone *,\u2020\n\nand luca longo \u2020\n\napplied intelligence research centre, technological university dublin, d08 x622 dublin, ireland;\nluca.longo@tudublin.ie\n* correspondence: giulia.vilone@tudublin.ie\n\u2020 these authors contributed equally to this work.\n\nabstract: machine and deep learning have proven their utility to generate data-driven models with\nhigh accuracy and precision. however, their non-linear, complex structures are often dif\ufb01cult to\ninterpret. consequently, many scholars have developed a plethora of methods to explain their\nfunctioning and the logic of their inferences. this systematic review aimed to organise these methods\ninto a hierarchical classi\ufb01cation system that builds upon and extends existing taxonomies by adding\na signi\ufb01cant dimension\u2014the output formats. the reviewed scienti\ufb01c papers were retrieved by\nconducting an initial search on google scholar with the keywords \u201cexp", "article\n\npulvinar-cortex interactions in vision and attention\n\nhighlights\nd neuronal properties and attentional modulation are similar in\n\npulvinar and v4\n\nauthors\n\nhuihui zhou, robert john schafer,\nrobert desimone\n\nd v4 leads pulvinar in gamma synchrony during attentive\n\nstimulus processing\n\ncorrespondence\nhh.zhou@siat.ac.cn\n\nd pulvinar deactivation reduces both sensory response and\n\nattentional effect in v4\n\nd following pulvinar deactivation, cortex appears to go to an\n\ninactive state\n\nin brief\nthe pulvinar is often proposed to\nmodulate cortical processing with\nattention. zhou et al. \ufb01nd that beyond any\nrole in attention, the pulvinar input to\ncortex seems necessary to maintain the\ncortex in an active state.\n\nzhou et al., 2016, neuron 89, 209\u2013220\njanuary 6, 2016 \u00aa2016 elsevier inc.\nhttp://dx.doi.org/10.1016/j.neuron.2015.11.034\n\n\f", "reviews\n\ndopamine, learning and\nmotivation\n\nroy a. wise\n\nthe hypothesis that dopamine is important for reward has been proposed in a number of\nforms, each of which has been challenged. normally, rewarding stimuli such as food, water,\nlateral hypothalamic brain stimulation and several drugs of abuse become ineffective as\nrewards in animals given performance-sparing doses of dopamine antagonists. dopamine\nrelease in the nucleus accumbens has been linked to the efficacy of these unconditioned\nrewards, but dopamine release in a broader range of structures is implicated in the\n\u2018stamping-in\u2019 of memory that attaches motivational importance to otherwise neutral\nenvironmental stimuli.\n\nthe  neurotransmitter  dopamine  \u2014  particularly \nnigrostriatal dopamine (box 1) \u2014 has long been identi-\nfied with motor function1. however, moderate doses of\nneuroleptic drugs (dopamine antagonists) attenuate the\nmotivation to act before they compromise the ability to\nact. such drugs do not immediately compromis", "interpreting neural computations by examining intrinsic and embedding dimensionality of\nneural activity\n\nmehrdad jazayeri1, srdjan ostojic2\n\nsummary\n\nthe ongoing exponential rise in recording capacity calls for new approaches for analysing and\ninterpreting neural data. effective dimensionality has emerged as an important property of neural\nactivity across populations of neurons, yet different studies rely on different definitions and\ninterpretations of this quantity. here we focus on intrinsic and embedding dimensionality, and discuss\nhow they might reveal computational principles from data. reviewing recent works, we propose that\nthe intrinsic dimensionality reflects information about the latent variables encoded in collective activity,\nwhile embedding dimensionality reveals the manner in which this information is processed. we\nconclude by highlighting the role of network models as an ideal substrate for testing more specifically\nvarious hypotheses on the computational principles refl", "r e v i e w s\n\nnormalization as a canonical neural \ncomputation\n\nmatteo carandini1 and david j.\u00a0heeger2\n\nabstract | there is increasing evidence that the brain relies on a set of canonical neural \ncomputations, repeating them across brain regions and modalities to apply similar \noperations to different problems. a promising candidate for such a computation is \nnormalization, in which the responses of neurons are divided by a common factor that \ntypically includes the summed activity of a pool of neurons. normalization was developed to \nexplain responses in the primary visual cortex and is now thought to operate\u00a0throughout the \nvisual system, and in many other sensory modalities and brain regions. normalization may \nunderlie\u00a0operations such as the representation of odours,\u00a0the modulatory effects of visual \nattention, the encoding of value and the integration of multisensory information. its \npresence in such a diversity of neural systems in multiple species, from invertebrates \nto\u00a0mamma", "a model-based approach to trial-by-trial p300 amplitude\n\ufb02uctuations\n\noriginal research article\npublished: 08 february 2013\ndoi: 10.3389/fnhum.2012.00359\n\nantonio kolossa1,tim fingscheidt 1*, karl wessel 2,3 and bruno kopp 2,4\n\n1 institute for communications technology, technische universit\u00e4t braunschweig, braunschweig, germany\n2 cognitive neurology, technische universit\u00e4t braunschweig, braunschweig, germany\n3 department of neurology, braunschweig hospital, braunschweig, germany\n4 department of neurology, hannover medical school, hannover, germany\n\nedited by:\nsven bestmann, university college\nlondon, uk\nreviewed by:\nfrancisco barcelo, university of illes\nbalears, spain\nsimon p. kelly, city college of new\nyork, usa\n*correspondence:\ntim fingscheidt, institute for\ncommunications technology,\ntechnische universit\u00e4t braunschweig,\nschleinitzstra\u00dfe 22, 38106\nbraunschweig, germany.\ne-mail: t.\ufb01ngscheidt@tu-bs.de\n\nit has long been recognized that the amplitude of the p300 component of event-relate", "the annals of statistics\n2009, vol. 37, no. 6b, 3779\u20133821\ndoi: 10.1214/09-aos692\n\u00a9 institute of mathematical statistics, 2009\n\nhigh-dimensional additive modeling\n\nby lukas meier, sara van de geer and peter b\u00fchlmann\n\neth z\u00fcrich\n\nwe propose a new sparsity-smoothness penalty for high-dimensional\ngeneralized additive models. the combination of sparsity and smoothness\nis crucial for mathematical theory as well as performance for \ufb01nite-sample\ndata. we present a computationally ef\ufb01cient algorithm, with provable numer-\nical convergence properties, for optimizing the penalized likelihood. further-\nmore, we provide oracle results which yield asymptotic optimality of our es-\ntimator for high dimensional but sparse additive models. finally, an adaptive\nversion of our sparsity-smoothness penalized approach yields large additional\nperformance gains.\n\n1. introduction. substantial progress has been achieved over the last years in\nestimating high-dimensional linear or generalized linear models where th", "j comput neurosci (2010) 29:107\u2013126\ndoi 10.1007/s10827-009-0179-x\n\na new look at state-space models for neural data\nliam paninski \u00b7 yashar ahmadian \u00b7\ndaniel gil ferreira \u00b7 shinsuke koyama \u00b7\nkamiar rahnama rad \u00b7 michael vidne \u00b7\njoshua vogelstein \u00b7 wei wu\n\nreceived: 22 december 2008 / revised: 6 july 2009 / accepted: 16 july 2009 / published online: 1 august 2009\n\u00a9 springer science + business media, llc 2009\n\nabstract state space methods have proven indispens-\nable in neural data analysis. however, common meth-\nods for performing inference in state-space models with\nnon-gaussian observations rely on certain approxima-\ntions which are not always accurate. here we review\ndirect optimization methods that avoid these approxi-\nmations, but that nonetheless retain the computational\nef\ufb01ciency of the approximate methods. we discuss a\nvariety of examples, applying these direct optimiza-\ntion techniques to problems in spike train smoothing,\nstimulus decoding, parameter estimation, and inference\nof", "7\n1\n0\n2\n\n \nr\np\na\n9\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n1\n8\n0\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nopening the black box of deep neural networks via information\n\nopening the black box of deep neural networks\n\nvia information\n\nravid schwartz-ziv\nedmond and lilly safra center for brain sciences\nthe hebrew university of jerusalem\njerusalem, 91904, israel\nnaftali tishby\u2217\nschool of engineering and computer science\nand edmond and lilly safra center for brain sciences\nthe hebrew university of jerusalem\njerusalem, 91904, israel\n\neditor: icri-ci\n\nravid.ziv@mail.huji.ac.il\n\ntishby@cs.huji.ac.il\n\nabstract\n\ndespite their great success, there is still no comprehensive theoretical understanding of learning\nwith deep neural networks (dnns) or their inner organization. previous work [tishby and za-\nslavsky (2015)] proposed to analyze dnns in the information plane; i.e., the plane of the mutual\ninformation values that each layer preserves on the input and output variables. they suggested that\nthe goal of the netw", "review\ninterneuron cell types \nare fit to function\n\nadam kepecs1 & gordon fishell2\n\ndoi:10.1038/nature12983\n\nunderstanding brain circuits begins with an appreciation of their component parts \u2014 the cells. although gabaergic interneu-\nrons are a minority population within the brain, they are crucial for the control of inhibition. determining the diversity of these \ninterneurons has been a central goal of neurobiologists, but this amazing cell type has so far defied a generalized classification \nsystem. interneuron complexity within the telencephalon could be simplified by viewing them as elaborations of a much more \nfinite group of developmentally specified cardinal classes that become further specialized as they mature. our perspective \nemphasizes that the ultimate goal is to dispense with classification criteria and directly define interneuron types by function.\n\ninterneurons, of all the cells within the forebrain, are the most diverse \n\nin terms of morphology, connectivity and physiol", "neuron\n\nviewpoint\n\ncortical preparatory activity: representation\nof movement or first cog in a dynamical machine?\n\nmark m. churchland,1,* john p. cunningham,1,3 matthew t. kaufman,2 stephen i. ryu,1,4 and krishna v. shenoy1,2,5\n1department of electrical engineering\n2neurosciences program\nstanford university, stanford, ca 94305, usa\n3department of engineering, university of cambridge, cambridge cb2 1pz, uk\n4department of neurosurgery, palo alto medical foundation, palo alto, ca 94301, usa\n5department of bioengineering, stanford university, stanford, ca 94705, usa\n*correspondence: church@stanford.edu\ndoi 10.1016/j.neuron.2010.09.015\n\nsummary\n\nthe motor cortices are active during both movement\nand movement preparation. a common assumption\nis that preparatory activity constitutes a subthreshold\nform of movement activity: a neuron active during\nrightward movements becomes modestly active\nduring preparation of a rightward movement. we\nasked whether this pattern of activity is, in fact,\nobser", "a framework for equilibrium equations \nauthor(s): gilbert strang \nsource: siam review, jun., 1988, vol. 30, no. 2 (jun., 1988), pp. 283-297  \npublished by: society for industrial and applied mathematics \n\nstable url: https://www.jstor.org/stable/2030801\n\njstor is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide \nrange of content in a trusted digital archive. we use information technology and tools to increase productivity and \nfacilitate new forms of scholarship. for more information about jstor, please contact support@jstor.org. \n \nyour use of the jstor archive indicates your acceptance of the terms & conditions of use, available at \nhttps://about.jstor.org/terms\n\nsociety for industrial and applied mathematics is collaborating with jstor to digitize, \npreserve and extend access to siam review\n\nthis content downloaded from \n\n(cid:0)216.165.95.167 on mon, 22 may 2023 17:15:00 +00:00(cid:0)\n(cid:0) \nall use subject to https://ab", "article\n\nhttps://doi.org/10.1038/s41467-020-17236-y\n\nopen\n\na solution to the learning dilemma for recurrent\nnetworks of spiking neurons\nguillaume bellec1,2, franz scherr\nrobert legenstein\n\n1, elias hajek1, darjan salaj\n\n1,2, anand subramoney\n\n1 & wolfgang maass\n\n1\u2709\n\n1,\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\nrecurrently connected networks of spiking neurons underlie the astounding information\nprocessing capabilities of the brain. yet in spite of extensive research, how they can learn\nthrough synaptic plasticity to carry out complex network computations remains unclear. we\nargue that two pieces of this puzzle were provided by experimental data from neuroscience.\na mathematical result tells us how these pieces need to be combined to enable biologically\nplausible online network learning through gradient descent, in particular deep reinforcement\nlearning. this learning method\u2013called e-prop\u2013approaches the performance of back-\npropagation through time (bptt), the best-known method for training recur", "functional  significance  of long-term\npotentiation for  sequence  learning and\nprediction\n\nl. f. abbott and  kenneth  i. blum\n\ncenter for complex  systems, brandeis university, waltham,\nmassachusetts 02254\n\npopulation  coding, where  neurons with  broad and  overlapping firing\nrate  tuning  curves  collectively  encode  information  about a stimulus,\nis  a  common feature  of  sensory  systems. we  use  decoding methods\nand  measured  properties  of  nmda-mediated  ltp  induction  to study\nthe  impact of long-term potentiation of synapses between the neurons\nof such a coding array. we find that  due to a temporal asymmetry in\nthe  induction  of  nmda-mediated  ltp, firing  patterns  in  a  neuronal\narray that initially  represent the current value of a sensory input will,\nafter  training, provide  an  experienced-based  prediction  of that  input\ninstead. we compute how this prediction  arises from and depends on\nthe  training  experience.  we  also  show  how  the  encoded  predictio", "article\n\ndoi:10.1038/nature11911\n\nfunctional organization of human\nsensorimotor cortex for speech articulation\n\nkristofer e. bouchard1,2, nima mesgarani1,2, keith johnson3 & edward f. chang1,2,4\n\nspeaking is one of the most complex actions that we perform, but nearly all of us learn to do it effortlessly. production of\nfluent speech requires the precise, coordinated movement of multiple articulators (for example, the lips, jaw, tongue and\nlarynx) over rapid time scales. here we used high-resolution, multi-electrode cortical recordings during the production\nof consonant-vowel syllables to determine the organization of speech sensorimotor cortex in humans. we found speech-\narticulator representations that are arranged somatotopically on ventral pre- and post-central gyri, and that partially\noverlap at individual electrodes. these representations were coordinated temporally as sequences during syllable\nproduction. spatial patterns of cortical activity showed an emergent, population-level ", "distributional reinforcement learning with quantile regression\n\nwill dabney\n\ndeepmind\n\nmark rowland\n\nuniversity of cambridge\u2217\n\nmarc g. bellemare\n\ngoogle brain\n\nr\u00b4emi munos\n\ndeepmind\n\n7\n1\n0\n2\n\n \nt\nc\no\n7\n2\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\nv\n4\n4\n0\n0\n1\n\n.\n\n0\n1\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nin reinforcement learning an agent interacts with the environ-\nment by taking actions and observing the next state and re-\nward. when sampled probabilistically, these state transitions,\nrewards, and actions can all induce randomness in the ob-\nserved long-term return. traditionally, reinforcement learn-\ning algorithms average over this randomness to estimate the\nvalue function. in this paper, we build on recent work ad-\nvocating a distributional approach to reinforcement learning\nin which the distribution over returns is modeled explicitly\ninstead of only estimating the mean. that is, we examine\nmethods of learning the value distribution instead of the value\nfunction. we give results that close a number of gap", "masked autoencoders are scalable vision learners\n\nkaiming he\u2217,\u2020 xinlei chen\u2217 saining xie yanghao li piotr doll\u00b4ar ross girshick\n\n\u2217equal technical contribution\n\n\u2020project lead\n\nfacebook ai research (fair)\n\n1\n2\n0\n2\n \nc\ne\nd\n9\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n3\nv\n7\n7\n3\n6\n0\n\n.\n\n1\n1\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nthis paper shows that masked autoencoders (mae) are\nscalable self-supervised learners for computer vision. our\nmae approach is simple: we mask random patches of the\ninput image and reconstruct the missing pixels. it is based\non two core designs. first, we develop an asymmetric\nencoder-decoder architecture, with an encoder that oper-\nates only on the visible subset of patches (without mask to-\nkens), along with a lightweight decoder that reconstructs\nthe original image from the latent representation and mask\ntokens. second, we \ufb01nd that masking a high proportion\nof the input image, e.g., 75%, yields a nontrivial and\nmeaningful self-supervisory task. coupling these two de-\nsigns enables us to ", "a r t i c l e s\n\ninhibitory suppression of heterogeneously tuned \nexcitation enhances spatial coding in ca1 place cells\nchristine grienberger1,2, aaron d milstein1,2, katie c bittner1, sandro romani1 & jeffrey c magee1\nplace cells in the ca1 region of the hippocampus express location-specific firing despite receiving a steady barrage of \nheterogeneously tuned excitatory inputs that should compromise output dynamic range and timing. we examined the role \nof synaptic inhibition in countering the deleterious effects of off-target excitation. intracellular recordings in behaving mice \ndemonstrate that bimodal excitation drives place cells, while unimodal excitation drives weaker or no spatial tuning in \ninterneurons. optogenetic hyperpolarization of interneurons had spatially uniform effects on place cell membrane potential \ndynamics, substantially reducing spatial selectivity. these data and a computational model suggest that spatially uniform \ninhibitory conductance enhances rate coding ", "letter\narithmetic and local circuitry underlying dopamine\nprediction errors\n\ndoi:10.1038/nature14855\n\nneir eshel1, michael bukwich1, vinod rao1, vivian hemmelder1, ju tian1 & naoshige uchida1\n\ndopamine neurons are thought to facilitate learning by comparing\nactual and expected reward1,2. despite two decades of investiga-\ntion, little is known about how this comparison is made. to deter-\nmine how dopamine neurons calculate prediction error, we\ncombined optogenetic manipulations with extracellular record-\nings in the ventral tegmental area while mice engaged in classical\nconditioning. here we demonstrate, by manipulating the temporal\nexpectation of reward, that dopamine neurons perform subtrac-\ntion, a computation that is ideal for reinforcement learning but\nrarely observed in the brain. furthermore, selectively exciting and\ninhibiting neighbouring gaba (c-aminobutyric acid) neurons in\nthe ventral tegmental area reveals that these neurons are a source of\nsubtraction: they inhibit dopamin", "continual learning of recurrent neural networks\nby locally aligning distributed representations\n\nalexander ororbia*, ankur mali, c. lee giles, fellow, ieee, and daniel kifer\n\n1\n\n9\n1\n0\n2\n\n \n\ng\nu\na\n1\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n4\nv\n1\n1\n4\n7\n0\n\n.\n\n0\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014temporal models based on recurrent neural net-\nworks have proven to be quite powerful in a wide variety of\napplications, including language modeling and speech processing.\nhowever, training these models often relies on back-propagation\nthrough time, which entails unfolding the network over many\ntime steps, making the process of conducting credit assignment\nconsiderably more challenging. furthermore, the nature of back-\npropagation itself does not permit the use of non-differentiable\nactivation functions and is inherently sequential, making paral-\nlelization of the underlying training process dif\ufb01cult.\n\nhere, we propose the parallel temporal neural coding net-\nwork (p-tncn), a biologically inspired model trained by t", "this analysis did not include medial and orbital frontal\nregions; neither of these regions showed content-\nspecific sustained activity.\n\n24. in the right hemisphere, the differences between the\nspatial extents of sustained activation for face and\nspatial working memory were 6.1 and 3.3 cm3 and\n0.31 and 0.23% signal change in the middle frontal\ncortex, and 1.8 and 1.8 cm3 and 0.39 and 0.25% in\nthe inferior frontal cortex (medians across subjects; p\n. 0.1 for all comparisons).\n\n25. others (2, 4, 20) have also directly contrasted object\nand spatial working memory and, as we did, they\nfound evidence for domain specificity in prefrontal\ncortex. however, their evidence indicated that do-\nmain specificity is primarily a hemispheric laterality\neffect, with left frontal regions specialized for object\nworking memory and right frontal regions specialized\nfor spatial working memory. in our previous studies\nof face (7, 14) and in this study of spatial working\nmemory, we found activations with simil", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/221615826\n\nparallel reinforcement learning with linear function approximation.\n\nconference paper \u00b7 may 2007\n\ndoi: 10.1145/1329125.1329179\u00a0\u00b7\u00a0source: dblp\n\ncitations\n12\n\n2 authors, including:\n\ndaniel kudenko\nforschungszentrum l3s\n\n202 publications\u00a0\u00a0\u00a03,431 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n646\n\nall content following this page was uploaded by daniel kudenko on 29 may 2014.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "9\n1\n0\n2\n\n \n\np\ne\ns\n6\n\n \n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n7\n6\n8\n1\n0\n\n.\n\n8\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nneuroscience-inspired online unsupervised learning algorithms\n\ncengiz pehlevan\u22171 and dmitri b. chklovskii\u20202,3\n\n1john a. paulson school of engineering and applied sciences, harvard university\n\n2flatiron institute, simons foundation\n\n3neuroscience institute, nyu medical center\n\nabstract\n\nalthough the currently popular deep learning networks achieve unprecedented performance\n\non some tasks, the human brain still has a monopoly on general intelligence. motivated by this\n\nand biological implausibility of deep learning networks, we developed a family of biologically\n\nplausible arti\ufb01cial neural networks (nns) for unsupervised learning. our approach is based on\n\noptimizing principled objective functions containing a term that matches the pairwise similarity\n\nof outputs to the similarity of inputs, hence the name - similarity-based. gradient-based online\n\noptimization of such similarity-based objective", "vol 454 | 21 august 2008 | doi:10.1038/nature07140\n\nletters\n\nspatio-temporal correlations and visual signalling in a\ncomplete neuronal population\njonathan w. pillow1, jonathon shlens2, liam paninski3, alexander sher4, alan m. litke4, e. j. chichilnisky2\n& eero p. simoncelli5\n\nthe\n\nindicate\n\nexistence of\n\nstatistical dependencies in the responses of sensory neurons gov-\nern both the amount of stimulus information conveyed and the\nmeans by which downstream neurons can extract it. although a\nvariety of measurements\nsuch\ndependencies1\u20133, their origin and importance for neural coding\nare poorly understood. here we analyse the functional significance\nof correlated firing in a complete population of macaque parasol\nretinal ganglion cells using a model of multi-neuron spike res-\nponses4,5. the model, with parameters fit directly to physiological\ndata, simultaneously captures both the stimulus dependence and\ndetailed spatio-temporal correlations in population responses,\nand provides two insight", "6\n1\n0\n2\n\n \n\ng\nu\na\n2\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n5\n1\n3\n6\n0\n\n.\n\n8\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nlfads - latent factor analysis via dynamical\n\nsystems\n\ndavid sussillo\u2217\ngoogle, inc.\n\nsussillo@google.com\n\nl.f. abbott\n\ndepartment of neuroscience\n\ncolumbia university\n\nlfabbott@columbia.edu\n\nrafal jozefowicz\u2020\n\ngoogle, inc.\n\nrafal@openai.com\n\nchethan pandarinath\u2021\ndepartment of neurosurgery\n\nstanford university\n\nchethan@gatech.edu\n\nabstract\n\nneuroscience is experiencing a data revolution in which many hundreds or thou-\nsands of neurons are recorded simultaneously. currently, there is little consensus\non how such data should be analyzed. here we introduce lfads (latent factor\nanalysis via dynamical systems), a method to infer latent dynamics from simul-\ntaneously recorded, single-trial, high-dimensional neural spiking data. lfads is\na sequential model based on a variational auto-encoder. by making a dynamical\nsystems hypothesis regarding the generation of the observed data, lfads reduces\nobserved spik", "the \n\njournal \n\nof  neuroscience, \n\nmarch \n\n15,  1996, \n\n16(6):2112-2126 \n\nrepresentation \ndynamics  of  the  head-direction \n\nof  spatial  orientation \n\nby  the  intrinsic \n\ncell  ensemble:  a  theory \n\nkechen  zhang \ndepartment \n\nof  cognitive  science,  university \n\nof  california  at  san  diego,  la  jolla,  california  92093-05 \n\n15 \n\nin \n\nfound \n\nin  the \n\nlandmarks \n\n(hd)  cells \n\ninformation \n\nrepresented \n\nlimbic  system \n\nthe  instantaneous \n\nrats  represent \nhead  direction \nin  the  horizontal  plane  regardless  of  the  location \nby  these  cells \nfor  inet-tially  based  updating \n\nthe  head-direction \nfreely  moving \nof  the  animal \nof  the  animal.  the  internal  direction \nuses  both  self-motion \nfor  calibration.  here,  a  model  of \nand  familiar  visual \nis  presented.  the  sta- \nthe  dynamics  of  the  hd  cell  ensemble \nin  the  network  and  a \nbility  of  a  localized  static  activity  profile \nnaturally  by  synaptic \ndynamic  shift  mechanism  are  ex", "9\n1\n0\n2\n\n \n\nb\ne\nf\n6\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n2\n2\n2\n0\n0\n\n.\n\n4\n0\n8\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2019\n\nmeta-learning update rules for unsuper-\nvised representation learning\n\nluke metz\ngoogle brain\nlmetz@google.com\n\nniru maheswaranathan\ngoogle brain\nnirum@google.com\n\nbrian cheung\nuniversity of california, berkeley\nbcheung@berkeley.edu\n\njascha sohl-dickstein\ngoogle brain\njaschasd@google.com\n\nabstract\n\na major goal of unsupervised learning is to discover data representations that are\nuseful for subsequent tasks, without access to supervised labels during training.\ntypically, this involves minimizing a surrogate objective, such as the negative log\nlikelihood of a generative model, with the hope that representations useful for\nsubsequent tasks will arise as a side effect. in this work, we propose instead to\ndirectly target later desired tasks by meta-learning an unsupervised learning rule\nwhich leads to representations useful for those tasks.\nspeci\ufb01cally, we ", "learning latent dynamics for planning from pixels\n\ndanijar hafner 1 2 timothy lillicrap 3 ian fischer 4 ruben villegas 1 5\n\ndavid ha 1 honglak lee 1 james davidson 1\n\nabstract\n\nplanning has been very successful for control\ntasks with known environment dynamics. to\nleverage planning in unknown environments,\nthe agent needs to learn the dynamics from\ninteractions with the world. however, learning\ndynamics models that are accurate enough for\nplanning has been a long-standing challenge,\nespecially in image-based domains. we propose\nthe deep planning network (planet), a purely\nmodel-based agent that learns the environment\ndynamics from images and chooses actions\nthrough fast online planning in latent space. to\nachieve high performance, the dynamics model\nmust accurately predict the rewards ahead for\nmultiple time steps. we approach this using a\nlatent dynamics model with both deterministic\nand stochastic transition components. moreover,\nwe propose a multi-step variational inference\nobjectiv", "hebbian and neuromodulatory mechanisms interact to\ntrigger associative memory formation\n\njoshua p. johansena,b,c,1,2, lorenzo diaz-mataixc,1, hiroki hamanakaa, takaaki ozawaa, edgar ycua, jenny koivumaaa,\nashwani kumara, mian houc, karl deisserothd,e, edward s. boydenf, and joseph e. ledouxc,g,2\n\nalaboratory for neural circuitry of memory, riken brain science institute, wako, saitama 351-0198, japan; bdepartment of life sciences, graduate school\nof arts and sciences, university of tokyo, tokyo 153-0198, japan; ccenter for neural science, new york university, new york, ny 10003; ddepartment of\nbioengineering, department of psychiatry and behavioral sciences, and ehoward hughes medical institute, stanford university, stanford, ca 94305; fmcgovern\ninstitute for brain research, department of brain and cognitive sciences, massachusetts institute of technology, cambridge, ma 02139; and gthe emotional\nbrain institute, nathan kline institute for psychiatric research, orangeburg, ny 10962\n\ncont", "9\n1\n0\n2\n\n \n\nv\no\nn\n5\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n7\n4\n2\n1\n\n.\n\n9\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nhidden strati\ufb01cation causes clinically meaningful\nfailures in machine learning for medical imaging\n\nluke oakden-rayner,\u2217gustavo carneiro\naustralian institute for machine learning\n\n{luke.oakden-rayner,gustavo.carneiro}\n\nuniversity of adelaide\n\nadelaide, sa 5000\n\n@adelaide.edu.au\n\njared dunnmon,\u2217 christopher r\u00e9\ndepartment of computer science\n\nstanford university\nstanford, ca 94305\n\n{jdunnmon,chrismre}\n\n@stanford.edu\n\nintroduction\n\n1\ndeep learning systems have shown remarkable promise in medical image analysis, often claiming\nperformance rivaling that of human experts [1]. however, performance results reported in the\nliterature may overstate the clinical utility and safety of these models. speci\ufb01cally, it is well known\nthat machine learning models often make mistakes that humans never would, despite having aggregate\nerror rates comparable to or better than those of human experts. an example of this \u201ci", "regularization of neural networks using dropconnect\n\nli wan\nmatthew zeiler\nsixin zhang\nyann lecun\nrob fergus\ndept. of computer science, courant institute of mathematical science, new york university\n\nwanli@cs.nyu.edu\nzeiler@cs.nyu.edu\nzsx@cs.nyu.edu\nyann@cs.nyu.edu\nfergus@cs.nyu.edu\n\nabstract\n\nwe introduce dropconnect, a generalization\nof dropout (hinton et al., 2012), for regular-\nizing large fully-connected layers within neu-\nral networks. when training with dropout,\na randomly selected subset of activations are\nset to zero within each layer. dropcon-\nnect instead sets a randomly selected sub-\nset of weights within the network to zero.\neach unit thus receives input from a ran-\ndom subset of units in the previous layer.\nwe derive a bound on the generalization per-\nformance of both dropout and dropcon-\nnect. we then evaluate dropconnect on a\nrange of datasets, comparing to dropout, and\nshow state-of-the-art results on several image\nrecognition benchmarks by aggregating mul-\ntiple dropc", "8\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n6\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n1\n9\n5\n8\n0\n\n.\n\n2\n0\n7\n1\n:\nv\ni\nx\nr\na\n\ntheshatteredgradientsproblem:ifresnetsaretheanswer,thenwhatisthequestion?davidbalduzzi1marcusfrean1lennoxleary1jplewis12kurtwan-duoma1brianmcwilliams3abstractalong-standingobstacletoprogressindeeplearningistheproblemofvanishingandex-plodinggradients.although,theproblemhaslargelybeenovercomeviacarefullyconstructedinitializationsandbatchnormalization,archi-tecturesincorporatingskip-connectionssuchashighwayandresnetsperformmuchbetterthanstandardfeedforwardarchitecturesdespitewell-choseninitializationandbatchnormalization.inthispaper,weidentifytheshatteredgradientsproblem.speci\ufb01cally,weshowthatthecor-relationbetweengradientsinstandardfeedfor-wardnetworksdecaysexponentiallywithdepthresultingingradientsthatresemblewhitenoisewhereas,incontrast,thegradientsinarchitec-tureswithskip-connectionsarefarmoreresis-tanttoshattering,decayingsublinearly.detailedempiricalevidenceispresentedinsupportoftheanalysis,onbothf", "1\n2\n0\n2\n\n \n\nb\ne\nf\n1\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n5\n1\n8\n5\n0\n\n.\n\n2\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nrepresentation matters: o\ufb04ine pretraining\n\nfor sequential decision making\n\nmengjiao yang\n\no\ufb01r nachum\n\nsherryy@google.com\n\nofirnachum@google.com\n\ngoogle research\n\ngoogle research\n\nabstract\n\nthe recent success of supervised learning methods on ever larger o\ufb04ine datasets has spurred interest in\nthe reinforcement learning (rl) \ufb01eld to investigate whether the same paradigms can be translated to rl\nalgorithms. this research area, known as o\ufb04ine rl, has largely focused on o\ufb04ine policy optimization,\naiming to \ufb01nd a return-maximizing policy exclusively from o\ufb04ine data. in this paper, we consider a\nslightly di\ufb00erent approach to incorporating o\ufb04ine data into sequential decision-making. we aim to\nanswer the question, what unsupervised objectives applied to o\ufb04ine datasets are able to learn state\nrepresentations which elevate performance on downstream tasks, whether those downstream tasks be\nonline rl, imitation", " \n\n\uf020 \n\ninternational journal of machine learning and computing, vol. 5, no. 2, april 2015\n\n \n\ncontrol policy with autocorrelated noise in \n\nreinforcement learning for robotics \n\npawe\u0142 wawrzy\u0144ski \n\nabstract\u2014direct  application  of  reinforcement  learning  in \nrobotics  rises  the  issue  of  discontinuity  of  control  signal. \nconsecutive  actions  are  selected  independently  on  random, \nwhich often makes them excessively far from one another. such \ncontrol is hardly ever appropriate in robots, it may even lead to \ntheir destruction. this paper considers a control policy in which \nconsecutive  actions  are  modified  by  autocorrelated  noise.  that \npolicy  generally  solves  the  aforementioned  problems  and  it  is \nreadily  applicable  in  robots.  in  the  experimental  study  it  is \napplied  to  three  robotic  learning  control  tasks:  cart-pole \nswingup, half-cheetah, and a walking humanoid. \n \n\nindex  terms\u2014machine  learning,  reinforcement  learning, \n\nactorcritics, ro", "further\nannual\nreviews\nclick here for quick links to \nannual reviews content online, \nincluding:\n\u2022 other articles in this volume\n\u2022 top cited articles\n\u2022 top downloaded articles\n\u2022 our comprehensive search\n\nhabits, rituals, and the\nevaluative brain\nann m. graybiel\ndepartment of brain and cognitive science and the mcgovern institute for brain research,\nmassachusetts institute of technology, cambridge, massachusetts 02139;\nemail: graybiel@mit.edu\n\nannu. rev. neurosci. 2008. 31:359\u201387\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev.neuro.29.051605.112851\ncopyright c(cid:2) 2008 by annual reviews.\nall rights reserved\n\n0147-006x/08/0721-0359$20.00\n\nkey words\nstriatum, reinforcement learning, stereotypy, procedural learning,\naddiction, automatization, obsessive-compulsive disorder\n\nabstract\nscientists in many different \ufb01elds have been attracted to the study of\nhabits because of the power habits have over behavior and because they\ninvo", "r e v i e w s\n\npyramidal neurons: dendritic structure \nand synaptic integration\n\nnelson spruston\nabstract | pyramidal neurons are characterized by their distinct apical and basal dendritic \ntrees and the pyramidal shape of their soma. they are found in several regions of the cns and, \nalthough the reasons for their abundance remain unclear, functional studies \u2014 especially of \nca1 hippocampal and layer v neocortical pyramidal neurons \u2014 have offered insights into the \nfunctions of their unique cellular architecture. pyramidal neurons are not all identical, but \nsome shared functional principles can be identified. in particular, the existence of dendritic \ndomains with distinct synaptic inputs, excitability, modulation and plasticity appears to be a \ncommon feature that allows synapses throughout the dendritic tree to contribute to action-\npotential generation. these properties support a variety of coincidence-detection \nmechanisms, which are likely to be crucial for synaptic integration ", "9\n1\n0\n2\n\n \nl\nu\nj\n \n\n9\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n0\n6\n3\n0\n\n.\n\n1\n1\n8\n1\n:\nv\ni\nx\nr\na\n\njournal of machine learning research 20 (2019) 1-49\n\nsubmitted 11/18; published 7/19\n\nmeasuring the e\ufb00ects of data parallelism\n\non neural network training\n\nchristopher j. shallue\u2217\njaehoon lee\u2217 \u2020\njoseph antognini\u2020\njascha sohl-dickstein\n\nroy frostig\n\ngeorge e. dahl\n\ngoogle brain\n1600 amphiteatre parkway\nmountain view, ca, 94043, usa\n\neditor: rob fergus\n\nshallue@google.com\n\njaehlee@google.com\n\njoe.antognini@gmail.com\n\njaschasd@google.com\n\nfrostig@google.com\n\ngdahl@google.com\n\nabstract\n\nrecent hardware developments have dramatically increased the scale of data parallelism\navailable for neural network training. among the simplest ways to harness next-generation\nhardware is to increase the batch size in standard mini-batch neural network training al-\ngorithms. in this work, we aim to experimentally characterize the e\ufb00ects of increasing the\nbatch size on training time, as measured by the number of steps ne", "on the power of over-parametrization in\n\nneural networks with quadratic activation\n\nsimon s. du 1 jason d. lee 2\n\n8\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n4\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n0\n2\n1\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe provide new theoretical insights on why over-\nparametrization is effective in learning neural\nnetworks. for a k hidden node shallow net-\nwork with quadratic activation and n training\n\ndata points, we show as long as k \u2265 \u221a2n, over-\n\nparametrization enables local search algorithms\nto \ufb01nd a globally optimal solution for general\nsmooth and convex loss functions. further, de-\nspite that the number of parameters may exceed\nthe sample size, using theory of rademacher\ncomplexity, we show with weight decay, the so-\nlution also generalizes well if the data is sampled\nfrom a regular distribution such as gaussian. to\n\nprove when k \u2265 \u221a2n, the loss function has be-\n\nnign landscape properties, we adopt an idea from\nsmoothed analysis, which may have other appli-\ncations in studying loss su", "generalized prioritized sweeping \n\ndavid andre  nir friedman  ronald parr \n\ncomputer science division, 387 soda hall \nuniversity of california, berkeley, ca 94720 \n\n{dandre,nir,parr}@cs.berkeley.edu \n\nabstract \n\nprioritized sweeping  is  a  model-based  reinforcement learning  method \nthat  attempts  to  focus  an  agent's  limited  computational  resources  to \nachieve a good estimate of the value of environment states.  to choose ef \nfectively where to spend a costly planning step, classic prioritized sweep \ning  uses  a simple  heuristic  to  focus  computation on  the  states  that are \nlikely to have the largest errors.  in this paper, we  introduce generalized \nprioritized sweeping, a principled method for generating such estimates \nin a representation-specific manner.  this allows us to extend prioritized \nsweeping beyond an explicit, state-based representation to deal with com \npact representations that are necessary for dealing with large state spaces. \nwe  apply  this  method", "i an update to this article is included at the end\n\na sensorimotor circuit in mouse cortex for visual\nflow predictions\n\narticle\n\nhighlights\nd mouse a24b/m2 sends a dense topographically organized\n\ninput to v1\n\nd motor-related signals from a24b/m2 drive motor and\n\nmismatch signals in v1\n\nd training to navigate a left-right inverted world reverses a24b/\n\nm2 visuomotor coding\n\nd stimulation of a24b/m2 axons in v1 in navigating mice elicits\n\nturning behavior\n\nauthors\n\nmarcus leinweber, daniel r. ward,\njan m. sobczak, alexander attinger,\ngeorg b. keller\n\ncorrespondence\ngeorg.keller@fmi.ch\n\nin brief\ntop-down input to visual cortex from\nprefrontal areas is involved in attentional\nand contextual modulation of sensory\nresponses. leinweber et al. argue that, in\nthe mouse, top-down input to v1 from\na24b/m2 carries a prediction of visual\n\ufb02ow given movement.\n\nleinweber et al., 2017, neuron 95, 1420\u20131432\nseptember 13, 2017 \u00aa 2017 elsevier inc.\nhttp://dx.doi.org/10.1016/j.neuron.2017.08.036\n\n\f", "7\n1\n0\n2\n\n \nr\np\na\n4\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n2\n3\n2\n1\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\ndeep information propagation\n\nsamuel s. schoenholz\u2217\ngoogle brain\n\njustin gilmer\u2217\ngoogle brain\n\nsurya ganguli\nstanford university\n\njascha sohl-dickstein\ngoogle brain\n\nabstract\n\nwe study the behavior of untrained neural networks whose weights and biases are\nrandomly distributed using mean \ufb01eld theory. we show the existence of depth\nscales that naturally limit the maximum depth of signal propagation through these\nrandom networks. our main practical result is to show that random networks may\nbe trained precisely when information can travel through them. thus, the depth\nscales that we identify provide bounds on how deep a network may be trained\nfor a speci\ufb01c choice of hyperparameters. as a corollary to this, we argue that in\nnetworks at the edge of chaos, one of these depth scales diverges. thus arbitrarily\ndeep networks may be trained only suf\ufb01ciently clos", "subject areas:\nplasticity\nsynaptic transmission\nreceptors\nsensory systems\n\nreceived\n10 april 2012\n\naccepted\n9 may 2012\n\npublished\n23 may 2012\n\ncorrespondence and\nrequests for materials\nshould be addressed to\nm.t. (mariomtv@\nhotmail.com) or g.k.\n(kohr@mpimf-\nheidelberg.mpg.de)\n\nnoradrenergic \u2018tone\u2019 determines\ndichotomous control of cortical\nspike-timing-dependent plasticity\n\nhumberto salgado2, georg ko\u00a8hr1 & mario trevin\u02dco1\n\n1department of molecular neurobiology, max planck institute for medical research, heidelberg, germany, 2departamento de\nneurociencias, centro de investigaciones regionales \u2018\u2018dr. hideyo noguchi\u2019\u2019, universidad auto\u00b4noma de yucata\u00b4n, yucata\u00b4n,\nme\u00b4xico.\n\nnorepinephrine (ne) is widely distributed throughout the brain. it modulates intrinsic currents, as well as\namplitude and frequency of synaptic transmission affecting the \u2018signal-to-noise ratio\u2019 of sensory responses.\nin the visual cortex, a1- and b-adrenergic receptors (ar) gate opposing effects on long-term plasticity ", "big neural networks waste capacity\n\nyann n. dauphin & yoshua bengio\n\nd\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle\n\nuniversit\u00b4e de montr\u00b4eal, montr\u00b4eal, qc, canada\n\n3\n1\n0\n2\n\n \nr\na\n\n \n\nm\n4\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n3\n8\n5\n3\n\n.\n\n1\n0\n3\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nthis article exposes the failure of some big neural networks to leverage added\ncapacity to reduce under\ufb01tting. past research suggest diminishing returns when\nincreasing the size of neural networks. our experiments on imagenet lsvrc-\n2010 show that this may be due to the fact there are highly diminishing returns for\ncapacity in terms of training error, leading to under\ufb01tting. this suggests that the\noptimization method - \ufb01rst order gradient descent - fails at this regime. directly\nattacking this problem, either through the optimization method or the choices of\nparametrization, may allow to improve the generalization error on large datasets,\nfor which a large capacity is required.\n\n1\n\nintroduction\n\ndeep learning and n", "a kernelized stein discrepancy for goodness-of-\ufb01t tests\n\nqiang liu\ncomputer science, dartmouth college, nh, 03755\njason d. lee\nmichael jordan\ndepartment of electrical engineering and computer science university of california, berkeley, ca 94709\n\njasondlee88@eecs.berkeley.edu\njordan@cs.berkeley.edu\n\nqliu@cs.dartmouth.edu\n\n6\n1\n0\n2\n\n \nl\nu\nj\n \n\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n3\n5\n2\n3\n0\n\n.\n\n2\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe derive a new discrepancy statistic for mea-\nsuring differences between two probability distri-\nbutions based on combining stein\u2019s identity with\nthe reproducing kernel hilbert space theory. we\napply our result to test how well a probabilis-\ntic model \ufb01ts a set of observations, and derive\na new class of powerful goodness-of-\ufb01t tests that\nare widely applicable for complex and high di-\nmensional distributions, even for those with com-\nputationally intractable normalization constants.\nboth theoretical and empirical properties of our\nmethods are studied thoroughly.\n\n1. intr", "transcriptional architecture of synaptic\ncommunication delineates gabaergic neuron\nidentity\n\narticle\n\ngraphical abstract\n\nauthors\nanirban paul, megan crow,\nricardo raudales, miao he, jesse gillis,\nz. josh huang\n\ncorrespondence\nhuangj@cshl.edu\n\nin brief\ngabaergic neuron types are\ndistinguished by a transcriptional\narchitecture that encodes their synaptic\ncommunication patterns.\n\nhighlights\nd single-cell transcriptome analysis of phenotype\n\ncharacterized gabaergic neurons\n\nd computation screen identi\ufb01es gene families that distinguish\n\ngaba subpopulations\n\nd 6 gene categories shape physiological input-output\n\nconnectivity of gaba neurons\n\nd transcription pro\ufb01les of synaptic communication\n\nencapsulate neuronal identity\n\npaul et al., 2017, cell 171, 522\u2013539\noctober 19, 2017 \u00aa 2017 elsevier inc.\nhttp://dx.doi.org/10.1016/j.cell.2017.08.032\n\n\f", "letter\nengineering a memory with ltd and ltp\n\nsadegh nabavi1*, rocky fox1*, christophe d. proulx1, john y. lin2, roger y. tsien2,3 & roberto malinow1\n\ndoi:10.1038/nature13294\n\nit has been proposed that memories are encoded by modification of\nsynaptic strengths through cellular mechanisms such as long-term\npotentiation (ltp) and long-term depression (ltd)1. however, the\ncausal link between these synaptic processes and memory has been\ndifficult to demonstrate2. here we show that fear conditioning3\u20138, a\ntype of associative memory, can be inactivated and reactivated by\nltd and ltp, respectively. we began by conditioning an animal to\nassociate a foot shock with optogenetic stimulation of auditory inputs\ntargeting the amygdala, a brain region known to be essential for fear\nconditioning3\u20138. subsequent optogenetic delivery of ltd condition-\ning to the auditory input inactivates memory of the shock. then sub-\nsequent optogenetic delivery of ltp conditioning to the auditory input\nreactivates mem", "article\n\ndoi:10.1038/nature10776\n\nconditional modulation of spike-timing-\ndependent plasticity forolfactory learning\n\nstijn cassenaer1,2 & gilles laurent1,3\n\nmushroom bodies are a well-known site for associative learning in insects. yet the precise mechanisms that underlie\nplasticity there and ensure their specificity remain elusive. in locusts, the synapses between the intrinsic mushroom\nbody neurons and their postsynaptic targets obey a hebbian spike-timing-dependent plasticity (stdp) rule. although\nthis property homeostatically regulates the timing of mushroom body output, its potential role in associative learning is\nunknown. here we show in vivo that pre\u2013post pairing causing stdp can, when followed by the local delivery of a\nreinforcement-mediating neuromodulator, specify the synapses that will undergo an associative change. at these\nsynapses, and there only, the change is a transformation of the stdp rule itself. these results illustrate the multiple\nactions of stdp, including a ", "olfactory landmarks and path integration converge\nto form a cognitive spatial map\n\narticle\n\nhighlights\nd localized odor cues can serve as landmarks to guide virtual\n\nnavigation in the dark\n\nd evolution of the ca1 spatial map re\ufb02ects iterative recognition\n\nof odor landmarks\n\nd path integration imposes spatial meaning on odor cues to\n\nestablish them as landmarks\n\nd a model reveals how odors and path integration interact to\n\nextend spatial maps\n\nauthors\n\nwalter fischler-ruiz, david g. clark,\nnarendra r. joshi, ..., mark schnitzer,\nl.f. abbott, richard axel\n\ncorrespondence\nlfa2103@columbia.edu (l.f.a.),\nra27@columbia.edu (r.a.)\n\nin brief\nthe recognition of a spatial landmark by\nits sensory features poses a problem for\nneural circuits. fischler-ruiz, et al. show\nhow this problem is solved when mice use\nodor cues to navigate in the dark. in the\nhippocampus, path integration imposes\nspatial meaning on odor cues, thereby\ncreating new landmarks.\n\nfischler-ruiz et al., 2021, neuron 109, 4036\u2013404", "1\n2\n0\n2\n\n \n\nb\ne\nf\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n8\n8\n2\n3\n0\n\n.\n\n1\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nhow to train your energy-based models\n\nyang song\nstanford university\ndiederik p. kingma\ngoogle research\n\nyangsong@cs.stanford.edu\n\ndpkingma@google.com\n\nabstract\n\nenergy-based models (ebms), also known as non-normalized probabilistic models, specify\nprobability density or mass functions up to an unknown normalizing constant. unlike\nmost other probabilistic models, ebms do not place a restriction on the tractability of\nthe normalizing constant, thus are more \ufb02exible to parameterize and can model a more\nexpressive family of probability distributions. however, the unknown normalizing constant\nof ebms makes training particularly di\ufb03cult. our goal is to provide a friendly introduction\nto modern approaches for ebm training. we start by explaining maximum likelihood\ntraining with markov chain monte carlo (mcmc), and proceed to elaborate on mcmc-free\napproaches, including score matching (sm) and noise const", "jmlr: workshop and conference proceedings vol 35:1\u201315, 2014\n\nlearning sparsely used overcomplete dictionaries\n\nalekh agarwal\nmicrosoft research, new york ny usa\nanimashree anandkumar\ndept of eecs, uc irvine, irvine, ca usa\nprateek jain\nmicrosoft research, bangalore, india\npraneeth netrapalli\ndept of ece, ut austin, austin, tx usa\n\nrashish tandon\ndept of cs, ut austin, austin, tx usa\n\nalekha@microsoft.com\n\na.anandkumar@uci.edu\n\nprajain@microsoft.com\n\npraneethn@utexas.edu\n\nrashish@cs.utexas.edu\n\nabstract\n\nwe consider the problem of learning sparsely used overcomplete dictionaries, where each observa-\ntion is a sparse combination of elements from an unknown overcomplete dictionary. we establish\nexact recovery when the dictionary elements are mutually incoherent. our method consists of a\nclustering-based initialization step, which provides an approximate estimate of the true dictionary\nwith guaranteed accuracy. this estimate is then re\ufb01ned via an iterative algorithm with the follow-\ning al", "deconvolutional networks\n\nmatthew d. zeiler, dilip krishnan, graham w. taylor and rob fergus\ndept. of computer science, courant institute, new york university\n\n{zeiler,dilip,gwtaylor,fergus}@cs.nyu.edu\n\nabstract\n\nbuilding robust low and mid-level image representa-\ntions, beyond edge primitives, is a long-standing goal in\nvision. many existing feature detectors spatially pool edge\ninformation which destroys cues such as edge intersections,\nparallelism and symmetry. we present a learning frame-\nwork where features that capture these mid-level cues spon-\ntaneously emerge from image data. our approach is based\non the convolutional decomposition of images under a spar-\nsity constraint and is totally unsupervised. by building a\nhierarchy of such decompositions we can learn rich feature\nsets that are a robust image representation for both the anal-\nysis and synthesis of images.\n\n1. introduction\n\nin this paper we propose deconvolutional networks, a\nframework that permits the unsupervised const", "3\n2\n0\n2\n\n \nl\nu\nj\n \n\n3\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n8\n1\n2\n9\n0\n\n.\n\n7\n0\n3\n2\n:\nv\ni\nx\nr\na\n\njournal of latex class files, vol. 14, no. 8, july 2023\n\n1\n\na comprehensive survey of forgetting in\ndeep learning beyond continual learning\n\nzhenyi wang, enneng yang, li shen, heng huang\n\nabstract\u2014forgetting refers to the loss or deterioration of previously acquired information or knowledge. while the existing surveys on\nforgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research\ndomains within deep learning. forgetting manifests in research fields such as generative models due to generator shifts, and federated\nlearning due to heterogeneous data distributions across clients. addressing forgetting encompasses several challenges, including\nbalancing the retention of old task knowledge with fast learning of new tasks, managing task interference with conflicting goals, and\npreventing privacy leakage, etc. moreover, most existing surve", "1\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n5\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n4\n5\n5\n4\n0\n\n.\n\n6\n0\n1\n2\n:\nv\ni\nx\nr\na\n\na survey of transformers\n\ntianyang lin, yuxin wang, xiangyang liu, and xipeng qiu\u2217, school of computer\nscience, fudan university, china and shanghai key laboratory of intelligent information processing, fudan\nuniversity, china\ntransformers have achieved great success in many artificial intelligence fields, such as natural language\nprocessing, computer vision, and audio processing. therefore, it is natural to attract lots of interest from\nacademic and industry researchers. up to the present, a great variety of transformer variants (a.k.a. x-formers)\nhave been proposed, however, a systematic and comprehensive literature review on these transformer variants\nis still missing. in this survey, we provide a comprehensive review of various x-formers. we first briefly\nintroduce the vanilla transformer and then propose a new taxonomy of x-formers. next, we introduce the\nvarious x-formers from three perspecti", "mach learn (2011) 84:137\u2013169\ndoi 10.1007/s10994-011-5235-x\n\nreinforcement learning in feedback control\nchallenges and benchmarks from technical process control\n\nroland hafner \u00b7 martin riedmiller\n\nreceived: 26 february 2010 / revised: 3 january 2011 / accepted: 8 january 2011 /\npublished online: 27 february 2011\n\u00a9 the author(s) 2011\n\nabstract technical process control is a highly interesting area of application serving a high\npractical impact. since classical controller design is, in general, a demanding job, this area\nconstitutes a highly attractive domain for the application of learning approaches\u2014in par-\nticular, reinforcement learning (rl) methods. rl provides concepts for learning controllers\nthat, by cleverly exploiting information from interactions with the process, can acquire high-\nquality control behaviour from scratch.\n\nthis article focuses on the presentation of four typical benchmark problems whilst high-\nlighting important and challenging aspects of technical process contr", "putting an end to end-to-end:\n\ngradient-isolated learning of representations\n\nsindy l\u00f6we\u2217\n\npeter o\u2019connor\n\nbastiaan s. veeling\u2217\n\namlab\n\nuniversity of amsterdam\n\nloewe.sindy@gmail.com, basveeling@gmail.com\n\nabstract\n\nwe propose a novel deep learning method for local self-supervised representation\nlearning that does not require labels nor end-to-end backpropagation but exploits\nthe natural order in data instead. inspired by the observation that biological neural\nnetworks appear to learn without backpropagating a global error signal, we split\na deep neural network into a stack of gradient-isolated modules. each module\nis trained to maximally preserve the information of its inputs using the infonce\nbound from oord et al. [2018]. despite this greedy training, we demonstrate that\neach module improves upon the output of its predecessor, and that the representa-\ntions created by the top module yield highly competitive results on downstream\nclassi\ufb01cation tasks in the audio and visual domain. th", "6\n1\n0\n2\n\n \nr\na\n\n \n\nm\n7\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n3\n8\n2\n0\n\n.\n\n2\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nbinarized neural networks: training neural networks with weights and\n\nactivations constrained to +1 or \u22121\n\nmatthieu courbariaux*1\nitay hubara*2\ndaniel soudry3\nran el-yaniv2\nyoshua bengio1,4\n1universit\u00b4e de montr\u00b4eal\n2technion - israel institute of technology\n3columbia university\n4cifar senior fellow\n*indicates equal contribution. ordering determined by coin \ufb02ip.\n\nmatthieu.courbariaux@gmail.com\nitayhubara@gmail.com\ndaniel.soudry@gmail.com\nrani@cs.technion.ac.il\nyoshua.umontreal@gmail.com\n\nabstract\n\nwe introduce a method to train binarized neu-\nral networks (bnns) - neural networks with bi-\nnary weights and activations at run-time. at\ntraining-time the binary weights and activations\nare used for computing the parameters gradi-\nents. during the forward pass, bnns drastically\nreduce memory size and accesses, and replace\nmost arithmetic operations with bit-wise opera-\ntions, which is expected to substan", "the  journal \n\nof  neuroscience, \n\nmarch \n\n1,  1996, \n\n76(5):1936-1947 \n\na  framework \npredictive  hebbian  learning \n\nfor  mesencephalic \n\ndopamine  systems  based  on \n\npeter  dayan,* \n\np.  read  montague,\u2019 \nl division  of  neuroscience, \ncognitive  science,  cambridge,  massachusetts \nfor  biological  studies,  la  jolla,  california  92037,  and  4the  department \nla  jolla,  california  92093 \n\nbaylor  college  of  medicine,  houston, \n\nand  terrence \n\nj.  sejnowskw \n\n02139,  3the  howard  hughes  medical \n\ninstitute  and  the  salk  institute \n\ntexas  77030,  *cbcl,  department \n\nof  brain  and \n\nof  biology,  university  of  california  at  san  diego, \n\ndopamine \n\na  theoretical \n\nthat  represents \nwe  show \n\nwe  develop \ncephalic \nsignal \nparticular, \nmake  predictions \ntuations \nsystems \nerrors in  these  predictions \nsubcottical \ncould  be  constructed \n\nabout \nin  the  activity \nabove  and  below \n\nframework \n\nthat  shows \n\nhow  mesen- \n\nsystems  could  distribute \n\ninforma", "supplementary materials \n\n \n \n\n \n \n \n\ncortical phenomenon \n\n \n\nstimulus onset quenches neural variability: a widespread  \n\nchurchland mm**, yu bm**, cunningham jp, sugrue lp, cohen mr, corrado gs, newsome \n\nwt, clark am, hosseini p, scott bb, bradley dc, smith ma, kohn a, movshon ja, \n\narmstrong km, moore t, chang sw, snyder lh, lisberger sg, priebe nj, finn im, ferster d, \n\nryu si, santhanam g, sahani m, and shenoy kv \n\n \n\n \n\n \n\nnature neuroscience: doi:10.1038/nn.2501\f", "4\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n1\n6\n6\n2\n\n.\n\n6\n0\n4\n1\n:\nv\ni\nx\nr\na\n\ngenerative adversarial nets\n\nian j. goodfellow, jean pouget-abadie\u2217, mehdi mirza, bing xu, david warde-farley,\n\nsherjil ozair\u2020, aaron courville, yoshua bengio\u2021\n\nd\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle\n\nuniversit\u00b4e de montr\u00b4eal\nmontr\u00b4eal, qc h3c 3j7\n\nabstract\n\nwe propose a new framework for estimating generative models via an adversar-\nial process, in which we simultaneously train two models: a generative model g\nthat captures the data distribution, and a discriminative model d that estimates\nthe probability that a sample came from the training data rather than g. the train-\ning procedure for g is to maximize the probability of d making a mistake. this\nframework corresponds to a minimax two-player game. in the space of arbitrary\nfunctions g and d, a unique solution exists, with g recovering the training data\ndistribution and d equal to 1\n2 everywhere. in the case where g and d ", "context encoders: feature learning by inpainting\n\ndeepak pathak\n\nphilipp kr\u00a8ahenb\u00a8uhl\n\njeff donahue\n\ntrevor darrell\n\nalexei a. efros\n\nuniversity of california, berkeley\n\n{pathak,philkr,jdonahue,trevor,efros}@cs.berkeley.edu\n\n6\n1\n0\n2\n\n \n\nv\no\nn\n1\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n9\n7\n3\n7\n0\n\n.\n\n4\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe present an unsupervised visual feature learning algo-\nrithm driven by context-based pixel prediction. by analogy\nwith auto-encoders, we propose context encoders \u2013 a con-\nvolutional neural network trained to generate the contents\nof an arbitrary image region conditioned on its surround-\nings.\nin order to succeed at this task, context encoders\nneed to both understand the content of the entire image,\nas well as produce a plausible hypothesis for the missing\npart(s). when training context encoders, we have experi-\nmented with both a standard pixel-wise reconstruction loss,\nas well as a reconstruction plus an adversarial loss. the\nlatter produces much sharper results be", "neural network dynamics\n\nfor model-based deep reinforcement learning\n\nwith model-free fine-tuning\n\nanusha nagabandi, gregory kahn, ronald s. fearing, sergey levine\n\nuniversity of california, berkeley\n\n7\n1\n0\n2\n\n \nc\ne\nd\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n9\n5\n2\n0\n\n.\n\n8\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014 model-free deep reinforcement\n\nlearning algo-\nrithms have been shown to be capable of learning a wide\nrange of robotic skills, but typically require a very large\nnumber of samples to achieve good performance. model-based\nalgorithms, in principle, can provide for much more ef\ufb01cient\nlearning, but have proven dif\ufb01cult to extend to expressive,\nhigh-capacity models such as deep neural networks. in this\nwork, we demonstrate that medium-sized neural network mod-\nels can in fact be combined with model predictive control\n(mpc) to achieve excellent sample complexity in a model-\nbased reinforcement learning algorithm, producing stable and\nplausible gaits to accomplish various complex locomotion tasks.\nwe ", "article\n\nvasoactive intestinal polypeptide-expressing\ninterneurons in the hippocampus support goal-\noriented spatial learning\n\nhighlights\nd ca2+ imaging indicates bimodal activity dynamics of vip\n\ninterneurons in vivo\n\nd activity of vip interneurons is modulated by task and learning\n\ndemands\n\nd vip-mediated disinhibition supports spatially guided reward\n\nlearning\n\nauthors\n\ngergely farkas turi, wen-ke li,\nspyridon chavlis, ...,\nboris valery zemelman,\npanayiota poirazi, attila losonczy\n\ncorrespondence\npoirazi@imbb.forth.gr (p.p.),\nal2856@columbia.edu (a.l.)\n\nin brief\nturi et al. imaged activity of vip-\nexpressing interneurons of hippocampal\narea ca1 in vivo. they show learning-\nrelated reorganization in vip population\ndynamics. vip interneurons provide\nbehavioral state-dependent disinhibition\nfor ca1 pyramidal cells that supports\nspatial reward learning.\n\nturi et al., 2019, neuron 101, 1150\u20131165\nmarch 20, 2019 \u00aa 2019 elsevier inc.\nhttps://doi.org/10.1016/j.neuron.2019.01.009\n\n\f", "review\npublished: 19 january 2016\ndoi: 10.3389/fncir.2015.00085\n\nneuromodulated\nspike-timing-dependent plasticity,\nand theory of three-factor learning\nrules\n\nnicolas fr\u00e9maux and wulfram gerstner *\n\nschool of computer science and brain mind institute, school of life sciences, \u00e9cole polytechnique f\u00e9d\u00e9rale de lausanne,\nlausanne, switzerland\n\nclassical hebbian learning puts the emphasis on joint pre- and postsynaptic activity,\nbut neglects the potential role of neuromodulators. since neuromodulators convey\ninformation about novelty or reward, the in\ufb02uence of neuromodulators on synaptic\nplasticity is useful not just for action learning in classical conditioning, but also to decide\n\u201cwhen\u201d to create new memories in response to a \ufb02ow of sensory stimuli. in this review, we\nfocus on timing requirements for pre- and postsynaptic activity in conjunction with one or\nseveral phasic neuromodulatory signals. while the emphasis of the text is on conceptual\nmodels and mathematical theories, we also disc", "reports\n\nfig. 3. theory, presented as the experiment (see\nfig. 1). the shg source is the magnetic compo-\nnent of the lorentz force on metal electrons in\nthe srrs.\n\nthe setup for measuring the shg is described\nin the supporting online material (22). we expect\nthat the shg strongly depends on the resonance\nthat is excited. obviously, the incident polariza-\ntion and the detuning of the laser wavelength\nfrom the resonance are of particular interest. one\npossibility for controlling the detuning is to\nchange the laser wavelength for a given sample,\nwhich is difficult because of the extremely broad\ntuning range required. thus, we follow an\nalternative route, lithographic tuning (in which\nthe incident laser wavelength of 1.5 mm, as well\nas the detection system, remains fixed), and tune\nthe resonance positions by changing the srr\nsize. in this manner, we can also guarantee that\nthe optical properties of the srr constituent\nmaterials are identical for all configurations. the\nblue bars in fig. 1 ", "annual review of neuroscience\nthe geometry of information\ncoding in correlated neural\npopulations\n\nrava azeredo da silveira1,2,3,4 and fred rieke5\n1department of physics, ecole normale sup\u00e9rieure, 75005 paris, france; email: rava@ens.fr\n2laboratoire de physique de l\u2019ens, universit\u00e9 paris sciences & lettres (psl), cnrs,\nsorbonne universit\u00e9, universit\u00e9 de paris, 75006 paris, france\n3institute of molecular and clinical ophthalmology basel, 4031 basel, switzerland\n4faculty of science, university of basel, 4056 basel, switzerland\n5department of physiology and biophysics, university of washington, seattle,\nwashington 98195, usa\n\nannu. rev. neurosci. 2021. 44:403\u201324\n\nfirst published as a review in advance on\napril 16, 2021\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nhttps://doi.org/10.1146/annurev-neuro-120320-\n082744\n\ncopyright \u00a9 2021 by annual reviews.\nall rights reserved\n\nkeywords\nneural coding, neural computation, correlations\n\nabstract\nneurons in the brain re", "2\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n6\n1\n\n \n \n]\n\nv\n\ni\n.\ns\ns\ne\ne\n[\n \n \n\n2\nv\n5\n0\n0\n8\n0\n\n.\n\n1\n1\n1\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2022\n\nsolving inverse problems in medical imaging\nwith score-based generative models\n\nyang song\u02da, liyue shen\u02da, lei xing & stefano ermon\nstanford university\n{yangsong@cs,liyues@,lei@,ermon@cs}.stanford.edu\n\nabstract\n\nreconstructing medical images from partial measurements is an important inverse\nproblem in computed tomography (ct) and magnetic resonance imaging (mri).\nexisting solutions based on machine learning typically train a model to directly map\nmeasurements to medical images, leveraging a training dataset of paired images and\nmeasurements. these measurements are typically synthesized from images using a\n\ufb01xed physical model of the measurement process, which hinders the generalization\ncapability of models to unknown measurement processes. to address this issue, we\npropose a fully unsupervised technique for inverse problem solving, leveraging the\nrece", "adversarially trained neural representations may already be as robust as\n\ncorresponding biological neural representations\n\nchong guo 1 michael j. lee 1 2 3 guillaume leclerc 4 joel dapello 1 2 5 yug rao 6 aleksander madry 4 7\n\njames j. dicarlo 1 2 3\n\nabstract\n\nvisual systems of primates are the gold standard\nof robust perception. there is thus a general be-\nlief that mimicking the neural representations that\nunderlie those systems will yield artificial visual\nsystems that are adversarially robust. in this work,\nwe develop a method for performing adversar-\nial visual attacks directly on primate brain activ-\nity. we then leverage this method to demonstrate\nthat the above-mentioned belief might not be well\nfounded. specifically, we report that the biolog-\nical neurons that make up visual systems of pri-\nmates exhibit susceptibility to adversarial pertur-\nbations that is comparable in magnitude to exist-\ning (robustly trained) artificial neural networks.\n\n1. introduction\ndeep neural networ", "similarity of neural network representations revisited\n\nsimon kornblith 1 mohammad norouzi 1 honglak lee 1 geoffrey hinton 1\n\n9\n1\n0\n2\n\n \nl\nu\nj\n \n\n9\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n4\n1\n4\n0\n0\n\n.\n\n5\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nrecent work has sought to understand the behav-\nior of neural networks by comparing representa-\ntions between layers and between different trained\nmodels. we examine methods for comparing neu-\nral network representations based on canonical\ncorrelation analysis (cca). we show that cca\nbelongs to a family of statistics for measuring mul-\ntivariate similarity, but that neither cca nor any\nother statistic that is invariant to invertible linear\ntransformation can measure meaningful similari-\nties between representations of higher dimension\nthan the number of data points. we introduce\na similarity index that measures the relationship\nbetween representational similarity matrices and\ndoes not suffer from this limitation. this simi-\nlarity index is equivalent to centered ker", "f o c u s   o n   n e u r a l   c o m p u tat i o n  a n d   t h e o r y  \n\nr e v i e w\n\nthe mechanics of state-dependent neural \ncorrelations\n\nbrent doiron1,2, ashok litwin-kumar1\u20133, robert rosenbaum1,2,4,5, gabriel k ocker1,2,6 & kre\u0161imir josi\u01077,8\n\nsimultaneous recordings from large neural populations are becoming increasingly common. an important feature of population \nactivity is the trial-to-trial correlated fluctuation of spike train outputs from recorded neuron pairs. similar to the firing rate of single \nneurons, correlated activity can be modulated by a number of factors, from changes in arousal and attentional state to learning and \ntask engagement. however, the physiological mechanisms that underlie these changes are not fully understood. we review recent \ntheoretical results that identify three separate mechanisms that modulate spike train correlations: changes in input correlations, \ninternal fluctuations and the transfer function of single neurons. we first examine these ", "proceedings of the twenty-sixth international joint conference on arti\ufb01cial intelligence (ijcai-17)\n\n4949\n\nvalueiterationnetworksavivtamar1,yiwu1,garrettthomas1,sergeylevine1,pieterabbeel1;21ucberkeley,2openaifavivt,jxwuyi,gwthomasg@berkeley.edu,svlevine@eecs.berkeley.edu,pabbeel@cs.berkeley.eduabstractweintroducethevalueiterationnetwork(vin):afullydifferentiableneuralnetworkwitha\u2018planningmodule\u2019embeddedwithin.vinscanlearntoplan,andaresuitableforpredictingoutcomesthatinvolveplanning-basedreasoning,suchaspoliciesforrein-forcementlearning.keytoourapproachisanoveldifferentiableapproximationofthevalue-iterationalgorithm,whichcanberepresentedasaconvolu-tionalneuralnetwork,andtrainedend-to-endusingstandardbackpropagation.weevaluatevinbasedpoliciesondiscreteandcontinuouspath-planningdomains,andonanatural-languagebasedsearchtask.weshowthatbylearninganexplicitplanningcomputation,vinpoliciesgeneralizebettertonew,unseendomains.thispaperisasigni\ufb01cantlyabridgedandijcaiaudiencetargetedversionoftheor", "biorxiv preprint \n\nthe copyright holder for this\npreprint (which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in \n\nhttps://doi.org/10.1101/2023.10.31.564922\n; \n\nthis version posted november 3, 2023. \n\ndoi: \n\nperpetuity. it is made available under a\n\ncc-by-nd 4.0 international license\n.\n\nfrom connectome to effectome: learning the\n\ncausal interaction map of the fly brain\n\ndean pospisil1*\u2020, max aragon1\u2020 and jonathan pillow1\n\n1*princeton neuroscience institute, princeton, washington road,\n\nprinceton, 08544, nj, usa.\n\n*corresponding author(s). e-mail(s): dp4846@princeton.edu;\n\ncontributing authors: mjaragon@princeton.edu; jpillow@princeton.edu;\n\n\u2020these authors contributed equally to this work.\n\nabstract\n\na long-standing goal of neuroscience is to obtain a causal model of the nervous\nsystem. this would allow neuroscientists to explain animal behavior in terms of\nthe dynamic interactions between neurons. the recently reporte", "\f", "review\n\nspecial  issue:  hippocampus  and  memory\n\na  neohebbian  framework  for  episodic\nmemory;  role  of  dopamine-dependent\nlate  ltp\njohn  lisman1,  anthony  a.  grace2 and  emrah  duzel3,4,5\n\n1 department  of  biology  and  volen  center  for  complex  systems,  brandeis  university,  waltham,  ma  02454-9110,  usa\n2 departments  of  neuroscience,  psychiatry  and  psychology,  university  of  pittsburgh,  pittsburgh,  pa  15260,  usa\n3 institute  of  cognitive  neuroscience,  university  college  london,  london  w1cn  4ar,  uk\n4 institute  of  cognitive  neurology  and  dementia  research,  otto-von-guericke-university,  leipziger  strasse  44,  39120  magdeburg,\ngermany\n5 german  centre  for  neurodegenerative  diseases  (dzne),  standort  magdeburg,  germany\n\naccording  to  the  hebb  rule,  the  change  in  the  strength  of\na  synapse  depends  only  on  the  local  interaction  of  pre-\nsynaptic  and  postsynaptic  events.  studies  at  many  types\nof  synapses  indicate ", "physiology & behavior 86 (2005) 717 \u2013 730\n\nneural bases of food-seeking: affect, arousal and reward\n\nb\n\nin corticostriatolimbic circuits\n\nbernard w. balleine *\n\ndepartment of psychology and the brain research institute, university of california, box 951563, los angeles, ca 90095-1563, united states\n\nreceived 28 july 2005; accepted 25 august 2005\n\nabstract\n\nf\n\n_\n\nf\n\n_\n\nor\n\nreward\n\nreward-like\n\nrecent studies suggest that there are multiple\n\nsystems that control food seeking; evidence points to two distinct\nlearning processes and four modulatory processes that contribute to the performance of food-related instrumental actions. the learning processes\nsubserve the acquisition of goal-directed and habitual actions and involve the dorsomedial and dorsolateral striatum, respectively. access to food\ncan function both to reinforce habits and as a reward or goal for actions. encoding and retrieving the value of a goal appears to be mediated by\ndistinct processes that, contrary to the somatic mar", "synaptic neuroscience\ndendritic synapse location and neocortical spike-timing-\ndependent plasticity\n\nreview article\npublished: 21 july 2010\ndoi: 10.3389/fnsyn.2010.00029\n\nrobert c. froemke1*, johannes j. letzkus 2, bj\u00f6rn m. kampa 3, giao b. hang4,5 and greg j. stuart 6\n\n1  departments of otolaryngology and physiology/neuroscience, molecular neurobiology program, the helen and martin kimmel center for biology and medicine, \n\nskirball institute of biomolecular medicine, new york university school of medicine, new york, ny, usa\n\n2  friedrich miescher institute for biomedical research, basel, switzerland\n3  brain research institute, university of zurich, zurich, switzerland\n4  howard hughes medical institute, university of california, berkeley, ca, usa\n5  department of molecular and cell biology, division of neurobiology, helen wills neuroscience institute, university of california, berkeley, ca, usa\n6  the john curtin school of medical research, australian national university, canberra, a", "psychlab: a psychology laboratory for deep\n\nreinforcement learning agents\n\njoel z. leibo, cyprien de masson d\u2019autume, daniel zoran, david amos,\n\ncharles beattie, keith anderson, antonio garc\u00eda casta\u00f1eda,\nmanuel sanchez, simon green, audrunas gruslys, shane legg,\n\ndemis hassabis, and matthew m. botvinick\n\n8\n1\n0\n2\n\n \n\nb\ne\nf\n4\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n2\nv\n6\n1\n1\n8\n0\n\n.\n\n1\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ndeepmind, london, uk\n\nfebruary 6, 2018\n\nabstract\n\npsychlab is a simulated psychology laboratory inside the \ufb01rst-person 3d\ngame world of deepmind lab (beattie et al., 2016). psychlab enables im-\nplementations of classical laboratory psychological experiments so that they\nwork with both human and arti\ufb01cial agents. psychlab has a simple and \ufb02ex-\nible api that enables users to easily create their own tasks. as examples,\nwe are releasing psychlab implementations of several classical experimen-\ntal paradigms including visual search, change detection, random dot motion\ndiscrimination, and multiple object tr", "robust compressed sensing mri with deep\n\ngenerative priors\n\najil jalal\u2217\n\nece, ut austin\n\najiljalal@utexas.edu\n\nmarius arvinte*\nece, ut austin\n\narvinte@utexas.edu\n\ngiannis daras\ncs, ut austin\n\ngiannisdaras@utexas.edu\n\n1\n2\n0\n2\n\n \nc\ne\nd\n6\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n8\n6\n3\n1\n0\n\n.\n\n8\n0\n1\n2\n:\nv\ni\nx\nr\na\n\neric price\n\ncs, ut austin\n\necprice@cs.utexas.edu\n\nalexandros g. dimakis\n\nece, ut austin\n\ndimakis@austin.utexas.edu\n\njonathan i. tamir\nece, ut austin\n\njtamir@utexas.edu\n\nabstract\n\nthe csgm framework (bora-jalal-price-dimakis\u201917) has shown that deep gen-\nerative priors can be powerful tools for solving inverse problems. however, to\ndate this framework has been empirically successful only on certain datasets (for\nexample, human faces and mnist digits), and it is known to perform poorly on\nout-of-distribution samples. in this paper, we present the \ufb01rst successful application\nof the csgm framework on clinical mri data. we train a generative prior on brain\nscans from the fastmri dataset, and sh", "neuron\n\nperspective\n\nsymmetry breaking in space-time hierarchies\nshapes brain dynamics and behavior\n\najay s. pillai1,2 and viktor k. jirsa3,*\n1department of neurology and developmental medicine, kennedy krieger institute, baltimore, md 21205, usa\n2department of neurology, johns hopkins university school of medicine, baltimore, md 21205, usa\n3institut de neurosciences des syste` mes, inserm, aix-marseille universite\u00b4 , 13005 marseille, france\n*correspondence: viktor.jirsa@univ-amu.fr\nhttp://dx.doi.org/10.1016/j.neuron.2017.05.013\n\nin order to maintain brain function, neural activity needs to be tightly coordinated within the brain network.\nhow this coordination is achieved and related to behavior is largely unknown. it has been previously argued\nthat the study of the link between brain and behavior is impossible without a guiding vision. here we propose\nbehavioral-level concepts and mechanisms embodied as structured \ufb02ows on manifold (sfm) that provide a\nformal description of behavior as", "continual learning in a multi-layer network of an\nelectric fish\n\narticle\n\ngraphical abstract\n\nauthors\nsalomon z. muller, abigail n. zadina,\nl.f. abbott, nathaniel b. sawtell\n\ncorrespondence\nns2635@columbia.edu\n\nin brief\nusing a cerebellum-like structure in an\nelectric \ufb01sh as a model system for\ninvestigating mechanisms of learning in\nmulti-layer networks, muller et al.\nobserved that functional\ncompartmentalization within individual\nneurons allows synaptic plasticity at an\nintermediate processing layer to\nadaptively shape network output.\n\nhighlights\nd biological solutions to general problems in multi-layer\n\nlearning are shown\n\nd intermediate layer function requires compartmentalization of\n\nlearning and signaling\n\nd circuit organization based on learning solves the credit\n\nassignment problem\n\nmuller et al., 2019, cell 179, 1382\u20131392\nnovember 27, 2019 \u00aa 2019 elsevier inc.\nhttps://doi.org/10.1016/j.cell.2019.10.020\n\n\f", "hogwild!: a lock-free approach to parallelizing\n\nstochastic gradient descent\n\nfeng niu\n\nleonn@cs.wisc.edu\n\nbenjamin recht\n\nbrecht@cs.wisc.edu\n\nchristopher r\u00b4e\n\nchrisre@cs.wisc.edu\n\nstephen j. wright\n\nswright@cs.wisc.edu\n\ncomputer sciences department\nuniversity of wisconsin-madison\n\nmadison, wi 53706\n\nabstract\n\nstochastic gradient descent (sgd) is a popular algorithm that can achieve state-\nof-the-art performance on a variety of machine learning tasks. several researchers\nhave recently proposed schemes to parallelize sgd, but all require performance-\ndestroying memory locking and synchronization. this work aims to show using\nnovel theoretical analysis, algorithms, and implementation that sgd can be im-\nplemented without any locking. we present an update scheme called hogwild!\nwhich allows processors access to shared memory with the possibility of overwrit-\ning each other\u2019s work. we show that when the associated optimization problem\nis sparse, meaning most gradient updates only modify sm", "a methods\n\na.1 network model\n\nwe consider a discrete-time implementation of a rate-based recurrent neural network (rnn) similar\nto the form in [56]. we denote the observable states, i.e. \ufb01ring rates, as zt at time t, and the\ncorresponding internal states as st. the dynamics of those states are governed by\n\nsj,t+1 = \u03b7 sj,t + (1 \u2212 \u03b7)\n\nwjl zl,t +\n\nw in\n\njm xm,t+1\n\n\uf8eb\uf8ed(cid:88)\n\nl(cid:54)=j\n\n(cid:88)\n\np\n\n\uf8f6\uf8f8\n\nzj,t = relu (sj,t),\n\n(s1)\nwhere \u03b7 = e\u2212dt/\u03c4m denotes the leak factor for simulation time step dt and membrane time constant\n\u03c4m, wlj denotes the weight of the synaptic connection from neuron j to l, w in\njm denotes the strength\nof the connection between the mth external input and neuron j and xt denotes the external input at\ntime t. threshold adaptation is not used here in order to focus on capacity of the temporal credit\npropagation mechanism. we focused on relu activation due to its wide adoption in both deep\nlearning and computational neuroscience communities; as discussed, we leave ext", "biorxiv preprint \nthe copyright holder for this preprint (which was not\ncertified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available under \n\nthis version posted october 10, 2018. \n\nhttps://doi.org/10.1101/440396\n; \n\ndoi: \n\na\n\ncc-by-nc-nd 4.0 international license\n.\n\nfunctional clustering of dendritic activity during decision-making \n\n \nkerlin a12, mohar b12, flickinger d1, maclennan bj1, davis c1, spruston n1, svoboda k1*  \n\nsummary \n\nthe active properties of dendrites support local nonlinear operations, but previous imaging and \nelectrophysiological measurements have produced conflicting views regarding the prevalence of \nlocal nonlinearities in vivo. we imaged calcium signals in pyramidal cell dendrites in the motor \ncortex of mice performing a tactile decision task. a custom microscope allowed us to image the \nsoma and up to 300 \u00b5m of contiguous dendrite at 15 hz, while resolving individual spines. new ", "training neural networks with local error signals\n\narild n\u00f8kland * 1 lars h. eidnes * 2\n\nabstract\n\nsupervised training of neural networks for classi-\n\ufb01cation is typically performed with a global loss\nfunction. the loss function provides a gradient\nfor the output layer, and this gradient is back-\npropagated to hidden layers to dictate an update\ndirection for the weights. an alternative approach\nis to train the network with layer-wise loss func-\ntions. in this paper we demonstrate, for the \ufb01rst\ntime, that layer-wise training can approach the\nstate-of-the-art on a variety of image datasets. we\nuse single-layer sub-networks and two different\nsupervised loss functions to generate local error\nsignals for the hidden layers, and we show that the\ncombination of these losses help with optimiza-\ntion in the context of local learning. using local\nerrors could be a step towards more biologically\nplausible deep learning because the global error\ndoes not have to be transported back to hidden\nlayers. ", "8\n1\n0\n2\n\n \n\ny\na\nm\n2\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n2\n9\n2\n8\n0\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nthe marginal value of adaptive gradient methods\n\nin machine learning\n\nashia c. wilson(cid:93), rebecca roelofs(cid:93), mitchell stern(cid:93), nathan srebro\u2020, and benjamin recht(cid:93)\n{ashia,roelofs,mitchell}@berkeley.edu, nati@ttic.edu, brecht@berkeley.edu\n\n(cid:93)university of california, berkeley\n\n\u2020toyota technological institute at chicago\n\nabstract\n\nadaptive optimization methods, which perform local optimization with a metric\nconstructed from the history of iterates, are becoming increasingly popular for\ntraining deep neural networks. examples include adagrad, rmsprop, and adam.\nwe show that for simple overparameterized problems, adaptive methods often \ufb01nd\ndrastically different solutions than gradient descent (gd) or stochastic gradient\ndescent (sgd). we construct an illustrative binary classi\ufb01cation problem where\nthe data is linearly separable, gd and sgd achieve zero test error, and a", "9\n1\n0\n2\n\n \nr\np\na\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n3\n6\n0\n1\n\n.\n\n9\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ngenerative replay with feedback connections as a\n\ngeneral strategy for continual learning\n\ngido m. van de ven1 & andreas s. tolias1,2\n\n1 center for neuroscience and arti\ufb01cial intelligence, baylor college of medicine, houston\n\n2 department of electrical and computer engineering, rice university, houston\n\n{ven,astolias}@bcm.edu\n\nabstract\n\na major obstacle to developing arti\ufb01cial intelligence applications capable of true\nlifelong learning is that arti\ufb01cial neural networks quickly or catastrophically for-\nget previously learned tasks when trained on a new one. numerous methods for\nalleviating catastrophic forgetting are currently being proposed, but differences\nin evaluation protocols make it dif\ufb01cult to directly compare their performance.\nto enable more meaningful comparisons, here we identi\ufb01ed three distinct scenar-\nios for continual learning based on whether task identity is known and, if it is\nnot, ", "1\n2\n0\n2\n\n \n\ng\nu\na\n4\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n2\n6\n0\n4\n0\n\n.\n\n1\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nmutual information neural estimation\n\nmohamed ishmael belghazi 1 aristide baratin 1 2 sai rajeswar 1 sherjil ozair 1 yoshua bengio 1 3 4\n\naaron courville 1 3 r devon hjelm 1 4\n\nabstract\n\nwe argue that the estimation of mutual informa-\ntion between high dimensional continuous ran-\ndom variables can be achieved by gradient descent\nover neural networks. we present a mutual infor-\nmation neural estimator (mine) that is linearly\nscalable in dimensionality as well as in sample\nsize, trainable through back-prop, and strongly\nconsistent. we present a handful of applications\non which mine can be used to minimize or max-\nimize mutual information. we apply mine to im-\nprove adversarially trained generative models. we\nalso use mine to implement the information bot-\ntleneck, applying it to supervised classi\ufb01cation;\nour results demonstrate substantial improvement\nin \ufb02exibility and performance in these settings.\n", "physical review x 4, 021039 (2014)\n\nbalanced networks of spiking neurons with spatially dependent recurrent connections\n\nrobert rosenbaum and brent doiron\n\ndepartment of mathematics, university of pittsburgh, pittsburgh, pennsylvania 15260, usa\n\nand center for the neural basis of cognition, pittsburgh, pennsylvania 15213, usa\n\n(received 27 august 2013; revised manuscript received 14 february 2014; published 28 may 2014)\n\nnetworks of model neurons with balanced recurrent excitation and inhibition capture the irregular and\nasynchronous spiking activity reported in cortex. while mean-field theories of spatially homogeneous\nbalanced networks are well understood, a mean-field analysis of spatially heterogeneous balanced networks\nhas not been fully developed. we extend the analysis of balanced networks to include a connection\nprobability that depends on the spatial separation between neurons. in the continuum limit, we derive that\nstable, balanced firing rate solutions require that the spati", "article\n\nprecision of inhibition: dendritic inhibition by\nindividual gabaergic synapses on hippocampal\npyramidal cells is con\ufb01ned in space and time\n\nhighlights\nd new paradigm to measure inhibition by individual gabaergic\n\nsynapses\n\nauthors\n\nfiona e. mu\u00a8 llner, corette j. wierenga,\ntobias bonhoeffer\n\nd a realistic model for dendritic ca2+ inhibition\n\nd ca2+ transients from back-propagating aps are inhibited with\n\nlarge dynamic range\n\nd ca2+ is inhibited with micrometer/millisecond precision in\n\nboth shafts and spines\n\ncorrespondence\nmuellner@neuro.mpg.de\n\nin brief\nby imaging action potential-evoked\ndendritic calcium signals and\nsimultaneously activating identi\ufb01ed\ninhibitory synapses, mu\u00a8 llner et al.\nmeasured the spatio-temporal pro\ufb01le of\ninhibition exerted by individual\ngabaergic synapses, which \ufb01lls a gap in\nthe biophysical understanding of\ndendritic inhibition.\n\nmu\u00a8 llner et al., 2015, neuron 87, 576\u2013589\naugust 5, 2015 \u00aa2015 elsevier inc.\nhttp://dx.doi.org/10.1016/j.neuron.2015.07.00", "department of computer science\nuniversity of toronto\nhttp://learning.cs.toronto.edu\n\n6 king\u2019s college rd, toronto\nm5s 3g4, canada\nfax: +1 416 978 1455\n\ncopyright c(cid:13) geo\ufb00rey hinton 2010.\n\naugust 2, 2010\n\nutml tr 2010\u2013003\n\na practical guide to training\nrestricted boltzmann machines\n\nversion 1\n\ndepartment of computer science, university of toronto\n\ngeo\ufb00rey hinton\n\n\f", "vs01ch17-kriegeskorte\n\nari\n\n4 november 2015\n\n10:24\n\ndeep neural networks:\na new framework for\nmodeling biological vision\nand brain information\nprocessing\nnikolaus kriegeskorte\nmedical research council cognition and brain sciences unit, university of cambridge,\ncambridge cb2 7ef, united kingdom; email: nikolaus.kriegeskorte@mrc-cbu.cam.ac.uk\n\nkeywords\nbiological vision, computer vision, object recognition, neural network,\ndeep learning, arti\ufb01cial intelligence, computational neuroscience\n\nabstract\nrecent advances in neural network modeling have enabled major strides in\ncomputer vision and other arti\ufb01cial intelligence applications. human-level\nvisual recognition abilities are coming within reach of arti\ufb01cial systems. arti-\n\ufb01cial neural networks are inspired by the brain, and their computations could\nbe implemented in biological neurons. convolutional feedforward networks,\nwhich now dominate computer vision, take further inspiration from the ar-\nchitecture of the primate visual hierarchy. ", "0\n2\n0\n2\n \nc\ne\nd\n1\n1\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n3\nv\n6\n1\n6\n2\n1\n\n.\n\n5\n0\n9\n1\n:\nv\ni\nx\nr\na\n\ndefending against neural fake news\n\nrowan zellers\u2660, ari holtzman\u2660, hannah rashkin\u2660, yonatan bisk\u2660\n\nali farhadi\u2660\u2665, franziska roesner\u2660, yejin choi\u2660\u2665\n\n\u2660paul g. allen school of computer science & engineering, university of washington\n\n\u2665allen institute for arti\ufb01cial intelligence\nhttps://rowanzellers.com/grover\n\nabstract\n\nrecent progress in natural language generation has raised dual-use concerns. while\napplications like summarization and translation are positive, the underlying tech-\nnology also might enable adversaries to generate neural fake news: targeted propa-\nganda that closely mimics the style of real news.\nmodern computer security relies on careful threat modeling: identifying potential\nthreats and vulnerabilities from an adversary\u2019s point of view, and exploring potential\nmitigations to these threats. likewise, developing robust defenses against neural\nfake news requires us \ufb01rst to carefully invest", "ne43ch12_vyas\n\narjats.cls\n\njune 23, 2020\n\n11:56\n\nannual review of neuroscience\ncomputation through neural\npopulation dynamics\n\nsaurabh vyas,1,3 matthew d. golub,2,3\ndavid sussillo,2,3,4 and krishna v. shenoy1,2,3,5\n1department of bioengineering, stanford university, stanford, california 94305, usa;\nemail: smvyas@stanford.edu\n2department of electrical engineering, stanford university, stanford, california 94305, usa\n3wu tsai neurosciences institute, stanford university, stanford, california 94305, usa\n4google ai, google inc., mountain view, california 94305, usa\n5department of neurobiology, bio-x institute, neurosciences program, and howard hughes\nmedical institute, stanford university, stanford, california 94305, usa\n\nkeywords\nneural computation, neural population dynamics, dynamical systems, state\nspaces\n\nabstract\nsignificant experimental, computational, and theoretical work has identified\nrich structure within the coordinated activity of interconnected neural pop-\nulations. an emergi", "cortical microcircuits as\n\ngated-recurrent neural networks\n\nrui ponte costa\u2217\n\nyannis m. assael\u2217\n\ncentre for neural circuits and behaviour\n\ndept. of computer science\n\ndept. of physiology, anatomy and genetics\n\nuniversity of oxford, oxford, uk\n\nuniversity of oxford, oxford, uk\n\nrui.costa@cncb.ox.ac.uk\n\nand deepmind, london, uk\n\nyannis.assael@cs.ox.ac.uk\n\nbrendan shillingford\u2217\n\ndept. of computer science\n\nuniversity of oxford, oxford, uk\n\nand deepmind, london, uk\n\nbrendan.shillingford@cs.ox.ac.uk\n\nnando de freitas\n\ndeepmind\nlondon, uk\n\nnandodefreitas@google.com\n\ntim p. vogels\n\ncentre for neural circuits and behaviour\n\ndept. of physiology, anatomy and genetics\n\nuniversity of oxford, oxford, uk\n\ntim.vogels@cncb.ox.ac.uk\n\nabstract\n\ncortical circuits exhibit intricate recurrent architectures that are remarkably similar\nacross different brain areas. such stereotyped structure suggests the existence of\ncommon computational principles. however, such principles have remained largely\nelusive. inspi", "8\n1\n0\n2\n\n \n\nb\ne\nf\n4\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n3\n0\n2\n6\n0\n\n.\n\n7\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nimagination-augmented agents\nfor deep reinforcement learning\n\nth\u00e9ophane weber\u2217 s\u00e9bastien racani\u00e8re\u2217 david p. reichert\u2217 lars buesing\narthur guez danilo rezende adria puigdom\u00e8nech badia oriol vinyals\n\nnicolas heess yujia li razvan pascanu\n\npeter battaglia\n\ndemis hassabis david silver daan wierstra\n\ndeepmind\n\nabstract\n\nwe introduce imagination-augmented agents (i2as), a novel architecture for deep\nreinforcement learning combining model-free and model-based aspects. in con-\ntrast to most existing model-based reinforcement learning and planning methods,\nwhich prescribe how a model should be used to arrive at a policy, i2as learn to\ninterpret predictions from a learned environment model to construct implicit plans\nin arbitrary ways, by using the predictions as additional context in deep policy\nnetworks. i2as show improved data ef\ufb01ciency, performance, and robustness to\nmodel misspeci\ufb01cation compared to se", "9\n1\n0\n2\n\n \n\nn\na\nj\n \n8\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n6\n8\n3\n2\n0\n\n.\n\n9\n0\n8\n1\n:\nv\ni\nx\nr\na\n\na geometrical analysis of global stability in trained feedback\nnetworks\n\nfrancesca mastrogiuseppe 1,2, srdjan ostojic 1\n\n1 laboratoire de neurosciences cognitives et computationelles, inserm u960 and\n2 laboratoire de physique statistique, cnrs umr 8550\n\u00e9cole normale sup\u00e9rieure - psl research university, paris, france\n\nrecurrent neural networks have been extensively studied in the context of neuroscience and machine learning\ndue to their ability to implement complex computations. while substantial progress in designing e\ufb00ective\nlearning algorithms has been achieved in the last years, a full understanding of trained recurrent networks is still\nlacking. speci\ufb01cally, the mechanisms that allow computations to emerge from the underlying recurrent dynamics\nare largely unknown. here we focus on a simple, yet underexplored computational setup: a feedback architecture\ntrained to associate a stationary ", "letter\n\ndoi:10.1038/nature23020\n\ndistinct timescales of population coding across \ncortex\n\ncaroline a. runyan1*, eugenio piasini2*, stefano panzeri2 & christopher d. harvey1\n\nthe  cortex  represents  information  across  widely  varying \ntimescales1\u20135. for instance, sensory cortex encodes stimuli that \nfluctuate over few tens of milliseconds6,7, whereas in association \ncortex behavioural choices can require the maintenance of \ninformation over seconds8,9. however, it remains poorly understood \nwhether diverse timescales result mostly from features intrinsic \nto individual neurons or from neuronal population activity. this \nquestion remains unanswered, because the timescales of coding \nin populations of neurons have not been studied extensively, and \npopulation codes have not been compared systematically across \ncortical regions. here we show that population codes can be \nessential to achieve long coding timescales. furthermore, we find \nthat the properties of population codes differ bet", "published as a conference paper at iclr 2016\n\nall you need is a good init\n\ndmytro mishkin, jiri matas\n\ncenter for machine perception\nczech technical university in prague\nczech republic {mishkdmy,matas}@cmp.felk.cvut.cz\n\nabstract\n\nlayer-sequential unit-variance (lsuv) initialization \u2013 a simple method for weight\ninitialization for deep net learning \u2013 is proposed. the method consists of the two\nsteps. first, pre-initialize weights of each convolution or inner-product layer with\northonormal matrices. second, proceed from the \ufb01rst to the \ufb01nal layer, normaliz-\ning the variance of the output of each layer to be equal to one.\nexperiment with different activation functions (maxout, relu-family, tanh) show\nthat the proposed initialization leads to learning of very deep nets that (i) produces\nnetworks with test accuracy better or equal to standard methods and (ii) is at least\nas fast as the complex schemes proposed speci\ufb01cally for very deep nets such as\nfitnets (romero et al. (2015)) and highway ", "implicit self-regularization in deep neural networks: evidence\n\nfrom random matrix theory and implications for learning\n\ncharles h. martin\u2217\n\nmichael w. mahoney\u2020\n\nabstract\n\nrandom matrix theory (rmt) is applied to analyze the weight matrices of deep neural\nnetworks (dnns), including both production quality, pre-trained models such as alexnet\nand inception, and smaller models trained from scratch, such as lenet5 and a miniature-\nalexnet. empirical and theoretical results clearly indicate that the dnn training process\nitself implicitly implements a form of self-regularization, implicitly sculpting a more regu-\nlarized energy or penalty landscape. in particular, the empirical spectral density (esd) of\ndnn layer matrices displays signatures of traditionally-regularized statistical models, even\nin the absence of exogenously specifying traditional forms of explicit regularization, such as\ndropout or weight norm constraints. building on relatively recent results in rmt, most\nnotably its extens", "attention is all you need\n\nashish vaswani\u2217\ngoogle brain\n\navaswani@google.com\n\nnoam shazeer\u2217\ngoogle brain\n\nnoam@google.com\n\nniki parmar\u2217\ngoogle research\n\nnikip@google.com\n\njakob uszkoreit\u2217\ngoogle research\nusz@google.com\n\nllion jones\u2217\ngoogle research\n\nllion@google.com\n\naidan n. gomez\u2217 \u2020\nuniversity of toronto\n\naidan@cs.toronto.edu\n\n\u0142ukasz kaiser\u2217\ngoogle brain\n\nlukaszkaiser@google.com\n\nillia polosukhin\u2217 \u2021\n\nillia.polosukhin@gmail.com\n\nabstract\n\nthe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. the best\nperforming models also connect the encoder and decoder through an attention\nmechanism. we propose a new simple network architecture, the transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring signi\ufb01cantly\nless time to", "neuron\n\nreview\n\nserotonin in the modulation\nof neural plasticity and networks:\nimplications for neurodevelopmental disorders\n\nklaus-peter lesch1,2,* and jonas waider1\n1division of molecular psychiatry, laboratory of translational neuroscience, adhd clinical research network, department of psychiatry,\npsychosomatics and psychotherapy, university of wu\u00a8 rzburg, 97080 wu\u00a8 rzburg, germany\n2department of neuroscience, school of mental health and neuroscience, maastricht university, 6211 lk maastricht, the netherlands\n*correspondence: kplesch@mail.uni-wuerzburg.de\nhttp://dx.doi.org/10.1016/j.neuron.2012.09.013\n\nserotonin (5-ht) shapes brain networks during development and modulates a wide spectrum of essential\nneuronal functions ranging from perception and cognitive appraisal to emotional responses in the mature\nbrain. de\ufb01cits in 5-ht-moderated synaptic signaling fundamentally impact the pathophysiology and long-\nterm outcome of neurodevelopmental disorders. our understanding of how 5-ht-dep", "article\n\ncommunicated by roger brockett\n\nestimating a state-space model from point process\nobservations\n\nanne c. smith\nasmith@neurostat.mgh.harvard.edu\nneuroscience statistics research laboratory, department of anesthesia and critical\ncare, massachusetts general hospital, boston, ma 02114, u.s.a.\n\nemery n. brown\nbrown@neurostat.mgh.harvard.edu\nneuroscience statistics research laboratory, department of anesthesia and critical\ncare, massachusetts general hospital, boston, ma 02114, u.s.a., and division of\nhealth sciences and technology, harvard medical school/massachusetts institute of\ntechnology, cambridge, ma 02139, u.s.a.\n\na widely used signal processing paradigm is the state-space model. the\nstate-space model is de\u0005ned by two equations: an observation equation\nthat describes how the hidden state or latent process is observed and a\nstate equation that de\u0005nes the evolution of the process through time. in-\nspired by neurophysiology experiments in which neural spiking activity\nis induced", "article\n\nhttps://doi.org/10.1038/s41467-023-40141-z\n\nexperimental validation of the free-energy\nprinciple with in vitro neural networks\n\nreceived: 12 october 2022\n\naccepted: 13 july 2023\n\ncheck for updates\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\ntakuya isomura 1\n\n, kiyoshi kotani2, yasuhiko jimbo3 & karl j. friston 4,5\n\nempirical applications of the free-energy principle are not straightforward\nbecause they entail a commitment to a particular process theory, especially at\nthe cellular and synaptic levels. using a recently established reverse engi-\nneering technique, we con\ufb01rm the quantitative predictions of the free-energy\nprinciple using in vitro networks of rat cortical neurons that perform causal\ninference. upon receiving electrical stimuli\u2014generated by mixing two hidden\nsources\u2014neurons self-organised to selectively encode the two sources. phar-\nmacological up- and downregulation of network excitability disrupted the\nensuing inference, consistent with change", "6\n1\n0\n2\n\n \nr\na\n\n \n\nm\n3\n2\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n5\n8\n2\n7\n0\n\n.\n\n3\n0\n6\n1\n:\nv\ni\nx\nr\na\n\na guide to convolution arithmetic for deep\n\nlearning\n\nvincent dumoulin1(cid:70) and francesco visin2(cid:70)\u2020\n\n(cid:70)mila, universit\u00e9 de montr\u00e9al\n\u2020airlab, politecnico di milano\n\nmarch 24, 2016\n\n1dumouliv@iro.umontreal.ca\n2francesco.visin@polimi.it\n\n\f", "journal of mathematical psychology 106 (2022) 102617\n\ncontents lists available at sciencedirect\n\njournal of mathematical psychology\n\njournal homepage: www.elsevier.com/locate/jmp\n\nperil, prudence and planning as risk, avoidance and worry\nchris gagne a,\u2217, peter dayan a,b\n\na max planck institute for biological cybernetics, t\u00fcbingen, germany\nb university of t\u00fcbingen, t\u00fcbingen, germany\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 14 june 2021\nreceived in revised form 22 october 2021\naccepted 11 november 2021\navailable online 10 december 2021\n\nkeywords:\ndecision making\nrisk sensitivity\nanxiety\navoidance behaviors\nworry\n\n1. introduction\n\nrisk occupies a central role in both the theory and practice of decision-making. although it is deeply\nimplicated in many conditions involving dysfunctional behavior and thought, modern theoretical\napproaches to understanding and mitigating risk, in either one-shot or sequential settings, have yet\nto permeate fully the fields of neural", "ne40ch17-uchida\n\nari\n\n4 july 2017\n\n17:51\n\nneural circuitry of reward\nprediction error\nmitsuko watabe-uchida,1,\u2217 neir eshel,1,2,\u2217\nand naoshige uchida1\n1department of molecular and cellular biology, center for brain science, harvard university,\ncambridge, massachusetts 02138; email: mitsuko@mcb.harvard.edu, uchida@mcb.harvard.edu\n2department of psychiatry and behavioral sciences, stanford university school of medicine,\nstanford, california 94305; email: neshel@stanford.edu\n\nannu. rev. neurosci. 2017. 40:373\u201394\n\nfirst published as a review in advance on\napril 24, 2017\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nhttps://doi.org/10.1146/annurev-neuro-072116-\n031109\ncopyright c(cid:2) 2017 by annual reviews.\nall rights reserved\n\u2217these authors contributed equally to the\npreparation of this review.\n\nannual \n\n reviews further\n\nclick here to view this article's \nonline features:\n\u2022 download \ufb01gures as ppt slides\n\u2022 navigate linked references\n\u2022 download citations\n\u2022 explo", "original research article\npublished: 04 april 2014\ndoi: 10.3389/fncom.2014.00038\n\nstochastic variational learning in recurrent spiking\nnetworks\ndanilo jimenez rezende 1,2* and wulfram gerstner 1,2\n\n1 laboratory of cognitive neuroscience, school of life sciences, brain mind institute, ecole polytechnique federale de lausanne, lausanne, vaud, switzerland\n2 laboratory of computational neuroscience, school of computer and communication sciences, ecole polytechnique federale de lausanne, lausanne, vaud,\n\nswitzerland\n\nedited by:\nnicolas brunel, centre national de la\nrecherche scienti\ufb01que, france\nreviewed by:\ntomoki fukai, riken brain science\ninstitute, japan\nanthony burkitt, university of\nmelbourne, australia\n*correspondence:\ndanilo jimenez rezende, laboratory\nof cognitive neuroscience, school\nof life sciences, brain mind\ninstitute, ecole polytechnique\nf\u00e9d\u00e9rale de lausannne, epfl - lcn\naab 135 (b\u00e2timent aab) station 15,\nch-1015 lausanne, switzerland\ne-mail: rezende.danilo@gmail.com\n\nthe abil", "describing multimedia content using\n\nattention-based encoder\u2013decoder networks\n\nkyunghyun cho\u25e6, aaron courville\u25e6 and yoshua bengio\u25e6(cid:63)\n\n1\n\n5\n1\n0\n2\n\n \nl\nu\nj\n \n\n4\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n3\n5\n0\n1\n0\n\n.\n\n7\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014whereas deep neural networks were \ufb01rst mostly\nused for classi\ufb01cation tasks, they are rapidly expanding in the\nrealm of structured output problems, where the observed target\nis composed of multiple random variables that have a rich joint\ndistribution, given the input. we focus in this paper on the case\nwhere the input also has a rich structure and the input and output\nstructures are somehow related. we describe systems that learn\nto attend to different places in the input, for each element of the\noutput, for a variety of tasks: machine translation, image caption\ngeneration, video clip description and speech recognition. all\nthese systems are based on a shared set of building blocks: gated\nrecurrent neural networks and convolutional neural networks,\nalong", "evidence for a hierarchy of predictions and prediction\nerrors in human cortex\n\ncatherine wacongnea,b,c,1,2, etienne labyta,b,c,1, virginie van wassenhovea,b,c, tristan bekinschteind, lionel naccachee,f,\nand stanislas dehaenea,b,c,g,2\n\nacognitive neuroimaging unit, institut national de la sant\u00e9 et de la recherche m\u00e9dicale, u992, f-91191 gif/yvette, france; bneurospin center, institute of\nbioimaging commissariat \u00e0 l\u2019energie atomique, f-91191 gif/yvette, france; cuniversit\u00e9 paris 11, orsay, france; dcognition and brain sciences unit, medical\nresearch council, cambridge, united kingdom; einstitut du cerveau et de la moelle \u00e9pini\u00e8re research center, institut national de la sant\u00e9 et de la recherche\nm\u00e9dicale, u975 paris, france; ffacult\u00e9 de m\u00e9decine piti\u00e9-salp\u00eatri\u00e8re, universit\u00e9 paris 6, paris, france; and gcoll\u00e8ge de france, f-75005 paris, france\n\ncontributed by stanislas dehaene, october 30, 2011 (sent for review june 24, 2011)\n\naccording to hierarchical predictive coding models, the cortex", "on the di\ufb03culty of training recurrent neural networks\n\n3\n1\n0\n2\n\n \n\nb\ne\nf\n6\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n3\n6\n0\n5\n\n.\n\n1\n1\n2\n1\n:\nv\ni\nx\nr\na\n\nrazvan pascanu\nuniversite de montreal\n\ntomas mikolov\nbrno university\n\nyoshua bengio\nuniversite de montreal\n\nabstract\n\nthere are two widely known issues with prop-\nerly training recurrent neural networks, the\nvanishing and the exploding gradient prob-\nlems detailed in bengio et al. (1994).\nin\nthis paper we attempt to improve the under-\nstanding of the underlying issues by explor-\ning these problems from an analytical, a geo-\nmetric and a dynamical systems perspective.\nour analysis is used to justify a simple yet ef-\nfective solution. we propose a gradient norm\nclipping strategy to deal with exploding gra-\ndients and a soft constraint for the vanishing\ngradients problem. we validate empirically\nour hypothesis and proposed solutions in the\nexperimental section.\n\n1. introduction\n\na recurrent neural network (rnn), e.g. fig. 1, is a\nneural network mode", "mastering atari, go, chess and shogi by \nplanning with a learned model\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nreceived: 3 april 2020\naccepted: 7 october 2020\npublished online: 23 december 2020\n\n check for updates\n\njulian schrittwieser1,3, ioannis antonoglou1,2,3, thomas hubert1,3, karen simonyan1, \nlaurent sifre1, simon schmitt1, arthur guez1, edward lockhart1, demis hassabis1, \nthore graepel1,2, timothy lillicrap1 & david silver1,2,3\u2009\u2709\n\nconstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and go2, where a perfect \nsimulator is available. however, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. here we present the muzero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually ", "article\n\nhttps://doi.org/10.1038/s41467-023-36583-0\n\nabstract representations emerge naturally\nin neural networks trained to perform\nmultiple tasks\n\nreceived: 14 june 2022\n\naccepted: 7 february 2023\n\ncheck for updates\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\nw. jeffrey johnston 1,2 & stefano fusi\n\n1,2\n\nhumans and other animals demonstrate a remarkable ability to generalize\nknowledge across distinct contexts and objects during natural behavior. we\nposit that this ability to generalize arises from a speci\ufb01c representational\ngeometry, that we call abstract and that is referred to as disentangled in\nmachine learning. these abstract representations have been observed in\nrecent neurophysiological studies. however, it is unknown how they emerge.\nhere, using feedforward neural networks, we demonstrate that the learning of\nmultiple tasks causes abstract representations to emerge, using both super-\nvised and reinforcement learning. we show that these abstract representati", "9\n1\n0\n2\n\n \n\nv\no\nn\n0\n3\n\n \n\n \n \n]\nt\ni\n.\ns\nc\n[\n \n \n\n9\nv\n6\n3\n4\n2\n0\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nnonlinear information bottleneck\n\nartemy kolchinsky,1, \u2217 brendan d. tracey,1, 2 and david h. wolpert1, 3, 4\n\n1santa fe institute, 1399 hyde park road, santa fe, nm, 87501, usa\n\n2dept aeronautics & astronautics, massachusetts institute of technology, cambridge, ma 02139, usa\n\n3complexity science hub, vienna, austria\n\n4arizona state university, tempe, az 85287, usa\n\ninformation bottleneck (ib) is a technique for extracting information in one random\nvariable x that is relevant for predicting another random variable y . ib works by encoding\nx in a compressed \u201cbottleneck\u201d random variable m from which y can be accurately decoded.\nhowever, \ufb01nding the optimal bottleneck variable involves a di\ufb03cult optimization problem,\nwhich until recently has been considered for only two limited cases: discrete x and y with\nsmall state spaces, and continuous x and y with a gaussian joint distribution (in which case\noptimal", "published as a conference paper at iclr 2022\n\nexposing the\nbehind masked\nmetropolis\u2013hastings\n\nimplicit energy networks\nvia\n\nlanguage models\n\n2\n2\n0\n2\n\n \nr\na\n\n \n\nm\n5\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n3\n7\n2\n0\n\n.\n\n6\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nkartik goyal1, chris dyer2, taylor berg-kirkpatrick3\n1carnegie mellon university, 2deepmind, 3uc san diego\nkartikgo@ttic.edu, cdyer@google.com, tberg@eng.ucsd.edu\n\nabstract\n\nwhile recent work has shown that scores from models trained by the ubiquitous\nmasked language modeling (mlm) objective effectively discriminate probable\nfrom improbable sequences, it is still an open question if these mlms specify\na principled probability distribution over the space of possible sequences.\nin\nthis paper, we interpret mlms as energy-based sequence models and propose\ntwo energy parametrizations derivable from the trained mlms. in order to draw\nsamples correctly from these models, we develop a tractable sampling scheme based\non the metropolis\u2013hastings monte carlo algorithm. in", "important gains from supervised fine-tuning of\n\ndeep architectures on large labeled sets\n\npascal lamblin\n\nuniversity of montreal\n\nlamblinp@iro.umontreal.ca\n\nyoshua bengio\n\nuniversity of montreal\n\nyoshua.bengio@umontreal.ca\n\nabstract\n\nthe declared goal of many investigations of deep learning algorithms is to ex-\nploit unsupervised learning algorithms to discover useful representations of the\ndata. but how useful are the representations discovered by the current learning\nalgorithms? this is often measured by exploiting these representations for a clas-\nsi\ufb01cation task. two of the main approaches that have been explored are (1) to\ndirectly feed these representations as input to classi\ufb01ers trained on mapping them\nto classes of interest, and (2), to initialize a deep supervised neural network with\nthe parameters found in the unsupervised pre-training stage, and perform super-\nvised \ufb01ne-tuning to improve not only the added classi\ufb01cation layers but also the\npre-trained layers. the objective of", "a survey of methods\n\nfor explaining black box models\n\nriccardo guidotti1,2, anna monreale1, salvatore ruggieri1, franco turini1,\n\ndino pedreschi1, fosca giannotti2\n\n1 university of pisa, {name.surname}@di.unipi.it\n2 isti-cnr, pisa, {name.surname}@isti.cnr.it\n\nabstract. in the last years many accurate decision support systems\nhave been constructed as black boxes, that is as systems that hide their\ninternal logic to the user. this lack of explanation constitutes both a\npractical and an ethical issue. the literature reports many approaches\naimed at overcoming this crucial weakness sometimes at the cost of scar-\nifying accuracy for interpretability. the applications in which black box\ndecision systems can be used are various, and each approach is typi-\ncally developed to provide a solution for a speci\ufb01c problem and, as a\nconsequence, delineating explicitly or implicitly its own de\ufb01nition of in-\nterpretability and explanation. the aim of this paper is to provide a\nclassi\ufb01cation of the main ", "vol 441|11 may 2006|doi:10.1038/nature04676\n\nletters\n\nneurons in the orbitofrontal cortex encode\neconomic value\ncamillo padoa-schioppa1 & john a. assad1\n\neconomic choice is the behaviour observed when individuals\nselect one among many available options. there is no intrinsically\n\u2018correct\u2019 answer: economic choice depends on subjective prefer-\nences. this behaviour is traditionally the object of economic\nanalysis1 and is also of primary interest in psychology2. however,\nthe underlying mental processes and neuronal mechanisms are\nnot well understood. theories of human and animal choice1\u20133 have\na cornerstone in the concept of \u2018value\u2019. consider, for example, a\nmonkey offered one raisin versus one piece of apple: behavioural\nevidence suggests that the animal chooses by assigning values to\nthe two options4. but where and how values are represented in the\nbrain is unclear. here we show that, during economic choice,\nneurons in the orbitofrontal cortex5\u201318 (ofc) encode the value of\noffered and c", "svcca: singular vector canonical correlation\n\nanalysis for deep learning dynamics and\n\ninterpretability\n\n7\n1\n0\n2\n\n \n\nv\no\nn\n8\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n6\n0\n8\n5\n0\n\n.\n\n6\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nmaithra raghu,1,2 justin gilmer,1 jason yosinski,3 & jascha sohl-dickstein1\n\n1google brain 2cornell university 3uber ai labs\n\nmaithrar gmail com, gilmer google com, yosinski uber com, jaschasd google com\n\nabstract\n\nwe propose a new technique, singular vector canonical correlation analysis\n(svcca), a tool for quickly comparing two representations in a way that is both\ninvariant to af\ufb01ne transform (allowing comparison between different layers and\nnetworks) and fast to compute (allowing more comparisons to be calculated than\nwith previous methods). we deploy this tool to measure the intrinsic dimension-\nality of layers, showing in some cases needless over-parameterization; to probe\nlearning dynamics throughout training, \ufb01nding that networks converge to \ufb01nal\nrepresentations from the bottom up; to ", "review\n\ntrends in neurosciences vol.30 no.5\n\ndopamine-mediated regulation of\ncorticostriatal synaptic plasticity\npaolo calabresi1,2, barbara picconi2, alessandro tozzi1,2\nand massimiliano di filippo1,2\n\n1 clinica neurologica, universita` di perugia, ospedale s. maria della misericordia, via s. andrea delle fratte, 06156, perugia, italy\n2 irccs fondazione santa lucia c/o cerc, via del fosso di fiorano, 00143, rome, italy\n\nthe striatum represents the main input into the basal\nganglia. neurons projecting from the striatum receive a\nlarge convergence of afferents from all areas of the\ncortex and transmit neural\ninformation to the basal\nganglia output structures. corticostriatal transmission\nis essential in the regulation of voluntary movement, in\naddition to behavioural control, cognitive function and\nreward mechanisms. long-term potentiation (ltp) and\nlong-term depression (ltd), the two main forms of\nsynaptic plasticity, are both represented at corticostria-\ntal synapses and strongly depe", "0\n2\n0\n2\n\n \n\ny\na\nm\n2\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\nv\n1\n8\n1\n2\n0\n\n.\n\n5\n0\n0\n2\n:\nv\ni\nx\nr\na\n\na neural network walks into a lab: towards using deep\n\nnets as models for human behavior\n\nwei ji ma and benjamin peters\n\nmay 6, 2020\n\nabstract\n\nwhat might sound like the beginning of a joke has become an attractive prospect for\nmany cognitive scientists: the use of deep neural network models (dnns) as models\nof human behavior in perceptual and cognitive tasks. although dnns have taken\nover machine learning, attempts to use them as models of human behavior are still in\nthe early stages. can they become a versatile model class in the cognitive scientist\u2019s\ntoolbox? we \ufb01rst argue why dnns have the potential to be interesting models of\nhuman behavior. we then discuss how that potential can be more fully realized. on\nthe one hand, we argue that the cycle of training, testing, and revising dnns needs\nto be revisited through the lens of the cognitive scientist\u2019s goals. speci\ufb01cally, we\nargue that methods for ", "neuroimage 52 (2010) 833\u2013847\n\ncontents lists available at sciencedirect\n\nneuroimage\n\nj o u r n a l h o m e p a g e : w w w. e l s e v i e r. c o m / l o c a t e / y n i m g\n\nattractor concretion as a mechanism for the formation of context representations\nmattia rigotti a,b, daniel ben dayan rubin a,b, sara e. morrison a, c. daniel salzman a,c,d,e,f,g, stefano fusi a,b,\u204e\na department of neuroscience, columbia university college of physicians and surgeons, new york, ny 10032-2695, usa\nb institute for neuroinformatics, university of z\u00fcrich, and eth z\u00fcrich, 8057 z\u00fcrich, switzerland\nc department of psychiatry, columbia university medical center, new york, ny 10032, usa\nd w. m. keck center on brain plasticity and cognition, columbia university medical center, new york, ny 10032, usa\ne kavli institute for brain sciences, columbia university medical center, new york, ny 10032, usa\nf mahoney center for brain and behavior, columbia university medical center, new york, ny 10032, usa\ng new york st", "article\n\ndoi: 10.1038/s41467-018-05873-3\n\nopen\n\nreconciling persistent and dynamic hypotheses\nof working memory coding in prefrontal cortex\n1,4,5 & steven w. kennerley\n\n1, john p. towers1, joni d. wallis2,3, laurence t. hunt\n\nsean e. cavanagh\n\n1,2,3\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\ncompeting accounts propose that working memory (wm) is subserved either by persistent\nactivity in single neurons or by dynamic (time-varying) activity across a neural population.\nhere, we compare these hypotheses across four regions of prefrontal cortex (pfc) in an\noculomotor-delayed-response task, where an intervening cue indicated the reward available\nfor a correct saccade. wm representations were strongest in ventrolateral pfc neurons with\nhigher intrinsic temporal stability (time-constant). at the population-level, although a stable\nmnemonic state was reached during the delay, this tuning geometry was reversed relative to\ncue-period selectivity, and was disrupted by the reward cue. single-neuron analysis ", "information dropout: learning optimal\n\nrepresentations through noisy computation\n\n1\n\nalessandro achille and stefano soatto\n\ndepartment of computer science\nuniversity of california, los angeles\n\n405 hilgard ave, los angeles, 90095, ca, usa\n\nemail: {achille,soatto}@cs.ucla.edu\n\n7\n1\n0\n2\n\n \n\nb\ne\nf\n2\n1\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n3\n5\n3\n1\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014the cross-entropy loss commonly used in deep learning is\nclosely related to the de\ufb01ning properties of optimal representations, but\ndoes not enforce some of the key properties. we show that this can be\nsolved by adding a regularization term, which is in turn related to injecting\nmultiplicative noise in the activations of a deep neural network, a special\ncase of which is the common practice of dropout. we show that our\nregularized loss function can be ef\ufb01ciently minimized using information\ndropout, a generalization of dropout rooted in information theoretic\nprinciples that automatically adapts to the data and can be", "i\n\ne\nc\nn\ne\nc\ns\no\nr\nu\ne\nn\ne\nr\nu\nt\na\nn\nm\no\nc\n\n/\n\n.\n\ne\nr\nu\nt\na\nn\nw\nw\nw\n\n.\n\n/\n/\n:\np\nt\nt\nh\n \n \n\n \n\np\nu\no\nr\ng\ng\nn\nh\ns\ni\nl\n\ni\n\nb\nu\np\n \ne\nr\nu\nt\na\nn\n \n5\n0\n0\n2\n\u00a9\n\na r t i c l e s\n\ncomputation and systems\n\nuncertainty-based competition between prefrontal and\ndorsolateral striatal systems for behavioral control\nnathaniel d daw1, yael niv1,2 & peter dayan1\n\na broad range of neural and behavioral data suggests that the brain contains multiple systems for behavioral choice, including\none associated with prefrontal cortex and another with dorsolateral striatum. however, such a surfeit of control raises an additional\nchoice problem: how to arbitrate between the systems when they disagree. here, we consider dual-action choice systems from a\nnormative perspective, using the computational theory of reinforcement learning. we identify a key trade-off pitting computational\nsimplicity against the \ufb02exible and statistically ef\ufb01cient use of experience. the trade-off is realized in a competition between the\ndors", "9\n1\n0\n2\n\n \n\nb\ne\nf\n4\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n2\n3\n0\n2\n0\n\n.\n\n0\n1\n8\n1\n:\nv\ni\nx\nr\na\n\ngradient descent aligns the layers of deep linear networks\n\nziwei ji\nmatus telgarsky\n{ziweiji2,mjt}@illinois.edu\n\nuniversity of illinois, urbana-champaign\n\nabstract\n\nthis paper establishes risk convergence and asymptotic weight matrix alignment \u2014 a form of implicit\nregularization \u2014 of gradient \ufb02ow and gradient descent when applied to deep linear networks on linearly\nseparable data. in more detail, for gradient \ufb02ow applied to strictly decreasing loss functions (with similar\nresults for gradient descent with particular decreasing step sizes): (i) the risk converges to 0; (ii) the\nnormalized ith weight matrix asymptotically equals its rank-1 approximation uiv(cid:62)\ni ; (iii) these rank-1\ni+1ui| \u2192 1. in the case of the logistic loss (binary cross\nmatrices are aligned across layers, meaning |v(cid:62)\nentropy), more can be said: the linear function induced by the network \u2014 the product of its weight\nma", "quasi-experimental causality in neuroscience  \nand behavioural research\n\nioana\u00a0e.\u00a0marinescu1*, patrick\u00a0n.\u00a0lawlor2 and konrad\u00a0p.\u00a0kording\u200a\n\n\u200a3,4\n\nin many scientific domains, causality is the key question. for example, in neuroscience, we might ask whether a medication \naffects perception, cognition or action. randomized controlled trials are the gold standard to establish causality, but they are \nnot always practical. the field of empirical economics has developed rigorous methods to establish causality even when ran-\ndomized controlled trials are not available. here we review these quasi-experimental methods and highlight how neuroscience \nand behavioural researchers can use them to do research that can credibly demonstrate causal effects.\n\nbehavioural research asks a broad range of questions, and most \n\nof them are of a causal nature1. when we ask how a drug affects \na patient, we want to know its causal effect: does it make the \npatient better? we do not want to ask the correlational ", "short report\n\nmidbrain dopamine neurons compute\ninferred and cached value prediction\nerrors in a common framework\nbrian f sadacca1*, joshua l jones1, geoffrey schoenbaum1,2,3*\n\n1intramural research program of the national institute on drug abuse, national\ninstitutes of health, bethesda, united states; 2department of anatomy and\nneurobiology, university of maryland school of medicine, baltimore, united states;\n3department of neuroscience, johns hopkins school of medicine, baltimore, united\nstates\n\nabstract midbrain dopamine neurons have been proposed to signal reward prediction errors as\ndefined in temporal difference (td) learning algorithms. while these models have been extremely\npowerful in interpreting dopamine activity, they typically do not use value derived through\ninference in computing errors. this is important because much real world behavior \u2013 and thus many\nopportunities for error-driven learning \u2013 is based on such predictions. here, we show that error-\nsignaling rat dopamine", "neural networks 19 (2006) 1153\u20131160\n\n2006 special issue\n\nwww.elsevier.com/locate/neunet\n\nthe misbehavior of value and the discipline of the will\n\npeter dayana,\u2217, yael niva,b, ben seymourc, nathaniel d. dawa\n\na gatsby computational neuroscience unit, ucl, 17 queen square, london wc1n 3ar, united kingdom\n\nb icnc, hebrew university, po box 1255, jerusalem 91904, israel\n\nc wellcome department of imaging neuroscience, ucl, 12 queen square, london wc1n 3bg, united kingdom\n\nreceived 31 october 2005; accepted 30 march 2006\n\nabstract\n\nmost reinforcement learning models of animal conditioning operate under the convenient, though \ufb01ctive, assumption that pavlovian\nconditioning concerns prediction learning whereas instrumental conditioning concerns action learning. however, it is only through pavlovian\nresponses that pavlovian prediction learning is evident, and these responses can act against the instrumental interests of the subjects. this can be\nseen in both experimental and natural circumstance", "2\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n6\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n3\n7\n5\n0\n0\n\n.\n\n3\n0\n2\n2\n:\nv\ni\nx\nr\na\n\ncontrasting random and learned features in deep bayesian linear regression\n\njacob a. zavatone-veth,1, 2, \u2217 william l. tong,3, \u2020 and cengiz pehlevan3, 2, \u2021\n1department of physics, harvard university, cambridge, massachusetts 02138, usa\n2center for brain science, harvard university, cambridge, massachusetts 02138, usa\n\n3john a. paulson school of engineering and applied sciences,\n\nharvard university, cambridge, massachusetts 02138, usa\n\n(dated: june 17, 2022)\n\nunderstanding how feature learning a\ufb00ects generalization is among the foremost goals of modern\ndeep learning theory. here, we study how the ability to learn representations a\ufb00ects the generalization\nperformance of a simple class of models: deep bayesian linear neural networks trained on unstructured\ngaussian data. by comparing deep random feature models to deep networks in which all layers are\ntrained, we provide a detailed characterization of t", "behavioral and brain sciences\n\nsuboptimality in perceptual decision making\n\ncambridge.org/bbs\n\ndobromir rahneva and rachel n. denisonb\n\ntarget article\n\nauthors d. rahnev and r. n. denison\ncontributed equally to this work.\n\ncite this article: rahnev d, denison rn. (2018)\nsuboptimality in perceptual decision making.\nbehavioral and brain sciences 41, e223: 1\u201366.\ndoi:10.1017/s0140525x18000936\n\ntarget article accepted: 9 february 2018\ntarget article manuscript online: 27 february\n2018\ncommentaries accepted: 5 june 2018\n\nkeywords:\nbayesian decision theory; cue combination;\nmodeling; optimality; perceptual decision\nmaking; suboptimality; uncertainty; vision\n\nwhat is open peer commentary? what\nfollows on these pages is known as a\ntreatment, in which a significant and\ncontroversial target article is published\nalong with commentaries (p. 17) and an\nauthor\u2019s response (p. 46). see bbsonline.\norg for more information.\n\n\u00a9 cambridge university press 2018\n\naschool of psychology, georgia institute of t", "journal of machine learning research 16 (2015) 2859-2900\n\nsubmitted 5/14; revised 3/15; published 12/15\n\nlinear dimensionality reduction:\n\nsurvey, insights, and generalizations\n\njohn p. cunningham\ndepartment of statistics\ncolumbia university\nnew york city, usa\n\nzoubin ghahramani\ndepartment of engineering\nuniversity of cambridge\ncambridge, uk\n\neditor: gert lanckriet\n\njpc2181@columbia.edu\n\nzoubin@eng.cam.ac.uk\n\nabstract\n\nlinear dimensionality reduction methods are a cornerstone of analyzing high dimensional\ndata, due to their simple geometric interpretations and typically attractive computational\nproperties. these methods capture many data features of interest, such as covariance, dy-\nnamical structure, correlation between data sets, input-output relationships, and margin\nbetween data classes. methods have been developed with a variety of names and motiva-\ntions in many \ufb01elds, and perhaps as a result the connections between all these methods\nhave not been highlighted. here we survey meth", "spontaneous cortical activity reveals hallmarks of an optimal\ninternal model of the environment\npietro berkes\n331\nscience\ndoi: 10.1126/science.1195870\n\n, et al.\n, 83 (2011);\n\n \n\nthis copy is for your personal, non-commercial use only.\n \n\nif you wish to distribute this article to others\ncolleagues, clients, or customers by \n\nclicking here.\n \n\n, you can order high-quality copies for your\n\n can be obtained by\n\n \nhere.\n\npermission to republish or repurpose articles or portions of articles\nfollowing the guidelines \n \nthe following resources related to this article are available online at\nwww.sciencemag.org (this infomation is current as of\n \n \nupdated information and services, \nversion of this article at: \nhttp://www.sciencemag.org/content/331/6013/83.full.html\n \n\njanuary 21, 2011\n\n ): \n\nsupporting online material \nhttp://www.sciencemag.org/content/suppl/2011/01/04/331.6013.83.dc1.html\n \n\ncan be found at: \n\nthis article \nhttp://www.sciencemag.org/content/331/6013/83.full.html#ref-list-1\n \n\n", "8\n1\n0\n2\n\n \nr\na\n\n \n\nm\n1\n2\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n8\n2\n7\n7\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2018\n\nunsupervised representation learning by pre-\ndicting image rotations\n\nspyros gidaris, praveer singh, nikos komodakis\nuniversity paris-est, ligm\necole des ponts paristech\n{spyros.gidaris,praveer.singh,nikos.komodakis}@enpc.fr\n\nabstract\n\nover the last years, deep convolutional neural networks (convnets) have trans-\nformed the \ufb01eld of computer vision thanks to their unparalleled capacity to learn\nhigh level semantic image features. however, in order to successfully learn those\nfeatures, they usually require massive amounts of manually labeled data, which\nis both expensive and impractical to scale. therefore, unsupervised semantic fea-\nture learning, i.e., learning without requiring manual annotation effort, is of crucial\nimportance in order to successfully harvest the vast amount of visual data that are\navailable today. in our work we propose to learn imag", "a r t i c l e s\n\ntemporal context calibrates interval timing\nmehrdad jazayeri1,2 & michael n shadlen2\n\nwe use our sense of time to identify temporal relationships between events and to anticipate actions. the degree to which we \ncan exploit temporal contingencies depends on the variability of our measurements of time. we asked humans to reproduce time \nintervals drawn from different underlying distributions. as expected, production times were more variable for longer intervals. \nhowever, production times exhibited a systematic regression toward the mean. consequently, estimates for a sample interval \ndiffered depending on the distribution from which it was drawn. a performance-optimizing bayesian model that takes the \nunderlying distribution of samples into account provided an accurate description of subjects\u2019 performance, variability and bias. \nthis finding suggests that the cns incorporates knowledge about temporal uncertainty to adapt internal timing mechanisms to \nthe temporal stat", "tools and resources\n\ninterpreting wide-band neural activity\nusing convolutional neural networks\nmarkus frey1,2*, sander tanni3, catherine perrodin4, alice o\u2019leary3,\nmatthias nau1,2, jack kelly5, andrea banino6, daniel bendor4, julie lefort3,\nchristian f doeller1,2,7\u2020, caswell barry3\u2020*\n\n1kavli institute for systems neuroscience, centre for neural computation, the egil\nand pauline braathen and fred kavli centre for cortical microcircuits, ntnu,\nnorwegian university of science and technology, trondheim, norway; 2max-planck-\ninsitute for human cognitive and brain sciences, leipzig, germany; 3cell &\ndevelopmental biology, ucl, london, united kingdom; 4institute of behavioural\nneuroscience, ucl, london, united kingdom; 5open climate fix, london, united\nkingdom; 6deepmind, london, united kingdom; 7institute of psychology, leipzig\nuniversity, leipzig, germany\n\nabstract rapid progress in technologies such as calcium imaging and electrophysiology has\nseen a dramatic increase in the size and exte", "0\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n2\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n3\n2\n7\n0\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\na practical sparse approximation for\n\nreal time recurrent learning\n\njacob menick\u2217\n\ndeepmind\n\nerich elsen\u2217\ndeepmind\n\nutku evci\ngoogle\n\nuniversity college london\n\nsimon osindero\n\nkaren simonyan\n\ndeepmind\n{jmenick, eriche, evcu, osindero, simonyan, gravesa}@google.com\n\ndeepmind\n\nalex graves\ndeepmind\n\nabstract\n\ncurrent methods for training recurrent neural networks are based on backpropagation through time,\nwhich requires storing a complete history of network states, and prohibits updating the weights \u2018online\u2019\n(after every timestep). real time recurrent learning (rtrl) eliminates the need for history storage and\nallows for online weight updates, but does so at the expense of computational costs that are quartic in the\nstate size. this renders rtrl training intractable for all but the smallest networks, even ones that are\nmade highly sparse. we introduce the sparse n-step approximation (snap) to the rtr", "2\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n3\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n7\n5\n0\n0\n\n.\n\n7\n0\n2\n1\n:\nv\ni\nx\nr\na\n\nimplicit density estimation by local moment\n\nmatching to sample from auto-encoders\n\nyoshua bengio, guillaume alain, and salah rifai\n\ndepartment of computer science and operations research\n\nuniversity of montreal\n\nmontreal, h3c 3j7\n\njuly 3, 2012\n\nabstract\n\nrecent work suggests that some auto-encoder variants do a good job of cap-\nturing the local manifold structure of the unknown data generating density. this\npaper contributes to the mathematical understanding of this phenomenon and helps\nde\ufb01ne better justi\ufb01ed sampling algorithms for deep learning based on auto-encoder\nvariants. we consider an mcmc where each step samples from a gaussian whose\nmean and covariance matrix depend on the previous state, de\ufb01nes through its\nasymptotic distribution a target density. first, we show that good choices (in the\nsense of consistency) for these mean and covariance functions are the local ex-\npected value and local ", ".\n\nd\ne\nv\nr\ne\ns\ne\nr\n \ns\nt\nh\ng\ni\nr\n \nl\nl\n\na\n\n \n.\n\ne\nr\nu\nt\na\nn\n \nr\ne\ng\nn\ni\nr\np\ns\n\n \nf\no\n \nt\nr\na\np\n\n \n,\n.\n\nc\nn\n\ni\n \n,\n\na\nc\n\n \n\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n8\n1\n0\n2\n \n\u00a9\n\n \n\na r t i c l e s\n\nthe hippocampus as a predictive map\n  & samuel j gershman4 \nkimberly l stachenfeld1,2, matthew m botvinick1,3 \n\n \n\na cognitive map has long been the dominant metaphor for hippocampal function, embracing the idea that place cells encode a \ngeometric representation of space. however, evidence for predictive coding, reward sensitivity and policy dependence in place \ncells suggests that the representation is not purely spatial. we approach this puzzle from a reinforcement learning perspective: \nwhat kind of spatial representation is most useful for maximizing future reward? we show that the answer takes the form of a \npredictive representation. this representation captures many aspects of place cell responses that fall outside the traditional \nview of a cognitive map. furthermore, we argue that entorhinal grid cell", "8\n1\n0\n2\n\n \n\nb\ne\nf\n3\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n4\nv\n8\n9\n1\n1\n1\n\n.\n\n0\n1\n7\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2018\n\naction-dependent control variates for pol-\nicy optimization via stein\u2019s identity\n\nhao liu\u2217\ncomputer science\nuestc\nchengdu, china\nuestcliuhao@gmail.com\n\nyihao feng\u2217\ncomputer science\nuniversity of texas at austin\naustin, tx, 78712\nyihao@cs.utexas.edu\n\nyi mao\nmicrosoft\nredmond, wa, 98052\nmaoyi@microsoft.com\n\ndengyong zhou\ngoogle\nkirkland, wa, 98033\ndennyzhou@google.com\n\njian peng\ncomputer science\nuiuc\nurbana, il 61801\njianpeng@illinois.edu\n\nqiang liu\ncomputer science\nuniversity of texas at austin\naustin, tx, 78712\nlqiang@cs.utexas.edu\n\nabstract\n\npolicy gradient methods have achieved remarkable successes in solving challeng-\ning reinforcement learning problems. however, it still often suffers from the large\nvariance issue on policy gradient estimation, which leads to poor sample ef\ufb01ciency\nduring training. in this work, we propose a control variate metho", "reducing spike train variability:\n\na computational theory of\n\nspike-timing dependent plasticity\n\nsander m. bohte1,2\n\ns.m.bohte@cwi.nl\n\nmichael c. mozer2\nmozer@cs.colorado.edu\n\n1dept. software engineering\n\n2dept. of computer science\n\ncwi, amsterdam, the netherlands\n\nuniversity of colorado, boulder, usa\n\nabstract\n\nexperimental studies have observed synaptic potentiation when a\npresynaptic neuron \ufb01res shortly before a postsynaptic neuron, and\nsynaptic depression when the presynaptic neuron \ufb01res shortly af-\nter. the dependence of synaptic modulation on the precise tim-\ning of the two action potentials is known as spike-timing depen-\ndent plasticity or stdp. we derive stdp from a simple compu-\ntational principle: synapses adapt so as to minimize the postsy-\nnaptic neuron\u2019s variability to a given presynaptic input, causing\nthe neuron\u2019s output to become more reliable in the face of noise.\nusing an entropy-minimization objective function and the biophys-\nically realistic spike-response model o", "6\n1\n0\n2\n\n \n\nb\ne\nf\n4\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n3\nv\n5\n2\n0\n2\n0\n\n.\n\n6\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nspatial transformer networks\n\nmax jaderberg\n\nkaren simonyan\n\nandrew zisserman\n\nkoray kavukcuoglu\n\ngoogle deepmind, london, uk\n\n{jaderberg,simonyan,zisserman,korayk}@google.com\n\nabstract\n\nconvolutional neural networks de\ufb01ne an exceptionally powerful class of models,\nbut are still limited by the lack of ability to be spatially invariant to the input data\nin a computationally and parameter ef\ufb01cient manner. in this work we introduce a\nnew learnable module, the spatial transformer, which explicitly allows the spa-\ntial manipulation of data within the network. this differentiable module can be\ninserted into existing convolutional architectures, giving neural networks the abil-\nity to actively spatially transform feature maps, conditional on the feature map\nitself, without any extra training supervision or modi\ufb01cation to the optimisation\nprocess. we show that the use of spatial transformers results in model", "tilburg centre for creative computing\ntilburg university\nhttp://www.uvt.nl/ticc\n\np.o. box 90153\n5000 le tilburg, the netherlands\nemail: ticc@uvt.nl\n\ncopyright c(cid:13) laurens van der maaten, eric postma, and jaap van den herik 2009.\n\noctober 26, 2009\n\nticc tr 2009\u2013005\n\ndimensionality reduction: a comparative\n\nreview\n\nlaurens van der maaten\n\neric postma\n\njaap van den herik\n\nticc, tilburg university\n\nabstract\n\nin recent years, a variety of nonlinear dimensionality reduction techniques have been\nproposed that aim to address the limitations of traditional techniques such as pca\nand classical scaling. the paper presents a review and systematic comparison of\nthese techniques. the performances of the nonlinear techniques are investigated on\narti\ufb01cial and natural tasks. the results of the experiments reveal that nonlinear tech-\nniques perform well on selected arti\ufb01cial tasks, but that this strong performance does\nnot necessarily extend to real-world tasks. the paper explains these results by", "journal of statistical mechanics:theory and experiment     paperasymptotics of representation learning in finitebayesian neural networks*to cite this article: jacob a zavatone-veth et al j. stat. mech. (2022) 114008 view the article online for updates and enhancements.you may also likethe axial anomaly and consistency of thefinite-width light-cone local duality sum rulefor the form factor of the transition * *  0minghai li, ze-kun guo and jueping liu-quantum transport through the edgestates of zigzag phosphorene nanoribbonsin presence of a single point defect:analytic green\u2019s function methodm amini and m soltani-detailed analysis and simulationverification for reconstruction of plasmaoptical boundary with spectrometrictechnique on hl-2m tokamakzhihao su,  , jinming gao et al.-this content was downloaded from ip address 98.59.161.237 on 27/09/2023 at 05:24\f", " \n \n \n \noriginal citation: \nsanborn, adam n. and silva, ricardo. (2013) constraining bridges between levels of analysis : \na computational justification for locally bayesian learning. journal of mathematical \npsychology, volume 57 (number 3-4). pp. 94-106. \n \npermanent wrap url: \nhttp://wrap.warwick.ac.uk/57365  \n \ncopyright and reuse: \nthe warwick research archive portal (wrap) makes this work by researchers of the \nuniversity of warwick available open access under the following conditions.  copyright \u00a9 \nand all moral rights to the version of the paper presented here belong to the individual \nauthor(s) and/or other copyright owners.  to the extent reasonable and practicable the \nmaterial made available in wrap has been checked for eligibility before being made \navailable. \n \ncopies of full items can be used for personal research or study, educational, or not-for-profit \npurposes without prior permission or charge.  provided that the authors, title and full \nbibliographic details are c", "policy  gradient  methods for \n\nreinforcement  learning with function \n\napproximation \n\nrichard s.  sutton, david mcallester, satinder singh, yishay mansour \n\nat&t labs - research,  180 park avenue,  florham park,  nj 07932 \n\nabstract \n\nfunction  approximation  is  essential  to  reinforcement  learning,  but \nthe standard approach of approximating a  value function and deter \nmining  a  policy  from  it  has so  far  proven theoretically  intractable. \nin this paper we explore an alternative approach in which the policy \nis explicitly represented by its own function approximator,  indepen \ndent of the value function,  and is  updated according to the gradient \nof expected reward with respect to the policy parameters.  williams's \nreinforce method and actor-critic methods are examples of this \napproach.  our  main  new  result  is  to  show  that  the  gradient  can \nbe  written  in  a  form  suitable  for  estimation  from  experience  aided \nby  an  approximate  action-value  or  adv", "r e v i e w s\n\nthe organization of recent\nand remote memories\n\npaul w. frankland*\u2021 and bruno bontempi\u00a7\n\nabstract | a fundamental question in memory research is how our brains can form enduring\nmemories. in humans, memories of everyday life depend initially on the medial temporal lobe\nsystem, including the hippocampus. as these memories mature, they are thought to become\nincreasingly dependent on other brain regions such as the cortex. little is understood about how\nnew memories in the hippocampus are transformed into remote memories in cortical networks.\nhowever, recent studies have begun to shed light on how remote memories are organized in the\ncortex, and the molecular and cellular events that underlie their consolidation.\n\nour memories of everyday life \u2014 of people, places and\nevents \u2014 define who we are1. however, these records of\nlife experience are not formed instantaneously. rather,\nnew memories are gradually transformed from an\ninitially labile state (in which they are vulnerable", "6\n1\n0\n2\n\n \nc\ne\nd\n9\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n4\n1\n2\n3\n0\n\n.\n\n2\n1\n6\n1\n:\nv\ni\nx\nr\na\n\ntowards deep learning with spiking neurons in energy\n\nbased models with contrastive hebbian plasticity\n\nthomas mesnard\n\nwulfram gerstner\n\n\u00e9cole normale sup\u00e9rieure\n\nparis, france\n\nthomas.mesnard@ens.fr\n\n\u00e9cole polytechnique f\u00e9d\u00e9ral de lausanne\n\n1015 lausanne, switzerland\n\nwulfram.gerstner@epfl.ch\n\njohanni brea\n\n\u00e9cole polytechnique f\u00e9d\u00e9ral de lausanne\n\n1015 lausanne, switzerland\njohanni.brea@epfl.ch\n\nabstract\n\nin machine learning, error back-propagation in multi-layer neural networks (deep\nlearning) has been impressively successful in supervised and reinforcement learning\ntasks. as a model for learning in the brain, however, deep learning has long been\nregarded as implausible, since it relies in its basic form on a non-local plasticity rule.\nto overcome this problem, energy-based models with local contrastive hebbian\nlearning were proposed and tested on a classi\ufb01cation task with networks of rate\nneurons.", "the chemical brain hypothesis for\nthe origin of nervous systems\n\nroyalsocietypublishing.org/journal/rstb\n\ng\u00e1sp\u00e1r j\u00e9kely\n\nresearch\n\ncite this article: j\u00e9kely g. 2021 the chemical\nbrain hypothesis for the origin of nervous\nsystems. phil. trans. r. soc. b 376: 20190761.\nhttps://doi.org/10.1098/rstb.2019.0761\n\naccepted: 25 november 2020\n\none contribution of 10 to a theme issue \u2018basal\ncognition: multicellularity, neurons and the\ncognitive lens\u2019.\n\nsubject areas:\nevolution, neuroscience, theoretical biology\n\nkeywords:\nnervous system evolution, neuropeptide,\nplacozoa, sponge, ctenophore, cnidaria\n\nauthor for correspondence:\ng\u00e1sp\u00e1r j\u00e9kely\ne-mail: g.jekely@exeter.ac.uk\n\nliving systems institute, university of exeter, stocker road, exeter ex4 4qd, uk\n\ngj, 0000-0001-8496-9836\n\nin nervous systems, there are two main modes of transmission for the propa-\ngation of activity between cells. synaptic transmission relies on close contact\nat chemical or electrical synapses while volume transmission is medi", "6\n1\n0\n2\n\n \n\ny\na\nm\n0\n2\n\n \n\n \n \n]\n\n.\n\nc\no\nh\nt\na\nm\n\n[\n \n \n\n3\nv\n2\n4\n6\n5\n0\n\n.\n\n3\n0\n6\n1\n:\nv\ni\nx\nr\na\n\noptimalblack-boxreductionsbetweenoptimizationobjectiveszeyuanallen-zhuzeyuan@csail.mit.eduprincetonuniversityeladhazanehazan@cs.princeton.eduprincetonuniversity\ufb01rstcirculatedonfebruary5,2016\u2217abstractthediverseworldofmachinelearningapplicationshasgivenrisetoaplethoraofalgorithmsandoptimizationmethods,\ufb01nelytunedtothespeci\ufb01cregressionorclassi\ufb01cationtaskathand.wereducethecomplexityofalgorithmdesignformachinelearningbyreductions:wedevelopreductionsthattakeamethoddevelopedforonesettingandapplyittotheentirespectrumofsmoothnessandstrong-convexityinapplications.furthermore,unlikeexistingresults,ournewreductionsareoptimalandmorepractical.weshowhowthesenewreductionsgiverisetonewandfasterrunningtimesontraininglinearclassi\ufb01ersforvariousfamiliesoflossfunctions,andconcludewithexperimentsshowingtheirsuccessesalsoinpractice.1introductionthebasicmachinelearningproblemofminimizingaregularizerplusalossfunctionco", "scaling limits of wide neural networks with weight sharing:\n\ngaussian process behavior, gradient independence, and neural tangent\n\nkernel derivation\n\n0\n2\n0\n2\n\n \nr\np\na\n4\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n0\n6\n7\n4\n0\n\n.\n\n2\n0\n9\n1\n:\nv\ni\nx\nr\na\n\ngreg yang 1\n\nabstract\n\nseveral recent trends in machine learning theory and practice, from the design of state-of-the-art gaussian process\nto the convergence analysis of deep neural nets (dnns) under stochastic gradient descent (sgd), have found\nit fruitful to study wide random neural networks. central to these approaches are certain scaling limits of such\nnetworks. we unify these results by introducing a notion of a straightline tensor program that can express most\nneural network computations, and we characterize its scaling limit when its tensors are large and randomized. from\nour framework follows 1. the convergence of random neural networks to gaussian processes for architectures such\nas recurrent neural networks, convolutional neural networks, residu", "machine  learning,  20, 273-297 (1995)\n\u00a9  1995 kluwer academic  publishers, boston.  manufactured in the  netherlands.\n\nsupport-vector  networks\n\ncorinna cortes \nvladimir vapnik \nat&t bell labs., holmdel, nj 07733, usa\n\neditor: lorenza saitta\n\ncorinna@neural.att.com\nvlad@neural.att.com\n\nabstract.  the support-vector  network is a new learning  machine for two-group  classification  problems.  the\nmachine  conceptually  implements  the  following  idea:  input  vectors  are  non-linearly  mapped  to  a  very  high-\ndimension  feature space.  in this feature space a linear decision surface  is constructed.  special properties  of the\ndecision  surface ensures high generalization  ability  of the learning  machine.  the idea behind the  support-vector\nnetwork  was previously  implemented  for the restricted  case where  the training  data can be separated  without\nerrors.  we here extend this result to non-separable  training data.\n\nhigh generalization  ability  of support-vector  network", "ann. n.y. acad. sci. issn 0077-8923\n\nannals of the new york academy of sciences\nspecial issue: the year in cognitive neuroscience\nreview\n\nbeyond the feedforward sweep: feedback computations in\nthe visual cortex\n\ngabriel kreiman1\n1children\u2019s hospital, harvard medical school and center for brains, minds, and machines, boston, massachusetts. 2cognitive\nlinguistic and psychological sciences, carney institute for brain science, brown university, providence, rhode island\n\nand thomas serre2\n\naddresses for correspondence: thomas serre, cognitive linguistic and psychological sciences, carney institute for brain\nscience, brown university, 190 thayer st, providence, ri 02912. thomas_serre@brown.edu; gabriel kreiman, children\u2019s\nhospital, harvard medical school and center for brains, minds, and machines, 3 blackfan circle, boston, ma 02115.\ngabriel.kreiman@childrens.harvard.edu\n\nvisual perception involves the rapid formation of a coarse image representation at the onset of visual processing,\nwhich ", "dynamic routing between capsules\n\nsara sabour\n\nnicholas frosst\n\ngeoffrey e. hinton\n\ngoogle brain\n\ntoronto\n\n{sasabour, frosst, geoffhinton}@google.com\n\nabstract\n\na capsule is a group of neurons whose activity vector represents the instantiation\nparameters of a speci\ufb01c type of entity such as an object or an object part. we use\nthe length of the activity vector to represent the probability that the entity exists and\nits orientation to represent the instantiation parameters. active capsules at one level\nmake predictions, via transformation matrices, for the instantiation parameters of\nhigher-level capsules. when multiple predictions agree, a higher level capsule\nbecomes active. we show that a discrimininatively trained, multi-layer capsule\nsystem achieves state-of-the-art performance on mnist and is considerably better\nthan a convolutional net at recognizing highly overlapping digits. to achieve these\nresults we use an iterative routing-by-agreement mechanism: a lower-level capsule\nprefers", "phil. trans. r. soc. b (2009) 364, 1183\u20131191\ndoi:10.1098/rstb.2008.0306\n\nthe neurobiology of memory based predictions\n\nhoward eichenbaum* and norbert j. fortin\n\ncenter for memory and brain, boston university, 2 cummington street, boston, ma 02215, usa\n\nrecent \ufb01ndings indicate that, in humans, the hippocampal memory system is involved in the capacity\nto imagine the future as well as remember the past. other studies have suggested that animals may\nalso have the capacity to recall the past and plan for the future. here, we will consider data that bridge\nbetween these sets of \ufb01ndings by assessing the role of the hippocampus in memory and prediction in\nrats. we will argue that animals have the capacity for recollection and that the hippocampus plays a\ncentral and selective role in binding information in the service of recollective memory. then we will\nconsider examples of transitive inference, a paradigm that requires the integration of overlapping\nmemories and \ufb02exible use of the resulting ", "exploring randomly wired neural networks for image recognition\n\nsaining xie alexander kirillov ross girshick kaiming he\n\nfacebook ai research (fair)\n\n9\n1\n0\n2\n\n \nr\np\na\n8\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n9\n6\n5\n1\n0\n\n.\n\n4\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nneural networks for image recognition have evolved\nthrough extensive manual design from simple chain-like\nmodels to structures with multiple wiring paths. the suc-\ncess of resnets [11] and densenets [16] is due in large\npart to their innovative wiring plans. now, neural architec-\nture search (nas) studies are exploring the joint optimiza-\ntion of wiring and operation types, however, the space of\npossible wirings is constrained and still driven by manual\ndesign despite being searched. in this paper, we explore a\nmore diverse set of connectivity patterns through the lens of\nrandomly wired neural networks. to do this, we \ufb01rst de\ufb01ne\nthe concept of a stochastic network generator that encap-\nsulates the entire network generation process. encapsula-\nt", "j neurophysiol 104: 1068 \u20131076, 2010.\nfirst published june 10, 2010; doi:10.1152/jn.00158.2010.\n\na pallidus-habenula-dopamine pathway signals inferred stimulus values\n\nethan s. bromberg-martin,1 masayuki matsumoto,1,2 simon hong,1 and okihide hikosaka1\n1laboratory of sensorimotor research, national eye institute, national institutes of health, bethesda, maryland; and 2primate research\ninstitute, kyoto university, inuyama, aichi, japan\n\nsubmitted 5 february 2010; accepted in \ufb01nal form 9 june 2010\n\nbromberg-martin es, matsumoto m, hong s, hikosaka o. a\npallidus-habenula-dopamine pathway signals\ninferred stimulus\nvalues. j neurophysiol 104: 1068 \u20131076, 2010. first published june\n10, 2010; doi:10.1152/jn.00158.2010. the reward value of a stimulus\ncan be learned through two distinct mechanisms: reinforcement learn-\ning through repeated stimulus-reward pairings and abstract inference\nbased on knowledge of the task at hand. the reinforcement mecha-\nnism is often identi\ufb01ed with midbrain dopami", "0\n2\n0\n2\n\n \nc\ne\nd\n1\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n8\n4\n8\n6\n0\n\n.\n\n9\n0\n8\n1\n:\nv\ni\nx\nr\na\n\non the learning dynamics of\ndeep neural networks\n\nremi tachet des combes1, mohammad pezeshki1,2, samira shabanian1,\naaron courville2, yoshua bengio2\n1microsoft research montreal\n2universite de montreal, mila\n\nabstract\n\nwhile a lot of progress has been made in recent years, the dynamics of learning\nin deep nonlinear neural networks remain to this day largely misunderstood. in\nthis work, we study the case of binary classi\ufb01cation and prove various properties\nof learning in such networks under strong assumptions such as linear separability\nof the data. extending existing results from the linear case, we con\ufb01rm empirical\nobservations by proving that the classi\ufb01cation error also follows a sigmoidal shape\nin nonlinear architectures. we show that given proper initialization, learning\nexpounds parallel independent modes and that certain regions of parameter space\nmight lead to failed training. we also demons", "dynamics of ongoing activity:\n\nexplanation of the large variability\n\nin evoked cortical responses\n\namos arieli, alexander sterkin, amiram grinvald, ad aertsen*\n\nevoked activity in the mammalian cortex and the resulting behavioral responses exhibit\na large variability to repeated presentations of the same stimulus. this study examined\nwhether the variability can be attributed to ongoing activity. ongoing and evoked spa-\ntiotemporal activity patterns in the cat visual cortex were measured with real-time optical\nimaging; local field potentials and discharges of single neurons were recorded simul-\ntaneously, by electrophysiological techniques. the evoked activity appeared determin-\nistic, and the variability resulted from the dynamics of ongoing activity, presumably\nreflecting the instantaneous state of cortical networks. in spite of the large variability,\nevoked responses in single trials could be predicted by linear summation of the deter-\nministic response and the preceding ongoing acti", "biorxiv preprint \nthe copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is \n\nhttps://doi.org/10.1101/2023.06.27.546656\nthis version posted july 11, 2023. \n; \n\ndoi: \n\nmade available under a\n\ncc-by-nd 4.0 international license\n.\n\nneuronal wiring diagram of an adult brain\n\nsven dorkenwald1,2, arie matsliah1, amy r sterling1,3, philipp schlegel4,5, szi-chieh yu1, claire e.\nmckellar1, albert lin1,6, marta costa5, katharina eichler5, yijie yin5, will silversmith1, casey\nschneider-mizell7, chris s. jordan1, derrick brittain7, akhilesh halageri1, kai kuehner1,\noluwaseun ogedengbe1, ryan morey1, jay gager1, krzysztof kruk3, eric perlman8, runzhe\nyang1,2, david deutsch1,9, doug bland1, marissa sorek1,3, ran lu1, thomas macrina1,2, kisuk\nlee1,10, j. alexander bae1,11, shang mu1, barak nehoran1,2, eric mitchell1, sergiy popovych1,2,\njingpeng wu1, zhen jia1, manuel castro1, ni", "cognitive, affective, & behavioral neuroscience\n2008, 8 (4), 429-453\ndoi:10.3758/cabn.8.4.429\n\nconnections between cn omputational\n\nand neurobiological perspectives\n\non decision making\n\ndecision theory, reinforcement learning,\n\nand the brain\n\npeter dayan\n\nuniversity college london, london, england\n\nand\n\nnathaniel d. daw\n\nnew york university, new york, new york\n\ndecision making is a core competence for animals and humans acting and surviving in environments they \nonly partially comprehend, gaining rewards and punishments for their troubles. decision-theoretic concepts \npermeate experiments and computational models in ethology, psychology, and neuroscience. here, we review \npermeate experiments and computational models in ethology, psychology, and neuroscience. here, we review\na well-\nknown, coherent bayesian approach to decision making, showing how it unifies issues in markovian\nn \ndecision problems, signal detection psychophysics, sequential sampling, and optimal exploration and discus", "a\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\nt\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nhhs public access\nauthor manuscript\nnature. author manuscript; available in pmc 2014 august 05.\n\npublished in final edited form as:\n\nnature. 2013 november 7; 503(7474): 78\u201384. doi:10.1038/nature12742.\n\ncontext-dependent computation by recurrent dynamics in \nprefrontal cortex\n\nvalerio mante1,+,*, david sussillo2,+, krishna v. shenoy2,3, and william t. newsome1\n1howard hughes medical institute and department of neurobiology\n\n2department of electrical engineering and neurosciences program\n\n3departments of neurobiology and bioengineering stanford university, stanford, ca, united \nstates of america\n\nsummary\n\nprefrontal cortex is thought to play a fundamental role in flexible, context-dependent behavior, but \nthe exact nature of the computations underlying this role remains largely mysterious. in particular, \nindividual prefrontal neurons o", "8486 \u2022 the journal of neuroscience, august 8, 2007 \u2022 27(32):8486 \u2013 8495\n\nbehavioral/systems/cognitive\n\nan integrated microcircuit model of attentional processing\nin the neocortex\n\nsalva ardid,1,2 xiao-jing wang,3 and albert compte1,2\n1instituto de neurociencias de alicante, universidad miguel herna\u00b4ndez\u2013consejo superior de investigaciones cient\u0131\u00b4ficas, 03550 sant joan d\u2019alacant, spain,\n2institut d\u2019investigacions biome`diques august pi i sunyer, 08036 barcelona, spain, and 3department of neurobiology and kavli institute for neuroscience,\nyale university school of medicine, new haven, connecticut 06520-8001\n\nselective attention is a fundamental cognitive function that uses top-down signals to orient and prioritize information processing in the\nbrain. single-cell recordings from behaving monkeys have revealed a number of attention-induced effects on sensory neurons, and have\ngiven rise to contrasting viewpoints about the neural underpinning of attentive processing. moreover, there is evid", "learning representations for neural\n\nnetwork-based classi\ufb01cation using the\n\ninformation bottleneck principle\n\n1\n\n9\n1\n0\n2\n\n \nr\np\na\n1\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n6\nv\n6\n6\n7\n9\n0\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nrana ali amjad, student member, ieee, and bernhard c. geiger, senior member, ieee\n\nabstract\u2014in this theory paper, we investigate training deep neural networks (dnns) for classi\ufb01cation via minimizing the information\nbottleneck (ib) functional. we show that the resulting optimization problem suffers from two severe issues: first, for deterministic dnns,\neither the ib functional is in\ufb01nite for almost all values of network parameters, making the optimization problem ill-posed, or it is piecewise\nconstant, hence not admitting gradient-based optimization methods. second, the invariance of the ib functional under bijections prevents\nit from capturing properties of the learned representation that are desirable for classi\ufb01cation, such as robustness and simplicity. we argue\nthat these issues are pa", "understanding black-box predictions via in\ufb02uence functions\n\n0\n2\n0\n2\n\n \nc\ne\nd\n9\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n0\n3\n7\n4\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\npang wei koh 1 percy liang 1\n\nabstract\n\nhow can we explain the predictions of a black-\nbox model? in this paper, we use in\ufb02uence func-\ntions \u2014 a classic technique from robust statis-\ntics \u2014 to trace a model\u2019s prediction through the\nlearning algorithm and back to its training data,\nthereby identifying training points most respon-\nsible for a given prediction. to scale up in\ufb02uence\nfunctions to modern machine learning settings,\nwe develop a simple, ef\ufb01cient implementation\nthat requires only oracle access to gradients and\nhessian-vector products. we show that even on\nnon-convex and non-differentiable models where\nthe theory breaks down, approximations to in\ufb02u-\nence functions can still provide valuable infor-\nmation. on linear models and convolutional neu-\nral networks, we demonstrate that in\ufb02uence func-\ntions are useful for multiple purpose", "markov chain monte carlo convergence diagnostics: a comparative review \nauthor(s): mary kathryn cowles and bradley p. carlin \nsource: journal of the american statistical association, jun., 1996, vol. 91, no. 434 \n(jun., 1996), pp. 883-904\npublished by: taylor & francis, ltd. on behalf of the american statistical association \n\n \n\nstable url: https://www.jstor.org/stable/2291683\n \nreferences \nlinked references are available on jstor for this article: \nhttps://www.jstor.org/stable/2291683?seq=1&cid=pdf-\nreference#references_tab_contents \nyou may need to log in to jstor to access the linked references.\n\njstor is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide \nrange of content in a trusted digital archive. we use information technology and tools to increase productivity and \nfacilitate new forms of scholarship. for more information about jstor, please contact support@jstor.org. \n \nyour use of the jstor archive indicates your acce", "world models\n\ndavid ha 1 j\u00a8urgen schmidhuber 2 3\n\n8\n1\n0\n2\n\n \n\ny\na\nm\n9\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n2\n2\n1\n0\n1\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nreinforcement\n\nwe explore building generative neural network\nmodels of popular\nlearning\nenvironments. our world model can be trained\nquickly in an unsupervised manner to learn a\ncompressed spatial and temporal representation\nof the environment. by using features extracted\nfrom the world model as inputs to an agent, we\ncan train a very compact and simple policy that\ncan solve the required task. we can even train\nour agent entirely inside of its own hallucinated\ndream generated by its world model, and transfer\nthis policy back into the actual environment.\n\nan interactive version of this paper is available at\nhttps://worldmodels.github.io\n\n1. introduction\nhumans develop a mental model of the world based on\nwhat they are able to perceive with their limited senses. the\ndecisions and actions we make are based on this internal\nmodel. jay wright for", "neuron, vol. 35, 773\u2013782, august 15, 2002, copyright \uf8e92002 by cell press\n\ngain modulation from background synaptic input\n\nfrances s. chance,1,3 l.f. abbott,2\nand alex d. reyes1\n1center for neural science\nnew york university\nnew york, new york 10003\n2 volen center for complex systems and\ndepartment of biology\nbrandeis university\nwaltham, massachusetts 02454\n\nsummary\n\ngain modulation is a prominent feature of neuronal\nactivity recorded in behaving animals, but the mecha-\nnism by which it occurs is unknown. by introducing a\nbarrage of excitatory and inhibitory synaptic conduc-\ntances that mimics conditions encountered in vivo into\npyramidal neurons in slices of rat somatosensory cor-\ntex, we show that the gain of a neuronal response to\nexcitatory drive can be modulated by varying the level\nof \u201cbackground\u201d synaptic input. simultaneously in-\ncreasing both excitatory and inhibitory background\nfiring rates in a balanced manner results in a divisive\ngain modulation of the neuronal response wit", "atoms of recognition in human and computer vision\n\nshimon ullmana,b,1,2, liav assifa,1, ethan fetayaa, and daniel hararia,c,1\n\nadepartment of computer science and applied mathematics, weizmann institute of science, rehovot 7610001, israel; bdepartment of brain and cognitive\nsciences, massachusetts institute of technology, cambridge, ma 02139; and cmcgovern institute for brain research, cambridge, ma 02139\n\nedited by michael e. goldberg, columbia university college of physicians, new york, ny, and approved january 11, 2016 (received for review july 8, 2015)\n\ndiscovering the visual features and representations used by the\nbrain to recognize objects is a central problem in the study of vision.\nrecently, neural network models of visual object recognition, includ-\ning biological and deep network models, have shown remarkable\nprogress and have begun to rival human performance in some chal-\nlenging tasks. these models are trained on image examples and\nlearn to extract features and representat", "article\n\nreceived 7 jul 2015 | accepted 21 mar 2016 | published 26 apr 2016\n\nopen\nneural substrates of cognitive biases during\nprobabilistic inference\nalireza soltani1, peyman khorsand1, clara guo1, shiva farashahi1 & janet liu1\n\ndoi: 10.1038/ncomms11393\n\ndecision making often requires simultaneously learning about and combining evidence from\nvarious sources of information. however, when making inferences from these sources,\nhumans show systematic biases that are often attributed to heuristics or limitations in\ncognitive processes. here we use a combination of experimental and modelling approaches to\nreveal neural substrates of probabilistic inference and corresponding biases. we \ufb01nd\nsystematic deviations from normative accounts of inference when alternative options are not\nequally rewarding; subjects\u2019 choice behaviour is biased towards the more rewarding option,\nwhereas their inferences about individual cues show the opposite bias. moreover, inference\nbias about combinations of cues d", "behavioral and brain sciences (2017), page 1 of 72\ndoi:10.1017/s0140525x16001837, e253\n\nbuilding machines that learn and\nthink like people\n\nbrenden m. lake\ndepartment of psychology and center for data science, new york university,\nnew york, ny 10011\nbrenden@nyu.edu\nhttp://cims.nyu.edu/~brenden/\n\ntomer d. ullman\ndepartment of brain and cognitive sciences and the center for brains, minds\nand machines, massachusetts institute of technology, cambridge, ma 02139\ntomeru@mit.edu\nhttp://www.mit.edu/~tomeru/\n\njoshua b. tenenbaum\ndepartment of brain and cognitive sciences and the center for brains, minds\nand machines, massachusetts institute of technology, cambridge, ma 02139\njbt@mit.edu\nhttp://web.mit.edu/cocosci/josh.html\n\nsamuel j. gershman\ndepartment of psychology and center for brain science, harvard university,\ncambridge, ma 02138, and the center for brains, minds and machines,\nmassachusetts institute of technology, cambridge, ma 02139\ngershman@fas.harvard.edu\nhttp://gershmanlab.webfaction", "the journal of neuroscience, march 25, 2009 \u2022 29(12):3685\u20133694 \u2022 3685\n\nbehavioral/systems/cognitive\n\ncorrelated connectivity and the distribution of firing rates\nin the neocortex\n\nalexei a. koulakov, toma\u00b4s\u02c7 hroma\u00b4dka, and anthony m. zador\ncold spring harbor laboratory, cold spring harbor, new york 11724\n\ntwo recent experimental observations pose a challenge to many cortical models. first, the activity in the auditory cortex is sparse, and\nfiring rates can be described by a lognormal distribution. second, the distribution of nonzero synaptic strengths between nearby cortical\nneurons can also be described by a lognormal distribution. here we use a simple model of cortical activity to reconcile these observations.\nthe model makes the experimentally testable prediction that synaptic efficacies onto a given cortical neuron are statistically correlated,\ni.e., it predicts that some neurons receive stronger synapses than other neurons. we propose a simple hebb-like learning rule that gives\nri", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/2022.03.28.485868\n; \n\nthis version posted july 1, 2023. \n\nthe copyright holder for this preprint (which\n\nwas not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\navailable under a\n\ncc-by-nc 4.0 international license\n.\n\nwhat can 1.8 billion regressions tell us about the pressures shaping\n\nhigh-level visual representation in brains and machines?\n\ncolin conwell1\u2217, jacob s. prince1, kendrick n. kay2, george a. alvarez1, talia konkle1,3,4\n\n1department of psychology, harvard university\n\n2center for magnetic resonance research, department of radiology, university of minnesota\n\n3center for brain science, harvard university\n\n4kempner institute for natural and artificial intelligence, harvard university\n\nabstract\n\nthe rapid development and open-source release of highly performant computer vision mod-\nels offers new potential for examining how different inductive ", "neural fitted q iteration - first experiences\nwith a data e\ufb03cient neural reinforcement\n\nlearning method\n\nmartin riedmiller\n\nneuroinformatics group,\n\nuniversity of onsabr\u00a8uck, 49078 osnabr\u00a8uck\n\nabstract. this paper introduces nfq, an algorithm for e\ufb03cient and ef-\nfective training of a q-value function represented by a multi-layer percep-\ntron. based on the principle of storing and reusing transition experiences,\na model-free, neural network based reinforcement learning algorithm is\nproposed. the method is evaluated on three benchmark problems. it is\nshown empirically, that reasonably few interactions with the plant are\nneeded to generate control policies of high quality.\n\n1 introduction\n\nwhen addressing interesting reinforcement learning (rl) problems in real\nworld applications, one sooner or later faces the problem of an appropriate\nmethod to represent the value function. neural networks, in particular multi-\nlayer perceptrons, o\ufb00er an interesting perspective due to their ability to ap", "physical review letters 130, 237401 (2023)\n\neditors' suggestion\n\nfeatured in physics\n\nshockwavelike behavior across social media\n\npedro d. manrique , frank yingjie huo, sara el oud , minzhang zheng , lucia illari\n\n, and neil f. johnson\n\nphysics department, george washington university, washington, dc 20052, usa\n\n(received 25 october 2022; revised 30 january 2023; accepted 28 march 2023; published 5 june 2023)\n\nonline communities featuring \u201canti-x\u201d hate and extremism, somehow thrive online despite moderator\npressure. we present a first-principles theory of their dynamics, which accounts for the fact that the online\npopulation comprises diverse individuals and evolves in time. the resulting equation represents a novel\ngeneralization of nonlinear fluid physics and explains the observed behavior across scales. its shockwave-\nlike solutions explain how, why, and when such activity rises from \u201cout-of-nowhere,\u201d and show how it can\nbe delayed, reshaped, and even prevented by adjusting the onli", "review\nneuromodulators and long-term synaptic\nplasticity in learning and memory:\na steered-glutamatergic perspective\n\namjad h. bazzari * and h. rheinallt parri\n\nschool of life and health sciences, aston university, birmingham b4 7et, uk; parrihr@aston.ac.uk\n* correspondence: bazzaria@aston.ac.uk; tel.: +44-(0)1212044186\n\nreceived: 7 october 2019; accepted: 29 october 2019; published: 31 october 2019\n\nabstract: the molecular pathways underlying the induction and maintenance of long-term synaptic\nplasticity have been extensively investigated revealing various mechanisms by which neurons\ncontrol their synaptic strength. the dynamic nature of neuronal connections combined with\nplasticity-mediated long-lasting structural and functional alterations provide valuable insights into\nneuronal encoding processes as molecular substrates of not only learning and memory but potentially\nother sensory, motor and behavioural functions that re\ufb02ect previous experience. however, one key\nelement receiving l", "matrix completion has no spurious local minimum\n\nrong ge\n\nduke university\n\n308 research drive, nc 27708\nrongge@cs.duke.edu.\n\njason d. lee\n\nuniversity of southern california\n3670 trousdale pkwy, ca 90089\n\njasonlee@marshall.usc.edu.\n\ntengyu ma\n\nprinceton university\n\n35 olden street, nj 08540\n\ntengyu@cs.princeton.edu.\n\nabstract\n\nmatrix completion is a basic machine learning problem that has wide applica-\ntions, especially in collaborative \ufb01ltering and recommender systems. simple\nnon-convex optimization algorithms are popular and effective in practice. despite\nrecent progress in proving various non-convex algorithms converge from a good\ninitial point, it remains unclear why random or arbitrary initialization suf\ufb01ces in\npractice. we prove that the commonly used non-convex objective function for\npositive semide\ufb01nite matrix completion has no spurious local minima \u2013 all local\nminima must also be global. therefore, many popular optimization algorithms such\nas (stochastic) gradient descent can p", "cnn features off-the-shelf: an astounding baseline for recognition\n\nali sharif razavian hossein azizpour\n\njosephine sullivan stefan carlsson\n\ncvap, kth (royal institute of technology)\n\n{razavian,azizpour,sullivan,stefanc}@csc.kth.se\n\nstockholm, sweden\n\n4\n1\n0\n2\n\n \n\ny\na\nm\n2\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n3\nv\n2\n8\n3\n6\n\n.\n\n3\n0\n4\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nrecent results indicate that the generic descriptors ex-\ntracted from the convolutional neural networks are very\npowerful. this paper adds to the mounting evidence that\nthis is indeed the case. we report on a series of exper-\niments conducted for different recognition tasks using the\npublicly available code and model of the overfeat net-\nwork which was trained to perform object classi\ufb01cation on\nilsvrc13. we use features extracted from the overfeat\nnetwork as a generic image representation to tackle the di-\nverse range of recognition tasks of object image classi\ufb01ca-\ntion, scene recognition, \ufb01ne grained recognition, attribute\ndetection and imag", "anne.churchland@\n\n*forcorrespondence:\ninternationalbrainlab.org (akc); liam.\n(lp); nicholas.steinmetz@\ninternationalbrainlab.org (nas)\n\npaninski@internationalbrainlab.org\n\nbiorxiv preprint \n\nhttps://doi.org/10.1101/2022.05.09.491042\n; \n\ndoi: \nwas not certified by peer review) is the author/funder. all rights reserved. no reuse allowed without permission. \n\nthis version posted may 12, 2022. \n\nthe copyright holder for this preprint (which\n\nreproducibilityofin-vivo\n1 electrophysiologicalmeasurements\n2 inmice\n\n3\n\ninternationalbrainlaboratory*,kushbanga7,juliusbenson11,niccol\u00f2bonacchi2,\n4 sebastianabruijns1,robcampbell13,ga\u00eblleachapuis5,annekchurchland6,m\n5 feliciadavatolhagh6,hyundonglee3,mayofaulkner7,feihu9,julia\n6 hunterberg2,anupkhanal6,christopherkrasniak10,guidotmeijer2,nathanielj\n7 miska7,zeinabmohammadi12,jean-paulnoel11,liampaninski3,alejandro\n8 pan-vazquez12,noamroth4,michaelschartner2,karolinasocha7,nicholasa\n9 steinmetz4,karelsvoboda14,marsataheri6,anneeurai8,mileswells7,steven", "1\n\na connection between score matching\n\nand denoising autoencoders\n\npascal vincent\n\nvincentp@iro.umontreal.ca\n\ndept. iro, universit\u00e9 de montr\u00e9al,\n\ncp 6128, succ. centre-ville, montr\u00e9al (qc) h3c 3j7, canada.\n\nd\u00e9partement d\u2019informatique et de recherche op\u00e9rationnelle\n\ntechnical report 1358\n\ndecember 2010\n\nthis is a preprint version of a note that has been\naccepted for publication in neural computation.\n\nkeywords: autoencoder, energy based models, score matching, denoising, density\n\nestimation.\n\nabstract\n\ndenoising autoencoders have been previously shown to be competitive alternatives\nto restricted boltzmann machines for unsupervised pre-training of each layer of a deep\narchitecture. we show that a simple denoising autoencoder training criterion is equiv-\nalent to matching the score (with respect to the data) of a speci\ufb01c energy based model\nto that of a non-parametric parzen density estimator of the data. this yields several\nuseful insights. it de\ufb01nes a proper probabilistic model for the ", "19-charter&oaksford-chap19  11/5/07  11:22 am  page 427\n\nchapter 19\nsemi-rational models of\nconditioning: the case of trial\norder\nnathaniel d. daw, aaron c. courville, and\npeter dayan\njune 15, 2007\n\n1 introduction\nbayesian treatments of animal conditioning start from a generative model that speci-\nfies precisely a set of assumptions about the structure of the learning task. optimal\nrules for learning are direct mathematical consequences of these assumptions. in\nterms of marr\u2019s (1982) levels of analyses, the main task at the computational level\nwould therefore seem to be to understand and characterize the set of assumptions\nfrom the observed behavior. however, a major problem with such marrian analyses \nis that most bayesian models for learning are presumptively untenable due to their\nradically intractable computational demands.\n\nthis tension between what the observer should and what she can do relates to\nmarr\u2019s (1982) distinction between the computational and algorithmic levels,\nchomsk", "de\ufb01nitions, methods, and applications in interpretable\nmachine learning\n\nw. james murdocha,1, chandan singhb,1, karl kumbiera,2, reza abbasi-aslb,c,d,2, and bin yua,b,3\n\nastatistics department, university of california, berkeley, ca 94720; belectrical engineering and computer science department, university of california,\nberkeley, ca 94720; cdepartment of neurology, university of california, san francisco, ca 94158; and dallen institute for brain science, seattle, wa 98109\n\ncontributed by bin yu, july 1, 2019 (sent for review january 16, 2019; reviewed by rich caruana and giles hooker)\n\nmachine-learning models have demonstrated great success in\nlearning complex patterns that enable them to make predic-\ntions about unobserved data. in addition to using models for\nprediction, the ability to interpret what a model has learned\nis receiving an increasing amount of attention. however, this\nincreased focus has led to considerable confusion about the\nnotion of interpretability. in particular, ", "the dif\ufb01culty of training deep architectures and the effect of unsupervised pre-training\n\n\u2217dumitru erhan\u2020, \u2217pierre-antoine manzagol, \u2217yoshua bengio, \u2021 samy bengio and \u2217pascal vincent\n\n\u2217diro, universit\u00b4e de montr\u00b4eal, montr\u00b4eal, qu\u00b4ebec, canada\n\n{erhandum, manzagop, bengioy, vincentp}@iro.umontreal.ca\n\n\u2021 google, mountain view, california, usa\n\nbengio@google.com\n\nabstract\n\nwhereas theoretical work suggests that deep ar-\nchitectures might be more ef\ufb01cient at represent-\ning highly-varying functions, training deep ar-\nchitectures was unsuccessful until the recent ad-\nvent of algorithms based on unsupervised pre-\ntraining. even though these new algorithms have\nenabled training deep models, many questions\nremain as to the nature of this dif\ufb01cult learning\nproblem. answering these questions is impor-\ntant if learning in deep architectures is to be fur-\nther improved. we attempt to shed some light\non these questions through extensive simulations.\nthe experiments con\ufb01rm and clarify the advan-\ntag", "a r t i c l e s\n\nfragmentation of grid cell maps in a \nmulticompartment environment\n\ndori derdikman1, jonathan r whitlock1, albert tsao1, marianne fyhn1,2, torkel hafting1,2,  \nmay-britt moser1 & edvard i moser1\n\nto determine whether entorhinal spatial representations are continuous or fragmented, we recorded neural activity in grid cells \nwhile rats ran through a stack of interconnected, zig-zagged compartments of equal shape and orientation (a hairpin maze). the \ndistribution of spatial firing fields was markedly similar across all compartments in which running occurred in the same direction, \nimplying that the grid representation was fragmented into repeating submaps. activity at neighboring positions was least \ncorrelated at the transitions between different arms, indicating that the map split regularly at the turning points. we saw similar \ndiscontinuities among place cells in the hippocampus. no fragmentation was observed when the rats followed similar trajectories \nin the absenc", "neuron\n\nreview\n\nthe spike-timing dependence of plasticity\n\ndaniel e. feldman1,*\n1department of molecular and cell biology, and helen wills neuroscience institute, university of california, berkeley, berkeley,\nca 94720-3200, usa\n*correspondence: dfeldman@berkeley.edu\nhttp://dx.doi.org/10.1016/j.neuron.2012.08.001\n\nin spike-timing-dependent plasticity (stdp), the order and precise temporal interval between presynaptic and\npostsynaptic spikes determine the sign and magnitude of long-term potentiation (ltp) or depression (ltd).\nstdp is widely utilized in models of circuit-level plasticity, development, and learning. however, spike timing\nis just one of several factors (including \ufb01ring rate, synaptic cooperativity, and depolarization) that govern\nplasticity induction, and its relative importance varies across synapses and activity regimes. this review\nsummarizes this broader view of plasticity, including the forms and cellular mechanisms for the spike-timing\ndependence of plasticity, and, t", "forward and backward inference in spatial cognition\n\nwill d. penny1*, peter zeidman1, neil burgess2\n\n1 wellcome trust centre for neuroimaging, university college, london, london, united kingdom, 2 institute for cognitive neuroscience, university college, london,\nlondon, united kingdom\n\nabstract\n\nthis paper shows that the various computations underlying spatial cognition can be implemented using statistical inference\nin a single probabilistic model. inference is implemented using a common set of \u2018lower-level\u2019 computations involving\nforward and backward inference over time. for example, to estimate where you are in a known environment, forward\ninference is used to optimally combine location estimates from path integration with those from sensory input. to decide\nwhich way to turn to reach a goal, forward inference is used to compute the likelihood of reaching that goal under each\noption. to work out which environment you are in, forward inference is used to compute the likelihood of sens", "p\ne\nr\ng\na\nm\no\nn\n \nn\ne\nu\nr\na\nl\n \nn\ne\nt\nw\no\nr\nk\ns\n,\n \nv\no\nl\n.\n \n7\n,\n \nn\no\n.\n \n3\n,\n \np\np\n.\n \n5\n0\n7\n-\n5\n2\n2\n,\n \n1\n9\n9\n4\n \nc\no\np\ny\nr\ni\ng\nh\nt\n \n\u00a9\n \n1\n9\n9\n4\n \ne\nl\ns\ne\nv\ni\ne\nr\n \ns\nc\ni\ne\nn\nc\ne\n \nl\nt\nd\n \np\nr\ni\nn\nt\ne\nd\n \ni\nn\n \nt\nh\ne\n \nu\ns\na\n.\n \na\nl\nl\n \nr\ni\ng\nh\nt\ns\n \nr\ne\ns\ne\nr\nv\ne\nd\n \n0\n8\n9\n3\n-\n6\n0\n8\n0\n/\n9\n4\n \n$\n6\n.\n0\n0\n \n+\n \n.\n0\n0\n \nc\no\nn\nt\nr\ni\nb\nu\nt\ne\nd\n \na\nr\nt\ni\nc\nl\ne\n \nt\no\np\no\nl\no\ng\ny\n \nr\ne\np\nr\ne\ns\ne\nn\nt\ni\nn\ng\n \nn\ne\nt\nw\no\nr\nk\ns\n \nt\nh\no\nm\na\ns\n \nm\na\nr\nt\ni\nn\ne\nt\nz\n \n1\n'\n2\n \na\nn\nd\n \nk\n_\nl\na\nu\ns\n \ns\nc\nh\nu\nl\nt\ne\nn\n \nl\n \nt\nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n \no\nf\n \ni\nl\nl\ni\nn\no\ni\ns\n \na\nn\nd\n \n2\ns\ni\ne\nm\ne\nn\ns\n \na\ng\n \n(\nr\ne\nc\ne\ni\nv\ne\nd\n \n4\n \nm\na\nr\nc\nh\n \n1\n9\n9\n3\n;\n \na\nc\nc\ne\np\nt\ne\nd\n \n2\n0\n \ns\ne\np\nt\ne\nm\nb\ne\nr\n \n1\n9\n9\n3\n \n)\n \na\nb\ns\nt\nr\na\nc\nt\n-\n-\na\n \nh\ne\nb\nb\ni\na\nn\n \na\nd\na\np\nt\na\nt\ni\no\nn\n \nr\nu\nl\ne\n \nw\ni\nt\nh\n \nw\ni\nn\nn\ne\nr\n-\nt\na\nk\ne\n-\na\nl\nl\n \nl\ni\nk\ne\n \nc\no\nm\np\ne\nt\ni\nt\ni\no\nn\n \ni\ns\n \ni\nn\nt\nr\no\nd\nu\nc\ne\nd\n.\n \ni\nt\n \ni\ns\n \ns\nh\no\nw\nn\n \nt\nh\na\nt\n \nt\nh\ni\ns\n \nc\no\nm\np\ne\nt\ni\nt\ni\nv\ne\n \nh\ne\nb\nb\ni\na\nn\n \nr\nu\nl\ne\n \nf\no\nr\nm\ns\n \ns\no\n-\nc\na\nl\nl\ne\nd\n", "7\n1\n0\n2\n\n \n\nv\no\nn\n7\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n1\n9\n3\n2\n0\n\n.\n\n1\n1\n7\n1\n:\nv\ni\nx\nr\na\n\n95\n\na tutorial on canonical correlation methods\n\nviivi uurtio, aalto university\njo \u02dcao m. monteiro, university college london\njaz kandola, imperial college london\njohn shawe-taylor, university college london\ndelmiro fernandez-reyes, university college london\njuho rousu, aalto university\n\ncanonical correlation analysis is a family of multivariate statistical methods for the analysis of paired sets\nof variables. since its proposition, canonical correlation analysis has for instance been extended to extract\nrelations between two sets of variables when the sample size is insuf\ufb01cient in relation to the data dimen-\nsionality, when the relations have been considered to be non-linear, and when the dimensionality is too large\nfor human interpretation. this tutorial explains the theory of canonical correlation analysis including its\nregularised, kernel, and sparse variants. additionally, the deep and bayesian ", "a r t i c l e s\n\npartitioning neuronal variability\nrobbe l t goris1, j anthony movshon1 & eero p simoncelli1,2\n\nresponses of sensory neurons differ across repeated measurements. this variability is usually treated as stochasticity  \narising within neurons or neural circuits. however, some portion of the variability arises from fluctuations in excitability due  \nto factors that are not purely sensory, such as arousal, attention and adaptation. to isolate these fluctuations, we developed  \na model in which spikes are generated by a poisson process whose rate is the product of a drive that is sensory in origin and \na gain summarizing stimulus-independent modulatory influences on excitability. this model provides an accurate account of \nresponse distributions of visual neurons in macaque lateral geniculate nucleus and cortical areas v1, v2 and mt, revealing \nthat variability originates in large part from excitability fluctuations that are correlated over time and between neurons, and \nthat", "biol cybern (2012) 106:523\u2013541\ndoi 10.1007/s00422-012-0512-8\n\nprospects\n\nactive inference and agency: optimal control without\ncost functions\nkarl friston \u00b7 spyridon samothrakis \u00b7\nread montague\n\nreceived: 1 february 2012 / accepted: 16 july 2012 / published online: 3 august 2012\n\u00a9 the author(s) 2012. this article is published with open access at springerlink.com\n\nabstract this paper describes a variational free-energy for-\nmulation of (partially observable) markov decision problems\nin decision making under uncertainty. we show that optimal\ncontrol can be cast as active inference. in active inference,\nboth action and posterior beliefs about hidden states mini-\nmise a free energy bound on the negative log-likelihood of\nobserved states, under a generative model. in this setting,\nreward or cost functions are absorbed into prior beliefs about\nstate transitions and terminal states. effectively, this con-\nverts optimal control into a pure inference problem, enabling\nthe application of standard", "vol 436|11 august 2005|doi:10.1038/nature03721\n\narticles\n\nmicrostructure of a spatial map in the\nentorhinal cortex\n\ntorkel hafting1*, marianne fyhn1*, sturla molden1\u2020, may-britt moser1 & edvard i. moser1\n\nthe ability to \ufb01nd one\u2019s way depends on neural algorithms that integrate information about place, distance and\ndirection, but the implementation of these operations in cortical microcircuits is poorly understood. here we show that\nthe dorsocaudal medial entorhinal cortex (dmec) contains a directionally oriented, topographically organized neural map\nof the spatial environment. its key unit is the \u2018grid cell\u2019, which is activated whenever the animal\u2019s position coincides with\nany vertex of a regular grid of equilateral triangles spanning the surface of the environment. grids of neighbouring cells\nshare a common orientation and spacing, but their vertex locations (their phases) differ. the spacing and size of\nindividual \ufb01elds increase from dorsal to ventral dmec. the map is anchored to ext", "a r t i c l e s\n\na neural network that finds a naturalistic solution for \nthe production of muscle activity\ndavid sussillo1, mark m churchland2, matthew t kaufman1,4 & krishna v shenoy1,3\nit remains an open question how neural responses in motor cortex relate to movement. we explored the hypothesis that motor \ncortex reflects dynamics appropriate for generating temporally patterned outgoing commands. to formalize this hypothesis, we \ntrained recurrent neural networks to reproduce the muscle activity of reaching monkeys. models had to infer dynamics that could \ntransform simple inputs into temporally and spatially complex patterns of muscle activity. analysis of trained models revealed \nthat the natural dynamical solution was a low-dimensional oscillator that generated the necessary multiphasic commands.  \nthis solution closely resembled, at both the single-neuron and population levels, what was observed in neural recordings from the \nsame monkeys. notably, data and simulations agreed o", "9\n1\n0\n2\n\n \n\nn\na\nj\n \n\n9\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n0\n9\n5\n0\n\n.\n\n2\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nsoft actor-critic algorithms and applications\n\ntuomas haarnoja\u2217\u2020\u2021\n\naurick zhou\u2217\u2020\n\nkristian hartikainen\u2217\u2020\n\ngeorge tucker\u2021\n\nsehoon ha\u2021\n\njie tan\u2021\n\nvikash kumar\u2021\n\nhenry zhu\u2020\n\nabhishek gupta\u2020\n\npieter abbeel\u2020\n\nsergey levine\u2020\u2021\n\nabstract\n\nmodel-free deep reinforcement learning (rl) algorithms have been successfully\napplied to a range of challenging sequential decision making and control tasks.\nhowever, these methods typically suffer from two major challenges: high sample\ncomplexity and brittleness to hyperparameters. both of these challenges limit the\napplicability of such methods to real-world domains. in this paper, we describe\nsoft actor-critic (sac), our recently introduced off-policy actor-critic algorithm\nbased on the maximum entropy rl framework. in this framework, the actor aims\nto simultaneously maximize expected return and entropy; that is, to succeed at\nthe task while acting as randomly as possi", "a r t i c l e s\n\nthe medial entorhinal cortex is necessary for temporal \norganization of hippocampal neuronal activity\nmagdalene i schlesiger1,2,6, christopher c cannova1,6, brittney l boublil1, jena b hales3, emily a mankin1, \nmark p brandon1, jill k leutgeb1, christian leibold4 & stefan leutgeb1,5\n\nthe superficial layers of the medial entorhinal cortex (mec) are a major input to the hippocampus. the high proportion \nof spatially modulated cells, including grid cells and border cells, in these layers suggests that mec inputs are critical for \nthe representation of space in the hippocampus. however, selective manipulations of the mec do not completely abolish \nhippocampal spatial firing. to determine whether other hippocampal firing characteristics depend more critically on mec inputs, \nwe recorded from hippocampal ca1 cells in rats with mec lesions. theta phase precession was substantially disrupted, even \nduring periods of stable spatial firing. our findings indicate that mec inputs ", "article\n\nrecurrent network models of sequence generation\nand memory\n\nhighlights\nd sequences emerge in random networks by modifying a small\n\nfraction of their connections\n\nauthors\n\nkanaka rajan, christopher d. harvey,\ndavid w. tank\n\nd analysis reveals new circuit mechanism for input-dependent\n\nsequence propagation\n\nd sequential activation may provide a dynamic mechanism for\n\nshort-term memory\n\ncorrespondence\nkrajan@princeton.edu (k.r.),\nharvey@hms.harvard.edu (c.d.h.),\ndwtank@princeton.edu (d.w.t.)\n\nin brief\nrajan et al. show that neural sequences\nsimilar to those observed during memory-\nbased decision-making tasks can be\ngenerated by minimally structured\nnetworks. sequences may effectively\nmediate the short-term memory engaged\nin these tasks.\n\nrajan et al., 2016, neuron 90, 128\u2013142\napril 6, 2016 \u00aa2016 elsevier inc.\nhttp://dx.doi.org/10.1016/j.neuron.2016.02.009\n\n\f", "6\n1\n0\n2\n\n \n\nb\ne\nf\n4\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n9\n7\n1\n5\n0\n\n.\n\n2\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ntowards a biologically plausible backprop\n\nbenjamin scellier and yoshua bengio\u2217\n\nuniversit\u00e9 de montr\u00e9al, montreal institute for learning algorithms\n\nseptember 1, 2021\n\nabstract\n\nthis work follows bengio and fischer (2015) in which theoretical foundations were laid to show how iterative in-\nference can backpropagate error signals. neurons move their activations towards con\ufb01gurations corresponding to lower\nenergy and smaller prediction error: a new observation creates a perturbation at visible neurons that propagates into hid-\nden layers, with these propagated perturbations corresponding to the back-propagated gradient. this avoids the need for\na lengthy relaxation in the positive phase of training (when both inputs and targets are observed), as was believed with\nprevious work on \ufb01xed-point recurrent networks. we show experimentally that energy-based neural networks with sev-\neral hidden layers can b", "a high-performance speech neuroprosthesis\n\nhttps://doi.org/10.1038/s41586-023-06377-x\nreceived: 21 january 2023\naccepted: 27 june 2023\npublished online: 23 august 2023\nopen access\n\n check for updates\n\nfrancis r. willett1,15\u2009\u2709, erin m. kunz2,3,15, chaofei fan4,15, donald t. avansino1, guy h. wilson5, \neun young choi6, foram kamdar6, matthew f. glasser7,8, leigh r. hochberg9,10,11, \nshaul druckmann12, krishna v. shenoy1,2,3,12,13,14 & jaimie m. henderson3,6\n\nspeech brain\u2013computer interfaces (bcis) have the potential to restore rapid \ncommunication to people with paralysis by decoding neural activity evoked by \nattempted speech into text1,2 or sound3,4. early demonstrations, although promising, \nhave not yet achieved accuracies sufficiently high for communication of unconstrained \nsentences from a large vocabulary1\u20137. here we demonstrate a speech-to-text bci that \nrecords spiking activity from intracortical microelectrode arrays. enabled by these \nhigh-resolution recordings, our study par", "recurrent  nets that time and count \n\nfelix  a.  gers \nfelix@idsia.ch \n\njiirgen schmidhuber \njuergen@idsia.ch \n\nidsia, corso elvezia 36, 6900 lugano, switzerland, www. idsia. ch \n\nabstract \n\nthe size of  the time intervals between events conveys information essential for numerous sequential \ntasks such as motor control  and  rhythm  detection.  while  hidden  markov  models tend  to  ignore this \ninformation,  recurrent  neural  networks (rnns) can  in  principle learn  to make use  of  it.  we  focus on \nlong  short-term  memory  (lstm) because  it  usually  outperforms  other  rnns.  surprisingly, lstm \naugmented by  \u201cpeephole connections\u201d from its internal  cells to its multiplicative gates can learn the fine \ndistinction between sequences of  spikes separated by either 50 or 49 discrete time steps, without the help \nof  any short  training  exemplars.  without  external  resets  or  teacher forcing or  loss of  performance on \ntasks reported earlier, our lstm variant also learns to", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/315570715\n\nwhy do similarity matching objectives lead to\nhebbian/anti-hebbian networks?\n\narticle\u00a0\u00a0in\u00a0\u00a0neural computation \u00b7 january 2018\n\ndoi: 10.1162/neco_a_01018\n\ncitations\n62\n\n3 authors:\n\nreads\n1,009\n\ncengiz pehlevan\nharvard university\n\nanirvan m sengupta\nrutgers, the state university of new jersey\n\n103 publications\u00a0\u00a0\u00a01,238 citations\u00a0\u00a0\u00a0\n\n147 publications\u00a0\u00a0\u00a05,427 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nsee profile\n\ndmitri b chklovskii\nsimons foundation\n\n188 publications\u00a0\u00a0\u00a018,455 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nsome of the authors of this publication are also working on these related projects:\n\nantisense transcription view project\n\ncalcium imaging view project\n\nall content following this page was uploaded by dmitri b chklovskii on 09 january 2018.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "double q-learning\n\nhado van hasselt\n\nmulti-agent and adaptive computation group\n\ncentrum wiskunde & informatica\n\nabstract\n\nin some stochastic environments the well-known reinforcement learning algo-\nrithm q-learning performs very poorly. this poor performance is caused by large\noverestimations of action values. these overestimations result from a positive\nbias that is introduced because q-learning uses the maximum action value as an\napproximation for the maximum expected action value. we introduce an alter-\nnative way to approximate the maximum expected value for any set of random\nvariables. the obtained double estimator method is shown to sometimes under-\nestimate rather than overestimate the maximum expected value. we apply the\ndouble estimator to q-learning to construct double q-learning, a new off-policy\nreinforcement learning algorithm. we show the new algorithm converges to the\noptimal policy and that it performs well in some settings in which q-learning per-\nforms poorly due to ", "0\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n3\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n3\n3\n3\n1\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nthalamocortical motor circuit insights for more\nrobust hierarchical control of complex sequences\n\nlaureline logiaco\u2217\nll3041@columbia.edu\n\ng. sean escola\u2217\n\ngse3@columbia.edu\n\nabstract\n\nwe study learning of recurrent neural networks that produce temporal sequences\nconsisting of the concatenation of re-usable \u2018motifs\u2019. in the context of neuroscience\nor robotics, these motifs would be the motor primitives from which complex\nbehavior is generated. given a known set of motifs, can a new motif be learned\nwithout affecting the performance of the known set and then used in new sequences\nwithout \ufb01rst explicitly learning every possible transition? two requirements enable\nthis: (i) parameter updates while learning a new motif do not interfere with the\nparameters used for the previously acquired ones; and (ii) a new motif can be\nrobustly generated when starting from the network state reached at the end of any\no", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nsingle  neuron  dynamics  and  computation\nnicolas  brunel1,*,  vincent  hakim2 and  magnus  je  richardson3\n\nat  the  single  neuron  level,  information  processing  involves  the\ntransformation  of  input  spike  trains  into  an  appropriate  output\nspike  train.  building  upon  the  classical  view  of  a  neuron  as  a\nthreshold  device,  models  have  been  developed  in  recent  years\nthat  take  into  account  the  diverse  electrophysiological  make-\nup  of  neurons  and  accurately  describe  their  input-output\nrelations.  here,  we  review  these  recent  advances  and  survey\nthe  computational  roles  that  they  have  uncovered  for  various\nelectrophysiological  properties,  for  dendritic  arbor  anatomy  as\nwell  as  for  short-term  synaptic  plasticity.\n\naddresses\n1 departments  of  statistics  and  neurobiology,  university  of  chicago,\nchicago,  usa\n2 laboratoire  de  physique  statistique,  cnrs,  un", "international journal of computer vision 70(1), 77\u201390, 2006\nc(cid:2) 2006 springer science + business media, llc. manufactured in the netherlands.\ndoi: 10.1007/s11263-005-4939-z\n\nunsupervised learning of image manifolds by semide\ufb01nite programming\n\ndepartment of computer and information science, university of pennsylvania, philadelphia, pa 19104-6389\n\nkilian q. weinberger and lawrence k. saul\n\nkilianw@cis.upenn.edu\nlsaul@cis.upenn.edu\n\nreceived april 5, 2005; revised august 12, 2005; accepted september 9, 2005\n\nfirst online version published in may, 2006\n\nabstract. can we detect low dimensional structure in high dimensional data sets of images? in this paper, we\npropose an algorithm for unsupervised learning of image manifolds by semide\ufb01nite programming. given a data set\nof images, our algorithm computes a low dimensional representation of each image with the property that distances\nbetween nearby images are preserved. more generally, it can be used to analyze high dimensional data that", "neuron\n\narticle\n\noverriding phasic dopamine signals\nredirects action selection\nduring risk/reward decision making\n\ncolin m. stopper,1 maric t.l. tse,1 david r. montes,1 candice r. wiedman,1 and stan b. floresco1,*\n1department of psychology and brain research centre, university of british columbia, vancouver, bc v6t 1z4, canada\n*correspondence: \ufb02oresco@psych.ubc.ca\nhttp://dx.doi.org/10.1016/j.neuron.2014.08.033\n\nsummary\n\nphasic increases and decreases in dopamine (da)\ntransmission encode reward prediction errors thought\nto facilitate reward-related learning, yet how these\nsignals guide action selection in more complex situa-\ntions requiring evaluation of different reward remains\nunclear. we manipulated phasic da signals while\nrats performed a risk/reward decision-making task,\nusing temporally discrete stimulation of either the\nlateral habenula (lhb) or rostromedial tegmental nu-\ncleus (rmtg) to suppress da bursts (con\ufb01rmed with\nneurophysiological studies) or the ventral tegmental\narea (", "research\n\nneuroscience\n\ndendritic action potentials and computation in\nhuman layer 2/3 cortical neurons\n\nalbert gidon1, timothy adam zolnik1, pawel fidzinski2,3, felix bolduan4, athanasia papoutsi5,\npanayiota poirazi5, martin holtkamp2, imre vida3,4, matthew evan larkum1,3*\n\nthe active electrical properties of dendrites shape neuronal input and output and are fundamental to\nbrain function. however, our knowledge of active dendrites has been almost entirely acquired from\nstudies of rodents. in this work, we investigated the dendrites of layer 2 and 3 (l2/3) pyramidal neurons\nof the human cerebral cortex ex vivo. in these neurons, we discovered a class of calcium-mediated\ndendritic action potentials (dcaaps) whose waveform and effects on neuronal output have not been\npreviously described. in contrast to typical all-or-none action potentials, dcaaps were graded; their\namplitudes were maximal for threshold-level stimuli but dampened for stronger stimuli. these dcaaps\nenabled the dendrites ", "\f", "highly accurate protein structure prediction \nwith alphafold\n\nhttps://doi.org/10.1038/s41586-021-03819-2\nreceived: 11 may 2021\naccepted: 12 july 2021\npublished online: 15 july 2021\nopen access\n\n check for updates\n\njohn jumper1,4\u2009\u2709, richard evans1,4, alexander pritzel1,4, tim green1,4, michael figurnov1,4, \nolaf ronneberger1,4, kathryn tunyasuvunakool1,4, russ bates1,4, augustin \u017e\u00eddek1,4, \nanna potapenko1,4, alex bridgland1,4, clemens meyer1,4, simon a. a. kohl1,4, \nandrew j. ballard1,4, andrew cowie1,4, bernardino romera-paredes1,4, stanislav nikolov1,4, \nrishub jain1,4, jonas adler1, trevor back1, stig petersen1, david reiman1, ellen clancy1, \nmichal zielinski1, martin steinegger2,3, michalina pacholska1, tamas berghammer1, \nsebastian bodenstein1, david silver1, oriol vinyals1, andrew w. senior1, koray kavukcuoglu1, \npushmeet kohli1 & demis hassabis1,4\u2009\u2709\n\nproteins are essential to life, and understanding their structure can facilitate a \nmechanistic understanding of their function. th", "0\n1\n0\n2\n\n \nt\nc\no\n8\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n7\n6\n4\n3\n\n.\n\n0\n1\n0\n1\n:\nv\ni\nx\nr\na\n\nfast inference in sparse coding algorithms\n\nwith applications to object recognition\n\nkoray kavukcuoglu\n\nmarc\u2019aurelio ranzato\n\nyann lecun\n\ndepartment of computer science\n\ncourant institute of mathematical sciences\nnew york university, new york, ny 10003\n\n{koray,ranzato,yann}@cs.nyu.edu\n\ndecember 4, 2008\n\ncomputational and biological learning laboratory\n\ntechnical report\n\ncbll-tr-2008-12-01\u2020\n\nabstract\n\nadaptive sparse coding methods learn a possibly overcomplete set of\nbasis functions, such that natural image patches can be reconstructed by\nlinearly combining a small subset of these bases. the applicability of\nthese methods to visual object recognition tasks has been limited because\nof the prohibitive cost of the optimization algorithms required to compute\nthe sparse representation. in this work we propose a simple and e\ufb03cient\nalgorithm to learn basis functions. after training, this model also provides\n", "differentiable plasticity: training plastic neural networks with\n\nbackpropagation\n\nthomas miconi 1 jeff clune 1 kenneth o. stanley 1\n\nabstract\n\nhow can we build agents that keep learning from\nexperience, quickly and ef\ufb01ciently, after their ini-\ntial training? here we take inspiration from the\nmain mechanism of learning in biological brains:\nsynaptic plasticity, carefully tuned by evolution\nto produce ef\ufb01cient lifelong learning. we show\nthat plasticity, just like connection weights, can\nbe optimized by gradient descent in large (mil-\nlions of parameters) recurrent networks with heb-\nbian plastic connections. first, recurrent plastic\nnetworks with more than two million parameters\ncan be trained to memorize and reconstruct sets\nof novel, high-dimensional (1,000+ pixels) nat-\nural images not seen during training. crucially,\ntraditional non-plastic recurrent networks fail to\nsolve this task. furthermore, trained plastic net-\nworks can also solve generic meta-learning tasks\nsuch as the omnig", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nbackpropagation  through  time  and  the  brain\ntimothy  p  lillicrap1,2,3 and  adam  santoro1,3\n\nit  has  long  been  speculated  that  the  backpropagation-of-error\nalgorithm  (backprop)  may  be  a  model  of  how  the  brain  learns.\nbackpropagation-through-time  (bptt)  is  the  canonical\ntemporal-analogue  to  backprop  used  to  assign  credit  in\nrecurrent  neural  networks  in  machine  learning,  but  there\u2019s  even\nless  conviction  about  whether  bptt  has  anything  to  do  with  the\nbrain.  even  in  machine  learning  the  use  of  bptt  in  classic\nneural  network  architectures  has  proven  insuf\ufb01cient  for  some\nchallenging  temporal  credit  assignment  (tca)  problems  that\nwe  know  the  brain  is  capable  of  solving.  nonetheless,  recent\nwork  in  machine  learning  has  made  progress  in  solving  dif\ufb01cult\ntca  problems  by  employing  novel  memory-based  and\nattention-based  architectures", "letter\npurkinje-cell plasticity and cerebellar motor\nlearning are graded by complex-spike duration\n\ndoi:10.1038/nature13282\n\nyan yang1 & stephen g. lisberger1,2\n\nbehavioural learning is mediated by cellular plasticity, such as changes\nin the strength of synapses at specific sites in neural circuits. the the-\nory of cerebellar motor learning1\u20133 relies on movement errors signalled\nby climbing-fibre inputs to cause long-term depression of synapses\nfrom parallel fibres to purkinje cells4,5. however, a recent review6 has\ncalled into question the widely held view that the climbing-fibre input\nis an \u2018all-or-none\u2019 event. in anaesthetized animals, there is wide vari-\nation in the duration of the complex spike (cs) caused in purkinje cells\nby a climbing-fibre input7. furthermore, the amount of plasticity in\npurkinje cells is graded according to the duration of electrically con-\ntrolled bursts in climbing fibres8,9. the duration of bursts depends\non the \u2018state\u2019 of the inferior olive and therefore", "physiol rev 95: 853\u2013951, 2015\npublished june 24, 2015; doi:10.1152/physrev.00023.2014\n\nneuronal reward and decision signals:\nfrom theories to data\nwolfram schultz\n\ndepartment of physiology, development and neuroscience, university of cambridge, cambridge, united\nkingdom\n\nl schultz w. neuronal reward and decision signals: from theories to data. physiol rev 95:\n\n853\u2013951, 2015. published june 24, 2015; doi:10.1152/physrev.00023.2014.\u2014re-\nwards are crucial objects that induce learning, approach behavior, choices, and emo-\ntions. whereas emotions are dif\ufb01cult to investigate in animals, the learning function is\nmediated by neuronal reward prediction error signals which implement basic con-\nstructs of reinforcement learning theory. these signals are found in dopamine neurons, which emit\na global reward signal to striatum and frontal cortex, and in speci\ufb01c neurons in striatum, amygdala,\nand frontal cortex projecting to select neuronal populations. the approach and choice functions\ninvolve subj", "published as a conference paper at iclr 2019\n\nffjord: free-form continuous dynamics for\nscalable reversible generative models\n\nwill grathwohl\u2217\u2020\u2021 , ricky t. q. chen\u2217\u2020, jesse bettencourt\u2020, ilya sutskever\u2021 , david duvenaud\u2020\n\nabstract\n\nreversible generative models map points from a simple distribution to a complex\ndistribution through an easily invertible neural network. likelihood-based training\nof these models requires restricting their architectures to allow cheap computation\nof jacobian determinants. alternatively, the jacobian trace can be used if the\ntransformation is speci\ufb01ed by an ordinary differential equation. in this paper, we\nuse hutchinson\u2019s trace estimator to give a scalable unbiased estimate of the log-\ndensity. the result is a continuous-time invertible generative model with unbiased\ndensity estimation and one-pass sampling, while allowing unrestricted neural net-\nwork architectures. we demonstrate our approach on high-dimensional density\nestimation, image generation, and v", "l\no\nn\ng\n-\nt\ne\nr\nm\n \nd\ne\np\nr\ne\ns\na\no\nn\n \no\nf\n \ne\nx\nd\nt\na\nt\no\nr\ny\n \ns\ny\nn\na\np\nt\ni\nc\n \nt\nr\na\nn\ns\nm\ni\ns\ns\ni\no\nn\n \na\nn\nd\n \ni\nt\ns\n \nr\ne\nl\na\nt\ni\no\nn\ns\nh\ni\np\n \nt\no\n \nl\no\nn\ng\n-\nt\ne\nr\nm\n \np\no\nt\ne\nn\n#\na\nt\ni\no\nn\n \na\nl\na\ni\nn\n \na\nr\nt\no\nl\na\n \na\nn\nd\n \nw\no\nl\nf\n \ns\ni\nn\ng\ne\nr\n \na\nl\na\ni\nn\n \na\nr\nt\no\nl\na\n \na\nn\nd\n \nw\no\nf\nf\n \ns\ni\nn\ng\ne\nr\n \na\nr\ne\n \na\nt\n \nt\nh\ne\n \nm\na\nx\n-\np\nl\na\nn\nc\nk\n \ni\nn\ns\nt\ni\nt\nu\nt\n \nf\ni\nj\nr\n \nh\ni\nm\nf\no\nr\ns\nc\nh\nu\nn\ng\n,\n \nd\ne\nu\nt\ns\nc\nh\no\nr\nd\ne\nn\n \ns\nt\nr\na\ns\ns\ne\n \n4\n6\n,\n \nd\n-\n6\n0\n5\n2\n8\n \nf\nr\na\nn\nk\nf\nu\nr\nt\n/\nm\n \n7\n1\n,\n \ng\ne\nr\nm\na\nn\ny\n.\n \ni\nn\n \nm\na\nn\ny\n \nb\nr\na\ni\nn\n \na\nr\ne\na\ns\n,\n \ni\nn\nc\nl\nu\nd\ni\nn\ng\n \nt\nh\ne\n \nc\ne\nr\ne\nb\ne\nl\nl\na\nr\n \nc\no\nr\nt\ne\nx\n,\n \nn\ne\no\nc\no\nr\nt\ne\nx\n,\n \nh\ni\np\np\no\nc\na\nm\np\nu\ns\n,\n \ns\nt\nr\ni\na\nt\nu\nm\n \na\nn\nd\n \nn\nu\nc\nl\ne\nu\ns\n \na\nc\nc\nu\nm\n-\n \nb\ne\nn\ns\n,\n \nb\nr\ni\ne\nf\n \na\nc\nt\ni\nv\na\nt\ni\no\nn\n \no\nf\n \na\nn\n \ne\nx\nc\ni\nt\na\nt\no\nr\ny\n \np\na\nt\nh\nw\na\ny\n \nc\na\nn\n \np\nr\no\nd\nu\nc\ne\n \nl\no\nn\ng\n-\nt\ne\nr\nm\n \nd\ne\np\nr\ne\ns\ns\ni\no\nn\n \n(\nl\n \nt\nd\n)\n \no\nf\n \ns\ny\nn\na\np\nt\ni\nc\n \nt\nr\na\nn\ns\n-\n \nm\ni\ns\ns\ni\no\nn\n.\n \ni\nn\n \nm\no\ns\nt\n \np\nr\ne\n", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/220500283\n\na novel spike distance\n\narticle\u00a0\u00a0in\u00a0\u00a0neural computation \u00b7 april 2001\n\ndoi: 10.1162/089976601300014321\u00a0\u00b7\u00a0source: dblp\n\ncitations\n520\n\n1 author:\n\nmark c w van rossum\nuniversity of nottingham\n\n119 publications\u00a0\u00a0\u00a06,021 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n1,325\n\nall content following this page was uploaded by mark c w van rossum on 30 may 2014.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "why is real-world visual object recognition\nhard?\nnicolas pinto1,2[\n\n, david d. cox1,2,3[\n\n, james j. dicarlo1,2*\n\n1 mcgovern institute for brain research, massachusetts institute of technology, cambridge, massachusetts, united states of america, 2 department of brain and cognitive\nsciences, massachusetts institute of technology, cambridge, massachusetts, united states of america, 3 the rowland institute at harvard, cambridge, massachusetts, united\nstates of america\n\nprogress in understanding the brain mechanisms underlying vision requires the construction of computational models\nthat not only emulate the brain\u2019s anatomy and physiology, but ultimately match its performance on visual tasks. in\nrecent years, \u2018\u2018natural\u2019\u2019 images have become popular in the study of vision and have been used to show apparently\nimpressive progress in building such models. here, we challenge the use of uncontrolled \u2018\u2018natural\u2019\u2019 images in guiding\nthat progress. in particular, we show that a simple v1-like model\u2014", "1254\n\nieee transactions on pattern analysis and machine intelligence,  vol.  20,  no.  11,  november  1998\n\nshort papers\n\na model of saliency-based visual attention\n\nfor rapid scene analysis\n\nlaurent itti, christof koch, and ernst niebur\n\nabstract\u2014a visual attention system, inspired by the behavior and the\nneuronal architecture of the early primate visual system, is presented.\nmultiscale image features are combined into a single topographical\nsaliency map. a dynamical neural network then selects attended\nlocations in order of decreasing saliency. the system breaks down the\ncomplex problem of scene understanding by rapidly selecting, in a\ncomputationally efficient manner, conspicuous locations to be analyzed\nin detail.\n\nindex terms\u2014visual attention, scene analysis, feature extraction,\ntarget detection, visual search.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014   f   \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\n1 introduction\nprimates  have  a  remarkable  ability  to  interpret  complex  scenes  in\nreal time, despite the limited speed of the neuronal har", "7\n1\n0\n2\n\n \n\ng\nu\na\n0\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n3\n3\n1\n4\n0\n\n.\n\n8\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nreproducibility of benchmarked deep reinforcement\n\nlearning tasks for continuous control\n\nriashat islam\u2217\u2020\n\nschool of computer science\n\nmcgill university\n\nmontreal, qc, canada\n\npeter henderson\u2020\n\nschool of computer science\n\nmcgill university\n\nmontreal, qc, canada\n\nriashat.islam@mail.mcgill.ca\n\npeter.henderson@mail.mcgill.ca\n\nmaziar gomrokchi\n\nschool of computer science\n\nmcgill university\n\nmontreal, qc, canada\n\nmaziar.gomrokchi@mail.mcgill.ca\n\ndoina precup\n\nschool of computer science\n\nmcgill university\n\nmontreal, qc, canada\n\ndprecup@cs.mcgill.ca\n\nabstract\n\npolicy gradient methods in reinforcement learning have become increasingly preva-\nlent for state-of-the-art performance in continuous control tasks. novel methods\ntypically benchmark against a few key algorithms such as deep deterministic pol-\nicy gradients and trust region policy optimization. as such, it is important to\npresent and use consistent ", "7\n1\n0\n2\n\n \nl\nu\nj\n \n\n0\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n4\n2\n2\n1\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\nsample efficient actor-critic with\nexperience replay\n\nziyu wang\ndeepmind\nziyu@google.com\n\nvictor bapst\ndeepmind\nvbapst@google.com\n\nnicolas heess\ndeepmind\nheess@google.com\n\nvolodymyr mnih\ndeepmind\nvmnih@google.com\n\nremi munos\ndeepmind\nmunos@google.com\n\nkoray kavukcuoglu\ndeepmind\nkorayk@google.com\n\nnando de freitas\ndeepmind, cifar, oxford university\nnandodefreitas@google.com\n\nabstract\n\nthis paper presents an actor-critic deep reinforcement learning agent with ex-\nperience replay that is stable, sample ef\ufb01cient, and performs remarkably well on\nchallenging environments, including the discrete 57-game atari domain and several\ncontinuous control problems. to achieve this, the paper introduces several inno-\nvations, including truncated importance sampling with bias correction, stochastic\ndueling network architectures, and a new trust region policy optimization ", "5\n1\n0\n2\n\n \nc\ne\nd\n4\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n5\n5\n4\n4\n0\n\n.\n\n2\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nmemory-based control with recurrent neural\n\nnetworks\n\nnicolas heess*\n\njonathan j hunt*\n\ntimothy p lillicrap david silver\n\ngoogle deepmind\n\n* these authors contributed equally.\n\nheess, jjhunt, countzero, davidsilver @ google.com\n\nabstract\n\npartially observed control problems are a challenging aspect of reinforcement\nlearning. we extend two related, model-free algorithms for continuous control\n\u2013 deterministic policy gradient and stochastic value gradient \u2013 to solve partially\nobserved domains using recurrent neural networks trained with backpropagation\nthrough time. we demonstrate that this approach, coupled with long-short term\nmemory is able to solve a variety of physical control problems exhibiting an as-\nsortment of memory requirements. these include the short-term integration of in-\nformation from noisy sensors and the identi\ufb01cation of system parameters, as well\nas long-term memory problems that r", "the journal of neuroscience, august 15, 2018 \u2022 38(33):7255\u20137269 \u2022 7255\n\nbehavioral/cognitive\n\nlarge-scale, high-resolution comparison of the core visual\nobject recognition behavior of humans, monkeys, and\nstate-of-the-art deep artificial neural networks\n\nx rishi rajalingham,* x elias b. issa,* x pouya bashivan, x kohitij kar, kailyn schmidt, and x james j. dicarlo\nmcgovern institute for brain research and department of brain and cognitive sciences, massachusetts institute of technology, cambridge, massachusetts\n02139\n\nprimates, including humans, can typically recognize objects in visual images at a glance despite naturally occurring identity-preserving\nimage transformations (e.g., changes in viewpoint). a primary neuroscience goal is to uncover neuron-level mechanistic models that\nquantitatively explain this behavior by predicting primate performance for each and every image. here, we applied this stringent\nbehavioral prediction test to the leading mechanistic models of primate vision ", "2015 ieee international conference on computer vision\n\nsurpassing human-level performance on imagenet classi\ufb01cation\n\ndelving deep into recti\ufb01ers:\n\nkaiming he\n\nxiangyu zhang\n\nshaoqing ren\n\njian sun\n\nmicrosoft research\n\nabstract\n\nrecti\ufb01ed activation units (recti\ufb01ers) are essential for\nstate-of-the-art neural networks.\nin this work, we study\nrecti\ufb01er neural networks for image classi\ufb01cation from two\naspects. first, we propose a parametric recti\ufb01ed linear\nunit (prelu) that generalizes the traditional recti\ufb01ed unit.\nprelu improves model \ufb01tting with nearly zero extra com-\nputational cost and little over\ufb01tting risk. second, we derive\na robust initialization method that particularly considers\nthe recti\ufb01er nonlinearities. this method enables us to train\nextremely deep recti\ufb01ed models directly from scratch and to\ninvestigate deeper or wider network architectures. based\non the learnable activation and advanced initialization, we\nachieve 4.94% top-5 test error on the imagenet 2012 clas-\nsi\ufb01cation d", "4\n1\n0\n2\n\n \nr\np\na\n4\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n5\nv\n6\n2\n0\n6\n\n.\n\n2\n1\n3\n1\n:\nv\ni\nx\nr\na\n\nhow to construct deep recurrent neural networks\n\nrazvan pascanu1, caglar gulcehre1, kyunghyun cho2, and yoshua bengio1\n\n1d\u00b4epartement d\u2019informatique et de recherche op\u00b4erationelle, universit\u00b4e de montr\u00b4eal,\n\n{pascanur, gulcehrc}@iro.umontreal.ca, yoshua.bengio@umontreal.ca\n\n2department of information and computer science, aalto university school of science,\n\nkyunghyun.cho@aalto.fi\n\nabstract\n\nin this paper, we explore different ways to extend a recurrent neural network\n(rnn) to a deep rnn. we start by arguing that the concept of depth in an rnn\nis not as clear as it is in feedforward neural networks. by carefully analyzing\nand understanding the architecture of an rnn, however, we \ufb01nd three points of\nan rnn which may be made deeper; (1) input-to-hidden function, (2) hidden-to-\nhidden transition and (3) hidden-to-output function. based on this observation, we\npropose two novel architectures of a deep rnn ", "ieee transactions on  neural  networks. vol.  5.  no.  3.  may  l y y 4  \n\nsos \n\nanalysis  of  the  back-propagation \n\nalgorithm  with  momentum \n\nv.  v.  phansalkar  and  p.  s. sastrq \n\npattems  are presented  frequently so  that  ( 5 )  is essentially  equivalent \nto  the  algorithm \n\n/ l j , ( / t  + 1) = u ( , ( / ) )   - ~ l ( h e / i i l / < , ) ( l / ( t i ) )  \n\n+ i l ( l l # / ( t t )  - t o / ( t t  - 1 ) ) .  \n\n(6) \n\nabsfracr-  in  this  letter,  the  hack-propagation  algorithm  with  the \nmomentum term is analyzed. it is shown that all local  minima of the sum \nof  least squares error are stable. other equilibrium points are unstable. \n\nis  small  enough.  equation  (6)  is  exactly \nthis  is  justified  if \nequivalent to ( 5 )  if the number of  pattems  are finite and the updating \nis done  only  after each  cycle  of  presentation  of  the  pattems. \n\ni.  introduction \n\n11.  analysis \n\nback-propagation  (bp)  [ i ]   is  one  of  the  most  widely  used  algo- \nrithms ", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/7973607\n\nadaptive coding of reward value by dopamine neurons\n\narticle\u00a0\u00a0in\u00a0\u00a0science \u00b7 april 2005\n\ndoi: 10.1126/science.1105370\u00a0\u00b7\u00a0source: pubmed\n\ncitations\n1,146\n\n3 authors, including:\n\nphilippe tobler\nuniversity of zurich\n\n230 publications\u00a0\u00a0\u00a010,666 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n1,284\n\nchristopher d fiorillo\nkorea advanced institute of science and technology\n\n40 publications\u00a0\u00a0\u00a04,853 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nall content following this page was uploaded by christopher d fiorillo on 21 july 2015.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "published as a conference paper at iclr 2018\n\ntowards deep learning models resistant to\nadversarial attacks\n\naleksander m \u02dbadry, aleksandar makelov, ludwig schmidt, dimitris tsipras, adrian vladu\u2217\ndepartment of electrical engineering and computer science\nmassachusetts institute of technology\ncambridge, ma 02139, usa\n{madry,amakelov,ludwigs,tsipras,avladu}@mit.edu\n\nabstract\n\nrecent work has demonstrated that neural networks are vulnerable to adversarial\nexamples, i.e., inputs that are almost indistinguishable from natural data and yet\nclassi\ufb01ed incorrectly by the network. to address this problem, we study the\nadversarial robustness of neural networks through the lens of robust optimization.\nthis approach provides us with a broad and unifying view on much prior work on\nthis topic. its principled nature also enables us to identify methods for both training\nand attacking neural networks that are reliable and, in a certain sense, universal.\nin particular, they specify a concrete security gu", "representational drift in the mouse visual cortex\n\ngraphical abstract\n\nauthors\n\narticle\n\ndaniel deitch, alon rubin, yaniv ziv\n\ncorrespondence\nyaniv.ziv@weizmann.ac.il\n\nin brief\ndeitch et al. \ufb01nd continuous changes in\nneuronal responses to the same stimuli\n(representational drift) over minutes to\ndays across multiple visual areas, cortical\nlayers, and cell types. despite these\nchanges in the coding of individual cells,\nthe structure of the relationships between\npopulation activity patterns remains\nstable and stereotypic.\n\nhighlights\nd the visual cortex exhibits representational drift over minutes\n\nto days\n\nd drift is characterized by both changes in cells\u2019 activity rates\n\nand tuning\n\nd neural-code stability does not follow the hierarchy of the\n\nvisual system\n\nd the relationships between population activity patterns\n\nremain stable over time\n\ndeitch et al., 2021, current biology 31, 4327\u20134339\noctober 11, 2021 \u00aa 2021 elsevier inc.\nhttps://doi.org/10.1016/j.cub.2021.07.062\n\nll\n\n\f", "perspective\n\nneuron\n\nwhat is optimal about motor control?\n\nkarl friston1,*\n1the wellcome trust centre for neuroimaging, institute of neurology, 12 queen square, london wc1n 3bg, uk\n*correspondence: k.friston@ucl.ac.uk\ndoi 10.1016/j.neuron.2011.10.018\n\nthis article poses a controversial question: is optimal control theory useful for understanding motor behavior\nor is it a misdirection? this question is becoming acute as people start to con\ufb02ate internal models in motor\ncontrol and perception (poeppel et al., 2008; hickok et al., 2011). however, the forward models in motor\ncontrol are not the generative models used in perceptual inference. this perspective tries to highlight the\ndifferences between internal models in motor control and perception and asks whether optimal control is\nthe right way to think about things. the issues considered here may have broader implications for optimal\ndecision theory and bayesian approaches to learning and behavior in general.\n\nintroduction\noptimal contro", "8\n1\n0\n2\n\n \n\ng\nu\na\n6\n1\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n1\n7\n2\n1\n1\n\n.\n\n4\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ngaussian process behaviour in wide deep neural networks\n\nalexander g. de g. matthews\ndepartment of engineering\ntrumpington street\nuniversity of cambridge, uk\n\nmark rowland\ndepartment of pure mathematics and mathematical statistics\nwilberforce road\nuniversity of cambridge, uk\n\njiri hron\ndepartment of engineering\ntrumpington street\nuniversity of cambridge, uk\n\nrichard e. turner\ndepartment of engineering\ntrumpington street\nuniversity of cambridge, uk\n\nzoubin ghahramani\ndepartment of engineering\ntrumpington street\nuniversity of cambridge, uk\nuber ai labs\n\nam554@cam.ac.uk\n\nmr504@cam.ac.uk\n\njh2084@cam.ac.uk\n\nret26@cam.ac.uk\n\nzoubin@eng.cam.ac.uk\n\nabstract\n\nwhilst deep neural networks have shown great empirical success, there is still much work\nto be done to understand their theoretical properties. in this paper, we study the rela-\ntionship between random, wide, fully connected, feedforward networks w", "this is a digitally remastered version of a paper published in the proceedings of the 18th international conference on\nmachine learning (2001), pp. 417\u2013424. this version should be crisper but otherwise identical to the published article.\n\no\ufb00-policy temporal-di\ufb00erence learning\n\nwith function approximation\n\ndoina precup\nschool of computer science, mcgill university, montreal, quebec, canada h3a 2a7\n\ndprecup@cs.mcgill.ca\n\nrichard s. sutton\nsanjoy dasgupta\nat&t shannon laboratory, 180 park ave., florham park, nj 07932 usa\n\nsutton@research.att.com\ndasgupta@research.att.com\n\nabstract\n\nwe introduce the \ufb01rst algorithm for o\ufb00-policy\ntemporal-di\ufb00erence learning that is stable\nwith linear function approximation. o\ufb00-\npolicy learning is of interest because it forms\nthe basis for popular reinforcement learning\nmethods such as q-learning, which has been\nknown to diverge with linear function approx-\nimation, and because it is critical to the prac-\ntical utility of multi-scale, multi-goal, learn-\ning f", "open\n\nreceived: 19 august 2015\naccepted: 11 august 2016\npublished: 07 september 2016\n\ndeep networks can resemble \nhuman feed-forward vision in \ninvariant object recognition\n\nsaeed reza kheradpisheh1,2, masoud ghodrati3,4, mohammad ganjtabesh1 & \ntimoth\u00e9e masquelier2,5,6,7\n\ndeep convolutional neural networks (dcnns) have attracted much attention recently, and have shown \nto be able to recognize thousands of object categories in natural image databases. their architecture \nis somewhat similar to that of the human visual system: both use restricted receptive fields, and a \nhierarchy of layers which progressively extract more and more abstracted features. yet it is unknown \nwhether dcnns match human performance at the task of view-invariant object recognition, whether \nthey make similar errors and use similar representations for this task, and whether the answers depend \non the magnitude of the viewpoint variations. to investigate these issues, we benchmarked eight state-\nof-the-art dcnns,", "neuroimage 99 (2014) 509\u2013524\n\ncontents lists available at sciencedirect\n\nneuroimage\n\nj ou r n a l h o m e p a ge : w ww . e l s e v i e r . c o m / l oc a te / y ni mg\n\ncorrespondences between retinotopic areas and myelin maps in human\nvisual cortex\nrouhollah o. abdollahi a,1, hauke kolster a,1, matthew f. glasser b,1, emma c. robinson c, timothy s. coalson b,\ndonna dierker b, mark jenkinson c, david c. van essen b,2, guy a. orban a,d,\u204e,2\na laboratorium voor neuro-en psychofysiologie, ku leuven, leuven, belgium\nb department of anatomy and neurobiology, washington university school of medicine, st louis, mo, usa\nc centre for functional magnetic resonance imaging of the brain (fmrib), university of oxford, oxford, uk\nd department of neuroscience, university of parma, parma, italy\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\naccepted 16 june 2014\navailable online 24 june 2014\n\nwe generated probabilistic area maps and maximum probability maps (mpms) for a set of 18 retinotopi", "transactions on neural networks and learning systems\n\n1\n\ndeep reinforcement learning with modulated\n\nhebbian plus q network architecture\n\npawel ladosz, eseoghene ben-iwhiwhu, jeffery dick, nicholas ketz, soheil kolouri, jeffrey l. krichmar,\n\npraveen pilly, andrea soltoggio,\n\n1\n2\n0\n2\n\n \nt\nc\no\n4\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n2\n0\n9\n9\n0\n\n.\n\n9\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014in this paper, we consider a subclass of partially\nobservable markov decision process (pomdp) problems which\nwe termed confounding pomdps. in these types of pomdps\ntemporal difference (td)-based rl algorithms struggle, as td\nerror cannot be easily derived from observations. we solve these\ntypes of problems using a new bio-inspired neural architecture\nthat combines a modulated hebbian network (mohn) with dqn,\nwhich we call modulated hebbian plus q network architecture\n(mohqa). the key idea is to use a hebbian network with rarely\ncorrelated bio-inspired neural traces to bridge temporal delays\nbetween actions and rewards w", "22. j. k. polansky et al., eur. j. immunol. 38, 1654\u20131663\n\n(2008).\n\n23. d. e. chatterton, d. n. nguyen, s. b. bering, p. t. sangild, int.\n\nj. biochem. cell biol. 45, 1730\u20131747 (2013).\n\n24. s. z. josefowicz et al., nature 482, 395\u2013399 (2012).\n25. g. longo, i. berti, a. w. burks, b. krauss, e. barbi, lancet 382,\n\n1656\u20131664 (2013).\n\n26. j. cahenzli, y. k\u00f6ller, m. wyss, m. b. geuking, k. d. mccoy,\n\ncell host microbe 14, 559\u2013570 (2013).\n\n27. k. d. mccoy et al., immunity 24, 329\u2013339 (2006).\n\nacknowledgments\nwe thank a. macpherson, k. mccoy, and d. artis for generously\nproviding various strains of germ-free mice to start our colony;\nt. k. kim, j. w. seo, m. o. lee, h. j. woo, h. j. jung, h.j. ko,\nj. kirundi, s. sakaguchi, and n. ohkura for technical support;\nand j. sprent, s. h. im, m. h. jang, d. rudra, and j. h. cho for\ndiscussions. the data from this study are tabulated in the main paper\nand in the supplementary materials. this work was supported by project\nibs-r005-d1 of the institute for", "article\n\nhigher-order thalamocortical inputs gate synaptic\nlong-term potentiation via disinhibition\n\ngraphical abstract\n\nauthors\n\nleena e. williams, anthony holtmaat\n\ncorrespondence\nanthony.holtmaat@unige.ch\n\nin brief\nusing ex vivo patch-clamp recordings,\noptogenetics, and chemogenetics,\nwilliams and holtmaat dissect the circuits\nunderlying sensory-driven ltp in the\ncortex. this reveals a circuit motif in\nwhich higher-order thalamocortical input\ngates plasticity of intracortical synapses\nvia vip-mediated disinhibition.\n\naav.chr2\n          \n\naav.dio.hm4di\n\naav.chr2\n          \n\nbarrel cortex\nl1\n\nl4\n\nfirst-order\nthalamus (vpm)\n\nhigher-order\nthalamus (pom)\n\noptical \nstimulation (os)\n\nl1\n\nl2/3\n\nl4\n\ng\nn\ni\nt\na\ng\n\ndisinhibition\n\nsst\n\nvip\n\npn\n\nltp\n\npre\nrps\n\nes\n\nelectrical \nstimulation (es)\n\nrythmic paired stimulation @8hz\n\n(rps, l4-es & pom-os)\n\ne\ns\nu\no\nm\n-\ne\nr\nc\np\nv\n\ni\n\n.\n\nos\n\nl1\n\nl2/3\n\nl4\n\nno\nltp\n\nreduced\ndisinhibition\n\nsst\n\ng\nn\ni\nt\na\ng\n \nd\ne\nc\nu\nd\ne\nr\n\nx\n\nvip\nhm4di\n+cno\n\npn\n\npre\nrps\n\npost\nrp", "measuring statistical dependence with\n\nhilbert-schmidt norms\n\narthur gretton1, olivier bousquet2, alex smola3, and bernhard sch\u00a8olkopf1\n\n1 mpi for biological cybernetics, spemannstr. 38, 72076 t\u00a8ubingen, germany\n\n{arthur, bernhard.schoelkopf}@tuebingen.mpg.de\n2 pertinence, 32, rue des je\u02c6uneurs, 75002 paris, france\n\n3 national ict australia, north road, canberra 0200 act, australia\n\nolivier.bousquet@pertinence.com\n\nalex.smola@nicta.com.au\n\nabstract. we propose an independence criterion based on the eigen-\nspectrum of covariance operators in reproducing kernel hilbert spaces\n(rkhss), consisting of an empirical estimate of the hilbert-schmidt\nnorm of the cross-covariance operator (we term this a hilbert-schmidt in-\ndependence criterion, or hsic). this approach has several advantages,\ncompared with previous kernel-based independence criteria. first, the\nempirical estimate is simpler than any other kernel dependence test, and\nrequires no user-de\ufb01ned regularisation. second, there is a clear", "neuron\n\narticle\n\ngenerating coherent patterns of activity\nfrom chaotic neural networks\n\ndavid sussillo1,* and l.f. abbott1,*\n1department of neuroscience, department of physiology and cellular biophysics, columbia university college of physicians and surgeons,\nnew york, ny 10032-2695, usa\n*correspondence: sussillo@neurotheory.columbia.edu (d.s.), lfa2103@columbia.edu (l.f.a.)\ndoi 10.1016/j.neuron.2009.07.018\n\nsummary\n\nneural circuits display complex activity patterns both\nspontaneously and when responding to a stimulus or\ngenerating a motor output. how are these two forms\nof activity related? we develop a procedure called\nforce learning for modifying synaptic strengths\neither external to or within a model neural network\nto change chaotic spontaneous activity into a wide\nvariety of desired activity patterns. force learning\nworks even though the networks we train are sponta-\nneously chaotic and we leave feedback loops intact\nand unclamped during learning. using this approach,\nwe construct", "regularized auto-encoders estimate local statistics\n\nguillaume alain, yoshua bengio and salah rifai\n\ndepartment of computer science and operations research\n\nuniversity of montreal\n\nmontreal, h3c 3j7, quebec, canada\n\nabstract\n\nwhat do auto-encoders learn about the underlying data generating distribution?\nrecent work suggests that some auto-encoder variants do a good job of capturing\nthe local manifold structure of the unknown data generating density. this paper\nclari\ufb01es some of these previous intuitive observations by showing that minimiz-\ning a particular form of regularized reconstruction error yields a reconstruction\nfunction that locally characterizes the shape of the data generating density. more\nprecisely, we show that the auto-encoder captures the score (derivative of the log-\ndensity with respect to the input) or the local mean associated with the unknown\ndata-generating density. this is the second result linking denoising auto-encoders\nand score matching, but in way that is dif", "brain-inspired predictive coding dynamics improve\n\nthe robustness of deep neural networks\n\nbhavin choksi\u2217\n\ncerco cnrs, umr 5549 &\n\nuniversit\u00e9 de toulouse\n\nbhavin.choksi@cnrs.fr\n\nmilad mozafari*\n\ncerco cnrs, umr 5549 &\n\nirit cnrs, umr 5505\n\nmilad.mozafari@cnrs.fr\n\ncallum biggs o\u2019may\n\ncerco cnrs\numr 5549\n\nbenjamin ador\ncerco cnrs\numr 5549\n\nandrea alamia\ncerco cnrs\numr 5549\n\nru\ufb01n vanrullen\n\ncerco cnrs, umr 5549 &\naniti, universit\u00e9 de toulouse\nrufin.vanrullen@cnrs.fr\n\nabstract\n\ndeep neural networks excel at image classi\ufb01cation, but their performance is far\nless robust to input perturbations than human perception. in this work we address\nthis shortcoming by incorporating brain-inspired recurrent dynamics in deep con-\nvolutional networks. we augment a pretrained feedforward classi\ufb01cation model\n(vgg16 trained on imagenet) with a \u201cpredictive coding\u201d strategy: a framework\npopular in neuroscience for characterizing cortical function. at each layer of the\nhierarchical model, generative feedback \u201c", "parmac: distributed optimisation of nested functions,\n\nwith application to learning binary autoencoders\n\nmiguel \u00b4a. carreira-perpi \u02dcn\u00b4an 1 mehdi alizadeh 1\n\nabstract\n\nmany powerful machine learning models are based on the composition of multiple processing layers, such as\ndeep nets, which gives rise to nonconvex objective functions. a general, recent approach to optimise such\n\u201cnested\u201d functions is the method of auxiliary coordinates (mac). mac introduces an auxiliary coordinate for\neach data point in order to decouple the nested model into independent submodels. this decomposes the optimi-\nsation into steps that alternate between training single layers and updating the coordinates. it has the advantage\nthat it reuses existing single-layer algorithms, introduces parallelism, and does not need to use chain-rule gradi-\nents, so it works with nondifferentiable layers. we describe parmac, a distributed-computation model for mac.\nthis trains on a dataset distributed across machines while lim", "s\nc\ni\nt\ns\ni\nt\na\nt\ns\n\nreconciling modern machine-learning practice and\nthe classical bias\u2013variance trade-off\n\nmikhail belkina,b,1, daniel hsuc, siyuan maa, and soumik mandala\n\nadepartment of computer science and engineering, the ohio state university, columbus, oh 43210; bdepartment of statistics, the ohio state university,\ncolumbus, oh 43210; and ccomputer science department and data science institute, columbia university, new york, ny 10027\n\nedited by peter j. bickel, university of california, berkeley, ca, and approved july 2, 2019 (received for review february 21, 2019)\n\nbreakthroughs in machine learning are rapidly changing science\nand society, yet our fundamental understanding of this technol-\nogy has lagged far behind. indeed, one of the central tenets of the\n\ufb01eld, the bias\u2013variance trade-off, appears to be at odds with the\nobserved behavior of methods used in modern machine-learning\npractice. the bias\u2013variance trade-off implies that a model should\nbalance under\ufb01tting and over\ufb01tt", "parameters as interacting particles: long time\n\nconvergence and asymptotic error scaling of neural\n\nnetworks\n\ncourant institute of mathematical sciences\n\ncourant institute of mathematical sciences\n\ngrant m. rotskoff\n\nnew york university\n\nrotskoff@cims.nyu.edu\n\neric vanden-eijnden\n\nnew york university\neve2@cims.nyu.edu\n\nabstract\n\nthe performance of neural networks on high-dimensional data distributions sug-\ngests that it may be possible to parameterize a representation of a given high-\ndimensional function with controllably small errors, potentially outperforming\nstandard interpolation methods. we demonstrate, both theoretically and numer-\nically, that this is indeed the case. we map the parameters of a neural network\nto a system of particles relaxing with an interaction potential determined by the\nloss function. we show that in the limit that the number of parameters n is large,\nthe landscape of the mean-squared error becomes convex and the representation\nerror in the function scales a", "the journal of neuroscience, january 11, 2012 \u2022 32(2):551\u2013562 \u2022 551\n\nbehavioral/systems/cognitive\n\nneural prediction errors reveal a risk-sensitive\nreinforcement-learning process in the human brain\n\nyael niv,1 jeffrey a. edlund,2 peter dayan,3 and john p. o\u2019doherty2,4\n1psychology department and princeton neuroscience institute, princeton university, princeton, new jersey 08540, 2division of humanities and social\nsciences and computation and neural systems program, california institute of technology, pasadena, california 91125, 3gatsby computational\nneuroscience unit, university college london, london, wc1n 3ar, united kingdom, and 4trinity college institute of neuroscience and school of\npsychology, trinity college dublin, dublin 2, ireland\n\nhumans and animals are exquisitely, though idiosyncratically, sensitive to risk or variance in the outcomes of their actions. economic,\npsychological, and neural aspects of this are well studied when information about risk is provided explicitly. ho", "open access\n\nedited by\nnicol\u00e1s navarro-guerrero,\nl3s research center, germany\n\nreviewed by\nandreas schweiger,\nairbus, netherlands\ntongle zhou,\nnanjing university of aeronautics and\nastronautics, china\nguangda chen,\nzhejiang university, china\n\n*correspondence\nmuhammad burhan hafez\n\nburhan.hafez@uni-hamburg.de\n\n\u2020these authors share \ufb01rst authorship\n\nreceived 19 december 2022\naccepted 28 april 2023\npublished 27 june 2023\n\ncitation\nhafez mb, immisch t, weber t and wermter s\n(2023) map-based experience replay: a\nmemory-e(cid:8)cient solution to catastrophic\nforgetting in reinforcement learning.\nfront. neurorobot. 17:1127642.\ndoi: 10.3389/fnbot.2023.1127642\n\ncopyright\n\u00a9 2023 hafez, immisch, weber and wermter.\nthis is an open-access article distributed under\nthe terms of the creative commons attribution\nlicense (cc by). the use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in", "brain and cognition 112 (2017) 92\u201397\n\ncontents lists available at sciencedirect\n\nbrain and cognition\n\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / b & c\n\nspecial invited review\na review of predictive coding algorithms\n\nm.w. spratling\n\n\u21d1\n\nking\u2019s college london, department of informatics, london, uk\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 21 may 2015\nrevised 9 november 2015\naccepted 13 november 2015\navailable online 19 january 2016\n\nkeywords:\npredictive coding\nsignal processing\nretina\ncortex\nfree energy\nneural networks\n\ncontents\n\npredictive coding is a leading theory of how the brain performs probabilistic inference. however, there\nare a number of distinct algorithms which are described by the term \u2018\u2018predictive coding\u201d. this article pro-\nvides a concise review of these different predictive coding algorithms, highlighting their similarities and\ndifferences. five algorithms are covered: linear predictive coding which has a long", "neuronal dynamics regulating brain and behavioral\nstate transitions\n\narticle\n\nauthors\naaron s. andalman, vanessa m. burns,\nmatthew lovett-barron, ..., marc levoy,\nkanaka rajan, karl deisseroth\n\ncorrespondence\ndeissero@stanford.edu\n\nin brief\nbrainwide imaging in zebra\ufb01sh and\nnetwork modeling reveal that switching\nfrom active to passive coping state arises\nfrom progressive activation of habenular\nneurons in response to behavioral\nchallenge.\n\ngraphical abstract\n\nbehavioral challenge causes passive coping response\n\nlarval zebrafish\n\nchallenge\n\ninescapable shock\n\npassive coping\nhb encodes stress by recruitment of active ensemble\n\nactive coping\n\nchallenge\n\nbrainwide \n2p & lfm\nimaging\n\nvhb\ndhb\n\nraphe\n\noptogenetics and network modeling show causal \nmechanisms of habenulo-raphe circuitry\n\nraphe \npassive\n\nvhb\nactive \npassive\n\nnphr inhibition(   )\nchr activation \n(   )\nbrain-wide recurrent network \nmodels implicate intra-hb \nand raphe-hb projections\n\nhighlights\nd passive coping in response to beh", "biological constraints on neural \nnetwork models of cognitive function\n\nfriedemann\u00a0pulverm\u00fcller \nmalte\u00a0r.\u00a0henningsen-schomers \n\n , rosario\u00a0tomasello \n\n , \n\n  and thomas\u00a0wennekers \n\n \n\nabstract | neural network models are potential tools for improving our under\u00ad\nstanding of complex brain functions. to address this goal, these models need to be \nneurobiologically realistic. however, although neural networks have advanced \ndramatically in recent years and even achieve human\u00adlike performance on com\u00ad\nplex perceptual and cognitive tasks, their similarity to aspects of brain anatomy \nand physiology is imperfect. here, we discuss different types of neural models, \nincluding localist, auto\u00adassociative, hetero\u00adassociative, deep and whole\u00adbrain  \nnetworks, and identify aspects under which their biological plausibility can be \nimproved. these aspects range from the choice of model neurons and of mecha\u00ad\nnisms of synaptic plasticity and learning to implementation of inhibition and  \ncontrol, along w", "article\n\nthe striatum organizes 3d behavior via moment-to-\nmoment action selection\n\ngraphical abstract\n\nidentify syllables and transitions using moseq\n\ndepth sensor\n\nauthors\njeffrey e. markowitz, winthrop f. gillis,\ncelia c. beron, ..., scott w. linderman,\nbernardo l. sabatini,\nsandeep robert datta\n\nneural recording\n\nreared\n pause\n\ndive\n\nlocomotion\n\ntime\n\ncorrespondence\nsrdatta@hms.harvard.edu\n\nin brief\nthe striatum concatenates sub-second\n3d behavioral motifs into action\nsequences during naturalistic behaviors.\n\nstriatal neurons encode syllable identity and sequence statistics\n\nsyllables\n\nsequences\n\ny\nt\ni\nv\ni\nt\nc\na\n \nl\na\nr\nu\ne\nn\n\nsyllable a\n\nsimilar\n\ndissimilar\n\nrare\n\ncommon\n\ntime\n\nlesions of the striatum randomize syllable sequences \n\na\n\nb\n\nc\n\nd\n\nnormal syllable sequence\n\nlesion\n\nc\n\na\n\nb\n\nd\n\nhighlights\nd dorsolateral striatum systematically represents behavioral\n\nsyllables and grammar\n\nd complementary direct and indirect pathway activity encodes\n\nfast 3d pose dynamics\n\nd dorsolateral", "8\n1\n0\n2\n\n \n\np\ne\ns\n1\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n0\n7\n3\n0\n\n.\n\n9\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nsparse attentive backtracking: temporal credit\n\nassignment through reminding\n\nnan rosemary ke1, anirudh goyal1, olexa bilaniuk1, jonathan binas1,\n\nmichael c. mozer5, chris pal1,6, yoshua bengio1\u2020\n\n1 mila, universit\u00e9 de montr\u00e9al\n\n5 university of colorado, boulder\n6 mila, polytechnique montr\u00e9al\n\n\u2020cifar senior fellow.\n\nabstract\n\nlearning long-term dependencies in extended temporal sequences requires credit\nassignment to events far back in the past. the most common method for training\nrecurrent neural networks, back-propagation through time (bptt), requires credit\ninformation to be propagated backwards through every single step of the forward\ncomputation, potentially over thousands or millions of time steps. this becomes\ncomputationally expensive or even infeasible when used with long sequences.\nimportantly, biological brains are unlikely to perform such detailed reverse replay\nover very long sequences", "deep cortical layers are activated\ndirectly by thalamus\n\nchristine m. constantinople and randy m. bruno*\n\nthe thalamocortical (tc) projection to layer 4 (l4) is thought to be the main route by which sensory\norgans communicate with cortex. sensory information is believed to then propagate through\nthe cortical column along the l4\u2192l2/3\u2192l5/6 pathway. here, we show that sensory-evoked responses\nof l5/6 neurons in rats derive instead from direct tc synapses. many l5/6 neurons exhibited\nsensory-evoked postsynaptic potentials with the same latencies as l4. paired in vivo recordings\nfrom l5/6 neurons and thalamic neurons revealed substantial convergence of direct tc synapses\nonto diverse types of infragranular neurons, particularly in l5b. pharmacological inactivation of l4\nhad no effect on sensory-evoked synaptic input to l5/6 neurons. l4 is thus not an obligatory\ndistribution hub for cortical activity, and thalamus activates two separate, independent \u201cstrata\u201d\nof cortex in parallel.\n\nwhisker d", "two routes to scalable credit assignment without weight symmetry\n\ndaniel kunin * 1 aran nayebi * 2 javier sagastuy-brena * 1\n\nsurya ganguli 3 jonathan m. bloom 4 5 daniel l. k. yamins 6 7 8\n\nabstract\n\nthe neural plausibility of backpropagation has\nlong been disputed, primarily for its use of non-\nlocal weight transport \u2014 the biologically dubi-\nous requirement that one neuron instantaneously\nmeasure the synaptic weights of another. until re-\ncently, attempts to create local learning rules that\navoid weight transport have typically failed in the\nlarge-scale learning scenarios where backpropa-\ngation shines, e.g. imagenet categorization with\ndeep convolutional networks. here, we investi-\ngate a recently proposed local learning rule that\nyields competitive performance with backpropa-\ngation and \ufb01nd that it is highly sensitive to meta-\nparameter choices, requiring laborious tuning that\ndoes not transfer across network architecture. our\nanalysis indicates the underlying mathematical\nreason f", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nneural  data  science:  accelerating\nthe  experiment-analysis-theory  cycle\nin  large-scale  neuroscience\nl  paninski1,2 and  jp  cunningham1\n\nmodern  large-scale  multineuronal  recording  methodologies,\nincluding  multielectrode  arrays,  calcium  imaging,  and\noptogenetic  techniques,  produce  single-neuron  resolution  data\nof  a  magnitude  and  precision  that  were  the  realm  of  science\n\ufb01ction  twenty  years  ago.  the  major  bottlenecks  in  systems  and\ncircuit  neuroscience  no  longer  lie  in  simply  collecting  data  from\nlarge  neural  populations,  but  also  in  understanding  this  data:\ndeveloping  novel  scienti\ufb01c  questions,  with  corresponding\nanalysis  techniques  and  experimental  designs  to  fully  harness\nthese  new  capabilities  and  meaningfully  interrogate  these\nquestions.  advances  in  methods  for  signal  processing,  network\nanalysis,  dimensionality  reduction,  and  optim", "can neocortical feedback alter the \nsign of plasticity?\n\nblake\u00a0a.\u00a0richards \n\n  and timothy\u00a0p.\u00a0lillicrap\n\nit can turn synaptic plasticity in a neuron on \nor off but cannot alter the sign of synaptic \nplasticity (for example, whether synapses \npotentiate or depress). instead, the term rpe \ndetermi nes the sign of plasticity. however, \nin our opinion it may be important for fbj \nto determine whether neurons potentiate or \ndepress their\u00a0synapses.\n\nroelfsema and holtmaat state that the \nweight changes from equation 1 are equivalent \nto those prescribed by backpropagation- \nof-error,  but  the  equivalence  is  on  the \nweight changes on average, and this point \nis  crucial.  notably,  even  random  search \nalgorithms, such as reinforce7, also agree \nwith backpropagation- of-error on average. \nthis means that for an individual stimulus, \nalgorithms  like  reinforce  or  those \nprescribed by equation 1 do not follow the \ntrue gradient. instead, these algorithms only \nfollow the true gradient ", "soft actor-critic:\n\noff-policy maximum entropy deep reinforcement\n\nlearning with a stochastic actor\n\ntuomas haarnoja 1 aurick zhou 1 pieter abbeel 1 sergey levine 1\n\n8\n1\n0\n2\n\n \n\ng\nu\na\n8\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n0\n9\n2\n1\n0\n\n.\n\n1\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nmodel-free deep reinforcement learning (rl) al-\ngorithms have been demonstrated on a range of\nchallenging decision making and control tasks.\nhowever, these methods typically suffer from two\nmajor challenges: very high sample complexity\nand brittle convergence properties, which necessi-\ntate meticulous hyperparameter tuning. both of\nthese challenges severely limit the applicability\nof such methods to complex, real-world domains.\nin this paper, we propose soft actor-critic, an off-\npolicy actor-critic deep rl algorithm based on the\nmaximum entropy reinforcement learning frame-\nwork. in this framework, the actor aims to maxi-\nmize expected reward while also maximizing en-\ntropy. that is, to succeed at the task while acting\nas rand", "j neurophysiol 90: 415\u2013 430, 2003.\nfirst published february 26, 2003; 10.1152/jn.01095.2002.\n\nwhat determines the frequency of fast network oscillations with irregular\nneural discharges? i. synaptic dynamics and excitation-inhibition balance\n\nnicolas brunel1 and xiao-jing wang2\n1centre national de la recherche scienti\ufb01que-neurophysique et physiologie du syste`me moteur-universite\u00b4 paris rene\u00b4 descartes, 75270\nparis cedex 06, france; and 2volen center, brandeis university, waltham, massachusetts 02454\n\nsubmitted 6 december 2002; accepted in \ufb01nal form 11 february 2003\n\nbrunel, nicolas and xiao-jing wang. what determines the fre-\nquency of fast network oscillations with irregular neural discharges?\ni. synaptic dynamics and excitation-inhibition balance. j neuro-\nphysiol 90: 415\u2013 430, 2003. first published february 26, 2003;\n10.1152/jn.01095.2002. when the local \ufb01eld potential of a cortical\nnetwork displays coherent fast oscillations (\u2b0340-hz gamma or\n\u2b03200-hz sharp-wave ripples), the spike ", "a r t i c l e s\n\ntwo types of asynchronous activity in networks of \nexcitatory and inhibitory spiking neurons\nsrdjan ostojic\nasynchronous activity in balanced networks of excitatory and inhibitory neurons is believed to constitute the primary medium \nfor the propagation and transformation of information in the neocortex. here we show that an unstructured, sparsely connected \nnetwork of model spiking neurons can display two fundamentally different types of asynchronous activity that imply vastly \ndifferent computational properties. for weak synaptic couplings, the network at rest is in the well-studied asynchronous state,  \nin which individual neurons fire irregularly at constant rates. in this state, an external input leads to a highly redundant response \nof different neurons that favors information transmission but hinders more complex computations. for strong couplings,  \nwe find that the network at rest displays rich internal dynamics, in which the firing rates of individual neurons", "prl 97, 048104 (2006)\n\np h y s i c a l r e v i e w l e t t e r s\n\nweek ending\n28 july 2006\n\ngradient learning in spiking neural networks by dynamic perturbation of conductances\n\n1kavli institute for theoretical physics, university of california, santa barbara, california 93106, usa\n\n2howard hughes medical institute and department of brain and cognitive sciences, massachusetts institute of technology,\n\nila r. fiete1 and h. sebastian seung2\n\ncambridge, massachusetts 02139, usa\n\n(received 19 january 2006; published 28 july 2006)\n\nwe present a method of estimating the gradient of an objective function with respect to the synaptic\nweights of a spiking neural network. the method works by measuring the \ufb02uctuations in the objective\nfunction in response to dynamic perturbation of the membrane conductances of the neurons. it is\ncompatible with recurrent networks of conductance-based model neurons with dynamic synapses. the\nmethod can be interpreted as a biologically plausible synaptic learning r", "composing graphical models with neural networks\nfor structured representations and fast inference\n\nmatthew james johnson\n\nharvard university\n\nmattjj@seas.harvard.edu\n\ndavid duvenaud\nharvard university\n\ndduvenaud@seas.harvard.edu\n\nalexander b. wiltschko\nharvard university, twitter\n\nawiltsch@fas.harvard.edu\n\nsandeep r. datta\n\nharvard medical school\n\nsrdatta@hms.harvard.edu\n\nryan p. adams\n\nharvard university, twitter\nrpa@seas.harvard.edu\n\nabstract\n\nwe propose a general modeling and inference framework that combines the com-\nplementary strengths of probabilistic graphical models and deep learning methods.\nour model family composes latent graphical models with neural network obser-\nvation likelihoods. for inference, we use recognition networks to produce local\nevidence potentials, then combine them with the model distribution using ef\ufb01cient\nmessage-passing algorithms. all components are trained simultaneously with a\nsingle stochastic variational inference objective. we illustrate this frame", "neuron, vol. 40, 1063\u20131073, december 18, 2003, copyright \uf8e92003 by cell press\n\nlearning in spiking neural\nnetworks by reinforcement of\nstochastic synaptic transmission\n\nviewpoint\n\nh. sebastian seung\nhoward hughes medical institute and\nbrain and cognitive sciences department\nmassachusetts institute of technology\ncambridge, massachusetts 02139\n\nsummary\n\nit is well-known that chemical synaptic transmission\nis an unreliable process, but the function of such unre-\nliability remains unclear. here i consider the hypothe-\nsis that the randomness of synaptic transmission is\nharnessed by the brain for learning, in analogy to the\nway that genetic mutation is utilized by darwinian evo-\nlution. this is possible if synapses are \u201chedonistic,\u201d\nresponding to a global reward signal by increasing\ntheir probabilities of vesicle release or failure, de-\npending on which action immediately preceded re-\nward. hedonistic synapses learn by computing a sto-\nchastic approximation to the gradient of the average\nrew", "reinforcement learning\n\nmemory\n\nbram bakker\n\ndept. of psychology, leiden university / idsia\np.o. box 9555; 2300 rb, leiden; the netherlands\n\nbbakker@fsw.leidenuniv.nl\n\nabstract\n\nthis paper presents reinforcement learning with a long short \nterm memory recurrent neural network: rl-lstm. model-free\nrl-lstm using advantage(,x) learning and directed exploration\ncan solve non-markovian tasks with long-term dependencies be \ntween relevant events. this is demonstrated in a t-maze task, as\nwell as in a difficult variation of the pole balancing task.\n\n1\n\nintroduction\n\nreinforcement learning (rl) is a way of learning how to behave based on delayed\nreward signals [12]. among the more important challenges for rl are tasks where\npart of the state of the environment is hidden from the agent. such tasks are called\nnon-markovian tasks or partially observable markov decision processes. many real\nworld tasks have this problem of hidden state. for instance, in a navigation task\ndifferent positions in the", "research\n\nresearch article summary \u25e5\n\nneuroscience\n\nspontaneous behaviors drive\nmultidimensional, brainwide activity\n\ncarsen stringer*\u2020, marius pachitariu*\u2020, nicholas steinmetz, charu bai reddy,\nmatteo carandini\u2021, kenneth d. harris\u2020\u2021\n\nintroduction: in the absence of sensory\ninputs, the brain produces structured patterns\nof activity, which can be as large as or larger\nthan sensory-driven activity. ongoing activity\nexists even in primary sensory cortices and\nhas been hypothesized to reflect recapitulation\nof previous sensory experiences, or expecta-\ntions of possible sensory events. alternatively,\nongoing activity could be related to behavioral\nand cognitive states.\n\nrationale: much previous work has linked\nspontaneous neural activity to behavior through\none-dimensional measures like running speed\nand pupil diameter. however, mice perform di-\nverse behaviors consisting of whisking, licking,\nsniffing, and other facial movements. we hy-\n\npothesized that there exists a multidimensional\nrepr", "proc. natl. acad. sci. usa\nvol. 93, pp. 13339\u201313344, november 1996\nneurobiology\n\nhow the brain keeps the eyes still\nh. s. seung\nbell laboratories, lucent technologies, murray hill, nj 07974\n\ncommunicated by john j. hopfield, california institute of technology, pasadena, ca, june 10, 1996 (received for review january 7, 1996)\n\nabstract\nthe brain can hold the eyes still because it\nstores a memory of eye position. the brain\u2019s memory of\nhorizontal eye position appears to be represented by persistent\nneural activity in a network known as the neural integrator,\nwhich is localized in the brainstem and cerebellum. existing\nexperimental data are reinterpreted as evidence for an \u2018\u2018at-\ntractor hypothesis\u2019\u2019 that the persistent patterns of activity\nobserved in this network form an attractive line of fixed points\nin its state space. line attractor dynamics can be produced in\nlinear or nonlinear neural networks by learning mechanisms\nthat precisely tune positive feedback.\n\nthe brain moves the eyes wi", " \n \n \n\ndeep learning incorporating  \nbiologically-inspired neural dynamics \n\nstanis\u0142aw wo\u017aniak1,*, angeliki pantazi1, thomas bohnstingl1,2, and evangelos eleftheriou1 \n1 ibm research \u2013 zurich, r\u00fcschlikon, switzerland \n2 institute of theoretical computer science, graz university of technology, graz, austria\n\n  \n\n \n\nneural networks have become the key technology of artificial intelligence and have contributed to breakthroughs in several \nmachine  learning  tasks,  primarily  owing  to  advances  in  deep  learning  applied  to  artificial  neural  networks  (anns). \nsimultaneously,  spiking  neural  networks  (snns)  incorporating  biologically-feasible  spiking  neurons  have  held  great \npromise because of their rich temporal dynamics and high-power efficiency. however, the developments in snns were \nproceeding separately from those in anns, effectively limiting the adoption of deep learning research insights. here we \nshow an alternative perspective on the spiking neuron that casts i", "neuron, vol. 32, 1149\u20131164, december 20, 2001, copyright \uf8e92001 by cell press\n\nrate, timing, and cooperativity jointly determine\ncortical synaptic plasticity\n\nper jesper sjo\u00a8 stro\u00a8 m, gina g. turrigiano,\nand sacha b. nelson1\nbrandeis university\ndepartment of biology\nvolen center for complex systems\nmailstop 008 415 south street\nwaltham, massachusetts 02454\n\nsummary\n\ncortical long-term plasticity depends on firing rate,\nspike timing, and cooperativity among inputs, but how\nthese factors interact during realistic patterns of activ-\nity is unknown. here we monitored plasticity while\nsystematically varying the rate, spike timing, and num-\nber of coincident afferents. these experiments dem-\nonstrate a novel form of cooperativity operating even\nwhen postsynaptic firing is evoked by current injec-\ntion, and reveal a complex dependence of ltp and\nltd on rate and timing. based on these data, we con-\nstructed and tested three quantitative models of corti-\ncal plasticity. one of these models, in w", "low-pass filtering sgd for recovering flat optima in the deep\n\nlearning optimization landscape\n\ndevansh bisla\ndb3484@nyu.edu\n\njing wang\n\njw5665@nyu.edu\n\nanna choromanska\n\nac5455@nyu.edu\n\n2\n2\n0\n2\n\n \n\nb\ne\nf\n4\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n2\n0\n8\n0\n\n.\n\n1\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nin this paper, we study the sharpness of a deep\nlearning (dl) loss landscape around local min-\nima in order to reveal systematic mechanisms\nunderlying the generalization abilities of dl\nmodels. our analysis is performed across vary-\ning network and optimizer hyper-parameters,\nand involves a rich family of di\ufb00erent sharp-\nness measures. we compare these measures\nand show that the low-pass \ufb01lter based mea-\nsure exhibits the highest correlation with the\ngeneralization abilities of dl models, has high\nrobustness to both data and label noise, and\nfurthermore can track the double descent be-\nhavior for neural networks. we next derive\nthe optimization algorithm, relying on the low-\npass \ufb01lter (lpf), that actively ", "deep generative stochastic networks trainable by backprop\n\nyoshua bengio\u2217\n\u00b4eric thibodeau-laufer\nguillaume alain\nd\u00b4epartement d\u2019informatique et recherche op\u00b4erationnelle, universit\u00b4e de montr\u00b4eal,\u2217& canadian inst. for advanced research\njason yosinski\ndepartment of computer science, cornell university\n\nfind.us@on.the.web\n\nabstract\n\nwe introduce a novel training principle for prob-\nabilistic models that is an alternative to max-\nimum likelihood.\nthe proposed generative\nstochastic networks (gsn) framework is based\non learning the transition operator of a markov\nchain whose stationary distribution estimates the\ndata distribution. the transition distribution of\nthe markov chain is conditional on the previ-\nous state, generally involving a small move, so\nthis conditional distribution has fewer dominant\nmodes, being unimodal in the limit of small\nmoves. thus, it is easier to learn because it\nis easier to approximate its partition function,\nmore like learning to perform supervised func-\ntion a", "the journal of neuroscience, july 21, 2010 \u2022 30(29):9659 \u20139669 \u2022 9659\n\nbehavioral/systems/cognitive\n\ndecoding complete reach and grasp actions from local\nprimary motor cortex populations\n\ncarlos e. vargas-irwin,1 gregory shakhnarovich,2,3 payman yadollahpour,2,3 john m. k. mislow,1,4 michael j. black,2\nand john p. donoghue1,5\ndepartments of 1neuroscience and 2computer science, brown university, providence, rhode island 02912, 3toyota technological institute, chicago,\nillinois 60637, 4department of neurosurgery, brigham and women\u2019s hospital, children\u2019s hospital boston, harvard medical school, boston, massachusetts\n02115, and 5department of veterans affairs, providence veterans affairs medical center, providence, rhode island 02908\n\nhow the activity of populations of cortical neurons generates coordinated multijoint actions of the arm, wrist, and hand is poorly\nunderstood. this study combined multielectrode recording techniques with full arm motion capture to relate neural activity in pr", "ieee transactions on xxxx, vol. xx, no. x, month year\n\n1\n\ngenetic programming-based evolutionary deep\nlearning for data-ef\ufb01cient image classi\ufb01cation\n\nying bi, member, ieee, bing xue, senior member, ieee, and mengjie zhang, fellow, ieee,\n\nabstract\u2014data-ef\ufb01cient image classi\ufb01cation is a challenging\ntask that aims to solve image classi\ufb01cation using small training\ndata. neural network-based deep learning methods are effective\nfor image classi\ufb01cation, but they typically require large-scale\ntraining data and have major limitations such as requiring\nexpertise to design network architectures and having poor inter-\npretability. evolutionary deep learning is a recent hot topic that\ncombines evolutionary computation with deep learning. however,\nmost evolutionary deep learning methods focus on evolving archi-\ntectures of neural networks, which still suffers from limitations\nsuch as poor interpretability. to address this, this paper proposes\na new genetic programming-based evolutionary deep learnin", "flexible brain\u2013computer interfaces\n\nhttps://doi.org/10.1038/s41928-022-00913-9\n\nreceived: 6 october 2022\n\naccepted: 7 december 2022\n\npublished online: 2 february 2023\n\n check for updates\n\nxin tang\u2009\n\n \u20091,2, hao shen\u2009\n\n \u20091,2, siyuan zhao\u2009\n\n \u20091, na li1 & jia liu\u2009\n\n \u20091 \n\nbrain\u2013computer interfaces\u2014which allow direct communication between the \nbrain and external computers\u2014have potential applications in neuroscience, \nmedicine and virtual reality. current approaches are, however, based on \nconventional rigid electronics and are limited by their intrinsic mechanical \nand geometrical mismatch with brain tissue. flexible electronics, which \ncan have mechanical properties compatible with the brain, could address \nthese limitations and be used to create the next generation of brain\u2013\ncomputer interfaces. here we explore the use of flexible electronics in \nthe development of brain\u2013computer interfaces. we examine the unique \nadvantages of flexible, stretchable and soft electronics in such interfaces ", "sparse bursts optimize information transmission\nin a multiplexed neural code\n\nrichard nauda,b,1 and henning sprekelerc,d\n\nauniversity of ottawa brain and mind research institute, department of cellular and molecular medicine, university of ottawa, ottawa, on k1h 8m5,\ncanada; bdepartment of physics, university of ottawa, ottawa, on k1n 6n5, canada; cbernstein center for computational neuroscience berlin, 10115\nberlin, germany; and dmodelling of cognitive processes, institute of software engineering and theoretical computer science, technische universit \u00a8at\nberlin, 10587 berlin, germany\n\nedited by terrence j. sejnowski, salk institute for biological studies, la jolla, ca, and approved may 21, 2018 (received for review december 4, 2017)\n\nmany cortical neurons combine the information ascending and\ndescending the cortical hierarchy. in the classical view, this infor-\nmation is combined nonlinearly to give rise to a single \ufb01ring-rate\noutput, which collapses all\ninput streams into one. we ana", "ps68ch05-gershman\n\nari\n\n4 november 2016\n\n10:31\n\nreinforcement learning and\nepisodic memory in humans\nand animals: an integrative\nframework\nsamuel j. gershman1 and nathaniel d. daw2\n1department of psychology and center for brain science, harvard university, cambridge,\nmassachusetts 02138; email: gershman@fas.harvard.edu\n2princeton neuroscience institute and department of psychology, princeton university,\nprinceton, new jersey 08544\n\nannu. rev. psychol. 2017. 68:101\u201328\n\nfirst published online as a review in advance on\nseptember 2, 2016\n\nkeywords\nreinforcement learning, memory, decision making\n\nthe annual review of psychology is online at\npsych.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev-psych-122414-033625\ncopyright c(cid:2) 2017 by annual reviews.\nall rights reserved\n\nabstract\nwe review the psychology and neuroscience of reinforcement learning (rl),\nwhich has experienced signi\ufb01cant progress in the past two decades, enabled\nby the comprehensive experimental study of simple lea", "letters\n\nvol 435|23 june 2005|doi:10.1038/nature03687\n\ninvariant visual representation by single neurons in\nthe human brain\nr. quian quiroga1,2\u2020, l. reddy1, g. kreiman3, c. koch1 & i. fried2,4\n\nit takes a fraction of a second to recognize a person or an object\neven when seen under strikingly different conditions. how such a\nrobust, high-level representation is achieved by neurons in the\nhuman brain is still unclear1\u20136. in monkeys, neurons in the upper\nstages of the ventral visual pathway respond to complex images\nsuch as faces and objects and show some degree of invariance to\nmetric properties such as the stimulus size, position and viewing\nangle2,4,7\u201312. we have previously shown that neurons in the human\nmedial temporal lobe (mtl) \ufb01re selectively to images of faces,\nanimals, objects or scenes13,14. here we report on a remarkable\nsubset of mtl neurons that are selectively activated by strikingly\ndifferent pictures of given individuals, landmarks or objects and\nin some cases even by let", "research article\n\nadaptive learning and decision-making\nunder uncertainty by metaplastic\nsynapses guided by a surprise detection\nsystem\nkiyohito iigaya1,2,3*\n\n1gatsby computational neuroscience unit, university college london, london,\nunited kingdom; 2center for theoretical neuroscience, college of physicians and\nsurgeons, columbia university , new york, united states; 3department of physics,\ncolumbia university, new york, united states\n\nabstract recent experiments have shown that animals and humans have a remarkable ability to\nadapt their learning rate according to the volatility of the environment. yet the neural mechanism\nresponsible for such adaptive learning has remained unclear. to fill this gap, we investigated a\nbiophysically inspired, metaplastic synaptic model within the context of a well-studied decision-\nmaking network, in which synapses can change their rate of plasticity in addition to their efficacy\naccording to a reward-based learning rule. we found that our model, whic", "towards scaling difference target propagation by learning backprop targets\n\nmaxence ernoult 1 fabrice normandin * 2 abhinav moudgil * 2 3 sean spinney 2 4 eugene belilovsky 2 3\n\nirina rish 2 4 blake richards 2 5 6 yoshua bengio 2 4\n\n2\n2\n0\n2\n\n \n\nn\na\nj\n \n\n1\n3\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n5\n1\n4\n3\n1\n\n.\n\n1\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nthe development of biologically-plausible learn-\ning algorithms is important for understanding\nlearning in the brain, but most of them fail to\nscale-up to real-world tasks, limiting their poten-\ntial as explanations for learning by real brains. as\nsuch, it is important to explore learning algorithms\nthat come with strong theoretical guarantees and\ncan match the performance of backpropagation\n(bp) on complex tasks. one such algorithm is dif-\nference target propagation (dtp), a biologically-\nplausible learning algorithm whose close relation\nwith gauss-newton (gn) optimization has been\nrecently established. however, the conditions un-\nder which this connection rig", "neural networks 141 (2021) 330\u2013343\n\ncontents lists available at sciencedirect\n\nneural networks\n\njournal homepage: www.elsevier.com/locate/neunet\n\nautoencoder networks extract latent variables and encode these\nvariables in their connectomes\nmatthew farrell a,b,\u2217, stefano recanatesi b, r. clay reid c, stefan mihalas c,\neric shea-brown a,b,c\na applied mathematics department, university of washington, seattle, wa, united states of america\nb computational neuroscience center, university of washington, seattle, wa, united states of america\nc allen institute for brain science, seattle, wa, united states of america\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 1 october 2020\nreceived in revised form 2 march 2021\naccepted 8 march 2021\navailable online 24 march 2021\n\nkeywords:\nautoencoder\nconnectome\ndimensionality reduction\nidentifiability\nlatent variable encoding\n\nadvances in electron microscopy and data processing techniques are leading to increasingly large\nand complete ", "letter\n\ncommunicated by naftali tishby\n\nasymptotic theory of information-theoretic experimental\ndesign\n\nliam paninski\nliam@gatsby.ucl.ac.uk\ngatsby computational neuroscience unit,\nuniversity college london, london, wc1n 3ar, u.k.\n\nwe discuss an idea for collecting data in a relatively ef\ufb01cient manner. our\npoint of view is bayesian and information-theoretic: on any given trial,\nwe want to adaptively choose the input in such a way that the mutual in-\nformation between the (unknown) state of the system and the (stochastic)\noutput is maximal, given any prior information (including data collected\non any previous trials). we prove a theorem that quanti\ufb01es the effective-\nness of this strategy and give a few illustrative examples comparing the\nperformance of this adaptive technique to that of the more usual non-\nadaptive experimental design. in particular, we calculate the asymptotic\nef\ufb01ciency of the information-maximization strategy and demonstrate that\nthis method is in a well-de\ufb01ned sense n", "the interplay between randomness and structure\n\nduring learning in rnns\n\nfriedrich schuessler\n\ntechnion\n\nschuessler@campus.technion.ac.il\n\nfrancesca mastrogiuseppe\n\ngatsby unit, ucl\n\nf.mastrogiuseppe@ucl.ac.uk\n\nalexis dubreuil\n\nens paris\n\nsrdjan ostojic\n\nens paris\n\nalexis.dubreuil@gmail.com\n\nsrdjan.ostojic@ens.fr\n\nomri barak\n\ntechnion\n\nomri.barak@gmail.com\n\nabstract\n\nrecurrent neural networks (rnns) trained on low-dimensional tasks have been\nwidely used to model functional biological networks. however, the solutions found\nby learning and the effect of initial connectivity are not well understood. here,\nwe examine rnns trained using gradient descent on different tasks inspired by\nthe neuroscience literature. we \ufb01nd that the changes in recurrent connectivity\ncan be described by low-rank matrices, despite the unconstrained nature of the\nlearning algorithm. to identify the origin of the low-rank structure, we turn to an\nanalytically tractable setting: training a linear rnn on a simpli\ufb01ed t", "m\no\nc\n\n.\n\ne\nr\nu\nt\na\nn\n\n.\ni\n\nc\ns\no\nr\nu\ne\nn\n\n/\n/\n:\np\nt\nt\nh\n \n\u2022\n \n.\n\nc\nn\n\ni\n \n\n \n\na\nc\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n0\n0\n0\n2\n\u00a9\n\n \n\n \n\nreview\n\n\u00a9 2000 nature america inc. \u2022 http://neurosci.nature.com\n\nsynaptic plasticity: taming the beast\n\nl. f. abbott and sacha b. nelson\n\ndepartment of biology and volen center, brandeis university, waltham, massachusetts 02454-9110, usa\n\ncorrespondence should be addressed to l.f.a. (abbott@brandeis.edu)\n\nsynaptic plasticity provides the basis for most models of learning, memory and development in neural\ncircuits. to generate realistic results, synapse-specific hebbian forms of plasticity, such as long-term\npotentiation and depression, must be augmented by global processes that regulate overall levels of\nneuronal and network activity. regulatory processes are often as important as the more intensively\nstudied hebbian processes in determining the consequences of synaptic plasticity for network\nfunction. recent experimental results suggest several novel mechanisms for", "ne38ch10-froemke\n\nari\n\n21 may 2015\n\n10:44\n\nplasticity of cortical\nexcitatory-inhibitory balance\n\nrobert c. froemke1,2\n1skirball institute for biomolecular medicine, neuroscience institute, and departments\nof otolaryngology, neuroscience, and physiology, new york university school of medicine,\nnew york, ny 10016; email: robert.froemke@med.nyu.edu\n2center for neural science, new york university, new york, ny 10003\n\nannu. rev. neurosci. 2015. 38:195\u2013219\n\nfirst published online as a review in advance on\napril 9, 2015\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev-neuro-071714-034002\ncopyright c(cid:2) 2015 by annual reviews.\nall rights reserved\n\nkeywords\ncortex, inhibition, neuromodulation, synaptic plasticity\n\nabstract\nsynapses are highly plastic and are modi\ufb01ed by changes in patterns of neu-\nral activity or sensory experience. plasticity of cortical excitatory synapses is\nthought to be important for learning and memory, leadin", "independent component analysis:\n\nalgorithms and applications\n\naapo hyv\u00e4rinen and erkki oja\n\nneural networks research centre\nhelsinki university of technology\n\np.o. box 5400, fin-02015 hut, finland\nneural networks, 13(4-5):411-430, 2000\n\nabstract\n\na fundamental problem in neural network research, as well as in many other disciplines, is \ufb01nding a suitable\nrepresentation of multivariate data, i.e. random vectors. for reasons of computational and conceptual simplicity,\nthe representation is often sought as a linear transformation of the original data. in other words, each component\nof the representation is a linear combination of the original variables. well-known linear transformation methods\ninclude principal component analysis, factor analysis, and projection pursuit. independent component analysis\n(ica) is a recently developed method in which the goal is to \ufb01nd a linear representation of nongaussian data so\nthat the components are statistically independent, or as independent as possibl", "ne41ch12_medina\n\nari\n\n24 may 2018\n\n7:30\n\nannu. rev. neurosci. 2018. 41:233\u201353\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nhttps://doi.org/10.1146/annurev-neuro-080317-\n061948\ncopyright c(cid:2) 2018 by annual reviews.\nall rights reserved\n\nannual review of neuroscience\ncomputational principles of\nsupervised learning in the\ncerebellum\njennifer l. raymond1 and javier f. medina2\n1department of neurobiology, stanford university school of medicine, stanford,\ncalifornia 94305, usa; email: jennifer.raymond@stanford.edu\n2department of neuroscience, baylor college of medicine, houston, texas 77030, usa;\nemail: jfmedina@bcm.edu\n\nkeywords\nmachine learning, decorrelation, consolidation, plasticity, climbing \ufb01ber,\npurkinje cell\n\nabstract\nsupervised learning plays a key role in the operation of many biological and\narti\ufb01cial neural networks. analysis of the computations underlying super-\nvised learning is facilitated by the relatively simple and uniform architec-\nture of t", "6\n1\n0\n2\n\n \nl\nu\nj\n \n\n1\n2\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n0\n5\n4\n6\n0\n\n.\n\n7\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nlayer normalization\n\njimmy lei ba\n\nuniversity of toronto\n\njamie ryan kiros\nuniversity of toronto\n\njimmy@psi.toronto.edu\n\nrkiros@cs.toronto.edu\n\ngeoffrey e. hinton\nuniversity of toronto\n\nand google inc.\n\nhinton@cs.toronto.edu\n\nabstract\n\ntraining state-of-the-art, deep neural networks is computationally expensive. one\nway to reduce the training time is to normalize the activities of the neurons. a\nrecently introduced technique called batch normalization uses the distribution of\nthe summed input to a neuron over a mini-batch of training cases to compute a\nmean and variance which are then used to normalize the summed input to that\nneuron on each training case. this signi\ufb01cantly reduces the training time in feed-\nforward neural networks. however, the effect of batch normalization is dependent\non the mini-batch size and it is not obvious how to apply it to recurrent neural net-\nworks. in this paper, w", "1\n\n0\n1\n0\n2\n\n \nr\na\n\nm\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n8\n5\n3\n0\n\n.\n\n3\n0\n0\n1\n:\nv\ni\nx\nr\na\n\ndeep big simple neural nets excel on hand-\n\nwritten digit recognition\n\ndan claudiu cires\u00b8an1, 2,\nueli meier1, 2,\nluca maria gambardella1, 2,\nj\u00a8urgen schmidhuber1, 2\n1idsia, galleria 2, 6928 manno-lugano, switzerland.\n2university of lugano & supsi, switzerland.\n\nkeywords: nn (neural network) , mlp (multilayer perceptron), gpu (graphics\n\nprocessing unit), training set deformations, mnist 1, bp (back-propagation).\n\nabstract\n\ngood old on-line back-propagation for plain multi-layer perceptrons yields a very\nlow 0.35% error rate on the famous mnist handwritten digits benchmark. all we\nneed to achieve this best result so far are many hidden layers, many neurons per layer,\nnumerous deformed training images, and graphics cards to greatly speed up learning.\n\n1\n\nintroduction\n\nautomatic handwriting recognition is of great academic and commercial interest. cur-\nrent algorithms are already pretty good at learning ", "on the universality of invariant networks\n\nhaggai maron 1 ethan fetaya 2 3 nimrod segol 1 yaron lipman 1\n\nabstract\n\nconstraining linear layers in neural networks to\nrespect symmetry transformations from a group\ng is a common design principle for invariant net-\nworks that has found many applications in ma-\nchine learning. in this paper, we consider a fun-\ndamental question that has received little atten-\ntion to date: can these networks approximate any\n(continuous) invariant function? we tackle the\nrather general case where g \u2264 sn (an arbitrary\nsubgroup of the symmetric group) that acts on\nrn by permuting coordinates. this setting in-\ncludes several recent popular invariant networks.\nwe present two main results: first, g-invariant\nnetworks are universal if high-order tensors are\nallowed. second, there are groups g for which\nhigher-order tensors are unavoidable for obtaining\nuniversality. g-invariant networks consisting of\nonly \ufb01rst-order tensors are of special interest due\nto their prac", "cortical areas interact through a communication\nsubspace\n\narticle\n\nhighlights\nd visual cortical areas interact through a communication\n\nsubspace (cs)\n\nd the cs de\ufb01nes which activity patterns in a source area relate\n\nto downstream activity\n\nd the largest activity patterns in a source area are not matched\n\nto the cs\n\nd the cs allows for selective and \ufb02exible routing of population\n\nsignals between areas\n\nauthors\n\njoa\u02dc o d. semedo, amin zandvakili,\nchristian k. machens, byron m. yu,\nadam kohn\n\ncorrespondence\njsemedo@cmu.edu\n\nin brief\nmost brain functions require the selective\nand \ufb02exible routing of neuronal activity\nbetween cortical areas. using paired\npopulation recordings from multiple\nvisual cortical areas, semedo et al. \ufb01nd a\npopulation-level mechanism that can\nachieve this routing, termed a\ncommunication subspace.\n\nsemedo et al., 2019, neuron 102, 249\u2013259\napril 3, 2019 \u00aa 2019 elsevier inc.\nhttps://doi.org/10.1016/j.neuron.2019.01.026\n\n\f", "y\nr\na\nt\nn\ne\nm\nm\no\nc\n\ne\ne\ns\n\ne\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\n\nperformance-optimized hierarchical models predict\nneural responses in higher visual cortex\n\ndaniel l. k. yaminsa,1, ha honga,b,1, charles f. cadieua, ethan a. solomona, darren seiberta, and james j. dicarloa,2\n\nadepartment of brain and cognitive sciences and mcgovern institute for brain research, massachusetts institute of technology, cambridge, ma 02139;\nand bharvard-mit division of health sciences and technology, institute for medical engineering and science, massachusetts institute of technology,\ncambridge, ma 02139\n\nedited by terrence j. sejnowski, salk institute for biological studies, la jolla, ca, and approved april 8, 2014 (received for review march 3, 2014)\n\nthe ventral visual stream underlies key human visual object re-\ncognition abilities. however, neural encoding in the higher areas of\nthe ventral stream remains poorly understood. here, we describe\na modeling approach that yields a quantitatively accurate model of\ninferio", "a r t i c l e s\n\ngating and control of primary visual cortex by pulvinar\ngopathy purushothaman1, roan marion2, keji li3 & vivien a casagrande1,3\n\nthe primary visual cortex (v1) receives its driving input from the eyes via the lateral geniculate nucleus (lgn) of the thalamus. \nthe lateral pulvinar nucleus of the thalamus also projects to v1, but this input is not well understood. we manipulated lateral \npulvinar neural activity in prosimian primates and assessed the effect on supra-granular layers of v1 that project to higher visual \ncortex. reversibly inactivating lateral pulvinar prevented supra-granular v1 neurons from responding to visual stimulation. \nreversible, focal excitation of lateral pulvinar receptive fields increased the visual responses in coincident v1 receptive fields \nfourfold and shifted partially overlapping v1 receptive fields toward the center of excitation. v1 responses to regions surrounding \nthe excited lateral pulvinar receptive fields were suppressed. lgn resp", "643 \n\nlearning sequential structure \nin simple recurrent networks \n\ndavid servan-schreiber. axel cleeremans. and james l. mcclelland \n\ndeparttnents of computer science and psycholgy \n\ncarnegie mellon university \n\npittsburgh, pa  15213 \n\nabstract \n\nwe  explore  a  network  architecture  introduced  by  elman  (1988)  for \npredicting  successive elements of a  sequence.  the  network  uses  the \npattern  of activation  over  a  set  of hidden  units  from  time-step  t-l, \ntogether  with element t,  to predict element t+ 1.  when  the  network  is \ntrained  with strings from  a particular finite-state grammar, it can learn \nto be a perfect finite-state recognizer for the grammar. cluster analyses \nof the  hidden-layer  patterns  of activation  showed  that  they  encode \nprediction-relevant information about the entire path traversed through \nthe network. we illustrate the phases of learning with cluster analyses \nperformed at different points during training. \n\nseveral connectionist arc", "the journal of neuroscience, january 21, 2015 \u2022 35(3):1319 \u20131334 \u2022 1319\n\ndevelopment/plasticity/repair\n\nsynaptic consolidation: from synapses to behavioral\nmodeling\n\nx lorric ziegler, x friedemann zenke, x david b. kastner, and x wulfram gerstner\nschool of computer and communication sciences and school of life sciences, brain mind institute, ecole polytechnique fe\u00b4de\u00b4rale de lausanne,\n1015 lausanne epfl, switzerland\n\nsynaptic plasticity, a key process for memory formation, manifests itself across different time scales ranging from a few seconds for plasticity\ninductionuptohoursorevenyearsforconsolidationandmemoryretention.wedevelopedathree-layeredmodelofsynapticconsolidationthat\naccounts for data across a large range of experimental conditions. consolidation occurs in the model through the interaction of the synaptic\nefficacywithascaffoldingvariablebyaread-writeprocessmediatedbyatagging-relatedvariable.plasticity-inducingstimulimodifytheefficacy,\nbut the state of tag and scaffold can o", "7\n1\n0\n2\n\n \nr\na\n\nm\n3\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n3\n1\n9\n9\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\ncapacity and trainability in recurrent\nneural networks\n\njasmine collins\u2217, jascha sohl-dickstein & david sussillo\ngoogle brain\ngoogle inc.\nmountain view, ca 94043, usa\n{jlcollins, jaschasd, sussillo}@google.com\n\nabstract\n\ntwo potential bottlenecks on the expressiveness of recurrent neural networks\n(rnns) are their ability to store information about the task in their parameters, and\nto store information about the input history in their units. we show experimentally\nthat all common rnn architectures achieve nearly the same per-task and per-unit\ncapacity bounds with careful training, for a variety of tasks and stacking depths.\nthey can store an amount of task information which is linear in the number of\nparameters, and is approximately 5 bits per parameter. they can additionally store\napproximately one real number from their input history per hidden unit.", "letters\n\nvol 440|20 april 2006|doi:10.1038/nature04671\n\nsynaptic scaling mediated by glial tnf-a\ndavid stellwagen1 & robert c. malenka1\n\ntwo general forms of synaptic plasticity that operate on different\ntimescales are thought to contribute to the activity-dependent\nre\ufb01nement of neural circuitry during development: (1) long-term\npotentiation (ltp) and long-term depression (ltd), which\ninvolve rapid adjustments in the strengths of individual synapses\nin response to speci\ufb01c patterns of correlated synaptic activity, and\n(2) homeostatic synaptic scaling, which entails uniform adjust-\nments in the strength of all synapses on a cell in response to\nprolonged changes in the cell\u2019s electrical activity1,2. without\nhomeostatic synaptic scaling, neural networks can become\nunstable and perform suboptimally1\u20133. although much is known\nabout the mechanisms underlying ltp and ltd4, little is known\nabout the mechanisms responsible for synaptic scaling except that\nsuch scaling is due, at least in part, t", "neuron, vol. 44, 691\u2013700, november 18, 2004, copyright \uf8e92004 by cell press\n\nbidirectional parallel fiber plasticity\nin the cerebellum under climbing fiber control\n\nmichiel coesmans, john t. weber,\nchris i. de zeeuw, and christian hansel*\ndepartment of neuroscience\nerasmus university medical center\n3000 dr rotterdam\nthe netherlands\n\nsummary\n\ncerebellar parallel fiber (pf)-purkinje cell (pc) syn-\napses can undergo postsynaptically expressed long-\nterm depression (ltd) or long-term potentiation (ltp)\ndepending on whether or not the climbing fiber (cf)\ninput is coactivated during tetanization. here, we show\nthat modifications of the postsynaptic calcium load\nusing the calcium chelator bapta or photolytic cal-\ncium uncaging result in a reversal of the expected\npolarity of synaptic gain change. at higher concentra-\ntions, bapta blocks pf-ltp. these data indicate that\npf-ltd requires a higher calcium threshold amplitude\nthan pf-ltp induction and suggest that cf activity\nacts as a polarity swi", "article\n\ndoi:10.1038/nature12160\n\nthe importance of mixed selectivity in\ncomplex cognitive tasks\n\nmattia rigotti1,2,3, omri barak1{, melissa r. warden4,5, xiao-jing wang2,6, nathaniel d. daw2,3, earl k. miller4 & stefano fusi1\n\nsingle-neuron activity in the prefrontal cortex (pfc) is tuned to mixtures of multiple task-related aspects. such mixed\nselectivity is highly heterogeneous, seemingly disordered and therefore difficult to interpret. we analysed the neural\nactivity recorded in monkeys during an object sequence memory task to identify a role of mixed selectivity in subserving\nthe cognitive functions ascribed to the pfc. we show that mixed selectivity neurons encode distributed information\nabout all task-relevant aspects. each aspect can be decoded from the population of neurons even when single-cell\nselectivity to that aspect is eliminated. moreover, mixed selectivity offers a significant computational advantage over\nspecialized responses in terms of the repertoire of input\u2013output", "neuron, vol. 30, 593\u2013607, may, 2001, copyright \u00aa 2001 by cell press\n\nneuronal correlates of motor performance and\nmotor learning in the primary motor cortex\nof monkeys adapting to an external force field\n\nchiang-shan ray li,1,3 camillo padoa-schioppa,1\nand emilio bizzi1,2\n1department of brain and cognitive sciences\nmassachusetts institute of technology\ncambridge, massachusetts 02139\n\nsummary\n\nthe primary motor cortex (m1) is known to control\nmotor performance. recent findings have also impli-\ncated m1 in motor learning, as neurons in this area\nshow learning-related plasticity. in the present study,\nwe analyzed the neuronal activity recorded in m1 in a\nforce field adaptation task. our goal was to investigate\nthe neuronal reorganization across behavioral epochs\n(before, during, and after adaptation). here we report\ntwo main findings. first, memory cells were present in\ntwo classes. with respect to the changes of preferred\ndirection (pd), these two classes complemented each\nother after re", "letter\n\ncommunicated by bruno averbeck\n\nlearning spike-based population codes by reward\nand population feedback\n\njohannes friedrich\nfriedrich@pyl.unibe.ch\nrobert urbanczik\nurbanczik@pyl.unibe.ch\nwalter senn\nsenn@pyl.unibe.ch\ndepartment of physiology, university of bern, ch-3012 bern, switzerland\n\nwe investigate a recently proposed model for decision learning in a pop-\nulation of spiking neurons where synaptic plasticity is modulated by a\npopulation signal in addition to reward feedback. for the basic model,\nbinary population decision making based on spike/no-spike coding, a de-\ntailed computational analysis is given about how learning performance\ndepends on population size and task complexity. next, we extend the\nbasic model to n-ary decision making and show that it can also be used\nin conjunction with other population codes such as rate or even latency\ncoding.\n\n1 introduction\n\nbehavioral decision making in the brain results from processes occurring at\nwidely differing spatial scales. ", "cognitive \n\nscience \n\n9,  147-169 \n\n(1985) \n\na  learning algorithm  for \n\nboltzmann machines* \n\ndavid  h.  ackley \n\ngeoffrey \ncomputer  science department \n\ne.  hinton \n\ncarnegie-mellon  university \n\nterrence \n\nj.  sejnowski \n\nbiophysics  department \n\nthe johns  hopkins  university \n\ncomputotionol \n\nof  massively \n\nparallel \n\nnetworks \n\nof  simple \n\nthe \nelements \nconnections \nfraction \nlem \nnetworks \nbut \nsearch \nthere \npreexisting \nstraints \nmethod, \neral \nknowledge \nexamples \nthot \ntivity \n\nore \n\nin  o  very \n\nshort \n\nto  use \n\nappear \nthe \ntechnique \nbe \nsome \nhardware \n\nmust \n\nthe \nin \nbased \n\ndomain \n\non \n\nrule \n\nlearning \n\nobout \nin  which \ndemonstrobly \n\nstructure. \n\npower \nthe \n\nin \n\nresides \n\ncommunication \n\nbetween \n\nelements. \n\nof \n\nthe \n\nknowledge \n\nof \ntime. \none \nto  be  well \n\nthe \nkind \nsuited \nefficiently \n\nconnections \n\nthat \n\nis  suitable \n\nfor  parallel \n\nway \n\nof  choosing \n\ninternal \n\nbandwidth \nconnections \nto  be  applied \n\nprovided \ncon \nto  an \n\nthese \nsystem", "article\na multi-modal parcellation of human \ncerebral cortex\n\nmatthew f. glasser1, timothy s. coalson1*, emma c. robinson2,3*, carl d. hacker4*, john harwell1, essa yacoub5, \nkamil ugurbil5, jesper andersson2, christian f. beckmann6,7, mark jenkinson2, stephen m. smith2 & david c. van essen1\n\ndoi:10.1038/nature18933\n\nunderstanding the amazingly complex human cerebral cortex requires a map (or parcellation) of its major subdivisions, \nknown as cortical areas. making an accurate areal map has been a century-old objective in neuroscience. using multi-\nmodal magnetic resonance images from the human connectome project (hcp) and an objective semi-automated \nneuroanatomical approach, we delineated 180 areas per hemisphere bounded by sharp changes in cortical architecture, \nfunction, connectivity, and/or topography in a precisely aligned group average of 210 healthy young adults. we \ncharacterized 97 new areas and 83 areas previously reported using post-mortem microscopy or other specialized s", "8\n1\n0\n2\n \nc\ne\nd\n1\n2\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n2\nv\n1\n2\n7\n6\n0\n\n.\n\n0\n1\n8\n1\n:\nv\ni\nx\nr\na\n\noptimizing agent behavior over long time scales by\ntransporting value\nchia-chun hung1\u2217\u2020, timothy lillicrap1\u2217\u2020, josh abramson1\u2217, yan wu1, mehdi mirza1, federico\ncarnevale1, arun ahuja1, greg wayne1\u2217\u2020.\n1deepmind, 5 new street square, london ec4a 3tw, uk.\n\u2217these authors contributed equally to this work.\n\u2020to whom correspondence should be addressed.\n\nhumans spend a remarkable fraction of waking life engaged in acts of \u201cmental time travel\u201d1.\nwe dwell on our actions in the past and experience satisfaction or regret. more than merely\nautobiographical storytelling, we use these event recollections to change how we will act in\nsimilar scenarios in the future. this process endows us with a computationally important\nability to link actions and consequences across long spans of time, which \ufb01gures prominently\nin addressing the problem of long-term temporal credit assignment; in arti\ufb01cial intelligence\n(ai) this is ", "0\n2\n0\n2\n\n \nr\np\na\n5\n1\n\n \n\n \n \n]\nh\np\n-\np\nm\no\nc\n.\ns\nc\ni\ns\ny\nh\np\n[\n \n \n\n2\nv\n1\n8\n4\n1\n1\n\n.\n\n5\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nai feynman: a physics-inspired method for symbolic regression\n\nsilviu-marian udrescu, max tegmark\u2217\n\ndept. of physics & center for brains, minds & machines,\n\nmassachusetts institute of technology, cambridge, ma 02139; sudrescu@mit.edu and\n\ntheiss research, la jolla, ca 92037, usa\n\n(dated: published in science advances, 6:eaay2631, april 15, 2020)\n\na core challenge for both physics and arti\ufb01cial intelligence (ai) is symbolic regression: \ufb01nding\na symbolic expression that matches data from an unknown function. although this problem is\nlikely to be np-hard in principle, functions of practical interest often exhibit symmetries, sepa-\nrability, compositionality and other simplifying properties.\nin this spirit, we develop a recursive\nmultidimensional symbolic regression algorithm that combines neural network \ufb01tting with a suite of\nphysics-inspired techniques. we apply it to 100 equations ", "a theory for how sensorimotor skills are learned and\nretained in noisy and nonstationary neural circuits\n\nrobert ajemiana,1, alessandro d\u2019ausiliob,c, helene moormand,e, and emilio bizzia,d,1\n\namcgovern institute for brain research and ddepartment of brain and cognitive sciences, massachusetts institute of technology, cambridge, ma 02139;\nbdepartment of psychology, university of rome \u201cla sapienza,\u201d 00185 rome, italy; cinter-university centre for research on cognitive processing in natural\nand arti\ufb01cial systems, rome, italy; and ehelen wills neuroscience institute, li ka shing biomedical center, university of california, berkeley, ca 94720\n\ncontributed by emilio bizzi, october 30, 2013 (sent for review july 12, 2013)\n\nduring the process of skill learning, synaptic connections in our\nbrains are modi\ufb01ed to form motor memories of learned sensori-\nmotor acts. the more plastic the adult brain is, the easier it is to\nlearn new skills or adapt to neurological injury. however, if the\nbrain is to", "neuron\n\narticle\n\nbalanced ampli\ufb01cation: a new mechanism\nof selective ampli\ufb01cation\nof neural activity patterns\n\nbrendan k. murphy1,2 and kenneth d. miller2,*\n1graduate group in biophysics, university of california, san francisco, san francisco, ca 94122, usa\n2center for theoretical neuroscience, sloan program in theoretical neuroscience, department of neuroscience, columbia university\ncollege of physicians and surgeons, new york, ny 10032, usa\n*correspondence: ken@neurotheory.columbia.edu\ndoi 10.1016/j.neuron.2009.02.005\n\nsummary\n\nin cerebral cortex, ongoing activity absent a stimulus\ncan resemble stimulus-driven activity in size and\nstructure. in particular, spontaneous activity in cat\nprimary visual cortex (v1) has structure signi\ufb01cantly\ncorrelated with evoked responses to oriented stimuli.\nthis suggests that, from unstructured input, cortical\ncircuits selectively amplify speci\ufb01c activity patterns.\ncurrent understanding of selective ampli\ufb01cation\ninvolves elongation of a neural assembl", "learning in implicit generative models\n\nshakir mohamed 1 balaji lakshminarayanan 1\n\n7\n1\n0\n2\n\n \n\nb\ne\nf\n7\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n4\nv\n3\n8\n4\n3\n0\n\n.\n\n0\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\ngenerative adversarial networks (gans) provide\nan algorithmic framework for constructing gen-\nerative models with several appealing proper-\nties:\nthey do not require a likelihood function\nto be speci\ufb01ed, only a generating procedure; they\nprovide samples that are sharp and compelling;\nand they allow us to harness our knowledge of\nbuilding highly accurate neural network classi-\n\ufb01ers. here, we develop our understanding of\ngans with the aim of forming a rich view of\nthis growing area of machine learning\u2014to build\nconnections to the diverse set of statistical think-\ning on this topic, of which much can be gained\nby a mutual exchange of ideas. we frame gans\nwithin the wider landscape of algorithms for learn-\ning in implicit generative models\u2014models that\nonly specify a stochastic procedure with which\nto genera", "recognition in terra incognita\n\nsara beery, grant van horn, and pietro perona\n\n{sbeery,gvanhorn,perona}@caltech.edu\n\ncaltech\n\nabstract. it is desirable for detection and classi\ufb01cation algorithms to\ngeneralize to unfamiliar environments, but suitable benchmarks for quan-\ntitatively studying this phenomenon are not yet available. we present a\ndataset designed to measure recognition generalization to novel environ-\nments. the images in our dataset are harvested from twenty camera traps\ndeployed to monitor animal populations. camera traps are \ufb01xed at one\nlocation, hence the background changes little across images; capture is\ntriggered automatically, hence there is no human bias. the challenge is\nlearning recognition in a handful of locations, and generalizing animal\ndetection and classi\ufb01cation to new locations where no training data is\navailable. in our experiments state-of-the-art algorithms show excellent\nperformance when tested at the same location where they were trained.\nhowever, we \ufb01", "dynamical isometry is achieved in residual networks in a universal way for any activation\n\nfunction\n\nwojciech tarnowski,1, \u2217 piotr warcho\u0142,1, \u2020 stanis\u0142aw jastrz\u02dbebski,2, \u2021 jacek tabor,2, \u00a7 and maciej a. nowak3, \u00b6\n\n1m. smoluchowski institute of physics, jagiellonian university, pl\u201330\u2013348 krak\u00f3w, poland\n2faculty of mathematics and computer science, jagiellonian university, krak\u00f3w, poland\n\n3m. smoluchowski institute of physics and mark kac complex systems research center, jagiellonian university, pl\u201330\u2013348 krak\u00f3w, poland\n\n(dated: march 5, 2019)\n\n9\n1\n0\n2\n\n \nr\na\n\nm\n4\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n8\n4\n8\n8\n0\n\n.\n\n9\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nwe demonstrate that in residual neural networks (resnets) dynamical isometry is achievable irrespective of\nthe activation function used. we do that by deriving, with the help of free probability and random matrix\ntheories, a universal formula for the spectral density of the input-output jacobian at initialization, in the large\nnetwork width and depth limit. ", "article\ndiscrete attractor dynamics underlies \npersistent activity in the frontal cortex\n\nhidehiko k. inagaki1, lorenzo fontolan1, sandro romani1* & karel svoboda1*\n\nhttps://doi.org/10.1038/s41586-019-0919-7\n\nshort-term memories link events separated in time, such as past sensation and future actions. short-term memories are \ncorrelated with slow neural dynamics, including selective persistent activity, which can be maintained over seconds. in a \ndelayed response task that requires short-term memory, neurons in the mouse anterior lateral motor cortex (alm) show \npersistent activity that instructs future actions. to determine the principles that underlie this persistent activity, here \nwe combined intracellular and extracellular electrophysiology with optogenetic perturbations and network modelling. \nwe show that during the delay epoch, the activity of alm neurons moved towards discrete end points that correspond \nto specific movement directions. these end points were robust to transien", "this is an open access article published under an acs authorchoice license, which permits\ncopying and redistribution of the article or any adaptations for non-commercial purposes.\n\nreview\n\npubs.acs.org/cr\n\ncoarse-grained protein models and their applications\nsebastian kmiecik,\u2020 dominik gront,\u2020 michal kolinski,\u2021 lukasz wieteska,\u2020,\u00a7 aleksandra elzbieta dawid,\u2020\nand andrzej kolinski*,\u2020\n\u2020\nfaculty of chemistry, university of warsaw, pasteura 1, 02-093 warsaw, poland\n\u2021\nbioinformatics laboratory, mossakowski medical research center of the polish academy of sciences, pawinskiego 5, 02-106\nwarsaw, poland\n\u00a7department of medical biochemistry, medical university of lodz, mazowiecka 6/8, 92-215 lodz, poland\n\nabstract: the traditional computational modeling of protein structure, dynamics,\nand interactions remains di\ufb03cult for many protein systems. it is mostly due to the size\nof protein conformational spaces and required simulation time scales that are still too\nlarge to be studied in atomistic detail", "review\n\nperceptual learning, motor learning and automaticity\n\nperceptual learning rules based on\nreinforcers and attention\npieter r. roelfsema1,2, arjen van ooyen2 and takeo watanabe3\n\n1 department of vision & cognition, netherlands institute for neurosciences, an institute of the royal netherlands academy of arts\nand sciences (knaw), meibergdreef 47, 1105 ba, amsterdam, the netherlands\n2 department of integrative neurophysiology, centre for neurogenomics and cognitive research, vu university, de boelelaan\n1085, 1081 hv amsterdam, amsterdam, the netherlands\n3 department of psychology, boston university, 64 cummington st., boston, ma 02215, usa\n\nhow does the brain learn those visual features that are\nrelevant for behavior? in this article, we focus on two\nfactors that guide plasticity of visual representations.\nfirst, reinforcers cause the global release of diffusive\nneuromodulatory signals that gate plasticity. second,\nattentional\nfeedback signals highlight the chain of\nneurons between", "stop explaining black box machine learning \nmodels for high stakes decisions and use \ninterpretable models instead\n\ncynthia rudin\u200a\n\n\u200a\n\nblack box machine learning models are currently being used for high-stakes decision making throughout society, causing prob-\nlems in healthcare, criminal justice and other domains. some people hope that creating methods for explaining these black box \nmodels will alleviate some of the problems, but trying to explain black box models, rather than creating models that are inter-\npretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. the way forward \nis to design models that are inherently interpretable. this perspective clarifies the chasm between explaining black boxes and \nusing inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-\nstakes decisions, identifies challenges to interpretable machine learning, and provides several exampl", "pergamon\n\nneural networks 14 (2001) 941\u00b1953\n\n2001 special issue\n\nneural\n\nnetworks\n\nwww.elsevier.com/locate/neunet\n\nfrom arti\u00aecial neural networks to spiking neuron populations\n\nand back again\n\nmarc de kamps*, frank van der velde\n\nunit of experimental and theoretical psyhology, leiden university, wassenaarsewag 52, 2333 ak leiden, the netherlands\n\nreceived 10 october 2000; revised 4 april 2001; accepted 4 april 2001\n\nabstract\n\nin this paper, we investigate the relation between arti\u00aecial neural networks (anns) and networks of populations of spiking neurons. the\nactivity of an arti\u00aecial neuron is usually interpreted as the \u00aering rate of a neuron or neuron population. using a model of the visual cortex, we\nwill show that this interpretation runs into serious dif\u00aeculties. we propose to interpret the activity of an arti\u00aecial neuron as the steady state of\na cross-inhibitory circuit, in which one population codes for `positive' arti\u00aecial neuron activity and another for `negative' activity. we ", "convolutional neural networks as a model of the visual\n\nsystem: past, present, and future\n\ngrace w. lindsay, phd\n\ngatsby computational unit/sainsbury wellcome centre\n\nuniversity college london\ngracewlindsay@gmail.com\n\nabstract\n\nconvolutional neural networks (cnns) were inspired by early findings in the study of biological\nvision. they have since become successful tools in computer vision and state-of-the-art models\nof both neural activity and behavior on visual tasks. this review highlights what, in the context\nof cnns, it means to be a good model in computational neuroscience and the various ways\nmodels can provide insight. specifically, it covers the origins of cnns and the methods by\n\nwhich we validate them as models of biological vision. it then goes on to elaborate on what we\ncan learn about biological vision by understanding and experimenting on cnns and discusses\nemerging opportunities for the use of cnns in vision research beyond basic object recognition.\n\nthis is the author\u2019s ", "8\n1\n0\n2\n\n \n\nn\na\nj\n \n\n1\n3\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n0\n9\n1\n0\n\n.\n\n6\n0\n7\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2018\n\nparameter space noise for exploration\n\nmatthias plappert\u2020\u2021, rein houthooft\u2020, prafulla dhariwal\u2020, szymon sidor\u2020,\nrichard y. chen\u2020, xi chen\u2020\u2020, tamim asfour\u2021, pieter abbeel\u2020\u2020, and marcin andrychowicz\u2020\n\u2020 openai\n\u2021 karlsruhe institute of technology (kit)\n\u2020\u2020 university of california, berkeley\ncorrespondence to matthias@openai.com\n\nabstract\n\ndeep reinforcement learning (rl) methods generally engage in exploratory be-\nhavior through noise injection in the action space. an alternative is to add noise\ndirectly to the agent\u2019s parameters, which can lead to more consistent exploration\nand a richer set of behaviors. methods such as evolutionary strategies use parameter\nperturbations, but discard all temporal structure in the process and require signif-\nicantly more samples. combining parameter noise with traditional rl methods\nallows to combine the best of both worlds", "hypothesis testing using pairwise distances and associated kernels\n\n2\n1\n0\n2\n\n \n\ny\na\nm\n1\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n1\n1\n4\n0\n\n.\n\n5\n0\n2\n1\n:\nv\ni\nx\nr\na\n\ndino sejdinovic\u22c6\narthur gretton\u22c6,\u2020,\u2217\nbharath sriperumbudur\u22c6,\u2217\nkenji fukumizu\u2021\n\u22c6gatsby computational neuroscience unit, csml, university college london, \u2020max planck institute for intel-\nligent systems, t\u00fcbingen, \u2021the institute of statistical mathematics, tokyo\n\narthur.gretton@gmail.com\n\ndino.sejdinovic@gmail.com\n\nbharath@gatsby.ucl.ac.uk\n\nfukumizu@ism.ac.jp\n\nabstract\n\nwe provide a unifying framework linking two\nclasses of statistics used in two-sample and\nindependence testing: on the one hand, the\nenergy distances and distance covariances\nfrom the statistics literature; on the other,\ndistances between embeddings of distribu-\ntions to reproducing kernel hilbert spaces\n(rkhs), as established in machine learning.\nthe equivalence holds when energy distances\nare computed with semimetrics of negative\ntype, in which case a kernel may be de\ufb01n", "volume 69, number 26\n\nph ysical revi ew letters\n\n28 december 1992\n\nsuppressing chaos in neural networks by noise\n\nl. molgedey, j. schuchhardt,\n\nand h. g. schuster\n\ninstitut\n\nfur theoretische physik, olshausenstrasse\n\n$0, d 290-0 kiel i, germany\n\n(received 20 july 1992)\n\nwe study discrete parallel dynamics of. a fully connected network of nonlinear\n\ncouplings under\n\nrandom asymmetric\n\nvia long-range\nmean-field equations, which become exact in the thermodynamical\nand the maximal lyapunov exponent of the network in dependence of a nonlinearity\nand the noise intensity.\npacs numbers: 05.45.+b, 05.20.\u2014y, 47.20.tg, 87.10.+e\n\ninteracting\nthe influence of external noise. using dynamical\nlimit, we calculate the activity\n(gain) parameter\n\nelements\n\nrecently there has been considerable\n\nsystems\n\ndynamical\n\n[1\u20143], ecological and economical models\n\ntially extended\nworks\npopulations\nmay be described by a set of coupled differential\ntions or iterated maps.\n\nof interacting\n\noscillators\n\ninterest\n\nin sp", "preprintrelease.fullcitation:yosinskij,clunej,bengioy,andlipsonh.howtransferablearefeaturesindeepneuralnetworks?inadvancesinneuralinformationprocessingsystems27(nips\u201914),nipsfoundation,2014.howtransferablearefeaturesindeepneuralnetworks?jasonyosinski,1jeffclune,2yoshuabengio,3andhodlipson41dept.computerscience,cornelluniversity2dept.computerscience,universityofwyoming3dept.computerscience&operationsresearch,universityofmontreal4dept.mechanical&aerospaceengineering,cornelluniversityabstractmanydeepneuralnetworkstrainedonnaturalimagesexhibitacuriousphe-nomenonincommon:onthe\ufb01rstlayertheylearnfeaturessimilartogabor\ufb01ltersandcolorblobs.such\ufb01rst-layerfeaturesappearnottobespeci\ufb01ctoaparticulardatasetortask,butgeneralinthattheyareapplicabletomanydatasetsandtasks.featuresmusteventuallytransitionfromgeneraltospeci\ufb01cbythelastlayerofthenetwork,butthistransitionhasnotbeenstudiedextensively.inthispaperweexperimentallyquantifythegeneralityversusspeci\ufb01cityofneuronsineachlayerofadeepconvolutionalneuralne", "the journal of neuroscience, april 1, 2000, 20(7):2451\u20132458\n\nsynaptic activity modulates the induction of bidirectional synaptic\nchanges in adult mouse hippocampus\n\nanaclet ngezahayo,1 melitta schachner,1,2 and alain artola1\n1department of neurobiology, swiss federal institute of technology zu\u00a8 rich, ho\u00a8 nggerberg, ch-8093 zu\u00a8 rich, switzerland,\nand 2zentrum fu\u00a8 r molekulare neurobiologie, universita\u00a8 t hamburg, d-20246 hamburg, germany\n\nactivity-dependent synaptic plasticity is critical for learning and\nmemory. considerable attention has been paid to mechanisms\nthat increase or decrease synaptic ef\ufb01cacy, referred to as long-\nterm potentiation (ltp) and long-term depression (ltd), respec-\ntively. it is becoming apparent that synaptic activity also mod-\nulates the ability to elicit subsequent synaptic changes. we\nprovide direct experimental evidence that this modulation is\nattributable, at least in part, to variations in the level of postsyn-\naptic depolarization required for inducing p", "rethinking atrous convolution for semantic image segmentation\n\nliang-chieh chen george papandreou florian schroff hartwig adam\n\n{lcchen, gpapan, fschroff, hadam}@google.com\n\ngoogle inc.\n\n7\n1\n0\n2\n \nc\ne\nd\n5\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n3\nv\n7\n8\n5\n5\n0\n\n.\n\n6\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nin this work, we revisit atrous convolution, a powerful tool\nto explicitly adjust \ufb01lter\u2019s \ufb01eld-of-view as well as control the\nresolution of feature responses computed by deep convolu-\ntional neural networks, in the application of semantic image\nsegmentation. to handle the problem of segmenting objects\nat multiple scales, we design modules which employ atrous\nconvolution in cascade or in parallel to capture multi-scale\ncontext by adopting multiple atrous rates. furthermore, we\npropose to augment our previously proposed atrous spatial\npyramid pooling module, which probes convolutional fea-\ntures at multiple scales, with image-level features encoding\nglobal context and further boost performance. we also elab-\nor", "greedy layer-wise training of deep networks\n\nyoshua bengio, pascal lamblin, dan popovici, hugo larochelle\n\nfbengioy,lamblinp,popovicd,larochehg@iro.umontreal.ca\n\nuniversit\u00b7e de montr\u00b7eal\n\nmontr\u00b7eal, qu\u00b7ebec\n\nabstract\n\ncomplexity theory of circuits strongly suggests that deep architectures can be much\nmore ef(cid:2)cient (sometimes exponentially) than shallow architectures, in terms of\ncomputational elements required to represent some functions. deep multi-layer\nneural networks have many levels of non-linearities allowing them to compactly\nrepresent highly non-linear and highly-varying functions. however, until recently\nit was not clear how to train such deep networks, since gradient-based optimization\nstarting from random initialization appears to often get stuck in poor solutions. hin-\nton et al. recently introduced a greedy layer-wise unsupervised learning algorithm\nfor deep belief networks (dbn), a generative model with many layers of hidden\ncausal variables. in the context of the a", "gradient-based  learning  algorithms  for  recurrent \n\nnetworks  and  their  computational  complexity \n\nronald  j.  williams \n\ncollege  of  computer  science \n\nnortheastern  university \n\nboston,  ma  02115 \n\nand \n\ndavid  zipser \n\ndepartment  of  cognitive  science \nuniversity  of  california,  san  diego \n\nla  jolla,  ca  92093 \n\nappears  in  y.  chauvin  &  d.  e.  rumelhart  (eds.) \n\nback-propagation:  theory,  architectures  and  applications. \n\nhillsdale,  nj:  erlbaum.  1995. \n\n1 \n\nintroduction \n\n1.1 \n\nlearning  in  recurrent  networks \n\nconnectionist  networks  having  feedback  connections  are  interesting  for  a  number  of  reasons.  bio- \nlogical  neural  networks  are  highly  recurrently  connected,  and  many  authors  have  studied  recurrent \nnetwork  models  of  various  types  of  perceptual  and  memory  processes.  the  general  property  making \nsuch  networks  interesting  and  potentially  useful  is  that  they  manifest  highly  nonlinear  dynami- \ncal  behav", "7\n1\n0\n2\n\n \n\np\ne\ns\n6\n2\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n3\nv\n7\n0\n9\n7\n0\n\n.\n\n7\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nmutual alignment transfer learning\n\nmarkus wulfmeier\u2217\noxford robotics institute\n\nuniversity of oxford\n\ningmar posner\n\noxford robotics institute\n\nuniversity of oxford\n\nmarkusw@robots.ox.ac.uk\n\ningmar@robots.ox.ac.uk\n\npieter abbeel\n\nberkeley ai research (bair)\n\nuniversity of california berkeley\n\nopenai, san francisco\n\npabbeel@cs.berkeley.edu\n\nabstract: training robots for operation in the real world is a complex, time con-\nsuming and potentially expensive task. despite signi\ufb01cant success of reinforce-\nment learning in games and simulations, research in real robot applications has not\nbeen able to match similar progress. while sample complexity can be reduced by\ntraining policies in simulation, such policies can perform sub-optimally on the real\nplatform given imperfect calibration of model dynamics. we present an approach\n\u2013 supplemental to \ufb01ne tuning on the real robot \u2013 to further bene\ufb01t from paralle", "the committee machine: computational to statistical gaps\n\nin learning a two-layers neural network\n\nbenjamin aubin(cid:63)\u2020, antoine maillard\u2020, jean barbier\u2666,\n\nflorent krzakala\u2020, nicolas macris\u2297 and lenka zdeborov\u00e1(cid:63)\n\nabstract\n\nheuristic tools from statistical physics have been used in the past to locate the phase transitions and\ncompute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural\nnetworks. in this contribution, we provide a rigorous justi(cid:27)cation of these approaches for a two-layers\nneural network model called the committee machine, under a technical assumption. we also introduce a\nversion of the approximate message passing (amp) algorithm for the committee machine that allows to\nperform optimal learning in polynomial time for a large set of parameters. we (cid:27)nd that there are regimes\nin which a low generalization error is information-theoretically achievable while the amp algorithm fails\nto deliver it; strongly ", "\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\t\f\u000b\u000e\r\u0010\u000f\u0012\u0011\u0013\u0001\u0015\u0014\n\u0016\u0017\u000f\u0019\u0018\u001b\u001a\u0010\u001a\u001d\u001c\u0004\u0016\u001f\u001e\u0010\u0014! \"\u0003#\u0001$\u0014\n\u0016\u0017\u000f%\u0014!\u000f'&)(\u001d\u000f\u0012\u0003* +\u0014\n\u0011-,.\u001c\u0004\u0016*/0\u001c1\u0003* + +\u0014!\u000f\u0012/\n\n2436587\u00159!36:<;>=$245?9a@b5dc\n\ne$fhg4ikjklnmporqksutvmpwdsum*xymzid[!onl\\g4mzw]l\ner[!o\\w8mp^atvm\u0006_\u000emp`v`vfaw\u0010arw8tcb!mzo\\d\\tvlne\n\ntclnl\\dngkjko\\^hh\nf?ikj\nlhmkjon\np6pkqhr6stqvudw8x6y{zuxo|6}rz\u007f~!sh}\n\n\u0080\u0082\u0081y\u0083h\u0084k\u0085k\u0086>\u0087?\u0084\n\nhkm1d\\jdsps\u0089mpd\\d\u0004fv\u008ako\\mptvw6\u008a\u008bfho\u007fsumzg4mpw6l\u008c`vm\n[vo\\wktvwk^rtvw#ikou[asn\u008d\nlnt\u008esz[!`\biko\\fhgk`vmpg4d\u0090\u008ftmpi\u0091mpwd\u008f6d\u0092faw\u0093l\\hkm\u0094[!gktv`vtvl\u0095e\u0096l\\f\nsufhg0g8tcw8m\u0010\u008a\u008bj8wdsul\\tvfhw\u0097[vikiko\\fo\u0098ktvg\u0012[vl\\tvfaw\u009a\u0099rtvl\\h\u0092l\\mpg\u0017\u008d\ni?faou[!`\u008c\u008f6t\u009c\u009b\u0091mpo\\mpw?sum0g\u009dmpl\\h8f]\u008f6drdnjdsuh\u0010[!drb![!`vjkm#tvl\\mpou[o\u008d\nlntcfhw\u008c\u009e\u008c\u009fb\u0098ki\u0091mpontvg4mpwhl\\d>tvw\u0006l\\hktvd\u0004[!onm\n[\u001fhd[obam$ikonft\u008ftjdsum\n\u008f\ng\u009dt\u00a0\u0098km\n\u008f\u009donmpd\\jk`vlnd\n\u00a1bl\\hkmponm\u0002hd[zbam*gdmpmzw\u00a2gdfhl\\h\u0010wkfhl\u007f[vgk`vm\ndnjdspsumpdnd\\mpd#[!wd\u008f\u00a3wkfhl\u007f[!g8`cmy\u008fttcdu[!iki?fatvwhl\\g4mzwal\\do\u009e\nhkmu\u008d\nfho\\e\u00a2hd[vd0g?mpmpw\u0082d\u007fsz[!o\u007fs\u0089ma\u00a4$g4fadnl\\`ve<\u008f6j8m\u0012lnf\u0010l\\h8m\u00a5\u008ftt\u00a0\u008a\u00a6\u008d\n\u00a7\u00a8s\u0089jk`vl\u00a9e\u00a2f!\u008ayo\\mo[!d\\fhwktvwk^<[vgdfhjkl\u0006\u008a\u00aajkwdsulntcfhw\u00ab[!iki8o\\fo\u0098kt\u009c\u008d\ng\u000e[!l\\fho\\d#l\\h?[!l\u0006^hm", "increasing the action gap:\n\nnew operators for reinforcement learning\n\nmarc g. bellemare and georg ostrovski and arthur guez\n\nphilip s. thomas\u2217 and r\u00b4emi munos\n\n{bellemare,ostrovski,aguez,munos}@google.com; philipt@cs.cmu.edu\n\ngoogle deepmind\n\n5\n1\n0\n2\n \nc\ne\nd\n5\n1\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\nv\n0\n6\n8\n4\n0\n\n.\n\n2\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nthis paper introduces new optimality-preserving oper-\nators on q-functions. we \ufb01rst describe an operator for\ntabular representations, the consistent bellman opera-\ntor, which incorporates a notion of local policy con-\nsistency. we show that this local consistency leads to\nan increase in the action gap at each state; increasing\nthis gap, we argue, mitigates the undesirable effects\nof approximation and estimation errors on the induced\ngreedy policies. this operator can also be applied to\ndiscretized continuous space and time problems, and\nwe provide empirical results evidencing superior per-\nformance in this context. extending the idea of a locally\nconsi", "scientific discovery in the age of artificial \nintelligence\n\nhttps://doi.org/10.1038/s41586-023-06221-2\nreceived: 30 march 2022\naccepted: 16 may 2023\npublished online: 2 august 2023\n\n check for updates\n\nhanchen wang1,2,37,38,39, tianfan fu3,39, yuanqi du4,39, wenhao gao5, kexin huang6, \n ziming liu7, payal chandak8, shengchao liu9,10, peter van katwyk11,12, andreea deac9,10, \nanima anandkumar2,13, karianne bergen11,12, carla p. gomes4, shirley ho14,15,16,17, \npushmeet kohli18, joan lasenby1, jure leskovec6, tie-yan liu19, arjun manrai20, \ndebora marks21,22, bharath ramsundar23, le song24,25, jimeng sun26, jian tang9,27,28, \npetar veli\u010dkovi\u010717,29, max welling30,31, linfeng zhang32,33, connor w. coley5,34, \nyoshua bengio9,10 & marinka zitnik20,22,35,36\u2009\u2709\n\nartificial intelligence (ai) is being increasingly integrated into scientific discovery to \naugment and accelerate research, helping scientists to generate hypotheses, design \nexperiments, collect and interpret large datasets, and gain ", "neuron, vol. 28, 329\u2013337, april 24, 2003, copyright \uf8e92003 by cell press\n\ntemporal difference models and\nreward-related learning in the human brain\n\njohn p. o\u2019doherty,1,* peter dayan,2 karl friston,1\nhugo critchley,1 and raymond j. dolan1\n1wellcome department of imaging neuroscience\ninstitute of neurology\n2 gatsby computational neuroscience unit\nuniversity college london\nlondon wc1n 3bg\nunited kingdom\n\nsummary\n\ntemporal difference learning has been proposed as a\nmodel for pavlovian conditioning, in which an animal\nlearns to predict delivery of reward following presenta-\ntion of a conditioned stimulus (cs). a key component\nof this model is a prediction error signal, which, before\nlearning, responds at the time of presentation of re-\nward but, after learning, shifts its response to the time\nof onset of the cs. in order to test for regions manifest-\ning this signal profile, subjects were scanned using\nevent-related fmri while undergoing appetitive condi-\ntioning with a pleasant taste rewar", "9\n1\n0\n2\n\n \n\ny\na\nm\n7\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n4\n8\n5\n8\n0\n\n.\n\n1\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nfine-grained analysis of optimization and generalization for\n\noverparameterized two-layer neural networks\n\nsanjeev arora\u2217\n\nsimon s. du\u2020 wei hu\u2021\n\nzhiyuan li\u00a7\n\nruosong wang\u00b6\n\nabstract\n\nrecent works have cast some light on the mystery of why deep nets \ufb01t any data and generalize despite\nbeing very overparametrized. this paper analyzes training and generalization for a simple 2-layer relu\nnet with random initialization, and provides the following improvements over recent works:\n\n(i) using a tighter characterization of training speed than recent papers, an explanation for why train-\ning a neural net with random labels leads to slower training, as originally observed in [zhang et al.\niclr\u201917].\n\n(ii) generalization bound independent of network size, using a data-dependent complexity measure.\nour measure distinguishes clearly between random labels and true labels on mnist and cifar,\nas shown by experiments.", "annealing between distributions by\n\naveraging moments\n\nroger grosse\n\nchris j. maddison\n\nruslan salakhutdinov\n\ncomp. sci. & ai lab\n\ndept. of computer science\n\ndepts. of statistics and comp. sci.,\n\nmit\n\ncambridge, ma 02139\n\nuniversity of toronto\ntoronto, on m5s 3g4\n\nuniversity of toronto\n\ntoronto, on m5s 3g4, canada\n\nabstract\n\nmany powerful monte carlo techniques for estimating partition functions, such\nas annealed importance sampling (ais), are based on sampling from a sequence\nof intermediate distributions which interpolate between a tractable initial distribu-\ntion and the intractable target distribution. the near-universal practice is to use\ngeometric averages of the initial and target distributions, but alternative paths can\nperform substantially better. we present a novel sequence of intermediate distribu-\ntions for exponential families de\ufb01ned by averaging the moments of the initial and\ntarget distributions. we analyze the asymptotic performance of both the geomet-\nric and moment a", "2\n2\n0\n2\n\n \nt\nc\no\n5\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n5\n1\n2\n4\n1\n\n.\n\n0\n1\n2\n2\n:\nv\ni\nx\nr\na\n\npreprint\n\nin-context reinforcement learning\nwith algorithm distillation\n\nmichael laskin\u2217\n\nluyu wang\u2217\n\njunhyuk oh\n\nemilio parisotto\n\nstephen spencer\n\nrichie steigerwald\n\ndj strouse\n\nsteven hansen\n\nangelos filos\n\nethan brooks\n\nmaxime gazeau\n\nhimanshu sahni\n\nsatinder singh\n\nvolodymyr mnih\n\ndeepmind\n\nabstract\n\nwe propose algorithm distillation (ad), a method for distilling reinforcement\nlearning (rl) algorithms into neural networks by modeling their training his-\ntories with a causal sequence model. algorithm distillation treats learning to\nreinforcement learn as an across-episode sequential prediction problem. a dataset\nof learning histories is generated by a source rl algorithm, and then a causal\ntransformer is trained by autoregressively predicting actions given their preceding\nlearning histories as context. unlike sequential policy prediction architectures that\ndistill post-learning or expert sequen", "a r t i c l e s\n\nseparate neural substrates for skill learning and\nperformance in the ventral and dorsal striatum\n\nhisham e atallah, dan lopez-paniagua, jerry w rudy & randall c o\u2019reilly\n\nit is widely accepted that the striatum of the basal ganglia is a primary substrate for the learning and performance of skills. we\nprovide evidence that two regions of the rat striatum, ventral and dorsal, play distinct roles in instrumental conditioning (skill\nlearning), with the ventral striatum being critical for learning and the dorsal striatum being important for performance but,\nnotably, not for learning. this implies an actor (dorsal) versus director (ventral) division of labor, which is a new variant of the\nwidely discussed actor-critic architecture. our results also imply that the successful performance of a skill can ultimately result\nin its establishment as a habit outside the basal ganglia.\n\nthere is extensive evidence that both the ventral and dorsal striatum are\ninvolved in instrumental ", "deep supervised, but not unsupervised, models may\nexplain it cortical representation\n\nseyed-mahdi khaligh-razavi*, nikolaus kriegeskorte*\n\nmedical research council, cognition and brain sciences unit, cambridge, united kingdom\n\nabstract\n\ninferior temporal (it) cortex in human and nonhuman primates serves visual object recognition. computational object-\nvision models, although continually improving, do not yet reach human performance. it is unclear to what extent the\ninternal representations of computational models can explain the it representation. here we investigate a wide range of\ncomputational model representations (37 in total), testing their categorization performance and their ability to account for\nthe it representational geometry. the models include well-known neuroscientific object-recognition models (e.g. hmax,\nvisnet) along with several models from computer vision (e.g. sift, gist, self-similarity features, and a deep convolutional\nneural network). we compared the representa", "the journal of neuroscience, november 1, 1999, 19(21):9587\u20139603\n\nsynaptic basis of cortical persistent activity: the importance of\nnmda receptors to working memory\n\nxiao-jing wang\nvolen center for complex systems and department of physics, brandeis university, waltham,\nmassachusetts 02454-9110\n\ndelay-period activity of prefrontal cortical cells, the neural hall-\nmark of working memory, is generally assumed to be sustained\nby reverberating synaptic excitation in the prefrontal cortical\ncircuit. previous model studies of working memory emphasized\nthe high ef\ufb01cacy of recurrent synapses, but did not investigate\nthe role of temporal synaptic dynamics. in this theoretical work,\ni show that biophysical properties of cortical synaptic transmis-\nsion are important to the generation and stabilization of a\nnetwork persistent state. this is especially the case when\nnegative feedback mechanisms (such as spike-frequency ad-\naptation, feedback shunting inhibition, and short-term depres-\nsion of recur", "ne39ch12-kohn\n\nari\n\n11 june 2016\n\n9:13\n\ncorrelations and neuronal\npopulation information\n\nadam kohn,1,2 ruben coen-cagli,3\ningmar kanitscheider,3,4,5 and alexandre pouget3,6,7\n1dominick purpura department of neuroscience, albert einstein college of medicine, bronx,\nnew york 10461; email: adam.kohn@einstein.yu.edu\n2department of ophthalmology and visual sciences, albert einstein college of medicine,\nbronx, new york 10461\n3department of basic neuroscience, university of geneva, ch-1211 geneva, switzerland;\nemail: ruben.coencagli@unige.ch, alexandre.pouget@unige.ch\n4center of learning and memory, the university of texas at austin, austin, texas 78712;\nemail: ikanitscheider@mail.clm.utexas.edu\n5department of neuroscience, the university of texas at austin, austin, texas 78712\n6department of brain and cognitive sciences, university of rochester, rochester,\nnew york 14627\n7gatsby computational neuroscience unit, university college london, w1t 4jg london,\nunited kingdom\n\nkeywords\nneural codin", "identification of causal effects using instrumental variables \nauthor(s): joshua d. angrist, guido w. imbens and donald b. rubin \nsource: journal of the american statistical association, jun., 1996, vol. 91, no. 434 \n(jun., 1996), pp. 444-455\npublished by: taylor & francis, ltd. on behalf of the american statistical association \n\n \n\nstable url: https://www.jstor.org/stable/2291629\n \nreferences \nlinked references are available on jstor for this article: \nhttps://www.jstor.org/stable/2291629?seq=1&cid=pdf-\nreference#references_tab_contents \nyou may need to log in to jstor to access the linked references.\n\njstor is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide \nrange of content in a trusted digital archive. we use information technology and tools to increase productivity and \nfacilitate new forms of scholarship. for more information about jstor, please contact support@jstor.org. \n \nyour use of the jstor archive indicates your ", "neuron 51, 227\u2013238, july 20, 2006 \u00aa2006 elsevier inc. doi 10.1016/j.neuron.2006.06.017\n\na cooperative switch determines the\nsign of synaptic plasticity in distal\ndendrites of neocortical pyramidal neurons\n\nper jesper sjo\u00a8 stro\u00a8 m1,2,* and michael ha\u00a8 usser1,2\n1 wolfson institute for biomedical research\n2 department of physiology\nuniversity college london\nlondon wc1e 6bt\nunited kingdom\n\nsummary\n\npyramidal neurons in the cerebral cortex span multi-\nple cortical layers. how the excitable properties of py-\nramidal neuron dendrites allow these neurons to both\nintegrate activity and store associations between dif-\nferent layers is not well understood, but is thought to\nrely in part on dendritic backpropagation of action po-\ntentials. here we demonstrate that the sign of synaptic\nplasticity in neocortical pyramidal neurons is regu-\nlated by the spread of the backpropagating action po-\ntential to the synapse. this creates a progressive gra-\ndient between ltp and ltd as the distance of the\nsyna", "5\n1\n0\n2\n\n \nt\nc\no\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n9\n1\n0\n0\n0\n\n.\n\n6\n0\n5\n1\n:\nv\ni\nx\nr\na\n\na critical review of recurrent neural networks\n\nfor sequence learning\n\nzachary c. lipton\n\njohn berkowitz\n\nzlipton@cs.ucsd.edu\n\njaberkow@physics.ucsd.edu\n\ncharles elkan\n\nelkan@cs.ucsd.edu\n\njune 5th, 2015\n\nabstract\n\ncountless learning tasks require dealing with sequential data. image\ncaptioning, speech synthesis, and music generation all require that a model\nproduce outputs that are sequences. in other domains, such as time series\nprediction, video analysis, and musical information retrieval, a model must\nlearn from inputs that are sequences. interactive tasks, such as translat-\ning natural language, engaging in dialogue, and controlling a robot, often\ndemand both capabilities. recurrent neural networks (rnns) are connec-\ntionist models that capture the dynamics of sequences via cycles in the\nnetwork of nodes. unlike standard feedforward neural networks, recurrent\nnetworks retain a state that can repr", "letter\nfeature detection and orientation tuning in the\ndrosophila central complex\n\ndoi:10.1038/nature12601\n\njohannes d. seelig1 & vivek jayaraman1\n\nmany animals, including insects, are known to use visual land-\nmarks to orient in their environment. in drosophilamelanogaster,\nbehavioural genetics studies have identified a higher brain struc-\nture called the central complex as being required for the fly\u2019s innate\nresponses to vertical visual features1 and its short- and long-term\nmemory for visual patterns2\u20134. but whether and how neurons of the\nfly central complex represent visual features are unknown. here we\nuse two-photon calcium imaging in head-fixed walking and flying\nflies to probe visuomotor responses of ring neurons\u2014a class of\ncentral complex neurons that have been implicated in landmark-\ndriven spatial memory in walking flies2,3 and memory for visual\npatterns in tethered flying flies5. we show that dendrites of ring\nneurons are visually responsive and arranged retinotopically. ri", "trends in cognitive sciences\n\nreview\nwhat are memories for? the hippocampus\nbridges past experience with future decisions\n\nnatalie biderman,1,2,4 akram bakkour,1,2,4 and daphna shohamy1,3,*\n\nmany decisions require \ufb02exible reasoning that depends on inference, generaliza-\ntion, and deliberation. here, we review emerging \ufb01ndings indicating that the\nhippocampus, known for its role in long-term memory, contributes to these \ufb02exible\naspects of value-based decision-making. this work offers new insights into the\nrole of memory in decision-making and suggests that memory may shape deci-\nsions even in situations that do not appear, at \ufb01rst glance, to depend on memory\nat all. uncovering the pervasive role of memory in decision-making challenges\nthe way we de\ufb01ne what memory is and what it does, suggesting that memory\u2019s\nprimary purpose may be to guide future behavior and that storing a record of the\npast is just one way to do so.\n\nwhat are memories for?\nlearning is essential for adaptive behavior, a", "available online at www.sciencedirect.com\n\nsciencedirect\n\ncurrent opinion in\n\nneurobiology\n\ninterpreting neural computations by examining\nintrinsic and embedding dimensionality of neural\nactivity\nmehrdad jazayeri1 and srdjan ostojic2\n\nabstract\nthe ongoing exponential rise in recording capacity calls for\nnew approaches for analysing and interpreting neural data.\neffective dimensionality has emerged as an important property\nof neural activity across populations of neurons, yet different\nstudies rely on different definitions and interpretations of this\nquantity. here, we focus on intrinsic and embedding dimen-\nsionality, and discuss how they might reveal computational\nprinciples from data. reviewing recent works, we propose that\nthe intrinsic dimensionality reflects information about the latent\nvariables encoded in collective activity while embedding\ndimensionality reveals the manner in which this information is\nprocessed. we conclude by highlighting the role of network\nmodels as an ideal", "using the output embedding to improve language models\n\no\ufb01r press and lior wolf\nschool of computer science\ntel-aviv university, israel\n\n{ofir.press,wolf}@cs.tau.ac.il\n\n7\n1\n0\n2\n\n \n\nb\ne\nf\n1\n2\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n3\nv\n9\n5\n8\n5\n0\n\n.\n\n8\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe study the topmost weight matrix of\nneural network language models. we\nshow that this matrix constitutes a valid\nword embedding. when training language\nmodels, we recommend tying the input\nembedding and this output embedding.\nwe analyze the resulting update rules and\nshow that the tied embedding evolves in\na more similar way to the output embed-\nding than to the input embedding in the\nuntied model. we also offer a new method\nof regularizing the output embedding. our\nmethods lead to a signi\ufb01cant reduction in\nperplexity, as we are able to show on a va-\nriety of neural network language models.\nfinally, we show that weight tying can re-\nduce the size of neural translation models\nto less than half of their original size with-\nou", "letter\ndendritic spikes enhance stimulus selectivity in\ncortical neurons in vivo\n\ndoi:10.1038/nature12600\n\nspencer l. smith1,2, ikuko t. smith1,2, tiago branco1,3 & michael ha\u00a8usser1\n\nneuronal dendrites are electrically excitable: they can generate\nregenerative events such as dendritic spikes in response to suffi-\nciently strong synaptic input1\u20133. although such events have been\nobserved in many neuronal types4\u20139, it is not well understood how\nactive dendrites contribute to the tuning of neuronal output invivo.\nhere we show that dendritic spikes increase the selectivity of neur-\nonal responses to the orientation of a visual stimulus (orientation\ntuning). we performed direct patch-clamp recordings from the den-\ndrites of pyramidal neurons in the primary visual cortex of lightly\nanaesthetized and awake mice, during sensory processing. visual\nstimulation triggered regenerative local dendritic spikes that were\ndistinct from back-propagating action potentials. these events were\norientation t", "under review as a conference paper at iclr 2021\n\nsingle layers of attention suffice to predict\nprotein contacts\n\nanonymous authors\npaper under double-blind review\n\nabstract\n\nthe established approach to unsupervised protein contact prediction estimates co-\nevolving positions using undirected graphical models. this approach trains a potts\nmodel on a multiple sequence alignment, then predicts that the edges with high-\nest weight correspond to contacts in the 3d structure. on the other hand, in-\ncreasingly large transformers are being pretrained on protein sequence databases\nbut have demonstrated mixed results for downstream tasks, including contact pre-\ndiction. this has sparked discussion about the role of scale and attention-based\nmodels in unsupervised protein representation learning. we argue that attention\nis a principled model of protein interactions, grounded in real properties of pro-\nfactored attention,\ntein family data. we introduce a simpli\ufb01ed attention layer,\nand show that it ", "machine learning: science and technologypaper \u2022 open accessstochasticity helps to navigate rough landscapes:comparing gradient-descent-based algorithms inthe phase retrieval problemto cite this article: francesca mignacco et al 2021 mach. learn.: sci. technol. 2 035029 view the article online for updates and enhancements.you may also likegd-1: the relic of an old metal-poorglobular clusterguang-wei li,  , brian yanny et al.-high-resolution spectroscopy of the gd-1stellar stream localizes the perturbernear the orbital plane of sagittariusana bonaca, charlie conroy, david w.hogg et al.-velocity dispersion of the gd-1 stellarstreammegan t. gialluca, rohan p. naidu andana bonaca-this content was downloaded from ip address 216.165.99.39 on 18/08/2023 at 03:27\f", "reinforcement learning by backpropagation through\n\nan lstm model/critic\n\nbram bakker\n\nintelligent systems laboratory amsterdam\n\ninformatics institute, university of amsterdam\n\nkruislaan 403, 1098 sj amsterdam, the netherlands\n\nemail: bram@science.uva.nl\n\nabstract\u2014 this paper describes backpropagation through an\nlstm recurrent neural network model/critic, for reinforcement\nlearning tasks in partially observable domains. this combines the\nadvantage of lstm\u2019s strength at learning long-term temporal\ndependencies to infer states in partially observable tasks, with\nthe advantage of being able to learn high-dimensional and/or\ncontinuous actions with backpropagation\u2019s focused credit assign-\nment mechanism.\n\ni. introduction\n\nthere are multiple reasons why neural networks (nns) are\nused for approximate dynamic programming or reinforcement\nlearning (rl) problems. probably the most common reason\nis that the particular rl problem under consideration has a\nlarge and/or continuous state space, and th", "available  online  at  www.sciencedirect.com\n\nneuroscience  of  affect:  brain  mechanisms  of  pleasure  and\ndispleasure\nkent  c  berridge1 and  morten  l  kringelbach2,3\n\naffective  neuroscience  aims  to  understand  how  affect\n(pleasure  or  displeasure)  is  created  by  brains.  progress  is  aided\nby  recognizing  that  affect  has  both  objective  and  subjective\nfeatures.  those  dual  aspects  re\ufb02ect  that  affective  reactions  are\ngenerated  by  neural  mechanisms,  selected  in  evolution  based\non  their  real  (objective)  consequences  for  genetic  \ufb01tness.  we\nreview  evidence  for  neural  representation  of  pleasure  in  the\nbrain  (gained  largely  from  neuroimaging  studies),  and  evidence\nfor  the  causal  generation  of  pleasure  (gained  largely  from  brain\nmanipulation  studies).  we  suggest  that  representation  and\ncausation  may  actually  re\ufb02ect  somewhat  separable\nneuropsychological  functions.  representation  reaches  an  apex\nin  limbic  regio", "the journal of neuroscience, november 27, 2013 \u2022 33(48):18999 \u201319011 \u2022 18999\n\nsystems/circuits\n\noptimizing working memory with heterogeneity of\nrecurrent cortical excitation\n\nzachary p. kilpatrick,1 bard ermentrout,2,3 and brent doiron2,3\n1department of mathematics, university of houston, houston, texas 77204, 2department of mathematics, university of pittsburgh, pittsburgh,\npennsylvania 15260, and 3center for the neural basis of cognition, pittsburgh, pennsylvania 15213\n\na neural correlate of parametric working memory is a stimulus-specific rise in neuron firing rate that persists long after the stimulus is\nremoved. network models with local excitation and broad inhibition support persistent neural activity, linking network architecture and\nparametric working memory. cortical neurons receive noisy input fluctuations that cause persistent activity to diffusively wander about\nthe network, degrading memory over time. we explore how cortical architecture that supports parametric working m", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/291693909\n\nonline evolution of deep convolutional network for vision-based\nreinforcement learning\n\nconference paper \u00b7 july 2014\n\ndoi: 10.1007/978-3-319-08864-8_25\n\ncitations\n75\n\n3 authors, including:\n\nfaustino gomez\nnnaisense sa\n\n71 publications\u00a0\u00a0\u00a09,884 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n1,861\n\nall content following this page was uploaded by faustino gomez on 29 march 2016.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "letter\nactivity in motor\u2013sensory projections reveals\ndistributed coding in somatosensation\n\ndoi:10.1038/nature11321\n\nleopoldo petreanu1{, diego a. gutnisky1, daniel huber1{, ning-long xu1, dan h. o\u2019connor1, lin tian1{, loren looger1\n& karel svoboda1\n\ncortical-feedback projections to primary sensory areas terminate\nmost heavily in layer 1 (l1) of the neocortex 1,2, where they make\nsynapses with tuft dendrites of pyramidal neurons. l1 input is\nthought to provide \u2018contextual\u2019 information3, but the signals trans-\nmitted by l1 feedback remain uncharacterized. in the rodent\nsomatosensory system, the spatially diffuse4 feedback projection\nfrom vibrissal motor cortex (vm1) to vibrissal somatosensory\ncortex (vs1, also known as the barrel cortex) may allow whisker\ntouch to be interpreted in the context of whisker position to\ncompute object location5,6. when mice palpate objects with their\nwhiskers to localize object features7,8, whisker touch excites vs19\nand later vm1 in a somatotopic manner10\u2013", "physical review x 8, 041029 (2018)\n\noptimal sequence memory in driven random networks\n\njannis schuecker,1,2,* sven goedeke,1,3,* and moritz helias1,4\n\n1institute of neuroscience and medicine (inm-6) and institute for advanced simulation (ias-6)\n\nand jara brain institute i, j\u00fclich research centre, 52428 j\u00fclich, germany\n\n2fraunhofer center for machine learning and fraunhofer iais, 53757 sankt augustin, germany\n\n3neural network dynamics and computation, institute of genetics, university of bonn,\n\n4department of physics, faculty 1, rwth aachen university, 52074 aachen, germany\n\n53115 bonn, germany\n\n \n\n(received 28 september 2017; revised manuscript received 13 august 2018; published 14 november 2018)\n\nautonomous, randomly coupled, neural networks display a transition to chaos at a critical coupling strength.\nhere, we investigate the effect of a time-varying input on the onset of chaos and the resulting consequences for\ninformation processing. dynamic mean-field theory yields the statistics", "ieee/acm transactions on computational biology and bioinformatics, vol. 4, no. 3,\n\njuly-september 2007\n\n441\n\nbidirectional long short-term memory\nnetworks for predicting the subcellular\n\nlocalization of eukaryotic proteins\n\ntrias thireou and martin reczko\n\nabstract\u2014an algorithm called bidirectional long short-term memory networks (blstm) for processing sequential data is\nintroduced. this supervised learning method trains a special recurrent neural network to use very long-range symmetric sequence\ncontext using a combination of nonlinear processing elements and linear feedback loops for storing long-range context. the algorithm\nis applied to the sequence-based prediction of protein localization and predicts 93.3 percent novel nonplant proteins and 88.4 percent\nnovel plant proteins correctly, which is an improvement over feedforward and standard recurrent networks solving the same problem.\nthe blstm system is available as a web service at http://stepc.stepc.gr/~synaptic/blstm.html.\n\ninde", "8\n1\n0\n2\n\n \n\nv\no\nn\n9\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n1\n3\n0\n0\n1\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nthe mirage of action-dependent baselines in reinforcement learning\n\ngeorge tucker 1 surya bhupatiraju 1 2 shixiang gu 1 3 4 richard e. turner 3 zoubin ghahramani 3 5\n\nsergey levine 1 6\n\nabstract\n\npolicy gradient methods are a widely used class\nof model-free reinforcement learning algorithms\nwhere a state-dependent baseline is used to reduce\ngradient estimator variance. several recent papers\nextend the baseline to depend on both the state and\naction and suggest that this signi\ufb01cantly reduces\nvariance and improves sample ef\ufb01ciency without\nintroducing bias into the gradient estimates. to\nbetter understand this development, we decom-\npose the variance of the policy gradient estimator\nand numerically show that learned state-action-\ndependent baselines do not in fact reduce vari-\nance over a state-dependent baseline in commonly\ntested benchmark domains. we con\ufb01rm this unex-\npected result by reviewing the", "distinct feedforward and feedback effects of\nmicrostimulation in visual cortex reveal neural\nmechanisms of texture segregation\n\narticle\n\nhighlights\nd microstimulation of visual cortex evokes local excitation\n\nfollowed by inhibition\n\nd microstimulation of v1 causes feedforward excitation and\n\ninhibition in v4\n\nd microstimulation of v4 only causes feedback-based\n\nreductions in v1 \ufb01ring rates\n\nd when v4 is suppressed by microstimulation, v1 \ufb01gure-\n\nground segregation is reduced\n\nauthors\n\np. christiaan klink, bruno dagnino,\nmarie-alice gariel-mathis,\npieter r. roelfsema\n\ncorrespondence\np.roelfsema@nin.knaw.nl\n\nin brief\nklink et al. probe interactions between\nvisual cortical areas v1 and v4 with\nelectrical microstimulation.\nmicrostimulation effects reliably\npropagated in the feedforward direction.\nin the feedback direction, they depended\non visual stimulation and \ufb01gure-ground\nsegregation. these results reveal the\ndriving and modulatory roles of\nfeedforward and feedback connections,\nrespecti", "revisiting natural gradient for deep networks\n\nrazvan pascanu\n\nuniversit\u00b4e de montr\u00b4eal\n\nmontr\u00b4eal qc h3c 3j7 canada\nr.pascanu@gmail.com\n\nyoshua bengio\n\nuniversit\u00b4e de montr\u00b4eal\n\nmontr\u00b4eal qc h3c 3j7 canada\n\nyoshua.bengio@umontreal.ca\n\n4\n1\n0\n2\n\n \n\nb\ne\nf\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n7\nv\n4\n8\n5\n3\n\n.\n\n1\n0\n3\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe evaluate natural gradient descent, an algorithm originally proposed in amari\n(1997), for learning deep models. the contributions of this paper are as follows.\nwe show the connection between natural gradient descent and three other recently\nproposed methods for training deep models: hessian-free optimization (martens,\n2010), krylov subspace descent (vinyals and povey, 2012) and tonga (le\nroux et al., 2008). we describe how one can use unlabeled data to improve the\ngeneralization error obtained by natural gradient and empirically evaluate the ro-\nbustness of the algorithm to the ordering of the training set compared to stochastic\ngradient descent. finally we", "local dynamics in trained recurrent neural networks\n\nfaculty of medicine, technion\u2013israel institute of technology, haifa 32000, israel and\n\nnetwork biology research labratories, technion - israel institute of technology, haifa 32000, israel\n\nalexander rivkind\u2217 and omri barak\u2020\n\n6\n1\n0\n2\n\n \nl\nu\nj\n \n3\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n5\nv\n2\n2\n2\n5\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nlearning a task induces connectivity changes in neural circuits, thereby changing their dynamics.\nto elucidate task related neural dynamics we study trained recurrent neural networks. we develop\na mean field theory for reservoir computing networks trained to have multiple \ufb01xed point attrac-\ntors. our main result is that the dynamics of the network\u2019s output in the vicinity of attractors is\ngoverned by a low order linear ordinary di\ufb00erential equation. stability of the resulting ode can\nbe assessed, predicting training success or failure. as a consequence, networks of recti\ufb01ed linear\n(rlu) and of sigmoidal nonlinearities ar", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\ncomputational  neuroscience:  beyond  the  local  circuit\nhaim  sompolinsky1,2\n\ncomputational  neuroscience  has  focused  largely  on  the\ndynamics  and  function  of  local  circuits  of  neuronal  populations\ndedicated  to  a  common  task,  such  as  processing  a  common\nsensory  input,  storing  its  features  in  working  memory,  choosing\nbetween  a  set  of  options  dictated  by  controlled  experimental\nsettings  or  generating  the  appropriate  actions.  most  of  current\ncircuit  models  suggest  mechanisms  for  computations  that  can\nbe  captured  by  networks  of  simpli\ufb01ed  neurons  connected  via\nsimple  synaptic  weights.  in  this  article  i  review  the  progress  of\nthis  approach  and  its  limitations.  it  is  argued  that  new\nexperimental  techniques  will  yield  data  that  might  challenge  the\npresent  paradigms  in  that  they  will  (1)  demonstrate  the\ncomputational  importance  of  micro", "published as a conference paper at iclr 2021\n\nbertology meets biology: interpreting\nattention in protein language models\n\nali madani1\n\nlav r. varshney1,2\n\njesse vig1\nrichard socher1\n1salesforce research, 2university of illinois at urbana-champaign\n{jvig,amadani,cxiong,rsocher,nazneen.rajani}@salesforce.com\nvarshney@illinois.edu\n\nnazneen fatema rajani1\n\ncaiming xiong1\n\n1\n2\n0\n2\n\n \nr\na\n\n \n\nm\n8\n2\n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n3\nv\n2\n2\n2\n5\n1\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\ntransformer architectures have proven to learn useful representations for pro-\ntein classi\ufb01cation and generation tasks. however, these representations present\nchallenges in interpretability. in this work, we demonstrate a set of methods for\nanalyzing protein transformer models through the lens of attention. we show that\nattention: (1) captures the folding structure of proteins, connecting amino acids that\nare far apart in the underlying sequence, but spatially close in the three-dimensional\nstructure, (2) targets binding site", "9\n1\n0\n2\n\n \n\nv\no\nn\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n6\nv\n1\n5\n2\n1\n0\n\n.\n\n7\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ntraining behavior of deep neural network in\n\nfrequency domain(cid:63)\n\nzhi-qin john xu(cid:63)(cid:63)1, yaoyu zhang2, and yanyang xiao2\n\n1 school of mathematical sciences and institute of natural sciences, shanghai jiao\n\ntong university, shanghai, china,\n\nxuzhiqin@sjtu.edu.cn,\n\n2 school of mathematics, institute for advanced study, princeton, nj 08540, usa,\n\n3 the brain cognition and brain disease institute, shenzhen institutes of advanced\n\nyaoyu@ias.edu,\n\ntechnology, chiniese academy of sciences, china,\n\nxyy82148@gmail.com\n\nabstract. why deep neural networks (dnns) capable of over\ufb01tting of-\nten generalize well in practice is a mystery [24]. to \ufb01nd a potential mech-\nanism, we focus on the study of implicit biases underlying the training\nprocess of dnns. in this work, for both real and synthetic datasets,\nwe empirically \ufb01nd that a dnn with common settings \ufb01rst quickly cap-\ntures the dominant low-freq", "supplementary figure 1: schematic of reinforcement learning in a multilayer network. the\nforward path is computed by the blue neurons at top. input enters at the leftmost input neurons,\nx, and is transformed to hidden activity by, h = \u03c6(w0x), where \u03c6(\u00b7) is the transfer function.\noutput via the rightmost neurons, y, is computed from the hidden neurons as, y = \u03c6(wh).\nan error, e, between a desired outcome and actual outcome is computed downstream of the\noutput neurons, and summarized by the scalar loss, l. for example, the loss could be the\nsum of squared error, l = (1/2)ete. the scalar loss signal, carried by the gold feedback path,\nis then globally broadcast to the network. thus, each neuron receives the same feedback\ninformation about success on the task. when weight updates are based on the correlation\nbetween recent changes in the loss and recent node perturbations, these changes will on\naverage be adaptive and reduce the loss. however, as the number of neurons in the network\ngrows,", "best practices for convolutional neural networks  \n\napplied to visual document analysis \n\npatrice y. simard, dave steinkraus, john c. platt \n\nmicrosoft research, one microsoft way, redmond wa 98052 \n\n{patrice,v-davste,jplatt}@microsoft.com \n\n \n\n \n \n\nabstract \n\n \n    neural  networks  are  a  powerful  technology  for \nclassification  of  visual  inputs  arising  from  documents. \nhowever, there is a confusing plethora of different neural \nnetwork  methods  that  are  used  in  the  literature  and  in \nindustry.  this  paper  describes  a  set  of  concrete  best \npractices  that  document  analysis researchers can use to \nget  good  results  with  neural  networks.  the  most \nimportant  practice  is  getting  a  training  set  as  large  as \npossible:  we  expand  the  training  set  by  adding  a  new \nform of distorted data. the next most important practice \nis that convolutional neural networks are better suited for \nvisual document tasks than fully connected networks. we \npropose", "catastrophic interference in connectionist \nnetworks: the sequential learning problem \n\nmichael  mccloskey \n\nneal j .  cohen \n\ni.  introduction \n\nconnectionist networks in  which  information  is  stored  in  weights  on \nconnections between simple processing units have attracted considerable \ninterest  in  cognitive  science (e.g., rumelhart,  mcclelland,  & the  pdp \nresearch  group,  1986;  mcclelland,  rumelhart,  & the  pdp  research \ngroup, 1986). much of the interest centers around two characteristics of \nthese networks. first, the weights on connections between units need not \nbe prewired  by the model builder but rather may be established through \ntraining in which items to be learned are presented repeatedly to the net- \nwork and the connection weights are adjusted in small increments accord- \ning  to a  learning  algorithm  (e.g.,  ackley,  hinton,  & sejnowski,  1985; \nrumelhart, hinton, & williams,  1986; hinton & sejnowski,  1986). sec- \nond,  the  networks  may  represen", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/257310494\n\nbayesian decoding using unsorted spikes in the rat hippocampus.\n\narticle\u00a0\u00a0in\u00a0\u00a0journal of neurophysiology \u00b7 october 2013\n\ndoi: 10.1152/jn.01046.2012\u00a0\u00b7\u00a0source: pubmed\n\ncitations\n88\n\n4 authors, including:\n\nreads\n446\n\nfabian kloosterman\nneuro-electronics research flanders\n\n63 publications\u00a0\u00a0\u00a02,548 citations\u00a0\u00a0\u00a0\n\nstuart p layton\nmassachusetts institute of technology\n\n6 publications\u00a0\u00a0\u00a0292 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nsee profile\n\nzhe sage chen\nnyu langone medical center\n\n159 publications\u00a0\u00a0\u00a04,963 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nsome of the authors of this publication are also working on these related projects:\n\ncardiovascular signal processing for assessing heart rate variability view project\n\nhigh-density electrode arrays view project\n\nall content following this page was uploaded by zhe sage chen on 05 september 2014.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nworking  models  of  working  memory\nomri  barak1 and  misha  tsodyks2\n\nworking  memory  is  a  system  that  maintains  and  manipulates\ninformation  for  several  seconds  during  the  planning  and\nexecution  of  many  cognitive  tasks.  traditionally,  it  was  believed\nthat  the  neuronal  underpinning  of  working  memory  is  stationary\npersistent  \ufb01ring  of  selective  neuronal  populations.  recent\nadvances  introduced  new  ideas  regarding  possible\nmechanisms  of  working  memory,  such  as  short-term  synaptic\nfacilitation,  precise  tuning  of  recurrent  excitation  and  inhibition,\nand  intrinsic  network  dynamics.  these  ideas  are  motivated  by\ncomputational  considerations  and  careful  analysis  of\nexperimental  data.  taken  together,  they  may  indicate  the\nplethora  of  different  processes  underlying  working  memory  in\nthe  brain.\n\naddresses\n1 faculty  of  medicine,  technion  \u2013  israel  inst", "p e r s p e c t i v e  \n\nf o c u s   o n   n e u r a l   c o m p u tat i o n  a n d   t h e o r y\n\nusing goal-driven deep learning models to understand \nsensory cortex\ndaniel l k yamins1,2 & james j dicarlo1,2\nfueled by innovation in the computer vision and artificial \nintelligence communities, recent developments in \ncomputational neuroscience have used goal-driven hierarchical \nconvolutional neural networks (hcnns) to make strides in \nmodeling neural single-unit and population responses in higher \nvisual cortical areas. in this perspective, we review the recent \nprogress in a broader modeling context and describe some of \nthe key technical innovations that have supported it. we then \noutline how the goal-driven hcnn approach can be used to \ndelve even more deeply into understanding the development \nand organization of sensory cortical processing. \n\nthe  space  of  possible  nonlinear  transformations  that  the  brains  \nneural networks could potentially compute is vast. a major chal", "vol 461 | 15 october 2009 | doi:10.1038/nature08499\n\narticles\n\nintracellular dynamics of hippocampal\nplace cells during virtual navigation\n\nchristopher d. harvey1,2,3, forrest collman1,2,3, daniel a. dombeck1,2,3 & david w. tank1,2,3\n\nhippocampal place cells encode spatial information in rate and temporal codes. to examine the mechanisms underlying\nhippocampal coding, here we measured the intracellular dynamics of place cells by combining in vivo whole-cell recordings\nwith a virtual-reality system. head-restrained mice, running on a spherical treadmill, interacted with a computer-generated\nvisual environment to perform spatial behaviours. robust place-cell activity was present during movement along a virtual\nlinear track. from whole-cell recordings, we identified three subthreshold signatures of place fields: an asymmetric ramp-like\ndepolarization of the baseline membrane potential, an increase in the amplitude of intracellular theta oscillations, and a\nphase precession of the intracel", "7\n1\n0\n2\n\n \n\nn\nu\nj\n \n9\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n8\n9\n6\n4\n0\n\n.\n\n6\n0\n7\n1\n:\nv\ni\nx\nr\na\n\ngradient descent for spiking neural networks\n\ndongsung huh\nsalk institute\n\nla jolla, ca 92037\nhuh@snl.salk.edu\n\nterrence j. sejnowski\n\nsalk institute\n\nla jolla, ca 92037\nterry@salk.edu\n\nabstract\n\nmuch of studies on neural computation are based on network models of static\nneurons that produce analog output, despite the fact that information processing in\nthe brain is predominantly carried out by dynamic neurons that produce discrete\npulses called spikes. research in spike-based computation has been impeded by\nthe lack of ef\ufb01cient supervised learning algorithm for spiking networks. here,\nwe present a gradient descent method for optimizing spiking network models by\nintroducing a differentiable formulation of spiking networks and deriving the exact\ngradient calculation. for demonstration, we trained recurrent spiking networks on\ntwo dynamic tasks: one that requires optimizing fast (\u2248 millisecond)", "on variational bounds of mutual information\n\nben poole 1 sherjil ozair 1 2 a\u00a8aron van den oord 3 alexander a. alemi 1 george tucker 1\n\n9\n1\n0\n2\n\n \n\ny\na\nm\n6\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n2\n9\n6\n0\n\n.\n\n5\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nestimating and optimizing mutual\ninforma-\ntion (mi) is core to many problems in machine\nlearning; however, bounding mi in high dimen-\nsions is challenging. to establish tractable and\nscalable objectives, recent work has turned to vari-\national bounds parameterized by neural networks,\nbut the relationships and tradeoffs between these\nbounds remains unclear. in this work, we unify\nthese recent developments in a single framework.\nwe \ufb01nd that the existing variational lower bounds\ndegrade when the mi is large, exhibiting either\nhigh bias or high variance. to address this prob-\nlem, we introduce a continuum of lower bounds\nthat encompasses previous bounds and \ufb02exibly\ntrades off bias and variance. on high-dimensional,\ncontrolled problems, we empirically character", "reports\n\na large-scale model of the\nfunctioning brain\nchris eliasmith,* terrence c. stewart, xuan choo, trevor bekolay, travis dewolf,\nyichuan tang, daniel rasmussen\na central challenge for cognitive and systems neuroscience is to relate the incredibly complex behavior\nof animals to the equally complex activity of their brains. recently described, large-scale neural models\nhave not bridged this gap between neural activity and biological function. in this work, we present a\n2.5-million-neuron model of the brain (called \u201cspaun\u201d) that bridges this gap by exhibiting many different\nbehaviors. the model is presented only with visual image sequences, and it draws all of its responses with\na physically modeled arm. although simplified, the model captures many aspects of neuroanatomy,\nneurophysiology, and psychological behavior, which we demonstrate via eight diverse tasks.\n\nlarge-scale neural simulations are becom-\n\ning increasingly common [see (1) for a\nreview]. these include the ambitious bl", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nwhy  neurons  mix:  high  dimensionality  for  higher\ncognition\nstefano  fusi1,  earl  k  miller2 and  mattia  rigotti3\n\nneurons  often  respond  to  diverse  combinations  of  task-\nrelevant  variables.  this  form  of  mixed  selectivity  plays  an\nimportant  computational  role  which  is  related  to  the\ndimensionality  of  the  neural  representations:  high-dimensional\nrepresentations  with  mixed  selectivity  allow  a  simple  linear\nreadout  to  generate  a  huge  number  of  different  potential\nresponses.  in  contrast,  neural  representations  based  on  highly\nspecialized  neurons  are  low  dimensional  and  they  preclude  a\nlinear  readout  from  generating  several  responses  that  depend\non  multiple  task-relevant  variables.  here  we  review  the\nconceptual  and  theoretical  framework  that  explains  the\nimportance  of  mixed  selectivity  and  the  experimental  evidence\nthat  recorded  neural  repr", "7\n1\n0\n2\n\n \nc\ne\nd\n2\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n4\n3\n7\n2\n0\n\n.\n\n2\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nlearning in the machine:\n\nrandom backpropagation and the deep\n\nlearning channel\n\npierre baldi1,\u2217, peter sadowski1, and zhiqin lu2\n\nabstract\n\nabstract: random backpropagation (rbp) is a variant of the\nbackpropagation algorithm for training neural networks, where the\ntranspose of the forward matrices are replaced by \ufb01xed random ma-\ntrices in the calculation of the weight updates. it is remarkable both\nbecause of its e\ufb00ectiveness, in spite of using random matrices to com-\nmunicate error information, and because it completely removes the\ntaxing requirement of maintaining symmetric weights in a physical\nneural system. to better understand random backpropagation, we\n\ufb01rst connect it to the notions of local learning and learning channels.\nthrough this connection, we derive several alternatives to rbp, in-\ncluding skipped rbp (srpb), adaptive rbp (arbp), sparse rbp,\nand their combinations (e.g. asrbp) and an", "a distributional code for value in dopamine-\nbased reinforcement learning\n\nhttps://doi.org/10.1038/s41586-019-1924-6\nreceived: 3 january 2019\naccepted: 19 november 2019\npublished online: 15 january 2020\n\nwill dabney1,5*, zeb kurth-nelson1,2,5, naoshige uchida3, clara kwon starkweather3,  \ndemis hassabis1, r\u00e9mi munos1 & matthew botvinick1,4,5\n\nsince its introduction, the reward prediction error theory of dopamine has explained \na wealth of empirical phenomena, providing a unifying framework for understanding \nthe representation of reward and value in the brain1\u20133. according to the now canonical \ntheory, reward predictions are represented as a single scalar quantity, which supports \nlearning about the expectation, or mean, of stochastic outcomes. here we propose an \naccount of dopamine-based reinforcement learning inspired by recent artificial \nintelligence research on distributional reinforcement learning4\u20136. we hypothesized \nthat the brain represents possible future rewards not as a si", "validationofsoftwareforbayesianmodelsusingposteriorquantilessamanthar.cook,andrewgelman,anddonaldb.rubinthisarticlepresentsasimulation-basedmethoddesignedtoestablishthecomputa-tionalcorrectnessofsoftwaredevelopedto\ufb01taspeci\ufb01cbayesianmodel,capitalizingonpropertiesofbayesianposteriordistributions.weillustratethevalidationtechniquewithtwoexamples.thevalidationmethodisshownto\ufb01nderrorsinsoftwarewhentheyexistand,moreover,thevalidationoutputcanbeinformativeaboutthenatureandlocationofsucherrors.wealsocompareourmethodwiththatofanearlierapproach.keywords:markovchainmontecarlo;gibbssampler;posteriordistribution;hier-archicalmodels.1.introductionasmoderncomputingenvironmentsbecomemoreadvanced,statisticiansareableto\ufb01tmorecomplicatedmodelsthataddressadequatelycomplicationssuchasmissingdata,complexsamplingschemes,andtreatmentnoncompliance.increasingmodelcomplexity,however,impliesincreasingopportunityforerrorsinthesoftwareusedto\ufb01tsuchmod-els,particularlywiththerapidlyexpandinguseofbayesianstatisticalmo", "a goal-driven modular neural network predicts\nparietofrontal neural dynamics during grasping\n\njonathan a. michaelsa,b,c\ue840, stefan schaffelhofera, andres agudelo-toroa\ue840, and hansj\u00f6rg scherbergera,d,1\ue840\n\naneurobiology laboratory, deutsches primatenzentrum gmbh, 37077 goettingen, germany; bbrain and mind institute, western university, london, on\nn6a 5b7, canada; choward hughes medical institute, stanford university, stanford, ca 94305; and dfaculty of biology and psychology, university of\ngoettingen, 37073 goettingen, germany\n\nedited by michael e. goldberg, columbia university, new york, ny, and approved october 27, 2020 (received for review march 18, 2020)\n\none of the primary ways we interact with the world is using our\nhands. in macaques, the circuit spanning the anterior intraparietal\narea, the hand area of the ventral premotor cortex, and the\nprimary motor cortex is necessary for transforming visual infor-\nmation into grasping movements. however, no comprehensive\nmodel exists that links", "d e c i s i o n m a k i n g\n\nr e v i e w\n\nchoice, uncertainty and value in prefrontal and\ncingulate cortex\n\nmatthew f s rushworth & timothy e j behrens\n\nreinforcement learning models that focus on the striatum and dopamine can predict the choices of animals and people.\nrepresentations of reward expectation and of reward prediction errors that are pertinent to decision making, however, are not\ncon\ufb01ned to these regions but are also found in prefrontal and cingulate cortex. moreover, decisions are not guided solely by the\nmagnitude of the reward that is expected. uncertainty in the estimate of the reward expectation, the value of information that might\nbe gained by taking a course of action and the cost of an action all in\ufb02uence the manner in which decisions are made through\nprefrontal and cingulate cortex.\n\nreward-maximizing behavior has been assumed to rely predomi-\nnantly on the ability to estimate the value of different stimuli in the\nenvironment and of different actions an animal mig", "a r t i c l e s\n\nextended practice of a motor skill is associated with \nreduced metabolic activity in m1\nnathalie picard1,2, yoshiya matsuzaka5 & peter l strick1\u20134\nhow does long-term training and the development of motor skills modify the activity of the primary motor cortex (m1)? to address \nthis issue, we trained monkeys for ~1\u20136 years to perform visually guided and internally generated sequences of reaching \nmovements. then, we used [14c]2-deoxyglucose (2dg) uptake and single-neuron recording to measure metabolic and neuron \nactivity in m1. after extended practice, we observed a profound reduction of metabolic activity in m1 for the performance of \ninternally generated compared to visually guided tasks. in contrast, measures of neuron firing displayed little difference during \nthe two tasks. these findings suggest that the development of skill through extended practice results in a reduction in the \nsynaptic activity required to produce internally generated, but not visually guided,", "validation of software for bayesian models using posterior quantiles \nauthor(s): samantha r. cook, andrew gelman and donald b. rubin \nsource: journal of computational and graphical statistics, sep., 2006, vol. 15, no. 3 \n(sep., 2006), pp. 675-692\npublished by: taylor & francis, ltd. on behalf of the american statistical association, \ninstitute of mathematical statistics, and interface foundation of america\n\n \n\n \n\nstable url: https://www.jstor.org/stable/27594203\n \nreferences \nlinked references are available on jstor for this article: \nhttps://www.jstor.org/stable/27594203?seq=1&cid=pdf-\nreference#references_tab_contents \nyou may need to log in to jstor to access the linked references.\n\njstor is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide \nrange of content in a trusted digital archive. we use information technology and tools to increase productivity and \nfacilitate new forms of scholarship. for more information about jstor", "available online at www.sciencedirect.com\n\nsciencedirect\n\ncurrent opinion in\n\nneurobiology\n\ntowards the next generation of recurrent network\nmodels for cognitive neuroscience\nguangyu robert yang1,2,3 and manuel molano-maz\u00f3n4\n\nabstract\nrecurrent neural networks (rnns) trained with machine\nlearning techniques on cognitive tasks have become a widely\naccepted tool for neuroscientists. in this short opinion piece,\nwe discuss fundamental challenges faced by the early work of\nthis approach and recent steps to overcome such challenges\nand build next-generation rnn models for cognition. we pro-\npose several essential questions that practitioners of this\napproach should address to continue to build future genera-\ntions of rnn models.\n\naddresses\n1 center for theoretical neuroscience, columbia university, usa\n2 department of brain and cognitive sciences, mit, usa\n3 department of electrical engineering and computer science, mit,\nusa\n4 brain circuits & behavior lab, idibaps, barcelona, spain\n\ncorres", "e\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\ne\nr\nu\nt\na\nn\nm\no\nc\n.\ne\nr\nu\nt\na\nn\nw\nw\nw\n\n/\n\n.\n\n/\n/\n:\np\nt\nt\nh\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n4\n0\n0\n2\n\u00a9\n\n \n\na r t i c l e s\n\nprefrontal cortex and decision making in a mixed-\nstrategy game\n\ndominic j barraclough, michelle l conroy & daeyeol lee\n\nin a multi-agent environment, where the outcomes of one\u2019s actions change dynamically because they are related to the behavior\nof other beings, it becomes difficult to make an optimal decision about how to act. although game theory provides normative\nsolutions for decision making in groups, how such decision-making strategies are altered by experience is poorly understood.\nthese adaptive processes might resemble reinforcement learning algorithms, which provide a general framework for finding\noptimal strategies in a dynamic environment. here we investigated the role of prefrontal cortex (pfc) in dynamic decision making\nin monkeys. as in reinforcement learning, the animal\u2019s choice during a competitive g", "3\n2\n0\n2\n\n \n\nv\no\nn\n \n4\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n3\nv\n9\n0\n8\n1\n1\n\n.\n\n8\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nexpressive probabilistic sampling in recurrent neural\n\nnetworks\n\nshirui chen\u2217\n\ndepartment of applied mathematics\nuniversity of washington, seattle\n\nsc256@uw.edu\n\nlinxing preston jiang\n\npaul g. allen school of computer science\n\n& engineering\n\nuniversity of washington, seattle\nprestonj@cs.washington.edu\n\nrajesh p. n. rao\n\npaul g. allen school of computer science\n\n& engineering and center for neurotechnology\n\nuniversity of washington, seattle\n\nrao@cs.washington.edu\n\nabstract\n\neric shea-brown\n\ndepartment of applied mathematics\ncomputational neuroscience center\nuniversity of washington, seattle\n\netsb@uw.edu\n\nin sampling-based bayesian models of brain function, neural activities are assumed\nto be samples from probability distributions that the brain uses for probabilistic\ncomputation. however, a comprehensive understanding of how mechanistic models\nof neural dynamics can sample from arbitrary distr", "geometric understanding of deep learning\n\nna lei \u2217\n\nzhongxuan luo \u2020\n\nshing-tung yau \u2021\n\ndavid xianfeng gu \u00a7\n\nabstract\n\ndeep learning is the mainstream technique for many machine learning tasks, including image recog-\nnition, machine translation, speech recognition, and so on. it has outperformed conventional methods in\nvarious \ufb01elds and achieved great successes. unfortunately, the understanding on how it works remains\nunclear. it has the central importance to lay down the theoretic foundation for deep learning.\n\nin this work, we give a geometric view to understand deep learning: we show that the fundamental\nprinciple attributing to the success is the manifold structure in data, namely natural high dimensional data\nconcentrates close to a low-dimensional manifold, deep learning learns the manifold and the probability\ndistribution on it.\n\nwe further introduce the concepts of recti\ufb01ed linear complexity for deep neural network measuring\nits learning capability, recti\ufb01ed linear complexity of", "a r t i c l e s\n\ngrid cells require excitatory drive from the hippocampus\ntora bonnevie1, benjamin dunn1, marianne fyhn1,3, torkel hafting1,3, dori derdikman1,3, john l kubie2, \nyasser roudi1, edvard i moser1 & may-britt moser1\nto determine how hippocampal backprojections influence spatially periodic firing in grid cells, we recorded neural activity in \nthe medial entorhinal cortex (mec) of rats after temporary inactivation of the hippocampus. we report two major changes in \nentorhinal grid cells. first, hippocampal inactivation gradually and selectively extinguished the grid pattern. second, the same \ngrid cells that lost their grid fields acquired substantial tuning to the direction of the rat\u2019s head. this transition in firing properties \nwas contingent on a drop in the average firing rate of the grid cells and could be replicated by the removal of an external \nexcitatory drive in an attractor network model in which grid structure emerges by velocity-dependent translation of activity", "dendritic spikes induce single-burst\nlong-term potentiation\n\nstefan remy and nelson spruston*\n\ndepartment of neurobiology and physiology, northwestern university, evanston, il 60208\n\ncommunicated by bert sakmann, max planck institute for medical research, heidelberg, germany, september 5, 2007 (received for review may 2, 2007)\n\nthe hippocampus is essential for episodic memory, which requires\nsingle-trial\nlearning. although long-term potentiation (ltp) of\nsynaptic strength is a candidate mechanism for learning, it is\ntypically induced by using repeated synaptic activation to produce\nprecisely timed, high-frequency, or rhythmic \ufb01ring. here we show\nthat hippocampal synapses potentiate robustly in response to\nstrong activation by a single burst. the induction mechanism of this\nsingle-burst ltp requires activation of nmda receptors, l-type\nvoltage-gated calcium channels, and dendritic spikes. thus, den-\ndritic spikes are a critical trigger for a form of ltp that is consistent\nwith the funct", "machine learning for aerial image labeling\n\nby\n\nvolodymyr mnih\n\na thesis submitted in conformity with the requirements\n\nfor the degree of doctor of philosophy\n\ngraduate department of computer science\n\nuniversity of toronto\n\nc(cid:13) copyright 2013 by volodymyr mnih\n\n\f", "an empirical exploration of recurrent network architectures\n\nrafal jozefowicz\ngoogle inc.\nwojciech zaremba\nnew york university, facebook1\nilya sutskever\ngoogle inc.\n\nrafalj@google.com\n\nwoj.zaremba@gmail.com\n\nilyasu@google.com\n\nabstract\n\nthe recurrent neural network (rnn) is an ex-\ntremely powerful sequence model that is often\ndif\ufb01cult to train. the long short-term memory\n(lstm) is a speci\ufb01c rnn architecture whose\ndesign makes it much easier to train. while\nwildly successful in practice, the lstm\u2019s archi-\ntecture appears to be ad-hoc so it is not clear if it\nis optimal, and the signi\ufb01cance of its individual\ncomponents is unclear.\nin this work, we aim to determine whether the\nlstm architecture is optimal or whether much\nbetter architectures exist. we conducted a thor-\nough architecture search where we evaluated\nover ten thousand different rnn architectures,\nand identi\ufb01ed an architecture that outperforms\nboth the lstm and the recently-introduced\ngated recurrent unit (gru) on some but not ", "letters to nature \n\na neuronal learning rule for \nsub-millisecond temporal \ncoding \n\nwulfram gerstner*:j:, richard kempter*, \nj.  leo van  hemmen* & hermann wagnert+ \n\n* physik-department and t fakultat fur chemie und  biologie, \ntechnische  universitat;  monchen, d-85747 garching bei  monchen, \ngermany \n\na \n\nb  o '---'---------~-'---\n\na:: ~ \n\na  paradox  that  exists  in  auditory  and  electrosensory  neural \nsystems1'2  is  that  they  encode  behaviourally relevant  signals  in \nthe range of a few microseconds with neurons that are at least one \norder of magnitude slower. the importance of temporal coding in \nneural  information  processing  is  not  clear  yet3-8.  a  central \nquestion  is  whether  neuronal  firing  can  be  more  precise  than \nthe time constants of the neuronal processes involved9\u2022  here we \naddress this problem using the auditory system of the barn owl as \nan  example.  we  present  a modelling  study based  on  computer \nsimulations of a neuron in the laminar", "locality preserving projections\n\nxiaofei he\n\ndepartment of computer science\n\nthe university of chicago\n\nchicago, il 60637\n\npartha niyogi\n\ndepartment of computer science\n\nthe university of chicago\n\nchicago, il 60637\n\nxiaofei@cs.uchicago.edu\n\nniyogi@cs.uchicago.edu\n\nabstract\n\nmany problems in information processing involve some form of dimen-\nsionality reduction. in this paper, we introduce locality preserving pro-\njections (lpp). these are linear projective maps that arise by solving a\nvariational problem that optimally preserves the neighborhood structure\nof the data set. lpp should be seen as an alternative to principal com-\nponent analysis (pca) \u2013 a classical linear technique that projects the\ndata along the directions of maximal variance. when the high dimen-\nsional data lies on a low dimensional manifold embedded in the ambient\nspace, the locality preserving projections are obtained by \ufb01nding the\noptimal linear approximations to the eigenfunctions of the laplace bel-\ntrami operator", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/269517554\n\na primer on reinforcement learning in the brain: psychological,\ncomputational, and neural perspectives\n\nchapter \u00b7 january 2011\n\ndoi: 10.4018/978-1-60960-021-1.ch006\n\ncitations\n31\n\n3 authors, including:\n\nelliot a ludvig\nthe university of warwick\n\n117 publications\u00a0\u00a0\u00a02,520 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n10,558\n\nall content following this page was uploaded by elliot a ludvig on 14 december 2014.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "article\n\ndoi:10.1038/nature11057\n\nbrain-wide neuronal dynamics during\nmotor adaptation in zebrafish\n\nmisha b. ahrens1,2, jennifer m. li1, michael b. orger3, drew n. robson1, alexander f. schier1, florian engert1 & ruben portugues1\n\na fundamental question in neuroscience is how entire neural circuits generate behaviour and adapt it to changes in\nsensory feedback. here we use two-photon calcium imaging to record the activity of large populations of neurons at the\ncellular level, throughout the brain of larval zebrafish expressing a genetically encoded calcium sensor, while the\nparalysed animals interact fictively with a virtual environment and rapidly adapt their motor output to changes in\nvisual feedback. we decompose the network dynamics involved in adaptive locomotion into four types of neuronal\nresponse properties, and provide anatomical maps of the corresponding sites. a subset of these signals occurred during\nbehavioural adjustments and are candidates for the functional elements th", "the loss surfaces of multilayer networks\n\n5\n1\n0\n2\n\n \n\nn\na\nj\n \n\n1\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n3\n3\n2\n0\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\nanna choromanska\nachoroma@cims.nyu.edu\n\nmikael hena\ufb00\nmbh305@nyu.edu\n\nmichael mathieu\nmathieu@cs.nyu.edu\n\ng\u00b4erard ben arous\n\nyann lecun\n\nbenarous@cims.nyu.edu\n\nyann@cs.nyu.edu\n\ncourant institute of mathematical sciences\n\nnew york, ny, usa\n\nabstract\n\n1 introduction\n\nwe study the connection between the highly\nnon-convex loss function of a simple model of\nthe fully-connected feed-forward neural net-\nwork and the hamiltonian of the spherical\nspin-glass model under the assumptions of:\ni) variable independence, ii) redundancy in\nnetwork parametrization, and iii) uniformity.\nthese assumptions enable us to explain the\ncomplexity of the fully decoupled neural net-\nwork through the prism of the results from\nrandom matrix theory. we show that for\nlarge-size decoupled networks the lowest crit-\nical values of the random loss function form\na layered structure and they are l", "phenotypic variation of transcriptomic cell \ntypes in mouse motor cortex\n\nhttps://doi.org/10.1038/s41586-020-2907-3\nreceived: 5 february 2020\naccepted: 16 october 2020\npublished online: 12 november 2020\nopen access\n\n check for updates\n\nfederico scala1,2,12, dmitry kobak3,12, matteo bernabucci1,2, yves bernaerts3,4,  \ncathryn ren\u00e9 cadwell5, jesus ramon castro1,2, leonard hartmanis6, xiaolong jiang1,2,7, \nsophie laturnus3, elanine miranda1,2, shalaka mulherkar2, zheng huan tan1,2, zizhen yao8, \nhongkui zeng8, rickard sandberg6, philipp berens3,9,10,11\u2009\u2709 & andreas s. tolias1,2\u2009\u2709\n\ncortical neurons exhibit extreme diversity in gene expression as well as in \nmorphological and electrophysiological properties1,2. most existing neural \ntaxonomies are based on either transcriptomic3,4 or morpho-electric5,6 criteria, as \nit has been technically challenging to study both aspects of neuronal diversity in \nthe same set of cells7. here we used patch-seq8 to combine patch-clamp recording, \nbiocytin st", "the bell system\n\ntechnical journal\n\nvolume xxxviii\n\nmay1959\n\nnumber 3\n\ncopi/\",itl 1969, a\"\",n'can t.z.\"aon. and t.1<graph companl/\n\nprobability of error for optimal codes\n\nin a gaussian channel\n\nby claude e. shannon\n\n(manuscript received october 17, 1958)\n\na study is made of coding and decoding systems for a continuous channel\nwith an additive gaussian noise and subject to an average power limitation\nat the transmitter. upper and lower bounds are found for the error prob \nability in decoding with optimal codes and decoding systems. these bounds\nare close together for signaling rates near channel capacity and also for sig \nnaling rates near zero, but diverge between. curves exhibiting these bounds\nare given.\n\ni.\n\nintroduction\n\nconsider a communication channel of the following type: once each\nsecond a real number may be chosen at the transmitting point. this\nnumber is transmitted to the receiving point but is perturbed by an\nadditive gaussian noise, so that the ith real number, s, , is r", "emotion enhances learning via\nnorepinephrine regulation of\nampa-receptor traf\ufb01cking\n\nhailan hu,1 eleonore real,1 kogo takamiya,2 myoung-goo kang,2 joseph ledoux,3 richard l. huganir,2\nand roberto malinow1,*\n1cold spring harbor laboratory, cold spring harbor, ny 11724, usa\n2howard hughes medical institute, department of neuroscience, johns hopkins university school of medicine, baltimore,\nmd 21205, usa\n3new york university, new york, ny 10003, usa\n*correspondence: malinow@cshl.org\ndoi 10.1016/j.cell.2007.09.017\n\nsummary\n\nemotion enhances our ability to form vivid\nmemories of even trivial events. norepinephrine\n(ne), a neuromodulator released during emo-\ntional arousal, plays a central role in the emo-\ntional regulation of memory. however, the un-\nderlying molecular mechanism remains elusive.\ntoward this aim, we have examined the role of\nne in contextual memory formation and in the\nsynaptic delivery of glur1-containing a-amino-\n3-hydroxy-5-methyl-4-isoxazoleproprionic acid\n(ampa)-type gl", "letter\n\ndoi:10.1038/nature25457\n\ndopamine neuron activity before action initiation \ngates and invigorates future movements\n\njoaquim alves da silva1,2, fatuel tecuapetla1,3, vitor paix\u00e3o1 & rui m. costa1,2,4\n\ndeciding when and whether to move is critical for survival. loss of \ndopamine neurons (dans) of the substantia nigra pars compacta \n(snc) in patients with parkinson\u2019s disease causes deficits in \nmovement initiation and slowness of movement1. the role of dans \nin self-paced movement has mostly been attributed to their tonic \nactivity, whereas phasic changes in dan activity have been linked \nto reward prediction2,3. this model has recently been challenged by \nstudies showing transient changes in dan activity before or during \nself-paced movement initiation4\u20137. nevertheless, the necessity of \nthis activity for spontaneous movement initiation has not been \ndemonstrated, nor has its relation to initiation versus ongoing \nmovement been described. here we show that a large proportion of \n", "neuron\n\nreview\n\nneuropeptide transmission in brain circuits\n\nanthony n. van den pol1,*\n1department of neurosurgery, yale university, new haven, ct 06520, usa\n*correspondence: anthony.vandenpol@yale.edu\nhttp://dx.doi.org/10.1016/j.neuron.2012.09.014\n\nneuropeptides are found in many mammalian cns neurons where they play key roles in modulating neuronal\nactivity. in contrast to amino acid transmitter release at the synapse, neuropeptide release is not restricted to\nthe synaptic specialization, and after release, a neuropeptide may diffuse some distance to exert its action\nthrough a g protein-coupled receptor. some neuropeptides such as hypocretin/orexin are synthesized\nonly in single regions of the brain, and the neurons releasing these peptides probably have similar functional\nroles. other peptides such as neuropeptide y (npy) are synthesized throughout the brain, and neurons that\nsynthesize the peptide in one region have no anatomical or functional connection with npy neurons in other\nb", "reports\n\nenhanced actin depolymerization at the mdia1-\nbound barbed end. this inhibition occurs in the\nsubmillimolar range of pi, which is two orders of\nmagnitude lower than the dissociation constant of\npi and g-actin (26). thus, binding of pi to f-actin\ninhibits profilin-induced depolymerization (fig. 4e).\nadp-g-actin (5 mm) elongated mdia1-bound\nf-actin faster in the presence of 20 mm pi than in\nits absence (fig. 3c). this effect of pi was more\nprominent in the presence of profilin than in its\nabsence (fig. 3c). the decrease in the actin off-\nrate (fig. 4d) corresponds well with the increase\nin the adp-actin elongation rate by 3 to 20 mm\npi (fig. 4e). we thus suggest that pi cancels the\ninhibitory effect of profilin on adp-actin elon-\ngation (fig. 3c) by abolishing the enhanced\nbarbed end off-rate. the discrepancy of the effect\nof 1 mm pi on depolymerization and assembly\n(fig. 4, d, and e) is because slow dissociation of\npi prebound to a fraction of the adp-f-actin\nsubunits [dissocia", "trust region policy optimization\n\njohn schulman\nsergey levine\nphilipp moritz\nmichael jordan\npieter abbeel\nuniversity of california, berkeley, department of electrical engineering and computer sciences\n\njoschu@eecs.berkeley.edu\nslevine@eecs.berkeley.edu\npcmoritz@eecs.berkeley.edu\njordan@cs.berkeley.edu\npabbeel@cs.berkeley.edu\n\nabstract\n\nin this article, we describe a method for optimiz-\ning control policies, with guaranteed monotonic\nimprovement. by making several approxima-\ntions to the theoretically-justi\ufb01ed scheme, we de-\nvelop a practical algorithm, called trust region\npolicy optimization (trpo). this algorithm is\neffective for optimizing large nonlinear poli-\ncies such as neural networks. our experiments\ndemonstrate its robust performance on a wide va-\nriety of tasks: learning simulated robotic swim-\nming, hopping, and walking gaits; and playing\natari games using images of the screen as input.\ndespite its approximations that deviate from the\ntheory, trpo tends to give monotonic imp", "article\n\ndoi:10.1038/nature11527\n\ninput-specific control of reward and\naversion in the ventral tegmental area\n\nstephan lammel1*, byung kook lim1*, chen ran1, kee wui huang1, michael j. betley1, kay m. tye2, karl deisseroth3\n& robert c. malenka1\n\nventral tegmental area (vta) dopamine neurons have important roles in adaptive and pathological brain functions\nrelated to reward and motivation. however, it is unknown whether subpopulations of vta dopamine neurons\nparticipate in distinct circuits that encode different motivational signatures, and whether inputs to the vta\ndifferentially modulate such circuits. here we show that, because of differences in synaptic connectivity, activation\nof inputs to the vta from the laterodorsal tegmentum and the lateral habenula elicit reward and aversion in mice,\nrespectively. laterodorsal tegmentum neurons preferentially synapse on dopamine neurons projecting to the nucleus\naccumbens lateral shell, whereas lateral habenula neurons synapse primarily on dop", "2254 \u2022 the journal of neuroscience, february 6, 2013 \u2022 33(6):2254 \u20132267\n\nsystems/circuits\n\nsignal multiplexing and single-neuron computations in\nlateral intraparietal area during decision-making\n\nmiriam l. r. meister,1 jay a. hennig,2 and alexander c. huk1,2,3,4\n1institute for neuroscience, 2center for perceptual systems, and departments of 3neurobiology and 4psychology, the university of texas at austin, austin,\ntexas 78712\n\nprevious work has revealed a remarkably direct neural correlate of decisions in the lateral intraparietal area (lip). specifically, firing rate\nhas been observed to ramp up or down in a manner resembling the accumulation of evidence for a perceptual decision reported by\nmaking a saccade into (or away from) the neuron\u2019s response field (rf). however, this link between lip response and decision formation\nemerged from studies where a saccadic target was always stimulating the rf during decisions, and where the neural correlate was the\naveraged activity of a restricted", "slayer: spike layer error reassignment in time\n\nsumit bam shrestha\u2217\n\ntemasek laboratories @ nus\nnational university of singapore\n\nsingapore, 117411\n\ngarrick orchard\u2020\n\ntemasek laboratories @ nus\nnational university of singapore\n\nsingapore, 117411\n\ntslsbs@nus.edu.sg\n\ntslgmo@nus.edu.sg\n\nabstract\n\ncon\ufb01guring deep spiking neural networks (snns) is an exciting research avenue\nfor low power spike event based computation. however, the spike generation\nfunction is non-differentiable and therefore not directly compatible with the stan-\ndard error backpropagation algorithm. in this paper, we introduce a new general\nbackpropagation mechanism for learning synaptic weights and axonal delays which\novercomes the problem of non-differentiability of the spike function and uses a\ntemporal credit assignment policy for backpropagating error to preceding layers.\nwe describe and release a gpu accelerated software implementation of our method\nwhich allows training both fully connected and convolutional neural", "2012 american control conference\nfairmont queen elizabeth, montr\u00e9al, canada\njune 27-june 29, 2012\n\n978-1-4577-1096-4/12/$26.00 \u00a92012 aacc\n\n2177\n\nauthorized licensed use limited to: university of washington libraries. downloaded on october 28,2023 at 19:13:50 utc from ieee xplore.  restrictions apply. \n\nmodel-freereinforcementlearningwithcontinuousactioninpracticethomasdegris,patrickm.pilarski,richards.suttonabstract\u2014reinforcementlearningmethodsareoftencon-sideredasapotentialsolutiontoenablearobottoadapttochangesinrealtimetoanunpredictableenvironment.however,withcontinuousaction,onlyafewexistingalgorithmsarepracticalforreal-timelearning.insuchasetting,mosteffectivemethodshaveusedaparameterizedpolicystructure,oftenwithaseparateparameterizedvaluefunction.thegoalofthispaperistoassesssuchactor\u2013criticmethodstoformafullyspeci\ufb01edpracticalalgorithm.ourspeci\ufb01ccontributionsinclude1)developingtheextensionofexistingincrementalpolicy-gradientalgorithmstouseeligibilitytraces,2)anempir-icalcomparisono", "in silico saturation mutagenesis of cancer \ngenes\n\nhttps://doi.org/10.1038/s41586-021-03771-1\nreceived: 2 july 2020\naccepted: 25 june 2021\npublished online: 28 july 2021\n\n check for updates\n\nferran mui\u00f1os1,4\u2009\u2709, francisco mart\u00ednez-jim\u00e9nez1,4, oriol pich1, abel gonzalez-perez1,2\u2009\u2709 & \nnuria lopez-bigas1,2,3\u2009\u2709\n\ndespite the existence of good catalogues of cancer genes1,2, identifying the specific \nmutations of those genes that drive tumorigenesis across tumour types is still a \nlargely unsolved problem. as a result, most mutations identified in cancer genes \nacross tumours are of unknown significance to tumorigenesis3. we propose that the \nmutations observed in thousands of tumours\u2014natural experiments testing their \noncogenic potential replicated across individuals and tissues\u2014can be exploited to \nsolve this problem. from these mutations, features that describe the mechanism of \ntumorigenesis of each cancer gene and tissue may be computed and used to build \nmachine learning models that enca", "attention-based selective plasticity\n\nsoheil kolouri 1 nicholas ketz 1 xinyun zou 2 jeffrey krichmar 2 praveen pilly 1\n\n9\n1\n0\n2\n\n \nr\na\n\nm\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n0\n7\n0\n6\n0\n\n.\n\n3\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nneural networks,\n\ncatastrophic forgetting/interference is a critical\nproblem for lifelong learning machines, which\nimpedes the agents from maintaining their pre-\nviously learned knowledge while learning new\ntasks.\nin particular, suf-\nfer plenty from the catastrophic forgetting phe-\nnomenon. recently there has been several efforts\ntowards overcoming catastrophic forgetting in\nneural networks. here, we propose a biologically\ninspired method toward overcoming catastrophic\nforgetting. speci\ufb01cally, we de\ufb01ne an attention-\nbased selective plasticity of synapses based on\nthe cholinergic neuromodulatory system in the\nbrain. we de\ufb01ne synaptic importance parame-\nters in addition to synaptic weights and then use\nhebbian learning in parallel with backpropaga-\ntion algorithm to learn syn", "444 \u2022 the journal of neuroscience, january 14, 2009 \u2022 29(2):444 \u2013 453\n\nbehavioral/systems/cognitive\n\nsingle nigrostriatal dopaminergic neurons form widely\nspread and highly dense axonal arborizations in the\nneostriatum\n\nwakoto matsuda,1 takahiro furuta,1 kouichi c. nakamura,1,2 hiroyuki hioki,1 fumino fujiyama,1 ryohachi arai,3\u2020\nand takeshi kaneko1,2\n1department of morphological brain science, graduate school of medicine, kyoto university, kyoto 606-8501, japan, 2core research for evolutionary\nscience and technology, japan science and technology agency, kawaguchi 332-0012, japan, and 3department of anatomy, shiga university of medical\nscience, otsu 520-2192, japan\n\nthe axonal arbors of single nigrostriatal dopaminergic neurons were visualized with a viral vector expressing membrane-targeted green\nfluorescent protein in rat brain. all eight reconstructed tyrosine hydroxylase-positive dopaminergic neurons possessed widely spread\nand highly dense axonal arborizations in the neostriatum. a", "deep learning for image super-resolution \n\na comprehensive review of deep learning-\n\nbased single image super-resolution \n\nsyed muhammad arsalan bashir1, 2,*\uf02a, yi wang1, and mahrukh khan3 \n\n \n\n1 school of electronics and information, northwestern polytechnical university, xi\u2019an, shaanxi, china  \n2 space and upper atmosphere research commission, karachi, sindh, pakistan  \n3 department of computer science, national university of computer and emerging sciences, karachi, sindh, \npakistan \n \nabstract  \n\nimage super-resolution (sr) is one of the vital image processing methods that improve the resolution of an image \nin the field of computer vision. in the last two decades, significant progress has been made in the field of super-\nresolution,  especially  utilizing  deep  learning  methods.  this  survey  is  an  effort  to  provide  a  detailed  survey  of \nrecent progress in the field of super-resolution in the perspective of deep learning while also informing about the \ninitial classical m", "7\n1\n0\n2\n\n \n\nn\na\nj\n \n\n3\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n8\n4\n6\n2\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nunder review as a conference paper at iclr 2017\n\ndeep unsupervised clustering with gaussian\nmixture variational autoencoders\n\nnat dilokthanakul1,\u2217, pedro a. m. mediano1, marta garnelo1,\nmatthew c. h. lee1, hugh salimbeni1, kai arulkumaran2 & murray shanahan1\n1department of computing, 2department of bioengineering\nimperial college london\nlondon, uk\n\u2217n.dilokthanakul14@imperial.ac.uk\n\nabstract\n\nwe study a variant of the variational autoencoder model (vae) with a gaussian\nmixture as a prior distribution, with the goal of performing unsupervised clus-\ntering through deep generative models. we observe that the known problem of\nover-regularisation that has been shown to arise in regular vaes also manifests\nitself in our model and leads to cluster degeneracy. we show that a heuristic\ncalled minimum information constraint that has been shown to mitigate this ef-\nfect in vaes can also be applied to improve u", "article\n\nreceived 24 sep 2013 | accepted 17 mar 2014 | published 28 apr 2014\n\ndoi: 10.1038/ncomms4675\n\nopen\n\nthe stimulus-evoked population response in visual\ncortex of awake monkey is a propagating wave\nlyle muller1,*, alexandre reynaud2,*, fre\u00b4de\u00b4ric chavane2 & alain destexhe1\n\npropagating waves occur in many excitable media and were recently found in neural systems\nfrom retina to neocortex. while propagating waves are clearly present under anaesthesia,\nwhether they also appear during awake and conscious states remains unclear. one possibility\nis that these waves are systematically missed in trial-averaged data, due to variability. here\nwe present a method for detecting propagating waves in noisy multichannel recordings.\napplying this method to single-trial voltage-sensitive dye imaging data, we show that the\nstimulus-evoked population response in primary visual cortex of the awake monkey propa-\ngates as a travelling wave, with consistent dynamics across trials. a network model sugge", "calcium-based plasticity model explains sensitivity of\nsynaptic changes to spike pattern, rate, and\ndendritic location\n\nmichael graupnera,b,1 and nicolas brunela\n\nalaboratory of neurophysics and physiology, unit\u00e9 mixte de recherche 8119, cnrs and universit\u00e9 paris descartes, 75270 paris cedex 06, france; and bcenter\nfor neural science, new york university, new york, ny 10003-6603\n\nedited by terrence j. sejnowski, salk institute for biological studies, la jolla, ca, and approved january 20, 2012 (received for review june 10, 2011)\n\nmultiple stimulation protocols have been found to be effective in\nchanging synaptic ef\ufb01cacy by inducing long-term potentiation or\ndepression. in many of those protocols, increases in postsynaptic\ncalcium concentration have been shown to play a crucial role.\nhowever, it is still unclear whether and how the dynamics of the\npostsynaptic calcium alone determine the outcome of synaptic\nplasticity. here, we propose a calcium-based model of a synapse in\nwhich potenti", "0\n2\n0\n2\n\n \n\nb\ne\nf\n4\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n9\n1\n6\n1\n0\n\n.\n\n0\n1\n9\n1\n:\nv\ni\nx\nr\na\n\nbeyond linearization: on quadratic and higher-order\n\napproximation of wide neural networks\n\nyu bai\u2217\n\njason d. lee\u2020\n\nfebruary 18, 2020\n\nabstract\n\nrecent theoretical work has established connections between over-parametrized neural net-\nworks and linearized models governed by the neural tangent kernels (ntks). ntk theory\nleads to concrete convergence and generalization results, yet the empirical performance of neu-\nral networks are observed to exceed their linearized models, suggesting insu\ufb03ciency of this\ntheory.\n\ntowards closing this gap, we investigate the training of over-parametrized neural networks\nthat are beyond the ntk regime yet still governed by the taylor expansion of the network.\nwe bring forward the idea of randomizing the neural networks, which allows them to escape\ntheir ntk and couple with quadratic models. we show that the optimization landscape of\nrandomized two-layer networks are n", "ieee transactions on neural networks and learning systems, vol. 25, no. 7, july 2014\n\n1229\n\na comprehensive review of stability analysis of\n\ncontinuous-time recurrent neural networks\n\nhuaguang zhang, senior member, ieee, zhanshan wang, member, ieee, and derong liu, fellow, ieee\n\nabstract\u2014 stability problems of continuous-time recurrent\nneural networks have been extensively studied, and many papers\nhave been published in the literature. the purpose of this paper\nis to provide a comprehensive review of the research on stability\nof continuous-time recurrent neural networks, including hop\ufb01eld\nneural networks, cohen\u2013grossberg neural networks, and related\nmodels. since time delay is inevitable in practice, stability results\nof recurrent neural networks with different classes of time delays\nare reviewed in detail. for the case of delay-dependent stability,\nthe results on how to deal with the constant/variable delay in\nrecurrent neural networks are summarized. the relationship\namong stability ", "b r a i n r e s e a r c h r e v i e w s 5 7 ( 2 0 0 8 ) 1 2 5 \u2013 1 3 3\n\nava i l a b l e a t w w w. s c i e n c e d i r e c t . c o m\n\nw w w. e l s ev i e r. c o m / l o c a t e / b r a i n r e s r ev\n\nreview\n\ncombining modules for movement\n\ne. bizzia,\u204e, v.c.k. cheungb, a. d'avellac, p. saltiela, m. treschd\nadepartment of brain and cognitive sciences and mcgovern institute for brain research, massachusetts institute of technolgy 46-6189,\n77 massachusetts avenue, cambridge, ma 02139, usa\nbdivision of health sciences and technology, harvard medical school and massachusetts institute of technology, usa\ncdepartment of neuromotor physiology, instituto di ricovero e cura a carattere scientifico, foundazione santa lucia, italy\nddepartment of biomedical engineering, northwestern university and rehabilitation institute of chicago, usa\n\na r t i c l e i n f o\n\na b s t r a c t\n\narticle history:\naccepted 4 august 2007\navailable online 5 september 2007\n\nkeywords:\nmodular organization\nspinal cord\nmuscl", "6\n1\n0\n2\n\n \n\nn\na\nj\n \n\n6\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n2\n3\n4\n5\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nunderstanding adversarial training: increasing\nlocal stability of neural nets through robust\n\noptimization\n\nuri shaham\nyale university\n\nuri.shaham@yale.edu\n\nyutaro yamada\nyale university\n\nyutaro.yamada@yale.edu\n\nsahand negahban\n\nyale university\n\nsahand.negahban@yale.edu\n\nabstract\n\nwe propose a general framework for increasing local stability of arti\ufb01cial neural\nnets (anns) using robust optimization (ro). we achieve this through an alter-\nnating minimization-maximization procedure, in which the loss of the network\nis minimized over perturbed examples that are generated at each parameter up-\ndate. we show that adversarial training of anns is in fact robusti\ufb01cation of the\nnetwork optimization, and that our proposed framework generalizes previous ap-\nproaches for increasing local stability of anns. experimental results reveal that\nour approach increases the robustness of the network to existing adv", "the shattered gradients problem:\n\nif resnets are the answer, then what is the question?\n\ndavid balduzzi 1 marcus frean 1 lennox leary 1 jp lewis 1 2 kurt wan-duo ma 1 brian mcwilliams 3\n\nabstract\n\na long-standing obstacle to progress in deep\nlearning is the problem of vanishing and ex-\nploding gradients. although, the problem has\nlargely been overcome via carefully constructed\ninitializations and batch normalization, archi-\ntectures incorporating skip-connections such as\nhighway and resnets perform much better than\nstandard feedforward architectures despite well-\nchosen initialization and batch normalization. in\nthis paper, we identify the shattered gradients\nproblem. speci\ufb01cally, we show that the cor-\nrelation between gradients in standard feedfor-\nward networks decays exponentially with depth\nresulting in gradients that resemble white noise\nwhereas, in contrast, the gradients in architec-\ntures with skip-connections are far more resis-\ntant to shattering, decaying sublinearly. detail", "6\n1\n0\n2\n\n \n\nb\ne\nf\n8\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n3\n4\n5\n7\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2016\n\nconvergent learning: do different neural\nnetworks learn the same representations?\n\nyixuan li1\u2217, jason yosinski1\u2217, jeff clune2, hod lipson3, & john hopcroft1\n1cornell university\n2university of wyoming\n3columbia university\n{yli,yosinski,jeh}@cs.cornell.edu\njeffclune@uwyo.edu, hod.lipson@columbia.edu\n\nabstract\n\nrecent successes in training large, deep neural networks have prompted active\ninvestigation into the representations learned on their intermediate layers. such\nresearch is dif\ufb01cult because it requires making sense of non-linear computations\nperformed by millions of learned parameters, but valuable because it increases\nour ability to understand current models and training algorithms and thus create\nimproved versions of them. in this paper we investigate the extent to which neural\nnetworks exhibit what we call convergent learning, which is when the re", "biorxiv preprint \nthe copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\nhttps://doi.org/10.1101/2022.11.14.516537\n; \n\nthis version posted november 15, 2022. \n\ndoi: \n\navailable under a\n\ncc-by 4.0 international license\n.\n\nwhen and why grid cells appear or not in trained path integrators\n\nben sorscher1, gabriel c. mel2, aran nayebi3, lisa giocomo4, daniel yamins5,6, and\n\nsurya ganguli1,4,6\n\n1department of applied physics, stanford university\n\n2neurosciences phd program, stanford university\n\n3mcgovern institute for brain research, mit\n\n4department of neurobiology, stanford university school of medicine\n\n5department of psychology, stanford university\n\n6department of computer science, stanford university\n\nabstract\n\nrecent work has claimed that the emergence of grid cells from trained path-integrator circuits is a\nmore fragile phenomenon than previously reporte", "cognitive \n\nscience  14,  179-211 \n\n(1990) \n\nfinding structure in time \n\njeffreyl.elman \n\nuniversity \n\nof  california, \n\nsan  diego \n\ntime \n\nunderlies \n\ndescribed \n\ntime \nrepresent \nresent \nspatial \nfirst \nto  provide \nare \nterns \nreflect \nthus \ntions \nis  reported \nof  xor) \nlearn \nable \nto \nwith  memory \nbound \ncably \nture, \nwhich \ngeneralizations \nfor \n\nrepresenting \n\ninteresting \n\nmany \nin  connectionist \n\nhuman \n\nbehaviors. \n\nmodels \n\nis  very \n\nimportant. \n\ntime \nimplicitly \nrepresentation). \n\nby \n\nits  effects \nthe  current \n\non  processing \nreport \n\ndevelops \n\nthus, \n\nthe  question \none  approach \nexplicitly \nthan \na  proposal \nalong \n\nrather \n\nby  jordan \n\nwith \nto \n\n(1986)  which \na  dynamic \n\ninvolves \nmemory. \nthe \nin  the  context \n\ninternal \nof  prior \n\nthemselves; \n\nthe  use  of \nin \n\napproach, \nthis \nrepresentations \ninternal \n\nstates. \n\nrecurrent \n\nnetworks \nback \nfed \n\ntask \n\n(as \n\nthese \n\nto \nof  how \nis  to  rep- \nin  a \nlines \nin  order \nlinks \nunit \npat- \nhidden \n", "neural  networks,  vol.  2,  pp.  53-58,  1989 \nprinted in  the  usa.  all  rights  reserved. \n\n0893-6080/89 $3.00  +  .00 \ncopyright \u00a9  1989  pergamon press pic \n\noriginal  contribution \n\nneural networks and  principal  component analysis: \n\nlearning from  examples without  local minima \n\npierre baldi  and  kurt hornik * \n\nuniversity of california.  san diego \n\n(received  18  may  1988;  revised and accepted 16 august 1988) \n\nabstract-we  consider the problem of learning from  examples  in  layered linear feed-forward  neural networks \nusing optimization methods, such as  back propagation,  with  respect to  the  usual quadratic error function  e of \nthe  connection  weights.  our  main  result  is  a complete  description  of the  landscape  attached  to  e  in  terms  of \nprincipal component analysis.  we  show that e has a unique minimum corresponding to  the projection onto the \nsubspace generated by the first principal vectors of a covariance matrix associated with  the training ", "7\n1\n0\n2\n\n \n\nb\ne\nf\n9\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n6\n0\n0\n3\n0\n\n.\n\n2\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nmulti-step o\ufb00-policy learning\n\nwithout importance sampling ratios\n\na. rupam mahmood\n\nhuizhen yu\n\nrichard s. sutton\n\nreinforcement learning and arti\ufb01cial intelligence laboratory\n\ndepartment of computing science, university of alberta\n\nedmonton, ab t6g 2e8 canada\n\nabstract\n\nto estimate the value functions of policies from exploratory data, most model-free o\ufb00-\npolicy algorithms rely on importance sampling, where the use of importance sampling\nratios often leads to estimates with severe variance. it is thus desirable to learn o\ufb00-policy\nwithout using the ratios. however, such an algorithm does not exist for multi-step learning\nwith function approximation. in this paper, we introduce the \ufb01rst such algorithm based\non temporal-di\ufb00erence (td) learning updates. we show that an explicit use of importance\nsampling ratios can be eliminated by varying the amount of bootstrapping in td updates\nin an action-dependen", "4\n1\n0\n2\n\n \n\nb\ne\nf\n9\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n0\n2\n1\n6\n\n.\n\n2\n1\n3\n1\n:\nv\ni\nx\nr\na\n\nexact solutions to the nonlinear dynamics of learning in\n\ndeep linear neural networks\n\nandrew m. saxe (asaxe@stanford.edu)\n\ndepartment of electrical engineering\n\njames l. mcclelland (mcclelland@stanford.edu)\n\ndepartment of psychology\n\nsurya ganguli (sganguli@stanford.edu)\n\ndepartment of applied physics\n\nstanford university, stanford, ca 94305 usa\n\nabstract\n\ndespite the widespread practical success of deep learning methods, our theoretical under-\nstanding of the dynamics of learning in deep neural networks remains quite sparse. we\nattempt to bridge the gap between the theory and practice of deep learning by systemati-\ncally analyzing learning dynamics for the restricted case of deep linear neural networks.\ndespite the linearity of their input-output map, such networks have nonlinear gradient de-\nscent dynamics on weights that change with the addition of each new hidden layer. we\nshow that deep linear n", "article\n\nreceived 19 dec 2015 | accepted 2 nov 2016 | published 5 jan 2017\n\ndoi: 10.1038/ncomms13804\n\nopen\n\nlayer-speci\ufb01city in the effects of attention\nand working memory on activity in primary\nvisual cortex\ntimo van kerkoerle1,*, matthew w. self2,* & pieter r. roelfsema2,3,4\n\nneuronal activity in early visual cortex depends on attention shifts but the contribution to\nworking memory has remained unclear. here, we examine neuronal activity in the different\nlayers of the primary visual cortex (v1) in an attention-demanding and a working memory\ntask. a current-source density analysis reveales top-down inputs in the super\ufb01cial layers and\nlayer 5, and an increase in neuronal \ufb01ring rates most pronounced in the super\ufb01cial and deep\nlayers and weaker in input layer 4. this increased activity is strongest in the attention task but\nit is also highly reliable during working memory delays. a visual mask erases the v1 memory\nactivity, but it reappeares at a later point in time. these results provid", "synaptic neuroscience\nspike-timing-dependent plasticity: a comprehensive overview\n\neditorial\npublished: 12 july 2012\ndoi: 10.3389/fnsyn.2012.00002\n\nh. markram1, w. gerstner 2 and p. j. sj\u00f6str\u00f6m 3*\n\n1  brain mind institute life science, ecole polytechnique federale de lausanne, lausanne, switzerland\n2  ecole polytechnique federale de lausanne, lausanne, switzerland\n3  department of neurology and neurosurgery, centre for research in neuroscience, the research institute of the mcgill university health centre, montreal, qc, canada\n*correspondence: jesper.sjostrom@mcgill.ca\nedited by:\nmary b. kennedy, caltech, usa\nreviewed by:\nmary b. kennedy, caltech, usa\n\nwhy timing matters\na neuron embedded in a neuronal network is bombarded with \nthousands of inputs every minute. but which ones are important? \nwhich information should the neuron listen to and pass along to \ndownstream neurons?\n\nduring brain development and during learning, this is a formi-\ndable problem that the vast majority of neurons", "a tutorial on deep learning for music information\n\nretrieval\n\nkeunwoo choi\n\nkeunwoo.choi@qmul.ac.uk\n\ngy\u00f6rgy fazekas\n\ng.fazekas@qmul.ac.uk\n\nkyunghyun cho\n\nkyunghyun.cho@nyu.edu\n\nmark sandler\n\nmark.sandler@qmul.ac.uk\n\n8\n1\n0\n2\n\n \n\ny\na\nm\n3\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n6\n9\n3\n4\n0\n\n.\n\n9\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\nfollowing their success in computer vision and other ar-\neas, deep learning techniques have recently become widely\nadopted in music information retrieval (mir) research. how-\never, the majority of works aim to adopt and assess methods\nthat have been shown to be e\ufb00ective in other domains, while\nthere is still a great need for more original research focus-\ning on music primarily and utilising musical knowledge and\ninsight. the goal of this paper is to boost the interest of\nbeginners by providing a comprehensive tutorial and reduc-\ning the barriers to entry into deep learning for mir. we\nlay out the basic principles and review prominent works in\nthis hard to navigate \ufb01eld. we then o", "published as a conference paper at iclr 2018\n\nunbiased online recurrent optimization\n\ncorentin tallec\nlaboratoire de recherche en informatique\nuniversit\u00e9 paris sud\ngif-sur-yvette, 91190, france\ncorentin.tallec@u-psud.fr\n\nyann ollivier\nlaboratoire de recherche en informatique\nuniversit\u00e9 paris sud\ngif-sur-yvette, 91190, france\nyann@yann-ollivier.org\n\nabstract\n\nthe novel unbiased online recurrent optimization (uoro) algorithm allows for\nonline learning of general recurrent computational graphs such as recurrent net-\nwork models. it works in a streaming fashion and avoids backtracking through\npast activations and inputs. uoro is computationally as costly as truncated\nbackpropagation through time (truncated bptt), a widespread algorithm for\nonline learning of recurrent networks jaeger (2002). uoro is a modi\ufb01cation of\nnobacktrack ollivier et al. (2015) that bypasses the need for model sparsity and\nmakes implementation easy in current deep learning frameworks, even for com-\nplex models. like ", "research article\n\nrepresentation of confidence\nassociated with a decision by\nneurons in the parietal cortex\nroozbeh kiani and michael n. shadlen\n\nthe degree of confidence in a decision provides a graded and probabilistic assessment of\nexpected outcome. although neural mechanisms of perceptual decisions have been studied\nextensively in primates, little is known about the mechanisms underlying choice certainty. we have\nshown that the same neurons that represent formation of a decision encode certainty about the\ndecision. rhesus monkeys made decisions about the direction of moving random dots, spanning a\nrange of difficulties. they were rewarded for correct decisions. on some trials, after viewing the\nstimulus, the monkeys could opt out of the direction decision for a small but certain reward.\nmonkeys exercised this option in a manner that revealed their degree of certainty. neurons in\nparietal cortex represented formation of the direction decision and the degree of certainty\nunderlying t", "a mathematical theory of semantic development in\ndeep neural networks\n\nandrew m. saxea,1, james l. mcclellandb, and surya gangulic,d\n\nadepartment of experimental psychology, university of oxford, oxford ox2 6gg, united kingdom; bdepartment of psychology, stanford university,\nstanford, ca 94305; cdepartment of applied physics, stanford university, stanford, ca 94305; and dgoogle brain, google, mountain view, ca 94043\n\nedited by terrence j. sejnowski, salk institute for biological studies, la jolla, ca, and approved april 9, 2019 (received for review december 6, 2018)\n\nan extensive body of empirical research has revealed remark-\nable regularities in the acquisition, organization, deployment, and\nneural representation of human semantic knowledge, thereby\nraising a fundamental conceptual question: what are the the-\noretical principles governing the ability of neural networks to\nacquire, organize, and deploy abstract knowledge by integrat-\ning across many individual experiences? we address ", "journal of mathematical psychology 47 (2003) 90\u2013100\n\ntutorial\n\ntutorial on maximum likelihood estimation\n\nin jae myung*\n\ndepartment of psychology, ohio state university, 1885 neil avenue mall, columbus, oh 43210-1222, usa\n\nreceived 30 november 2001; revised 16 october 2002\n\nabstract\n\nin this paper, i provide a tutorial exposition on maximum likelihood estimation (mle). the intended audience of this tutorial are\nresearchers who practice mathematical modeling of cognition but are unfamiliar with the estimation method. unlike least-squares\nestimation which is primarily a descriptive tool, mle is a preferred method of parameter estimation in statistics and is an\nindispensable tool for many statistical modeling techniques, in particular in non-linear modeling with non-normal data. the purpose\nof this paper is to provide a good conceptual explanation of the method with illustrative examples so the reader can have a grasp of\nsome of the basic principles.\nr 2003 elsevier science (usa). all rig", "neuron, vol. 38, 473\u2013485, may 8, 2003, copyright \uf8e92003 by cell press\n\nrobust spatial working memory\nthrough homeostatic synaptic scaling\nin heterogeneous cortical networks\n\nalfonso renart, pengcheng song,\nand xiao-jing wang*\nvolen center for complex systems\nbrandeis university\nwaltham, massachusetts 02454\n\nsummary\n\nthe concept of bell-shaped persistent neural activity\nrepresents a cornerstone of the theory for the internal\nrepresentation of analog quantities, such as spatial\nlocation or head direction. previous models, however,\nrelied on the unrealistic assumption of network homo-\ngeneity. we investigate this issue in a network model\nwhere fine tuning of parameters is destroyed by heter-\nogeneities in cellular and synaptic properties. hetero-\ngeneities result in the loss of stored spatial information\nin a few seconds. accurate encoding is recovered\nwhen a homeostatic mechanism scales the excitatory\nsynapses to each cell to compensate for the heteroge-\nneity in cellular excitability and", "m\no\nc\n\n.\n\ne\nr\nu\nt\na\nn\n\n.\ni\n\nc\ns\no\nr\nu\ne\nn\n\n/\n/\n:\np\nt\nt\nh\n \n\u2022\n \n.\n\nc\nn\n\ni\n \n\n \n\na\nc\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n0\n0\n0\n2\n\u00a9\n\n \n\n \n\n\u00a9 2000 nature america inc. \u2022 http://neurosci.nature.com\n\narticles\n\nsomatic epsp amplitude is\nindependent of synapse location in\nhippocampal pyramidal neurons\n\njeffrey c. magee1 and erik p. cook2\n\n1 neuroscience center, louisiana state university medical center, 2020 gravier st., new orleans, louisiana 70112, usa\n2 howard hughes medical institute, baylor college of medicine, one baylor plaza, houston, texas 77050, usa \n\ncorrespondence should be addressed to j.c.m. (jmagee@lsumc.edu)\n\nmost neurons receive thousands of synaptic inputs onto widely spread dendrites. because of\ndendritic filtering, distant synapses should have less efficacy than proximal ones. to investigate this,\nwe characterized the amplitude and kinetics of excitatory synaptic input across the apical dendrites\nof ca1 pyramidal neurons using dual whole-cell recordings. we found that dendritic epsp ampli", "on layer normalization in the transformer architecture\n\nruibin xiong\u2020* 1 2 yunchang yang* 3 di he 4 5 kai zheng 4 shuxin zheng 5 chen xing 6 huishuai zhang 5\n\nyanyan lan 1 2 liwei wang 4 3 tie-yan liu 5\n\nabstract\n\nthe transformer is widely used in natural lan-\nguage processing tasks. to train a transformer\nhowever, one usually needs a carefully designed\nlearning rate warm-up stage, which is shown to\nbe crucial to the \ufb01nal performance but will slow\ndown the optimization and bring more hyper-\nparameter tunings. in this paper, we \ufb01rst study\ntheoretically why the learning rate warm-up stage\nis essential and show that the location of layer nor-\nmalization matters. speci\ufb01cally, we prove with\nmean \ufb01eld theory that at initialization, for the\noriginal-designed post-ln transformer, which\nplaces the layer normalization between the resid-\nual blocks, the expected gradients of the parame-\nters near the output layer are large. therefore, us-\ning a large learning rate on those gradients makes\nthe tra", "fourier analysis of sinusoidally driven thalamocortical relay\nneurons and a minimal integrate-and-fire-or-burst model\n\ngregory d. smith,1,4 charles l. cox,3 s. murray sherman,3 and john rinzel1,2,4\n1center for neural science and 2courant institute of mathematical sciences, new york university, new york, new york\n10003; 3department of neurobiology, state university of new york, stony brook, new york 11794; and 4mathematical\nresearch branch, national institute of diabetes and digestive and kidney diseases, national institutes of health, bethesda,\nmaryland 20814\n\nsmith, gregory d., charles l. cox, s. murray sherman, and\njohn rinzel. fourier analysis of sinusoidally driven thalamocortical\nrelay neurons and a minimal integrate-and-\ufb01re-or-burst model. j.\nneurophysiol. 83: 588 \u2013 610, 2000. we performed intracellular record-\nings of relay neurons from the lateral geniculate nucleus of a cat\nthalamic slice preparation. we measured responses during both tonic\nand burst \ufb01ring modes to sinusoidal ", "neurobiology of learning and memory 82 (2004) 171\u2013177\n\nwww.elsevier.com/locate/ynlme\n\nminireview\n\nmemory systems of the brain: a brief history and current perspective\n\nlarry r. squire*\n\ndepartments of psychiatry, neurosciences, and psychology, university of california, san diego, la jolla, ca 92093, usa\n\nveterans a\ufb00airs healthcare system, san diego, ca 92161, usa\n\nreceived 23 april 2004; accepted 14 june 2004\n\navailable online 4 august 2004\n\nabstract\n\nthe idea that memory is composed of distinct systems has a long history but became a topic of experimental inquiry only after\nthe middle of the 20th century. beginning about 1980, evidence from normal subjects, amnesic patients, and experimental animals\nconverged on the view that a fundamental distinction could be drawn between a kind of memory that is accessible to conscious rec-\nollection and another kind that is not. subsequent work shifted thinking beyond dichotomies to a view, grounded in biology, that\nmemory is composed of multiple ", "230\n\nieee transactions on autonomous mental development, vol. 2, no. 3, september 2010\n\nformal theory of creativity, fun, and\n\nintrinsic motivation (1990\u20132010)\n\nj\u00fcrgen schmidhuber\n\nabstract\u2014the simple, but general formal theory of fun and\nintrinsic motivation and creativity (1990\u20132010) is based on the\nconcept of maximizing intrinsic reward for the active creation\nor discovery of novel, surprising patterns allowing for improved\nprediction or data compression. it generalizes the traditional\n\ufb01eld of active learning, and is related to old, but less formal ideas\nin aesthetics theory and developmental psychology. it has been\nargued that the theory explains many essential aspects of intel-\nligence including autonomous development, science, art, music,\nand humor. this overview \ufb01rst describes theoretically optimal\n(but not necessarily practical) ways of implementing the basic\ncomputational principles on exploratory, intrinsically motivated\nagents or robots, encouraging them to provoke event seq", "8\n1\n0\n2\n\n \nc\ne\nd\n1\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n7\n6\n5\n3\n0\n\n.\n\n1\n1\n8\n1\n:\nv\ni\nx\nr\na\n\ncbmmmemono.92november21,2018biologically-plausiblelearningalgorithmscanscaletolargedatasetswillxiao1,honglinchen2,qianliliao2andtomasopoggio21departmentofmolecularandcellularbiology,harvarduniveristy2centerforbrains,minds,andmachines,mitabstractthebackpropagation(bp)algorithmisoftenthoughttobebiologicallyimplausibleinthebrain.oneofthemainreasonsisthatbprequiressymmetricweightmatricesinthefeedforwardandfeed-backpathways.toaddressthis\u201cweighttransportproblem\u201d(grossberg,1987),twomorebiologicallyplausiblealgorithms,proposedbyliaoetal.(2016)andlillicrapetal.(2016),relaxbp\u2019sweightsym-metryrequirementsanddemonstratecomparablelearningcapabilitiestothatofbponsmalldatasets.however,arecentstudybybartunovetal.(2018)evaluatevariantsoftarget-propagation(tp)andfeedbackalignment(fa)onminist,cifar,andimagenetdatasets,and\ufb01ndthatalthoughmanyoftheproposedalgorithmsperformwellonmnistandcifar,theyperformsigni\ufb01cantlyworset", "leading edge\n\nreview\n\nhallmarks of cancer: the next generation\n\ndouglas hanahan1,2,* and robert a. weinberg3,*\n1the swiss institute for experimental cancer research (isrec), school of life sciences, epfl, lausanne ch-1015, switzerland\n2the department of biochemistry & biophysics, ucsf, san francisco, ca 94158, usa\n3whitehead institute for biomedical research, ludwig/mit center for molecular oncology, and mit department of biology, cambridge,\nma 02142, usa\n*correspondence: dh@ep\ufb02.ch (d.h.), weinberg@wi.mit.edu (r.a.w.)\ndoi 10.1016/j.cell.2011.02.013\n\nthe hallmarks of cancer comprise six biological capabilities acquired during the multistep develop-\nment of human tumors. the hallmarks constitute an organizing principle for rationalizing the\ncomplexities of neoplastic disease. they include sustaining proliferative signaling, evading growth\nsuppressors, resisting cell death, enabling replicative immortality, inducing angiogenesis, and acti-\nvating invasion and metastasis. underlying these ", "(v)\n\ntwo\n\nlearning  internal  representations\n\nberror  propagation\n\ndavid  e. ruineihart,  geoffrey  e.  hint..,\n\nand  ronald  j. williams\n\n0 \n\n4 \n\nseptember  1985\n\nics  report  8506\n\ncognitive \n\nscience\n\niaq  i\n\ninstitute  for  cognitive  science\n\nuniversity  of california,  san  diego \n\n862 18 \n\nla jolla, california  92093\n120,\n\n\f", "classification of electrophysiological and \nmorphological neuron types in the mouse  \nvisual cortex\n\nnathan w. gouwens1,2, staci a. sorensen1,2, jim berg1,2, changkyu lee1, tim jarsky1, jonathan ting1, \nsusan m. sunkin1, david feng\u200a\n\u200a1, costas a. anastassiou1, eliza barkan1, kris bickley1, nicole blesie1, \nthomas braun1, krissy brouner1, agata budzillo1, shiella caldejon1, tamara casper1, dan castelli1, \npeter chong1, kirsten crichton1, christine cuhaciyan1, tanya l. daigle1, rachel dalley1, nick dee1, \ntsega desta1, song-lin ding1, samuel dingman1, alyse doperalski1, nadezhda dotson\u200a\ntom egdorf1, michael fisher1, rebecca a. de frates1, emma garren1, marissa garwood1, \namanda gary1, nathalie gaudreault1, keith godfrey1, melissa gorham1, hong gu1, caroline habel1, \nkristen hadley1, james harrington1, julie a. harris1, alex henry1, dijon hill1, sam josephsen1, \nsara kebede1, lisa kim1, matthew kroll1, brian lee1, tracy lemon1, katherine e. link\u200a\nbrian long1, rusty mann1, medea mcgraw1, s", "neuron\n\nperspective\n\nthe life of behavior\n\nalex gomez-marin1,* and asif a. ghazanfar2,3,*\n1behavior of organisms laboratory, instituto de neurociencias csic-umh, 03550 alicante, spain\n2princeton neuroscience institute, princeton university, princeton, nj 08544, usa\n3departments of psychology and ecology & evolutionary biology, princeton university, princeton, nj 08544, usa\n*correspondence: agomezmarin@gmail.com (a.g.-m.), asifg@princeton.edu (a.a.g.)\nhttps://doi.org/10.1016/j.neuron.2019.09.017\n\nneuroscience needs behavior. however, it is daunting to render the behavior of organisms intelligible without\nsuppressing most, if not all, references to life. when animals are treated as passive stimulus-response, dis-\nembodied and identical machines, the life of behavior perishes. here, we distill three biological principles\n(materiality, agency, and historicity), spell out their consequences for the study of animal behavior, and illus-\ntrate them with various examples from the literature. we", "neuron\n\nreport\n\nrewards evoke learning of unconsciously\nprocessed visual stimuli in adult humans\n\naaron r. seitz,1,2,3,* dongho kim,1,3 and takeo watanabe1\n1department of psychology, boston university, 64 cummington street, boston, ma 02215, usa\n2department of psychology, university of california, riverside, 900 university avenue, riverside, ca 92521, usa\n3these authors contributed equally to this work\n*correspondence: aseitz@ucr.edu\ndoi 10.1016/j.neuron.2009.01.016\n\nsummary\n\nthe study of human learning is complicated by the\nmyriad of processing elements involved in conduct-\ning any behavioral task. in the case of visual percep-\ntual learning, there has been signi\ufb01cant controversy\nregarding the task processes that guide the forma-\ntion of this learning. however, there is a developing\nconsensus that top-down, task-related factors are\nrequired for such learning to take place. here we\nchallenge this idea by use of a novel procedure in\nwhich human participants, who were deprived of\nfood an", "8\n1\n0\n2\n\n \nl\nu\nj\n \n\n2\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n4\n3\n8\n1\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nconducting credit assignment by aligning local\n\ndistributed representations\n\nalexander g. ororbia\ncollege of information & sciences technology\npenn state university\nuniversity park, pa 16802, usa\n\nankur mali\ncollege of information & sciences technology\npenn state university\nuniversity park, pa 16802, usa\n\ndaniel kifer\nschool of electrical engineering and computer science\npenn state university\nuniversity park, pa 16802, usa\n\nc. lee giles\ncollege of information & sciences technology\npenn state university\nuniversity park, pa 16802, usa\n\neditor:\n\nago109@psu.edu\n\naam35@psu.edu\n\ndkifer@cse.psu.edu\n\ngiles@ist.psu.edu\n\nabstract\n\nusing back-propagation and its variants to train deep networks is often problematic for new\nusers. issues such as exploding gradients, vanishing gradients, and high sensitivity to weight\ninitialization strategies often make networks di\ufb03cult to train, especially when users are\nexperi", "p\ne\nr\ng\na\nm\no\nn\n \n0\n3\n0\n6\n-\n4\n5\n2\n2\n(\n9\n5\n)\n0\n0\n4\n3\n6\n-\nx\n \nn\ne\nu\nr\no\ns\nc\ni\ne\nn\nc\ne\n \nv\no\nl\n.\n \n7\n0\n,\n \nn\no\n.\n \n1\n,\n \np\np\n.\n \n1\n-\n5\n,\n \n1\n9\n9\n6\n \ne\nl\ns\ne\nv\ni\ne\nr\n \ns\nc\ni\ne\nn\nc\ne\n \nl\nt\nd\n \nc\no\np\ny\nr\ni\ng\nh\nt\n \n\u00a9\n \n1\n9\n9\n5\n \ni\nb\nr\no\n \np\nr\ni\nn\nt\ne\nd\n \ni\nn\n \ng\nr\ne\na\nt\n \nb\nr\ni\nt\na\ni\nn\n.\n \na\nl\nl\n \nr\ni\ng\nh\nt\ns\n \nr\ne\ns\ne\nr\nv\ne\nd\n \n0\n3\n0\n6\n-\n4\n5\n2\n2\n/\n9\n6\n \n$\n9\n.\n5\n0\n \n+\n \n0\n.\n0\n0\n \nl\ne\nt\nt\ne\nr\n \nt\no\n \nn\ne\nu\nr\no\ns\nc\ni\ne\nn\nc\ne\n \nd\no\np\na\nm\ni\nn\ne\n \nr\ne\nv\ne\nr\ns\ne\ns\n \nt\nh\ne\n \nd\ne\np\nr\ne\ns\ns\ni\no\nn\n \no\nf\n \nr\na\nt\n \nc\no\nr\nt\ni\nc\no\ns\nt\nr\ni\na\nt\na\nl\n \ns\ny\nn\na\np\ns\ne\ns\n \nw\nh\ni\nc\nh\n \nn\no\nr\nm\na\nl\nl\ny\n \nf\no\nl\nl\no\nw\ns\n \nh\ni\ng\nh\n-\nf\nr\ne\nq\nu\ne\nn\nc\ny\n \ns\nt\ni\nm\nu\nl\na\nt\ni\no\nn\n \no\nf\n \nc\no\nr\nt\ne\nx\n \ni\nn\n \nv\ni\nt\nr\no\n \nj\n.\n \nr\n.\n \nw\nl\nc\nk\ne\nn\ns\n,\n*\n \na\n.\n \nj\n.\n \nb\ne\ng\ng\nt\n \na\nn\nd\n \ng\n.\n \nw\n.\n \na\nr\nb\nu\nt\nh\nn\no\nt\nt\n~\n'\n:\n~\n \n*\nd\ne\np\na\nr\nt\nm\ne\nn\nt\n \no\nf\n \na\nn\na\nt\no\nm\ny\n \na\nn\nd\n \ns\nt\nr\nu\nc\nt\nu\nr\na\nl\n \nb\ni\no\nl\no\ng\ny\n,\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n \no\nf\n \no\nt\na\ng\no\n,\n \nd\ne\nn\ne\nd\ni\nn\n,\n \nn\ne\nw\n \nz\ne\na\nl\na\nn\nd\n \n~\n'\nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n \n", "representations for stable off-policy reinforcement learning\n\ndibya ghosh 1 marc g. bellemare 1\n\n0\n2\n0\n2\n\n \nt\nc\no\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n0\n2\n5\n5\n0\n\n.\n\n7\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nreinforcement learning with function approxima-\ntion can be unstable and even divergent, especially\nwhen combined with off-policy learning and bell-\nman updates.\nin deep reinforcement learning,\nthese issues have been dealt with empirically by\nadapting and regularizing the representation, in\nparticular with auxiliary tasks. this suggests that\nrepresentation learning may provide a means to\nguarantee stability. in this paper, we formally\nshow that there are indeed nontrivial state repre-\nsentations under which the canonical td algo-\nrithm is stable, even when learning off-policy. we\nanalyze representation learning schemes that are\nbased on the transition matrix of a policy, such as\nproto-value functions, along three axes: approxi-\nmation error, stability, and ease of estimation. in\nthe most general ", "the power of interpolation: understanding the effectiveness of sgd in\n\nmodern over-parametrized learning\u2020\n\nsiyuan ma 1 raef bassily 1 mikhail belkin 1\n\nabstract\n\n1\n\nintroduction\n\nin this paper we aim to formally explain the phe-\nnomenon of fast convergence of stochastic gradi-\nent descent (sgd) observed in modern machine\nlearning. the key observation is that most mod-\nern learning architectures are over-parametrized\nand are trained to interpolate the data by driving\nthe empirical loss (classi\ufb01cation and regression)\nclose to zero. while it is still unclear why these\ninterpolated solutions perform well on test data,\nwe show that these regimes allow for fast con-\nvergence of sgd, comparable in number of iter-\nations to full gradient descent. for convex loss\nfunctions we obtain an exponential convergence\nbound for mini-batch sgd parallel to that for full\ngradient descent. we show that there is a criti-\ncal batch size m\u2217 such that: (a) sgd iteration\nwith mini-batch size m \u2264 m\u2217 is nearly equ", "biorxiv preprint \nthe copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is \n\nhttps://doi.org/10.1101/2022.03.07.483196\n; \n\nthis version posted march 8, 2022. \n\ndoi: \n\nmade available under a\n\ncc-by-nd 4.0 international license\n.\n\nbio-inspired neural networks implement di\ufb00erent recurrent visual\n\nprocessing strategies than task-trained ones do\ngrace w. lindsaya,b\u2217, thomas d. mrsic-flogela, maneesh sahanib\n\nasainsbury wellcome centre, university college london, london, uk\n\nb gatsby computational neuroscience unit, university college london, london, uk\n\n\u2217corresponding author: gracewlindsay@gmail.com\n\nabstract\n\nbehavioral studies suggest that recurrence in the visual system is important for processing degraded\nstimuli. there are two broad anatomical forms this recurrence can take, lateral or feedback, each with\ndi\ufb00erent assumed functions. here we add four di\ufb00erent kinds", "ieice transactions on information and systems,\nvol.e95-d, no.10, pp.2564{2567, 2012.\n\n1\n\non kernel parameter selection\n\nin hilbert-schmidt independence criterion\n\nmasashi sugiyama\n\ntokyo institute of technology, japan.\n\nsugi@cs.titech.ac.jp\n\nhttp://sugiyama-www.cs.titech.ac.jp/~sugi\n\nmakoto yamada\n\ntokyo institute of technology, japan.\n\nyamada@sg.cs.titech.ac.jp\n\nabstract\n\nthe hilbert-schmidt independence criterion (hsic) is a kernel-based statistical in-\ndependence measure that can be computed very e\ufb03ciently. however, it requires\nus to determine the kernel parameters heuristically because no objective model se-\nlection method is available. least-squares mutual information (lsmi) is another\nstatistical independence measure that is based on direct density-ratio estimation.\nalthough lsmi is computationally more expensive than hsic, lsmi is equipped\nwith cross-validation, and thus the kernel parameter can be determined objectively.\nin this paper, we show that hsic can actually be regarded", "16. tunnicliffe, v., mcarthur, a. g. & mchugh, d. a biogeographical perspective of the deep-sea\n\nhydrothermal vent fauna. adv. mar. biol. 34, 353\u2013442 (1998).\n\n17. sibuet, m. & olu, k. biogeography, biodiversity and \ufb02uid dependence of deep-sea cold-seep\n\ncommunities at active and passive margins. deep-sea res. ii 45, 517\u2013567 (1998).\n\n18. heezen, b. c. & tharp, m. world ocean floor (map). united states navy, of\ufb01ce of naval research\n\n(washington, d.c., 1977).\n\n19. bouchet, p. & metivier, b. living pleurotomariidae (mollusca: gastropoda) from the south paci\ufb01c.\n\nnew zealand j. zoo., 9, 309\u2013318 (1982).\n\n20. ameziane-cominardi, n., bourseau, j. p. & roux, m. les crinoides pedoncules de nouvelle-\n\ncaledonie (s.w. paci\ufb01que) : une faune ancestrale issue de la mesogee mesozoique. c.r. acad. sc. paris,\n304, 15\u201318 (1987).\n\n21. laurin, b. decouverte d\u2019un squelette de soutien du lophophore de type \u2018\u2018crura\u2019\u2019 chez un brachiopode\ninarticule : description de neoancistrocrania norfolki gen. sp. nov. (cran", "research\n\nresearch article summary \u25e5\n\nrna imaging\n\nspatially resolved, highly multiplexed\nrna profiling in single cells\n\nkok hao chen,1* alistair n. boettiger,1* jeffrey r. moffitt,1*\nsiyuan wang,1 xiaowei zhuang1,2\u2020\n\nintroduction: the copy number and in-\ntracellular localization of rna are important\nregulators of gene expression. measurement\nof these properties at the transcriptome scale\nin single cells will give answers to many ques-\ntions related to gene expression and regulation.\nsingle-molecule rna imaging approaches, such\nas single-molecule fluorescence in situ hybrid-\nization (smfish), are powerful tools for count-\ning and mapping rna; however, the number\nof rna species that can be simultaneously im-\naged in individual cells has been limited. this\nmakes it challenging to perform transcriptomic\nanalysis of single cells in a spatially resolved\nmanner. here, we report multiplexed error-\nrobust fish (merfish), a single-molecule im-\naging method that allows thousands of rna\n\nspecies ", "transformer-xl: attentive language models\n\nbeyond a fixed-length context\n\nzihang dai\u221712, zhilin yang\u221712, yiming yang1, jaime carbonell1,\n\nquoc v. le2, ruslan salakhutdinov1\n\n1carnegie mellon university, 2google brain\n\n{dzihang,zhiliny,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com\n\n9\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n6\n8\n2\n0\n\n.\n\n1\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\ntransformers have a potential of learning\nlonger-term dependency, but are limited by a\n\ufb01xed-length context in the setting of language\nmodeling. we propose a novel neural ar-\nchitecture transformer-xl that enables learn-\ning dependency beyond a \ufb01xed length with-\nout disrupting temporal coherence.\nit con-\nsists of a segment-level recurrence mechanism\nand a novel positional encoding scheme. our\nmethod not only enables capturing longer-term\ndependency, but also resolves the context frag-\nmentation problem. as a result, transformer-\nxl learns dependency that is 80% longer than\nrnns and 450% longer than vanilla trans-", "biorxiv preprint \n\nthe copyright holder for this preprint (which was\nnot certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available \n\nthis version posted september 20, 2018. \n\nhttps://doi.org/10.1101/418939\n; \n\ndoi: \n\nunder a\n\ncc-by-nc-nd 4.0 international license\n.\n\nprobing variability in a cognitive map using manifold inference from neural dynamics\n\nryan j. lowy,1, sam lewalleny,1,3, dmitriy aronov1,4, rhino nevers1, david w. tank1,2\n\ny these authors contributed equally to this work. 1princeton neuroscience institute, princeton university, 2bezos center for neural\ncircuit dynamics, princeton university, 3present address: center for brain science, harvard university, 4present address: department of\nneuroscience, columbia university\n\nhippocampal neurons \ufb01re selectively in local behavioral contexts such as the position in an environment\nor phase of a task,1\u20133 and are thought to form a cognitive map of tas", "original research\npublished: 17 january 2020\ndoi: 10.3389/fncom.2019.00097\n\nunsupervised learning of persistent\nand sequential activity\n\nulises pereira 1 and nicolas brunel 1,2,3,4*\n\n1 department of statistics, the university of chicago, chicago, il, united states, 2 department of neurobiology, the\nuniversity of chicago, chicago, il, united states, 3 department of neurobiology, duke university, durham, nc,\nunited states, 4 department of physics, duke university, durham, nc, united states\n\ntwo strikingly distinct types of activity have been observed in various brain structures\nduring delay periods of delayed response tasks: persistent activity (pa),\nin which\na sub-population of neurons maintains an elevated \ufb01ring rate throughout an entire\ndelay period; and sequential activity (sa), in which sub-populations of neurons are\nactivated sequentially in time. it has been hypothesized that both types of dynamics\ncan be \u201clearned\u201d by the relevant networks from the statistics of their inputs, than", "n\ne\nu\nr\na\nl\n \nn\ne\nt\nw\no\nr\nk\ns\n,\n \nv\no\nl\n.\n \n3\n,\n \np\np\n.\n \n3\n6\n7\n-\n3\n7\n5\n,\n \n1\n9\n9\n0\n \n0\n8\n9\n3\n-\n6\n0\n8\n0\n/\n9\n0\n \n$\n3\n.\n0\n0\n \n+\n \n.\n0\n0\n \np\nr\ni\nn\nt\ne\nd\n \ni\nn\n \nt\nh\ne\n \nu\ns\na\n.\n \na\nl\nl\n \nr\ni\ng\nh\nt\ns\n \nr\ne\ns\ne\nr\nv\ne\nd\n.\n \nc\no\np\ny\nr\ni\ng\nh\nt\n \n~\n'\n:\n \n1\n9\n9\n0\n \np\ne\nr\ng\na\nm\no\nn\n \np\nr\ne\ns\ns\n \np\nl\nc\n \no\nr\ni\ng\ni\nn\na\nl\n \nc\no\nn\nt\nr\ni\nb\nu\nt\ni\no\nn\n \nt\nh\ne\n \no\np\nt\ni\nm\ni\ns\ne\nd\n \ni\nn\nt\ne\nr\nn\na\nl\n \nr\ne\np\nr\ne\ns\ne\nn\nt\na\nt\ni\no\nn\n \no\nf\n \nm\nu\nl\nt\ni\nl\na\ny\ne\nr\n \nc\nl\na\ns\ns\ni\nf\ni\ne\nr\n \nn\ne\nt\nw\no\nr\nk\ns\n \np\ne\nr\nf\no\nr\nm\ns\n \nn\no\nn\nl\ni\nn\ne\na\nr\n \nd\ni\ns\nc\nr\ni\nm\ni\nn\na\nn\nt\n \na\nn\na\nl\ny\ns\ni\ns\n \na\nn\nd\nr\ne\nw\n \nr\n.\n \nw\ne\nb\nb\n \na\nn\nd\n \nd\na\nv\ni\nd\n \nl\no\nw\ne\n \nr\no\ny\na\nl\n \ns\ni\ng\nn\na\nl\ns\n \na\nn\nd\n \nr\na\nd\na\nr\n \ne\ns\nt\na\nb\nl\ni\ns\nh\nm\ne\nn\nt\n,\n \ng\nr\ne\na\nt\n \nm\na\nl\nv\ne\nr\nn\n,\n \nu\n.\nk\n.\n \n(\nr\ne\nc\ne\ni\nv\ne\nd\n \n3\n1\n \no\nc\nt\no\nb\ne\nr\n \n1\n9\n8\n8\n;\n \nr\ne\nv\ni\ns\ne\nd\n \na\nn\nd\n \na\nc\nc\ne\np\nt\ne\nd\n \n2\n4\n \na\nu\ng\nu\ns\nt\n \n1\n9\n8\n9\n)\n \na\nb\ns\nt\nr\na\nc\nt\n-\n-\nt\nh\ni\ns\n \np\na\np\ne\nr\n \ni\nl\nl\nu\ns\nt\nr\na\nt\ne\ns\n \nw\nh\ny\n \na\n \nn\no\nn\nl\ni\nn\ne\na\nr\n \na\nd\na\np\nt\ni\nv\ne\n \nf\ne\ne\nd\n-\nf\no\n", "1\n2\n0\n2\n\n \n\ny\na\nm\n0\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n0\n6\n4\n1\n\n.\n\n5\n0\n1\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2021\n\non the geometry of generalization and memo-\nrization in deep neural networks\n\ncory stephenson*,1, suchismita padhy*,1, abhinav ganesh1, yue hui2,\n\nhanlin tang1 and sueyeon chung+,3\n\n1intel labs, 2stanford university, 3columbia university,\n\n{cory.stephenson,suchismita.padhy,abhinav.ganesh,}@intel.com, yueh@stanford.edu,\n\nhanlin.tang@intel.com, sueyeon.chung@columbia.edu\n\nabstract\n\nunderstanding how large neural networks avoid memorizing training data is key\nto explaining their high generalization performance. to examine the structure\nof when and where memorization occurs in a deep network, we use a recently\ndeveloped replica-based mean \ufb01eld theoretic geometric analysis method. we \ufb01nd\nthat all layers preferentially learn from examples which share features, and link\nthis behavior to generalization performance. memorization predominately occurs\nin the deepe", "regularizing and optimizing lstm language models\n\nstephen merity 1 nitish shirish keskar 1 richard socher 1\n\n7\n1\n0\n2\n\n \n\ng\nu\na\n7\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n1\nv\n2\n8\n1\n2\n0\n\n.\n\n8\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nrecurrent neural networks (rnns), such as long\nshort-term memory networks (lstms), serve as\na fundamental building block for many sequence\nlearning tasks,\nincluding machine translation,\nlanguage modeling, and question answering. in\nthis paper, we consider the speci\ufb01c problem of\nword-level language modeling and investigate\nstrategies for regularizing and optimizing lstm-\nbased models. we propose the weight-dropped\nlstm which uses dropconnect on hidden-to-\nhidden weights as a form of recurrent regulariza-\ntion. further, we introduce nt-asgd, a vari-\nant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined us-\ning a non-monotonic condition as opposed to be-\ning tuned by the user. using these and other reg-\nularization strategies, we achieve state-of-the", "anrv314-ne30-21\n\nari\n\n21 may 2007\n\n13:44\n\nthe neural basis of\ndecision making\n\njoshua i. gold1 and michael n. shadlen2\n1department of neuroscience, university of pennsylvania, philadelphia,\npennsylvania 19104-6074; email: jigold@mail.med.upenn.edu\n2howard hughes medical institute and department of physiology and biophysics,\nuniversity of washington, seattle, washington 98195-7290;\nemail: shadlen@u.washington.edu\n\nannu. rev. neurosci. 2007. 30:535\u201374\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev.neuro.29.051605.113038\ncopyright c(cid:2) 2007 by annual reviews.\nall rights reserved\n\n0147-006x/07/0721-0535$20.00\n\nkey words\npsychophysics, signal detection theory, sequential analysis, motion\nperception, vibrotactile perception, choice, reaction time\n\nabstract\nthe study of decision making spans such varied \ufb01elds as neuro-\nscience, psychology, economics, statistics, political science, and com-\nputer science. despite this diversity ", "hippocampal contributions to model-based\nplanning and spatial memory\n\narticle\n\nhighlights\nd we tested planning and spatial memory in patients with\n\nhippocampal damage and controls\n\nd patients relied less on both model-based planning and\n\nallocentric spatial memory\n\nd the planning impairment was related to the amount of\n\ndamage to right hippocampus\n\nd planning and place memory covaried in controls but were\n\nless related in patients\n\nauthors\n\noliver m. vikbladh, michael r. meager,\njohn king, ..., daphna shohamy,\nneil burgess, nathaniel d. daw\n\ncorrespondence\nomv208@nyu.edu (o.m.v.),\nndaw@princeton.edu (n.d.d.)\n\nin brief\ntesting patients with hippocampal\ndamage, vikbladh et al. demonstrate that\nmodel-based planning and place memory\nrely on a common hippocampal\nsubstrate. the study bridges the\nreinforcement learning and spatial\nmemory literatures to clarify the scope of\nhippocampal contributions to behavior.\n\nvikbladh et al., 2019, neuron 102, 683\u2013693\nmay 8, 2019 \u00aa 2019 elsevier inc.\nhttps", "untangling in invariant speech recognition\n\ncory stephenson\n\nintel ai lab\n\njenelle feather\n\nmit\n\nsuchismita padhy\n\nintel ai lab\n\ncory.stephenson@intel.com\n\njfeather@mit.edu\n\nsuchismita.padhy@intel.com\n\noguz elibol\nintel ai lab\n\noguz.h.elibol@intel.com\n\nhanlin tang\nintel ai lab\n\nhanlin.tang@intel.com\n\nmit/ center for brains, minds, and machines\n\njosh mcdermott\n\njhm@mit.edu\n\nsueyeon chung\n\ncolumbia university/ mit\n\nsueyeon@mit.edu\n\nabstract\n\nencouraged by the success of deep neural networks on a variety of visual tasks,\nmuch theoretical and experimental work has been aimed at understanding and in-\nterpreting how vision networks operate. meanwhile, deep neural networks have\nalso achieved impressive performance in audio processing applications, both as\nsub-components of larger systems and as complete end-to-end systems by them-\nselves. despite their empirical successes, comparatively little is understood about\nhow these audio models accomplish these tasks. in this work, we employ a re-\ncen", "research article\n\nan event map of memory space in the\nhippocampus\nlorena deuker1,2*, jacob ls bellmund1,3, tobias navarro schro\u00a8 der1,3,\nchristian f doeller1,3*\n\n1donders institute for brain, cognition and behaviour, radboud university\nnijmegen, nijmegen, the netherlands; 2department of neuropsychology, institute\nof cognitive neuroscience, ruhr-university bochum, bochum, germany; 3kavli\ninstitute for systems neuroscience, centre for neural computation, egil and\npauline braathen and fred kavli centre for cortical microcircuits, ntnu -\nnorwegian university of science and technology, st. olavs university hospital,\ntrondheim, norway\n\nabstract the hippocampus has long been implicated in both episodic and spatial memory,\nhowever these mnemonic functions have been traditionally investigated in separate research\nstrands. theoretical accounts and rodent data suggest a common mechanism for spatial and\nepisodic memory in the hippocampus by providing an abstract and flexible representation of the\n", "neuron\n\nperspective\n\nneural manifolds for the control of movement\n\njuan a. gallego,1,2 matthew g. perich,3 lee e. miller,1,3,4 and sara a. solla1,5,*\n1department of physiology, northwestern university, chicago, il 60611, usa\n2neural and cognitive engineering group, centre for robotics and automation csic-upm, arganda del rey 28500, spain\n3department of biomedical engineering, northwestern university, evanston, il 60208, usa\n4department of physical medicine and rehabilitation, northwestern university, chicago, il 60611, usa\n5department of physics and astronomy, northwestern university, evanston, il 60208, usa\n*correspondence: solla@northwestern.edu\nhttp://dx.doi.org/10.1016/j.neuron.2017.05.025\n\nthe analysis of neural dynamics in several brain cortices has consistently uncovered low-dimensional mani-\nfolds that capture a signi\ufb01cant fraction of neural variability. these neural manifolds are spanned by speci\ufb01c\npatterns of correlated neural activity, the \u2018\u2018neural modes.\u2019\u2019 we discuss a mode", "original research\npublished: 21 june 2017\ndoi: 10.3389/fnins.2017.00324\n\nevent-driven random\nback-propagation: enabling\nneuromorphic deep learning\nmachines\n\nemre o. neftci 1*, charles augustine 2, somnath paul 2 and georgios detorakis 1\n\n1 neuromorphic machine intelligence laboratory, department of cognitive sciences, university of california, irvine, irvine, ca,\nunited states, 2 circuit research lab, intel corporation, hilsboro, or, united states\n\nan ongoing challenge in neuromorphic computing is to devise general and\ncomputationally ef\ufb01cient models of inference and learning which are compatible with the\nspatial and temporal constraints of the brain. one increasingly popular and successful\napproach is to take inspiration from inference and learning algorithms used in deep\nneural networks. however, the workhorse of deep learning, the gradient descent gradient\nback propagation (bp) rule, often relies on the immediate availability of network-wide\ninformation stored with high-precision me", "research | reports\n\nneuronal modeling\n\nsingle-trial spike trains in parietal\ncortex reveal discrete steps during\ndecision-making\n\nkenneth w. latimer,1,2 jacob l. yates,1,2 miriam l. r. meister,2,3\nalexander c. huk,1,2,4,5 jonathan w. pillow1,2,5,6*\n\nneurons in the macaque lateral intraparietal (lip) area exhibit firing rates that appear to\nramp upward or downward during decision-making. these ramps are commonly assumed\nto reflect the gradual accumulation of evidence toward a decision threshold. however,\nthe ramping in trial-averaged responses could instead arise from instantaneous jumps at\ndifferent times on different trials. we examined single-trial responses in lip using\nstatistical methods for fitting and comparing latent dynamical spike-train models. we\ncompared models with latent spike rates governed by either continuous diffusion-to-bound\ndynamics or discrete \u201cstepping\u201d dynamics. roughly three-quarters of the choice-selective\nneurons we recorded were better described by the stepp", "the journal of neuroscience, february 16, 2011 \u2022 31(7):2481\u20132487 \u2022 2481\n\ndevelopment/plasticity/repair\n\ndopaminergic projections from midbrain to primary motor\ncortex mediate motor skill learning\n\njonas a. hosp,1,2* ana pekanovic,1,2* mengia s. rioult-pedotti,1,2,3 and andreas r. luft1,2,4\n1clinical neurorehabilitation, department of neurology, university of zurich, ch-8091 zurich, switzerland, 2rehabilitation initiative and technology\ncenter zurich, ch-8008 zurich, switzerland, 3department of neurosciences, brown university, providence, rhode island 02912, and 4department of\nneurology, johns hopkins university, baltimore, maryland 21231\n\nthe primary motor cortex (m1) of the rat contains dopaminergic terminals. the origin of this dopaminergic projection and its functional\nrole for movement are obscure. other areas of cortex receive dopaminergic projections from the ventral tegmental area (vta) of the\nmidbrain, and these projections are involved in learning phenomena. we therefore hypot", "unsupervised neural network models of the ventral\nvisual stream\n\nchengxu zhuanga,1\ndaniel l. k. yaminsa,e,f\n\n, siming yanb\n\n, aran nayebic\n\n, martin schrimpfd\n\n, michael c. franka\n\n, james j. dicarlod\n\n, and\n\nadepartment of psychology, stanford university, stanford, ca 94305; bdepartment of computer science, the university of texas at austin, austin, tx\n78712; cneurosciences phd program, stanford university, stanford, ca 94305; dbrain and cognitive sciences, massachusetts institute of technology,\ncambridge, ma 02139; edepartment of computer science, stanford university, stanford, ca 94305; and fwu tsai neurosciences institute, stanford\nuniversity, stanford, ca 94305\n\nedited by marlene behrmann, carnegie mellon university, pittsburgh, pa, and approved december 9, 2020 (received for review july 7, 2020)\n\ndeep neural networks currently provide the best quantitative\nmodels of the response patterns of neurons throughout the\nprimate ventral visual stream. however, such networks have\nremained", "j neurophysiol 119: 2347\u20132357, 2018.\nfirst published march 14, 2018; doi:10.1152/jn.00872.2017.\n\nresearch article control of movement\n\nvigor of reaching movements: reward discounts the cost of effort\n\nx erik m. summerside,1 x reza shadmehr,2 and alaa a. ahmed1\n1department of integrative physiology, university of colorado, boulder, colorado; and 2department of biomedical\nengineering, johns hopkins university, baltimore, maryland\n\nsubmitted 8 december 2017; accepted in \ufb01nal form 7 march 2018\n\nsummerside em, shadmehr r, ahmed aa. vigor of reaching\nmovements: reward discounts the cost of effort. j neurophysiol 119:\n2347\u20132357, 2018. first published march 14, 2018; doi:10.1152/\njn.00872.2017.\u2014making a movement may be thought of as an eco-\nnomic decision in which one spends effort to acquire reward. time\ndiscounts reward, which predicts that the magnitude of reward should\naffect movement vigor: we should move faster, spending greater\neffort, when there is greater reward at stake. indeed, sacc", "4\n1\n0\n2\n\n \n\np\ne\ns\n8\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n6\n0\n9\n7\n\n.\n\n7\n0\n4\n1\n:\nv\ni\nx\nr\na\n\nhow auto-encoders could provide credit\n\nassignment in deep networks via target propagation\n\nyoshua bengio\n\nuniversit\u00b4e de montr\u00b4eal\n\ncifar fellow\n\nabstract\n\nwe propose to exploit reconstruction as a layer-local training signal for deep learn-\ning. reconstructions can be propagated in a form of target propagation playing a\nrole similar to back-propagation but helping to reduce the reliance on derivatives\nin order to perform credit assignment across many levels of possibly strong non-\nlinearities (which is dif\ufb01cult for back-propagation). a regularized auto-encoder\ntends produce a reconstruction that is a more likely version of its input, i.e., a\nsmall move in the direction of higher likelihood. by generalizing gradients, target\npropagation may also allow to train deep networks with discrete hidden units. if\nthe auto-encoder takes both a representation of input and target (or of any side in-\nformation) ", "multitask representations in the human \ncortex transform along a sensory-to-motor \nhierarchy\n\nhttps://doi.org/10.1038/s41593-022-01224-0\n\nreceived: 2 december 2021\n\naccepted: 28 october 2022\n\npublished online: 19 december 2022\n\n check for updates\n\ntakuya ito\u2009\n\n \u20091 & john d. murray\u2009\n\n \u20091,2,3 \n\nhuman cognition recruits distributed neural processes, yet the organizing \ncomputational and functional architectures remain unclear. here, we \ncharacterized the geometry and topography of multitask representations \nacross the human cortex using functional magnetic resonance imaging \nduring 26 cognitive tasks in the same individuals. we measured the \nrepresentational similarity across tasks within a region and the alignment \nof representations between regions. representational alignment varied \nin a graded manner along the sensory\u2013association\u2013motor axis. multitask \ndimensionality exhibited compression then expansion along this gradient. \nto investigate computational principles of multitask represe", "neuron\n\nreview\n\nthe normalization model of attention\n\njohn h. reynolds1,* and david j. heeger2\n1salk institute for biological studies, la jolla, ca 92037-1099, usa\n2department of psychology and center for neural science, new york university, new york, ny 10003, usa\n*correspondence: reynolds@salk.edu\ndoi 10.1016/j.neuron.2009.01.002\n\nattention has been found to have a wide variety of effects on the responses of neurons in visual cortex. we\ndescribe a model of attention that exhibits each of these different forms of attentional modulation, depending\non the stimulus conditions and the spread (or selectivity) of the attention \ufb01eld in the model. the model helps\nreconcile proposals that have been taken to represent alternative theories of attention. we argue that the\nvariety and complexity of the results reported in the literature emerge from the variety of empirical protocols\nthat were used, such that the results observed in any one experiment depended on the stimulus conditions\nand the sub", "neuron\n\noverview\n\nneuromodulation of neuronal circuits:\nback to the future\n\neve marder1,*\n1biology department and volen center, brandeis university, waltham, ma 02454, usa\n*correspondence: marder@brandeis.edu\nhttp://dx.doi.org/10.1016/j.neuron.2012.09.010\n\nall nervous systems are subject to neuromodulation. neuromodulators can be delivered as local hormones,\nas cotransmitters in projection neurons, and through the general circulation. because neuromodulators can\ntransform the intrinsic \ufb01ring properties of circuit neurons and alter effective synaptic strength, neuromodu-\nlatory substances recon\ufb01gure neuronal circuits, often massively altering their output. thus, the anatomical\nconnectome provides a minimal structure and the neuromodulatory environment constructs and speci\ufb01es\nthe functional circuits that give rise to behavior.\n\nadds\n\nintroduction\nneuromodulation\nextraordinary\nrichness to the dynamics that networks\ncan display. it also adds confounds of\nmany kinds that require that we rel", " \n\nphasic firing in dopaminergic neurons is sufficient for behavioral conditioning \nhsing-chen tsai,1,2* feng zhang,2,* antoine adamantidis,3 garret d. stuber,4 antonello bonci,4 luis de lecea,3 karl \ndeisseroth2,3\u2020 \n1neuroscience program, w083 clark center, 318 campus drive west, stanford university, stanford, ca 94305, usa. \n2department of bioengineering, w083 clark center, 318 campus drive west, stanford university, stanford, ca 94305, usa. \n3department of psychiatry and behavioral sciences, w083 clark center, 318 campus drive west, stanford university, \nstanford, ca 94305, usa. 4ernest gallo clinic and research center, department of neurology, wheeler center for the \nneurobiology of drug addiction, university of california san francisco, san francisco, ca 94158, usa. \n*these authors contributed equally to this work. \n\u2020to whom correspondence should be addressed. e-mail: deissero@stanford.edu \n\nnatural rewards and drugs of abuse can alter dopamine \nsignaling, and ventral tegmental ar", "i an update to this article is included at the end\n\nneuron\n\narticle\n\nbalanced ampli\ufb01cation: a new mechanism\nof selective ampli\ufb01cation\nof neural activity patterns\n\nbrendan k. murphy1,2 and kenneth d. miller2,*\n1graduate group in biophysics, university of california, san francisco, san francisco, ca 94122, usa\n2center for theoretical neuroscience, sloan program in theoretical neuroscience, department of neuroscience, columbia university\ncollege of physicians and surgeons, new york, ny 10032, usa\n*correspondence: ken@neurotheory.columbia.edu\ndoi 10.1016/j.neuron.2009.02.005\n\nsummary\n\nin cerebral cortex, ongoing activity absent a stimulus\ncan resemble stimulus-driven activity in size and\nstructure. in particular, spontaneous activity in cat\nprimary visual cortex (v1) has structure signi\ufb01cantly\ncorrelated with evoked responses to oriented stimuli.\nthis suggests that, from unstructured input, cortical\ncircuits selectively amplify speci\ufb01c activity patterns.\ncurrent understanding of selective ", "8\n1\n0\n2\n\n \n\nv\no\nn\n2\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n4\nv\n4\n4\n6\n1\n0\n\n.\n\n0\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nunderstanding intermediate layers\n\nusing linear classi\ufb01er probes\n\nguillaume alain\n\nmila, university of montreal\n\nguillaume.alain.umontreal@gmail.com\n\nyoshua bengio\n\nmila, university of montreal\n\nabstract\n\nneural network models have a reputation for being black boxes. we propose to\nmonitor the features at every layer of a model and measure how suitable they are\nfor classi\ufb01cation. we use linear classi\ufb01ers, which we refer to as \u201cprobes\u201d, trained\nentirely independently of the model itself.\nthis helps us better understand the roles and dynamics of the intermediate layers.\nwe demonstrate how this can be used to develop a better intuition about models\nand to diagnose potential problems.\nwe apply this technique to the popular models inception v3 and resnet-50.\namong other things, we observe experimentally that the linear separability of fea-\ntures increase monotonically along the depth of the model.\n\n1", "neural population geometry reveals the role of\n\nstochasticity in robust perception\n\njoel dapello\u2217,1,2,3, jenelle feather\u2217,1,2,4, hang le\u2217,1, tiago marques1,2,4\n\ndavid d. cox5, josh h. mcdermott1,2,4,6, james j. dicarlo1,2,4, sueyeon chung1,7,8\n\n\u2217equal contribution, ordered alphabetically\n\n1department of brain and cognitive sciences, massachusetts institute of technology\n\n2mcgovern institute for brain research, massachusetts institute of technology\n\n3school of engineering and applied sciences, harvard university\n\n4center for brains, minds and machines, massachusetts institute of technology\n\n5mit-ibm watson ai lab\n\n6speech and hearing bioscience and technology, harvard university\n\n7center for theoretical neuroscience, columbia university\n\n8zuckerman institute, columbia univeristy\n\nabstract\n\nadversarial examples are often cited by neuroscientists and machine learning re-\nsearchers as an example of how computational models diverge from biological\nsensory systems. recent work has proposed a", "3\n2\n0\n2\n\n \n\nb\ne\nf\n2\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n9\n2\n5\n1\n1\n\n.\n\n2\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nmodular deep learning\n\njonas pfei\ufb00er\u2217\ngoogle research\nsebastian ruder\u2217\ngoogle research\n\nivan vuli\u0107\nuniversity of cambridge\nedoardo m. ponti\u2217\nuniversity of edinburgh\nuniversity of cambridge\n\njonaspfei\ufb00er@google.com\n\nruder@google.com\n\niv250@cam.ac.uk\n\neponti@ed.ac.uk\n\nabstract\n\ntransfer learning has recently become the dominant paradigm of machine learning. pre-\ntrained models \ufb01ne-tuned for downstream tasks achieve better performance with fewer labelled\nexamples. nonetheless, it remains unclear how to develop models that specialise towards\nmultiple tasks without incurring negative interference and that generalise systematically\nto non-identically distributed tasks. modular deep learning has emerged as a promising\nsolution to these challenges. in this framework, units of computation are often implemented\nas autonomous parameter-e\ufb03cient modules. information is conditionally routed to a subset\nof module", "supplementary material: predictive learning as a network mechanism\n\nfor extracting low-dimensional latent space representations.\n\ncontents\n\n1 predictive learning and representations in the simple \u201ccard game\u201d example: further analysis\n\n1.1 learning neural representations in the card game example . . . . . . . . . . . . . . . . . . . . . .\n1.2 analysis of the regularity of representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n2 theoretical analysis of predictive learning and latent space representations\n\n2.1 low-dimensional neural representation manifolds and how they code latent variables . . . . . . .\n2.2 linear dimensionality analysis: participation ratio and dimensionality gain . . . . . . . . . . .\n2.3 how latent space signal transfer follows from translation-invariant representations of neural states\n2.4 participation ratio and linear dimensionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.5 further considerations on the locality of recep", "article\nhybrid computing using a neural \nnetwork with dynamic external memory\n\ndoi:10.1038/nature20101\n\nalex graves1*, greg wayne1*, malcolm reynolds1, tim harley1, ivo danihelka1, agnieszka grabska-barwi\u0144ska1, \nsergio g\u00f3mez colmenarejo1, edward grefenstette1, tiago ramalho1, john agapiou1, adri\u00e0 puigdom\u00e8nech badia1, \nkarl moritz hermann1, yori zwols1, georg ostrovski1, adam cain1, helen king1, christopher summerfield1, phil blunsom1, \nkoray kavukcuoglu1 & demis hassabis1\n\nartificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, \nbut are limited in their ability to represent variables and data structures and to store data over long timescales, owing to \nthe lack of an external memory. here we introduce a machine learning model called a differentiable neural computer \n(dnc), which consists of a neural network that can read from and write to an external memory matrix, analogous to the \nrandom-access memory in a conventional comp", "diet selection and optimization by northwestern crows feeding on japanese littleneck \nclams\n \nauthor(s): howard richardson and nicolaas a. m. verbeek \nsource: ecology, oct., 1986, vol. 67, no. 5 (oct., 1986), pp. 1219-1226 \npublished by: wiley on behalf of the ecological society of america \n\nstable url: https://www.jstor.org/stable/1938677\n \nreferences \nlinked references are available on jstor for this article: \nhttps://www.jstor.org/stable/1938677?seq=1&cid=pdf-\nreference#references_tab_contents \nyou may need to log in to jstor to access the linked references.\n\njstor is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide \nrange of content in a trusted digital archive. we use information technology and tools to increase productivity and \nfacilitate new forms of scholarship. for more information about jstor, please contact support@jstor.org. \n \nyour use of the jstor archive indicates your acceptance of the terms & conditions of us", "learning multiagent communication\n\nwith backpropagation\n\nsainbayar sukhbaatar\ndept. of computer science\n\ncourant institute, new york university\n\nsainbar@cs.nyu.edu\n\narthur szlam\n\nrob fergus\n\nfacebook ai research\n\nfacebook ai research\n\nnew york\n\naszlam@fb.com\n\nnew york\n\nrobfergus@fb.com\n\nabstract\n\nmany tasks in ai require the collaboration of multiple agents. typically, the\ncommunication protocol between agents is manually speci\ufb01ed and not altered\nduring training. in this paper we explore a simple neural model, called commnet,\nthat uses continuous communication for fully cooperative tasks. the model consists\nof multiple agents and the communication between them is learned alongside their\npolicy. we apply this model to a diverse set of tasks, demonstrating the ability\nof the agents to learn to communicate amongst themselves, yielding improved\nperformance over non-communicative agents and baselines. in some cases, it\nis possible to interpret the language devised by the agents, revealing s", "research article\n\na connectome of the drosophila central \ncomplex reveals network motifs suitable \nfor flexible navigation and context- \ndependent action\u00a0selection\nbrad k hulse*\u2020, hannah haberkern*\u2020, romain franconville*\u2020, \ndaniel turner- evans*\u2020, shin- ya takemura, tanya wolff, marcella noorman, \nmarisa dreher, chuntao dan, ruchi parekh, ann m hermundstad, gerald m rubin, \nvivek jayaraman*\n\njanelia research campus, howard hughes medical institute, ashburn, united states\n\nabstract flexible behaviors over long timescales are thought to engage recurrent neural \nnetworks in deep brain regions, which are experimentally challenging to study. in insects, recur-\nrent circuit dynamics in a brain region called the central complex (cx) enable directed locomotion, \nsleep, and context- and experience- dependent spatial navigation. we describe the first complete \nelectron microscopy- based connectome of the drosophila cx, including all its neurons and circuits \nat synaptic resolution. we identified", "3\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n6\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n6\n4\n7\n8\n0\n\n.\n\n5\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nseeing is believing: brain-inspired modular\ntraining for mechanistic interpretability\n\nziming liu, eric gan & max tegmark\ndepartment of physics, institute for ai and fundamental interactions, mit\n{zmliu,ejgan,tegmark}@mit.edu\n\nabstract\n\nwe introduce brain-inspired modular training (bimt), a method for making neural\nnetworks more modular and interpretable. inspired by brains, bimt embeds neu-\nrons in a geometric space and augments the loss function with a cost proportional\nto the length of each neuron connection. we demonstrate that bimt discovers\nuseful modular neural networks for many simple tasks, revealing compositional\nstructures in symbolic formulas, interpretable decision boundaries and features\nfor classification, and mathematical structure in algorithmic datasets. the ability\nto directly see modules with the naked eye can complement current mechanistic\ninterpretability strategies such as probe", "letter\ndiverse coupling of neurons to populations in\nsensory cortex\n\ndoi:10.1038/nature14273\n\nmichael okun1,2,3, nicholas a. steinmetz1,2,3,4, lee cossell2,5, m. florencia iacaruso2,5, ho ko2{, pe\u00b4ter bartho\u00b4 6{, tirin moore4,\nsonja b. hofer2,5, thomas d. mrsic-flogel2,5, matteo carandini31 & kenneth d. harris1,2,61\n\na large population of neurons can, in principle, produce an astro-\nnomical number of distinct firing patterns. in cortex, however,\nthese patterns lie in a space of lower dimension1\u20134, as if individual\nneurons were \u2018\u2018obedient members of a huge orchestra\u2019\u20195. here we use\nrecordings from the visual cortex of mouse (mus musculus) and\nmonkey (macacamulatta) to investigate the relationship between\nindividual neurons and the population, and to establish the under-\nlying circuit mechanisms. we show that neighbouring neurons can\ndiffer in their coupling to the overall firing of the population, rang-\ning from strongly coupled \u2018choristers\u2019 to weakly coupled \u2018soloists\u2019.\npopulation coup", "european journal of neuroscience, vol. 22, pp. 505\u2013512, 2005\n\n\u00aa federation of european neuroscience societies\n\nblockade of nmda receptors in the dorsomedial\nstriatum prevents action\u2013outcome learning in instrumental\nconditioning\n\nhenry h. yin, barbara j. knowlton and bernard w. balleine\ndepartment of psychology and brain research institute, university of california, los angeles, box 951563, los angeles, ca\n90095\u20131563, usa\n\nkeywords: apv, basal ganglia, goal-directed action, learning\n\nabstract\n\nalthough there is consensus that instrumental conditioning depends on the encoding of action\u2013outcome associations, it is not known\nwhere this learning process is localized in the brain. recent research suggests that the posterior dorsomedial striatum (pdms) may be\nthe critical locus of these associations. we tested this hypothesis by examining the contribution of n-methyl-d-aspartate receptors\n(nmdars) in the pdms to action\u2013outcome learning. rats with bilateral cannulae in the pdms were \ufb01rst train", "letter\n\ncommunicated by yann le cun\n\na fast learning algorithm for deep belief nets\n\ngeoffrey e. hinton\nhinton@cs.toronto.edu\nsimon osindero\nosindero@cs.toronto.edu\ndepartment of computer science, university of toronto, toronto, canada m5s 3g4\n\nyee-whye teh\ntehyw@comp.nus.edu.sg\ndepartment of computer science, national university of singapore,\nsingapore 117543\n\nwe show how to use \u201ccomplementary priors\u201d to eliminate the explaining-\naway effects that make inference dif\ufb01cult in densely connected belief nets\nthat have many hidden layers. using complementary priors, we derive a\nfast, greedy algorithm that can learn deep, directed belief networks one\nlayer at a time, provided the top two layers form an undirected associa-\ntive memory. the fast, greedy algorithm is used to initialize a slower\nlearning procedure that \ufb01ne-tunes the weights using a contrastive ver-\nsion of the wake-sleep algorithm. after \ufb01ne-tuning, a network with three\nhidden layers forms a very good generative model of the joi", "2\n2\n0\n2\n\n \n\np\ne\ns\n5\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n4\n8\n4\n7\n0\n\n.\n\n9\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nhydra attention:\n\nefficient attention with many heads\n\ndaniel bolya1,2\u22c6, cheng-yang fu2, xiaoliang dai2, peizhao zhang2, and judy\n\nhoffman1\n\n1 georgia tech\n\n{dbolya,judy}@gatech.edu\n\n{chengyangfu,xiaoliangdai,stzpz}@fb.com\n\n2 meta ai\n\nabstract. while transformers have begun to dominate many tasks in\nvision, applying them to large images is still computationally difficult. a\nlarge reason for this is that self-attention scales quadratically with the\nnumber of tokens, which in turn, scales quadratically with the image size.\non larger images (e.g., 1080p), over 60% of the total computation in the\nnetwork is spent solely on creating and applying attention matrices. we\ntake a step toward solving this issue by introducing hydra attention, an\nextremely efficient attention operation for vision transformers (vits).\nparadoxically, this efficiency comes from taking multi-head attention to\nits extreme: by using", "clusterability in neural networks\n\ndaniel filan1, *, stephen casper2, *, shlomi hod3, *,\n\ncody wild1, andrew critch1, stuart russell1\n\n{daniel \ufb01lan, codywild, critch, russell}@berkeley.edu, scasper@college.harvard.edu, shlomi@bu.edu\n\n1 uc berkeley, 2 harvard, 3 boston university\n\n1\n2\n0\n2\n\n \nr\na\n\nm\n4\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n6\n8\n3\n3\n0\n\n.\n\n3\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nthe learned weights of a neural network have often been con-\nsidered devoid of scrutable internal structure. in this paper,\nhowever, we look for structure in the form of clusterabil-\nity: how well a network can be divided into groups of neu-\nrons with strong internal connectivity but weak external con-\nnectivity. we \ufb01nd that a trained neural network is typically\nmore clusterable than randomly initialized networks, and of-\nten clusterable relative to random networks with the same dis-\ntribution of weights. we also exhibit novel methods to pro-\nmote clusterability in neural network training, and \ufb01nd that\nin multi-laye", "mm...\nated with the largest activation, the move-\nment field, was determined by presenting a\nsaccade target at different locations. mon-\nkeys then performed a countermanding task\nthat manipulates the ability to inhibit a\nmovement at different stages of preparation\n(fig. 2) (9). we analyzed the growth of\nmovement-related activity of fef neurons\nduring trials with different reaction times to\ntest the merits of the two models (10). to\nrepresent the form of the accumulating sig-\nnal, we derived a particular activation func-\ntion from the trains of action potentials\n(11).\n\nwe recorded from two m. mulatta a total\nof 118 cells, 25 of which exhibited move-\nment-related activity and provided suffi-\ncient data in the necessary trial conditions\nto be included in this report. the response\nof a representative fef movement cell is\nshown during 150 no-signal trials when the\nsaccade target was in the movement field of\nthe cell (fig. 3a). the activity of the cell\nbegan to increase -100 ms before saccad", "a tutorial on training recurrent neural \nnetworks, covering bptt, rtrl, ekf and the \n\"echo state network\" approach  \n \n \nherbert jaeger \nfraunhofer institute for autonomous intelligent systems (ais) \nsince 2003: international university bremen \n \nfirst published: oct. 2002 \nfirst revision:  feb. 2004 \nsecond revision: march 2005  \nthird revision: april 2008 \nforth revision: july 2013  fifth revision: dec 2013 \n \nabstract:  \n \nthis tutorial is a worked-out version of a 5-hour course originally held at ais in \nseptember/october 2002. it has two distinct components. first, it contains a \nmathematically-oriented crash course on traditional training methods for recurrent \nneural networks, covering back-propagation through time (bptt), real-time recurrent \nlearning (rtrl), and extended kalman filtering approaches (ekf). this material is \ncovered in sections 2 \u2013 5. the remaining sections 1 and 6 \u2013 9 are much more gentle, \nmore detailed, and illustrated with simple examples. they are intended ", "https://doi.org/10.1038/s41583-023-00756-z\n\n check for updates\n\nhow deep is the brain? the shallow  \nbrain hypothesis\n\nmototaka suzuki\u2009\nabstract\n\n \u20091 \n\n, cyriel m. a. pennartz\u2009\n\n \u20091 & jaan aru\u2009\n\n \u20092 \n\ndeep learning and predictive coding architectures commonly assume \nthat inference in neural networks is hierarchical. however, largely \nneglected in deep learning and predictive coding architectures is the \nneurobiological evidence that all hierarchical cortical areas, higher \nor lower, project to and receive signals directly from subcortical \nareas. given these neuroanatomical facts, today\u2019s dominance of \ncortico-centric, hierarchical architectures in deep learning and \npredictive coding networks is highly questionable; such architectures \nare likely to be\u00a0missing essential computational principles the brain \nuses. in this perspective, we present the shallow brain hypothesis: \nhierarchical cortical processing is integrated with a massively parallel \nprocess to which subcortical areas sub", "a mathematical theory of deep convolutional\n\nneural networks for feature extraction\n\nthomas wiatowski and helmut b\u00a8olcskei, fellow, ieee\n\n1\n\n7\n1\n0\n2\n\n \nt\nc\no\n4\n2\n\n \n\n \n \n]\nt\ni\n.\ns\nc\n[\n \n \n\n3\nv\n3\n9\n2\n6\n0\n\n.\n\n2\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014deep convolutional neural networks have led to\nbreakthrough results in numerous practical machine learning\ntasks such as classi\ufb01cation of images in the imagenet data\nset, control-policy-learning to play atari games or the board\ngame go, and image captioning. many of these applications \ufb01rst\nperform feature extraction and then feed the results thereof into\na trainable classi\ufb01er. the mathematical analysis of deep convo-\nlutional neural networks for feature extraction was initiated by\nmallat, 2012. speci\ufb01cally, mallat considered so-called scattering\nnetworks based on a wavelet transform followed by the modulus\nnon-linearity in each network layer, and proved translation\ninvariance (asymptotically in the wavelet scale parameter) and\ndeformation stability of t", "systematic errors in connectivity inferred from \nactivity in strongly recurrent networks\n\nabhranil das\u200a\n\n\u200a1,2 and ila r. fiete\u200a\n\n\u200a1,2,3\u2009\u2709\n\nunderstanding the mechanisms of neural computation and learning will require knowledge of the underlying circuitry. because \nit is difficult to directly measure the wiring diagrams of neural circuits, there has long been an interest in estimating them algo-\nrithmically from multicell activity recordings. we show that even sophisticated methods, applied to unlimited data from every \ncell in the circuit, are biased toward inferring connections between unconnected but highly correlated neurons. this failure to \n\u2018explain away\u2019 connections occurs when there is a mismatch between the true network dynamics and the model used for infer-\nence, which is inevitable when modeling the real world. thus, causal inference suffers when variables are highly correlated, \nand activity-based estimates of connectivity should be treated with special caution in strongly co", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nfrozen  algorithms:  how  the  brain\u2019s  wiring\nfacilitates  learning\ndhruva  v  raman  and  timothy  o\u2019leary\n\nsynapses  and  neural  connectivity  are  plastic  and  shaped  by\nexperience.  but  to  what  extent  does  connectivity  itself\nin\ufb02uence  the  ability  of  a  neural  circuit  to  learn?  insights  from\noptimization  theory  and  ai  shed  light  on  how  learning  can  be\nimplemented  in  neural  circuits.  though  abstract  in  their  nature,\nlearning  algorithms  provide  a  principled  set  of  hypotheses  on\nthe  necessary  ingredients  for  learning  in  neural  circuits.  these\ninclude  the  kinds  of  signals  and  circuit  motifs  that  enable\nlearning  from  experience,  as  well  as  an  appreciation  of  the\nconstraints  that  make  learning  challenging  in  a  biological\nsetting.  remarkably,  some  simple  connectivity  patterns  can\nboost  the  ef\ufb01ciency  of  relatively  crude  learning  rule", "a r t i c l e s\n\ndecorrelation and efficient coding by retinal  \nganglion cells\nxaq pitkow1 & markus meister2\nan influential theory of visual processing asserts that retinal center-surround receptive fields remove spatial correlations in  \nthe visual world, producing ganglion cell spike trains that are less redundant than the corresponding image pixels. for bright, \nhigh-contrast images, this decorrelation would enhance coding efficiency in optic nerve fibers of limited capacity. we tested the \ncentral prediction of the theory and found that the spike trains of retinal ganglion cells were indeed decorrelated compared with \nthe visual input. however, most of the decorrelation was accomplished not by the receptive fields, but by nonlinear processing in \nthe retina. we found that a steep response threshold enhanced efficient coding by noisy spike trains and that the effect of this \nnonlinearity was near optimal in both salamander and macaque retina. these results offer an explanation for ", "meta-reinforcement learning via \norbitofrontal cortex\n\nhttps://doi.org/10.1038/s41593-023-01485-3\n\nreceived: 24 march 2023\n\naccepted: 6 october 2023\n\npublished online: xx xx xxxx\n\n check for updates\n\nryoma hattori\u2009\nshuqi chen\u2009\nbyung kook lim\u2009\n\n \u20091,2,3,4,6 \n\n, nathan g. hedrick\u2009\n\n \u20091,2,3,4, anant jain5, \n\n \u20091,2,3,4, hanjia you\u2009\n\n \u20091,2,3,4, mariko hattori1,2,3,4, jun-hyeok choi1, \n\n \u20091, ryohei yasuda\u2009\n\n \u20095 & takaki komiyama\u2009\n\n \u20091,2,3,4 \n\nthe meta-reinforcement learning (meta-rl) framework, which involves rl \nover multiple timescales, has been successful in training deep rl models \nthat generalize to new environments. it has been hypothesized that the \nprefrontal cortex may mediate meta-rl in the brain, but the evidence \nis scarce. here we show that the orbitofrontal cortex (ofc) mediates \nmeta-rl. we trained mice and deep rl models on a probabilistic reversal \nlearning task across sessions during which they improved their trial-by-trial \nrl policy through meta-learning. ca2+/calmodulin-d", "6\n1\n0\n2\n\n \n\np\ne\ns\n0\n1\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n1\n3\n3\n4\n0\n\n.\n\n7\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nrandom projections of random manifolds\n\nsubhaneil lahiri,1 peiran gao,2 and surya ganguli1\n\n1department of applied physics, stanford university, stanford, ca 94305, usa\n\n2space exploration technologies co., hawthorne, ca 90250, usa\n\nabstract\n\ninteresting data often concentrate on low dimensional smooth manifolds inside a\nhigh dimensional ambient space. random projections are a simple, powerful tool for di-\nmensionality reduction of such data. previous works have studied bounds on how many\nprojections are needed to accurately preserve the geometry of these manifolds, given\ntheir intrinsic dimensionality, volume and curvature. however, such works employ def-\ninitions of volume and curvature that are inherently di\ufb03cult to compute. therefore\nsuch theory cannot be easily tested against numerical simulations to understand the\ntightness of the proven bounds. we instead study typical distortions arisin", "available online at www.sciencedirect.com\n\nsciencedirect\n\ncurrent opinion in\n\nneurobiology\n\ntranscriptomic evidence for dense peptidergic\nnetworks within forebrains of four widely divergent\ntetrapods\nstephen j smith\n\nabstract\nthe primary function common to every neuron is communi-\ncation with other neurons. such cell\u2013cell signaling can take\nnumerous forms, including fast synaptic transmission and\nslower neuromodulation via secreted messengers, such as\nneuropeptides, dopamine, and many other diffusible small\nmolecules. individual neurons are quite diverse, however, in all\nparticulars of both synaptic and neuromodulatory communi-\ncation. neuron classification schemes have therefore proven\nvery useful in exploring the emergence of network function,\nbehavior, and cognition from the communication functions of\nindividual neurons. recently published single-cell mrna\nsequencing data and corresponding transcriptomic neuron\nclassifications from turtle, songbird, mouse, and human pro-\nvide eviden", "4\n1\n0\n2\n\n \nr\np\na\n9\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n4\n3\n0\n6\n\n.\n\n2\n1\n3\n1\n:\nv\ni\nx\nr\na\n\ndeep inside convolutional networks: visualising\nimage classi\ufb01cation models and saliency maps\n\nkaren simonyan\n\nandrea vedaldi\n\nandrew zisserman\n\nvisual geometry group, university of oxford\n\n{karen,vedaldi,az}@robots.ox.ac.uk\n\nabstract\n\nthis paper addresses the visualisation of image classi\ufb01cation models, learnt us-\ning deep convolutional networks (convnets). we consider two visualisation\ntechniques, based on computing the gradient of the class score with respect to\nthe input image. the \ufb01rst one generates an image, which maximises the class\nscore [5], thus visualising the notion of the class, captured by a convnet. the\nsecond technique computes a class saliency map, speci\ufb01c to a given image and\nclass. we show that such maps can be employed for weakly supervised object\nsegmentation using classi\ufb01cation convnets. finally, we establish the connection\nbetween the gradient-based convnet visualisation methods ", "journal of machine learning research 19 (2018) 1-57\n\nsubmitted 4/18; published 11/18\n\nthe implicit bias of gradient descent on separable data\n\ndaniel soudry\nelad hoffer\nmor shpigel nacson\ndepartment of electrical engineering,technion\nhaifa, 320003, israel\nsuriya gunasekar\nnathan srebro\ntoyota technological institute at chicago\nchicago, illinois 60637, usa\n\neditor: leon bottou\n\ndaniel.soudry@gmail.com\nelad.hoffer@gmail.com\nmor.shpigel@gmail.com\n\nsuriya@ttic.edu\nnati@ttic.edu\n\nabstract\n\nwe examine gradient descent on unregularized logistic regression problems, with homogeneous\nlinear predictors on linearly separable datasets. we show the predictor converges to the direction\nof the max-margin (hard margin svm) solution. the result also generalizes to other monotone\ndecreasing loss functions with an in\ufb01mum at in\ufb01nity, to multi-class problems, and to training a\nweight layer in a deep network in a certain restricted setting. furthermore, we show this convergence\nis very slow, and only logari", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\ncomputational  models  as  statistical  tools\ndaniel  durstewitz,  georgia  koppe1 and  hazem  toutounji1\n\ntraditionally,  models  in  statistics  are  relatively  simple  \u2018general\npurpose\u2019  quantitative  inference  tools,  while  models  in\ncomputational  neuroscience  aim  more  at  mechanistically\nexplaining  speci\ufb01c  observations.  research  on  methods  for\ninferring  behavioral  and  neural  models  from  data,  however,  has\nshown  that  a  lot  could  be  gained  by  merging  these  approaches,\naugmenting  computational  models  with  distributional\nassumptions.  this  enables  estimation  of  parameters  of  such\nmodels  in  a  principled  way,  comes  with  con\ufb01dence  regions  that\nquantify  uncertainty  in  estimates,  and  allows  for  quantitative\nassessment  of  prediction  quality  of  computational  models  and\ntests  of  speci\ufb01c  hypotheses  about  underlying  mechanisms.\nthus,  unlike  in  conventional  stat", "a large-scale neural network training \nframework for generalized estimation of \nsingle-trial population dynamics\n\nhttps://doi.org/10.1038/s41592-022-01675-0\n\nreceived: 12 january 2021\n\naccepted: 14 october 2022\n\npublished online: 28 november 2022\n\n check for updates\n\nmohammad reza keshtkaran1,12, andrew r. sedler\u2009\nraeed h. chowdhury\u2009\nhansem sohn\u2009\nchethan pandarinath\u2009\n\n \u20097, mehrdad jazayeri7, lee e. miller\u2009\n\n \u20093,4, raghav tandon\u2009\n\n \u20091,2,11 \n\n \u20091,2,12, \n\n \u20093,8,9,10 & \n\n \u20091,2, diya basrai1,5, sarah l. nguyen6, \n\nachieving state-of-the-art performance with deep neural population \ndynamics models requires extensive hyperparameter tuning for each \ndataset. autolfads is a model-tuning framework that automatically \nproduces high-performing autoencoding models on data from a variety \nof brain areas and tasks, without behavioral or task information. we \ndemonstrate its broad applicability on several rhesus macaque datasets: \nfrom motor cortex during free-paced reaching, somatosensory cortex \ndur", "neuroimage 258 (2022) 119360 \n\ncontents lists available at  sciencedirect \n\nneuroimage \n\njournal homepage:  www.elsevier.com/locate/neuroimage \n\nempirical transmit \ufb01eld bias correction of t1w/t2w myelin maps \n\nmatthew f. glasser \ngraham l. baum \ndavid c. van essen \n\na , b , \u2217 , timothy s. coalson \n\nf , joonas a. autio \n\nb , nicholas a. bock \n\nb , michael p. harms \n\nc , junqian xu \ng , edward j. auerbach \nd , douglas n. greve \ng \ni , takuya hayashi \n\nd , e , \nh , essa yacoub \n\nd , \n\na departments of radiology, \nb neuroscience, and \nc psychiatry, washington university medical school, st. louis, mo, united states \nd center for magnetic resonance research, university of minnesota, minneapolis, mn, united states \ne departments of radiology and psychiatry, baylor college of medicine, houston, tx, united states \nf department of psychology, harvard university, cambridge, ma, united states \ng riken center for biosystems dynamics research, kobe, japan \nh athinoula a. martinos center for biomedic", "flexible timing by temporal scaling of cortical \nresponses\n\njing wang\u200a\n\n\u200a1,5, devika narain1,2,3,4, eghbal a. hosseini2,5 and mehrdad jazayeri\u200a\n\n\u200a1,2,5*\n\nmusicians can perform at different tempos, speakers can control the cadence of their speech, and children can flexibly vary \ntheir temporal expectations of events. to understand the neural basis of such flexibility, we recorded from the medial frontal \ncortex of nonhuman primates trained to produce different time intervals with different effectors. neural responses were het-\nerogeneous, nonlinear, and complex, and they exhibited a remarkable form of temporal invariance: firing rate profiles were \ntemporally scaled to match the produced intervals. recording from downstream neurons in the caudate and from thalamic \nneurons projecting to the medial frontal cortex indicated that this phenomenon originates within cortical networks. recurrent \nneural network models trained to perform the task revealed that temporal scaling emerges from nonl", "the neural and computational bases \nof semantic cognition\n\nmatthew a.\u00a0lambon ralph1, elizabeth jefferies2, karalyn patterson3,4  \nand timothy t.\u00a0rogers5\nabstract | semantic cognition refers to our ability to use, manipulate and generalize knowledge \nthat is acquired over the lifespan to support innumerable verbal and non-verbal behaviours. \nthis review summarizes key findings and issues arising from a decade of research into the \nneurocognitive and neurocomputational underpinnings of this ability, leading to a new \nframework that we term controlled semantic cognition (csc). csc offers solutions to \nlong-standing queries in philosophy and cognitive science, and yields a convergent framework \nfor understanding the neural and computational bases of healthy semantic cognition and its \ndysfunction in brain disorders.\n\nsemantic cognition refers to the collection of neurocog-\nnitive mechanisms that support semantically imbued \nbehaviours. we deploy our semantic knowledge not \nonly to produce ", "7\n1\n0\n2\n\n \n\nb\ne\nf\n5\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n8\n7\n5\n1\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nunder review as a conference paper at iclr 2017\n\nneural architecture search with\nreinforcement learning\n\nbarret zoph\u2217, quoc v. le\ngoogle brain\n{barretzoph,qvl}@google.com\n\nabstract\n\nneural networks are powerful and \ufb02exible models that work well for many dif\ufb01-\ncult learning tasks in image, speech and natural language understanding. despite\ntheir success, neural networks are still hard to design. in this paper, we use a re-\ncurrent network to generate the model descriptions of neural networks and train\nthis rnn with reinforcement learning to maximize the expected accuracy of the\ngenerated architectures on a validation set. on the cifar-10 dataset, our method,\nstarting from scratch, can design a novel network architecture that rivals the best\nhuman-invented architecture in terms of test set accuracy. our cifar-10 model\nachieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster t", "8\n1\n0\n2\n\n \n\np\ne\ns\n \n0\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n5\nv\n5\n0\n2\n7\n0\n\n.\n\n1\n1\n7\n1\n:\nv\ni\nx\nr\na\n\ndecoding of neural data using cohomological feature extraction\n\nerik rybakkena, nils baasa, and benjamin dunnb\n\na{erik.rybakken, nils.baas}@ntnu.no, department of mathematical sciences, norwegian university of science and\n\nbbenjamin.dunn@ntnu.no, kavli institute for systems neuroscience, norwegian university of science and technology,\n\ntechnology, 7491 trondheim, norway\n\n7491 trondheim, norway\n\nseptember 11, 2018\n\nabstract\n\nwe introduce a novel data-driven approach to discover and decode features in the neural code coming from\nlarge population neural recordings with minimal assumptions, using cohomological feature extraction. we\napply our approach to neural recordings of mice moving freely in a box, where we \ufb01nd a circular feature. we\nthen observe that the decoded value corresponds well to the head direction of the mouse. thus we capture head\ndirection cells and decode the head direction from", "9\n1\n0\n2\n\n \n\np\ne\ns\n0\n2\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n2\nv\n8\n5\n8\n5\n0\n\n.\n\n9\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nctrl: a conditional transformer language\nmodel for controllable generation\n\nnitish shirish keskar\u2217, bryan mccann\u2217, lav r. varshney, caiming xiong, richard socher\nsalesforce research\u2020\n\nabstract\n\nlarge-scale language models show promising text generation capabilities, but\nusers cannot easily control particular aspects of the generated text. we release\nctrl, a 1.63 billion-parameter conditional transformer language model, trained\nto condition on control codes that govern style, content, and task-speci\ufb01c behav-\nior. control codes were derived from structure that naturally co-occurs with raw\ntext, preserving the advantages of unsupervised learning while providing more\nexplicit control over text generation. these codes also allow ctrl to predict\nwhich parts of the training data are most likely given a sequence. this provides\na potential method for analyzing large amounts of data via model-based source\natt", "ieee transactions on pattern analysis and machine intelligence, vol. 24, no. 12, december 2002\n\nx\n\ninput feature selection by mutual\n\ninformation based on parzen window\n\nnojun kwak, student member,\nieee computer society, and\nchong-ho choi, member, ieee\n\nabstract\u2014mutual information is a good indicator of relevance between variables,\nand have been used as a measure in several feature selection algorithms.\nhowever, calculating the mutual information is difficult, and the performance of a\nfeature selection algorithm depends on the accuracy of the mutual information. in\nthis paper, we propose a new method of calculating mutual information between\ninput and class variables based on the parzen window, and we apply this to a\nfeature selection algorithm for classification problems.\n\nindex terms\u2014feature selection, mutual information, parzen window.\n\n1\n\nintroduction\n\n(cid:230)\n\nmutual information is considered as a good indicator of relevance\nbetween two random variables [1]. recently, efforts to", "hippocampus 25:1073\u20131188 (2015)\n\nhippocampal sharp wave-ripple: a cognitive biomarker\n\nfor episodic memory and planning\n\ngy\u20acorgy buzs\u0013aki*\n\nabstract:\nsharp wave ripples (spw-rs) represent the most synchro-\nnous population pattern in the mammalian brain. their excitatory output\naffects a wide area of the cortex and several subcortical nuclei. spw-rs\noccur during \u201coff-line\u201d states of the brain, associated with consummatory\nbehaviors and non-rem sleep, and are in\ufb02uenced by numerous neurotrans-\nmitters and neuromodulators. they arise from the excitatory recurrent sys-\ntem of the ca3 region and the spw-induced excitation brings about a fast\nnetwork oscillation (ripple) in ca1. the spike content of spw-rs is tempo-\nrally and spatially coordinated by a consortium of interneurons to replay\nfragments of waking neuronal sequences in a compressed format. spw-rs\nassist in transferring this compressed hippocampal representation to dis-\ntributed circuits to support memory consolidation; selective di", "provably ef\ufb01cient maximum entropy exploration\n\nelad hazan 1 2 sham m. kakade 3 4 2 karan singh 1 2 abby van soest 1 2\n\nabstract\n\nsuppose an agent is in a (possibly unknown)\nmarkov decision process in the absence of a re-\nward signal, what might we hope that an agent\ncan ef\ufb01ciently learn to do? this work studies a\nbroad class of objectives that are de\ufb01ned solely as\nfunctions of the state-visitation frequencies that\nare induced by how the agent behaves. for exam-\nple, one natural, intrinsically de\ufb01ned, objective\nproblem is for the agent to learn a policy which\ninduces a distribution over state space that is as\nuniform as possible, which can be measured in an\nentropic sense. we provide an ef\ufb01cient algorithm\nto optimize such such intrinsically de\ufb01ned objec-\ntives, when given access to a black box planning\noracle (which is robust to function approximation).\nfurthermore, when restricted to the tabular setting\nwhere we have sample based access to the mdp,\nour proposed algorithm is provably ef", "a r t i c l e s\n\nexplicit information for category-orthogonal object \nproperties increases along the ventral stream\nha hong1\u20133,5, daniel l k yamins1,2,5, najib j majaj1,2,4 & james j dicarlo1,2\nextensive research has revealed that the ventral visual stream hierarchically builds a robust representation for supporting \nvisual object categorization tasks. we systematically explored the ability of multiple ventral visual areas to support a variety of \n\u2018category-orthogonal\u2019 object properties such as position, size and pose. for complex naturalistic stimuli, we found that the inferior \ntemporal (it) population encodes all measured category-orthogonal object properties, including those properties often considered \nto be low-level features (for example, position), more explicitly than earlier ventral stream areas. we also found that the it \npopulation better predicts human performance patterns across properties. a hierarchical neural network model based on simple \ncomputational principles gene", "m\no\nc\n\n.\n\ne\nr\nu\nt\na\nn\n\n.\ni\n\nc\ns\no\nr\nu\ne\nn\n\n/\n/\n:\np\nt\nt\nh\n\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n1\n0\n0\n2\n\u00a9\n\n \n\n\u00a9 2001 nature publishing group  http://neurosci.nature.com\n\narticles\n\nnatural signal statistics and sensory\ngain control\n\nodelia schwartz1 and eero p. simoncelli2\n\n1 center for neural science, new york university, 4 washington place, room 809, new york, new york 10003, usa\n2 howard hughes medical institute, center for neural science, and courant institute of mathematical sciences, new york university, new york, \n\nnew york 10003, usa\n\ncorrespondence should be addressed to e. s. (eero.simoncelli@nyu.edu)\n\nwe describe a form of nonlinear decomposition that is well-suited for efficient encoding of natural\nsignals. signals are initially decomposed using a bank of linear filters. each filter response is then rec-\ntified and divided by a weighted sum of rectified responses of neighboring filters. we show that this\ndecomposition, with parameters optimized for the stat", "learning by neural reassociation\n\ncorrected: publisher correction\n\nmatthew d. golub\u200a\nelizabeth c. tyler-kabara\u200a\n\n\u200a1,2,3, patrick t. sadtler2,4,5, emily r. oby2,4,5, kristin m. quick2,4,5, stephen i. ryu3,6, \n\u200a1,2,9,10*\n\n\u200a4,7,8, aaron p. batista2,4,5, steven m. chase\u200a\n\n\u200a2,9,10* and byron m. yu\u200a\n\nbehavior is driven by coordinated activity across a population of neurons. learning requires the brain to change the neural \npopulation activity produced to achieve a given behavioral goal. how does population activity reorganize during learning? we \nstudied intracortical population activity in the primary motor cortex of rhesus macaques during short-term learning in a brain\u2013\ncomputer interface (bci) task. in a bci, the mapping between neural activity and behavior is exactly known, enabling us to rig-\norously define hypotheses about neural reorganization during learning. we found that changes in population activity followed \na suboptimal neural strategy of reassociation: animals relied on a fixe", "articles\n\nhttps://doi.org/10.1038/s43588-022-00282-5\n\ndisentangling the flow of signals between \npopulations of neurons\n\nevren gokcen!\nchristian k. machens5,7 and byron m. yu!\n\n!1,6,7\u2009\u2709\n\n!1, anna i. jasper2, jo\u00e3o d. semedo1, amin zandvakili!\n\n!2, adam kohn2,3,4,7, \n\ntechnological advances now allow us to record from large populations of neurons across multiple brain areas. these recordings \nmay illuminate how communication between areas contributes to brain function, yet a substantial barrier remains: how do we \ndisentangle the concurrent, bidirectional flow of signals between populations of neurons? we propose here a dimensionality \nreduction framework, delayed latents across groups (dlag), that disentangles signals relayed in each direction, identifies how \nthese signals are represented by each population and characterizes how they evolve within and across trials. we demonstrate \nthat dlag performs well on synthetic datasets similar in scale to current neurophysiological recordings. ", "closed-form continuous-time neural \nnetworks\n\nhttps://doi.org/10.1038/s42256-022-00556-7\n\nreceived: 23 march 2022\n\naccepted: 5 october 2022\n\npublished online: 15 november 2022\n\n check for updates\n\n \u20091,5 \nramin hasani\u2009\nlucas liebenwein\u2009\ndaniela rus1\n\n, mathias lechner1,2,5, alexander amini1, \n \u20091, aaron ray1, max tschaikowski3, gerald teschl\u2009\n\n \u20094 & \n\ncontinuous-time neural networks are a class of machine learning systems \nthat can tackle representation learning on spatiotemporal decision-making \ntasks. these models are typically represented by continuous differential \nequations. however, their expressive power when they are deployed on \ncomputers is bottlenecked by numerical differential equation solvers. \nthis limitation has notably slowed down the scaling and understanding of \nnumerous natural physical phenomena such as the dynamics of nervous \nsystems. ideally, we would circumvent this bottleneck by solving the given \ndynamical system in closed form. this is known to be intractable ", "letter\n\ncommunicated by peter dayan\n\nstimulus representation and the timing of reward-prediction\nerrors in models of the dopamine system\n\nelliot a. ludvig\nelliot@cs.ualberta.ca\nrichard s. sutton\nsutton@cs.ualberta.ca\nuniversity of alberta, edmonton, alberta t6g 2e8, canada\n\ne. james kehoe\nj.kehoe@unsw.edu.au\nuniversity of new south wales, sydney 2052, new south wales, australia\n\nthe phasic \ufb01ring of dopamine neurons has been theorized to encode a\nreward-prediction error as formalized by the temporal-difference (td)\nalgorithm in reinforcement learning. most td models of dopamine have\nassumed a stimulus representation, known as the complete serial com-\npound, in which each moment in a trial is distinctly represented. we\nintroduce a more realistic temporal stimulus representation for the td\nmodel. in our model, all external stimuli, including rewards, spawn a\nseries of internal microstimuli, which grow weaker and more diffuse\nover time. these microstimuli are used by the td learning algori", "4\n1\n0\n2\n\n \n\nv\no\nn\n6\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n4\n8\n7\n1\n\n.\n\n1\n1\n4\n1\n:\nv\ni\nx\nr\na\n\nconditional generative adversarial nets\n\nd\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle\n\nmehdi mirza\n\nuniversit\u00b4e de montr\u00b4eal\nmontr\u00b4eal, qc h3c 3j7\n\nmirzamom@iro.umontreal.ca\n\nsimon osindero\nflickr / yahoo inc.\n\nsan francisco, ca 94103\n\nosindero@yahoo-inc.com\n\nabstract\n\ngenerative adversarial nets [8] were recently introduced as a novel way to train\ngenerative models. in this work we introduce the conditional version of generative\nadversarial nets, which can be constructed by simply feeding the data, y, we wish\nto condition on to both the generator and discriminator. we show that this model\ncan generate mnist digits conditioned on class labels. we also illustrate how\nthis model could be used to learn a multi-modal model, and provide preliminary\nexamples of an application to image tagging in which we demonstrate how this\napproach can generate descriptive tags which are not part of training", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nbeyond  stdp  \u2014  towards  diverse  and  functionally\nrelevant  plasticity  rules\naparna  suvrathan\n\nsynaptic  plasticity,  induced  by  the  close  temporal  association\nof  two  neural  signals,  supports  associative  forms  of  learning.\nhowever,  the  millisecond  timescales  for  association  often  do\nnot  match  the  much  longer  delays  for  behaviorally  relevant\nsignals  that  supervise  learning.  in  particular,  information  about\nthe  behavioral  outcome  of  neural  activity  can  be  delayed,\nleading  to  a  problem  of  temporal  credit  assignment.  recent\nstudies  suggest  that  synaptic  plasticity  can  have  temporal  rules\nthat  not  only  accommodate  the  delays  relevant  to  the  circuit,\nbut  also  be  precisely  tuned  to  the  behavior  the  circuit  supports.\nthese  discoveries  highlight  the  diversity  of  plasticity  rules,\nwhose  temporal  requirements  may  depend  on  circuit  de", "proc. natl. acad. sci. usa\nvol. 92, pp. 3844-3848, april 1995\nneurobiology\n\ntheory of orientation tuning in visual cortex\n\n(neural networks/cross-correlations/symmetry breaking)\n\nr. ben-yishai*, r. lev bar-or*, and h. sompolinskyt\n*racah institute of physics and center for neural computation, hebrew university, jerusalem 91904, israel; and tat&t bell laboratories,\nmurray hill, nj 07974\n\ncommunicated by pierre c. hohenberg, at&t bell laboratories, murray hill, nj, december 21, 1994 (received for review july 28, 1994)\n\nabstract\nthe role of intrinsic cortical connections in\nprocessing sensory input and in generating behavioral\noutput is poorly understood. we have examined this issue in\nthe context of the tuning of neuronal responses in cortex to\nthe orientation of a visual stimulus. we analytically study a\nsimple network model that incorporates both orientation-\nselective input from the lateral geniculate nucleus and\norientation-specific cortical interactions. depending on the\nmodel param", "n\ne\nw\ns\n\nf\ne\na\nt\nu\nr\ne\n\nnews feature\n\nwhat are the limits of deep learning?\n\nthe much-ballyhooed artificial intelligence approach boasts impressive feats but still falls\nshort of human brainpower. researchers are determined to figure out what\u2019s missing.\n\nm. mitchell waldrop, science writer\n\nthere\u2019s no mistaking the image: it\u2019s a banana\u2014a big,\nripe, bright-yellow banana. yet the artificial intelligence\n(ai) identifies it as a toaster, even though it was trained\nwith the same powerful and oft-publicized deep-learning\ntechniques that have produced a white-hot revolution in\ndriverless cars, speech understanding, and a multitude of\nother ai applications. that means the ai was shown sev-\neral thousand photos of bananas, slugs, snails, and\nsimilar-looking objects, like so many flash cards, and then\ndrilled on the answers until it had the classification down\ncold. and yet this advanced system was quite easily con-\nfused\u2014all it took was a little day-glow sticker, digitally\npasted in one corner ", "7\n1\n0\n2\n\n \n\ny\na\nm\n2\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n5\n1\n9\n0\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\n1\n\ndeeplab: semantic image segmentation with\ndeep convolutional nets, atrous convolution,\n\nand fully connected crfs\n\nliang-chieh chen, george papandreou, senior member, ieee, iasonas kokkinos, member, ieee,\n\nkevin murphy, and alan l. yuille, fellow, ieee\n\nabstract\u2014in this work we address the task of semantic image segmentation with deep learning and make three main contributions\nthat are experimentally shown to have substantial practical merit. first, we highlight convolution with upsampled \ufb01lters, or\n\u2018atrous convolution\u2019, as a powerful tool in dense prediction tasks. atrous convolution allows us to explicitly control the resolution at\nwhich feature responses are computed within deep convolutional neural networks. it also allows us to effectively enlarge the \ufb01eld of\nview of \ufb01lters to incorporate larger context without increasing the number of parameters or the amount of computation. second, we\nprop", "\f", "published as a conference paper at iclr 2018\n\nnervenet: learning structured policy with\ngraph neural networks\n\ntingwu wang\u2217, renjie liao\u2217, jimmy ba & sanja fidler\ndepartment of computer science\nuniversity of toronto\nvector institute\n{tingwuwang,rjliao}@cs.toronto.edu,\njimmy@psi.toronto.edu, fidler@cs.toronto.edu\n\nabstract\n\nwe address the problem of learning structured policies for continuous control. in\ntraditional reinforcement learning, policies of agents are learned by multi-layer\nperceptrons (mlps) which take the concatenation of all observations from the en-\nvironment as input for predicting actions. in this work, we propose nervenet to\nexplicitly model the structure of an agent, which naturally takes the form of a\ngraph. speci\ufb01cally, serving as the agent\u2019s policy network, nervenet \ufb01rst propa-\ngates information over the structure of the agent and then predict actions for differ-\nent parts of the agent. in the experiments, we \ufb01rst show that our nervenet is com-\nparable to state-of-", "article\n\nflexible sensorimotor computations through rapid\nrecon\ufb01guration of cortical dynamics\n\ngraphical abstract\n\nauthors\n\nevan d. remington, devika narain,\neghbal a. hosseini, mehrdad jazayeri\n\ncorrespondence\nmjaz@mit.edu\n\nin brief\nremington et al. employ a dynamical\nsystems perspective to understand how\nthe brain \ufb02exibly controls timed\nmovements. results suggest that\nneurons in the frontal cortex form a\nrecurrent network whose behavior is\n\ufb02exibly controlled by inputs and initial\nconditions.\n\nhighlights\nd monkeys performed a timing task demanding \ufb02exible\n\ncognitive control\n\nd the organization of neural trajectories in frontal cortex\n\nre\ufb02ected task demands\n\nd flexible control was best explained in terms of inputs and\n\ninitial conditions\n\nd recurrent neural network models validated the inferred\n\ncontrol principles\n\nremington et al., 2018, neuron 98, 1005\u20131019\njune 6, 2018 \u00aa 2018 elsevier inc.\nhttps://doi.org/10.1016/j.neuron.2018.05.020\n\n\f", "4\n1\n0\n2\n \nc\ne\nd\n4\n1\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n3\nv\n5\n1\n2\n3\n\n.\n\n9\n0\n4\n1\n:\nv\ni\nx\nr\na\n\nsequence to sequence learning\n\nwith neural networks\n\nilya sutskever\n\ngoogle\n\noriol vinyals\n\ngoogle\n\nquoc v. le\n\ngoogle\n\nilyasu@google.com\n\nvinyals@google.com\n\nqvl@google.com\n\nabstract\n\ndeep neural networks (dnns) are powerful models that have achieved excel-\nlent performance on dif\ufb01cult learning tasks. although dnns work well whenever\nlarge labeled training sets are available, they cannot be used to map sequences to\nsequences. in this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. our method\nuses a multilayered long short-term memory (lstm) to map the input sequence\nto a vector of a \ufb01xed dimensionality, and then another deep lstm to decode the\ntarget sequence from the vector. our main result is that on an english to french\ntranslation task from the wmt\u201914 dataset, the translations produced by the lstm\nachieve a bleu score of", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/2703232\n\nfunction optimization using connectionist reinforcement learning algorithms\n\narticle\u00a0\u00a0in\u00a0\u00a0connection science \u00b7 september 1991\n\ndoi: 10.1080/09540099108946587\u00a0\u00b7\u00a0source: citeseer\n\ncitations\n278\n\n2 authors, including:\n\njing peng\nmontclair state university\n\n162 publications\u00a0\u00a0\u00a04,056 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n3,571\n\nall content following this page was uploaded by jing peng on 01 december 2014.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "6\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n2\n2\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n9\n2\n1\n7\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nexplainable restricted boltzmann machines for collaborative\n\nfiltering\n\nbehnoush abdollahi\ndept. of computer engineering & computer science, knowledge discovery and web mining lab, university of\nlouisville, louisville, ky 40222, usa\n\nb.abdollahi@louisville.edu\n\nolfa nasraoui\ndept. of computer engineering & computer science, knowledge discovery and web mining lab, university of\nlouisville, louisville, ky 40222, usa\n\nolfa.nasraoui@louisville.edu\n\nabstract\n\ntheir\n\nrecommendations.\n\nmost accurate recommender systems are\nreasoning\nblack-box models, hiding the\nbehind\nyet\nexplanations have been shown to increase\nthe user\u2019s trust in the system in addition to\nproviding other bene\ufb01ts such as scrutability,\nmeaning the ability to verify the validity\nof recommendations.\nthis gap between\naccuracy and transparency or explainability\nhas generated an interest\nin automated\nexplanation generation methods. restric", "learning stochastic feedforward neural networks\n\nyichuan tang\n\nruslan salakhutdinov\n\ndepartment of computer science\n\ndepartment of computer science and statistics\n\nuniversity of toronto\n\ntoronto, ontario, canada.\ntang@cs.toronto.edu\n\nuniversity of toronto\n\ntoronto, ontario, canada.\n\nrsalakhu@cs.toronto.edu\n\nabstract\n\nmultilayer perceptrons (mlps) or neural networks are popular models used for\nnonlinear regression and classi\ufb01cation tasks. as regressors, mlps model the\nconditional distribution of the predictor variables y given the input variables x.\nhowever, this predictive distribution is assumed to be unimodal (e.g. gaussian).\nfor tasks involving structured prediction, the conditional distribution should be\nmulti-modal, resulting in one-to-many mappings. by using stochastic hidden vari-\nables rather than deterministic ones, sigmoid belief nets (sbns) can induce a rich\nmultimodal distribution in the output space. however, previously proposed learn-\ning algorithms for sbns are not ef\ufb01ci", "report\n\nmanipulating hippocampal place cell activity by\nsingle-cell stimulation in freely moving mice\n\ngraphical abstract\n\nauthors\n\nmaria diamantaki, stefano coletta,\nkhaled nasr, ..., philipp berens,\npatricia preston-ferrer,\nandrea burgalossi\n\ncorrespondence\nandrea.burgalossi@cin.uni-tuebingen.de\n\nin brief\nplace cells can serve as a readout of\nhippocampal memory. diamantaki et al.\nshow that the activity of single place cells\ncan be rapidly modi\ufb01ed by single-cell\nstimulation in freely moving mice. this\n\ufb01nding provides insights into the cellular\nmechanisms that support the rapid\nreorganization of hippocampal place\nmaps.\n\nhighlights\nd juxtacellular stimulation of single hippocampal neurons in\n\nfreely moving mice\n\nd stimulation in silent neurons can induce place \ufb01elds\n\nd stimulation in single place cells can induce place \ufb01eld\n\nremapping\n\ndiamantaki et al., 2018, cell reports 23, 32\u201338\napril 3, 2018 \u00aa 2018 the author(s).\nhttps://doi.org/10.1016/j.celrep.2018.03.031\n\n\f", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/2022.08.15.503870\n; \n\nthis version posted august 15, 2022. \n\nthe copyright holder for this preprint\n\n(which was not certified by peer review) is the author/funder. all rights reserved. no reuse allowed without permission. \n\nflexible multitask computation in recurrent networks utilizes shared dynamical motifs \n\nlaura driscoll1, krishna shenoy1,2-7, david sussillo1,5 \n\n1 department of electrical engineering, stanford university, stanford, ca, usa \n\n2 department of neurosurgery, stanford university, stanford, ca, usa \n3 department of bioengineering, stanford university, stanford, ca, usa \n4 department of neurobiology, stanford university, stanford, ca, usa \n\n5 wu tsai neurosciences institute, stanford university, stanford, ca, usa \n\n6 bio-x institute, stanford university, stanford, ca, usa \n\n7 howard hughes medical institute at stanford university, stanford, ca, usa \n\n \nflexible  computation  is  a  hallmark  of  intelligent  behavior.  ye", "e\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\ne\nr\nu\nt\na\nn\nm\no\nc\n.\ne\nr\nu\nt\na\nn\nw\nw\nw\n\n/\n\n.\n\n/\n/\n:\np\nt\nt\nh\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n4\n0\n0\n2\n\u00a9\n\n \n\na r t i c l e s\n\nmodulation of spike timing by sensory deprivation\nduring induction of cortical map plasticity\n\ntansu celikel1,2, vanessa a szostak1 & daniel e feldman1\n\ndeprivation-induced plasticity of sensory cortical maps involves long-term potentiation (ltp) and depression (ltd) of cortical\nsynapses, but how sensory deprivation triggers ltp and ltd in vivo is unknown. here we tested whether spike timing\u2013dependent forms\nof ltp and ltd are involved in this process. we measured spike trains from neurons in layer 4 (l4) and layers 2 and 3 (l2/3) of rat\nsomatosensory cortex before and after acute whisker deprivation, a manipulation that induces whisker map plasticity involving ltd at\nl4-to-l2/3 (l4\u2013l2/3) synapses. whisker deprivation caused an immediate reversal of firing order for most l4 and l2/3 neurons and a\nsubstantial decorrelati", "vol 454 | 14 august 2008 | doi:10.1038/nature07150\n\nletters\n\ninternal brain state regulates membrane potential\nsynchrony in barrel cortex of behaving mice\njames f. a. poulet1 & carl c. h. petersen1\n\nas\n\nextracellular\n\nrevealed by\n\nrecordings of\n\ninternal brain states form key determinants for sensory percep-\ntion, sensorimotor coordination and learning1,2. a prominent\nreflection of different brain states in the mammalian central nerv-\nous system is the presence of distinct patterns of cortical syn-\nchrony,\nthe\nelectroencephalogram, local field potential and action potentials.\nsuch temporal correlations of cortical activity are thought to be\nfundamental mechanisms of neuronal computation3\u201311. however,\nit is unknown how cortical synchrony is reflected in the intracel-\nlular membrane potential (vm) dynamics of behaving animals.\nhere we show, using dual whole-cell recordings from layer 2/3\nprimary somatosensory barrel cortex in behaving mice, that the\nvm of nearby neurons is highly correla", "actor-critic algorithms \n\nvijay  r.  konda \n\njohn  n.  tsitsiklis \n\nlaboratory for  information and decision  systems , \n\nmassachusetts institute of technology, \n\ncambridge,  ma,  02139. \n\nkonda@mit.edu,  jnt@mit.edu \n\nabstract \n\nwe  propose  and  analyze  a  class  of  actor-critic  algorithms  for \nsimulation-based  optimization  of  a  markov  decision  process  over \na  parameterized  family  of randomized  stationary  policies.  these \nare two-time-scale  algorithms in  which  the critic uses td learning \nwith  a  linear approximation architecture and the actor is  updated \nin  an  approximate  gradient  direction  based  on  information  pro \nvided by the critic.  we  show that the features for  the critic should \nspan a subspace prescribed by the choice of parameterization of the \nactor.  we  conclude by discussing convergence properties and some \nopen problems. \n\n1 \n\nintroduction \n\nthe  vast  majority  of  reinforcement  learning  (rl)  [9j  and  neuro-dynamic  pro \ngramming (n", "the thirty-fourth aaai conference on arti\ufb01cial intelligence (aaai-20)\n\nthe hsic bottleneck: deep learning without back-propagation\n\nwan-duo kurt ma, j.p. lewis, w. bastiaan kleijn\n{mawand, bastiaan.kleijn}@ecs.vuw.ac.nz, jplewis@google.com\n\nvictoria university\n\nabstract\n\nwe introduce the hsic (hilbert-schmidt independence crite-\nrion) bottleneck for training deep neural networks. the hsic\nbottleneck is an alternative to the conventional cross-entropy\nloss and backpropagation that has a number of distinct advan-\ntages. it mitigates exploding and vanishing gradients, resulting\nin the ability to learn very deep networks without skip con-\nnections. there is no requirement for symmetric feedback or\nupdate locking. we \ufb01nd that the hsic bottleneck provides per-\nformance on mnist/fashionmnist/cifar10 classi\ufb01cation\ncomparable to backpropagation with a cross-entropy target,\neven when the system is not encouraged to make the output\nresemble the classi\ufb01cation labels. appending a single layer\ntrain", "7\n1\n0\n2\n\n \n\np\ne\ns\n8\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n3\n5\n9\n1\n0\n\n.\n\n9\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nimplicit regularization in deep learning\n\nby\n\nbehnam neyshabur\n\na thesis submitted\n\nin partial ful\ufb01llment of the requirements for\n\nthe degree of\n\ndoctor of philosophy in computer science\n\nat the\n\ntoyota technological institute at chicago\n\naugust, 2017\n\nthesis committee:\n\nnathan srebro (thesis advisor),\n\nyury makarychev,\n\nruslan salakhutdinov,\ngregory shakhnarovich\n\n\f", "numer. math. (2015) 130:567\u2013577\ndoi 10.1007/s00211-014-0673-6\n\nnumerische\nmathematik\n\non non-ergodic convergence rate of douglas\u2013rachford\nalternating direction method of multipliers\nbingsheng he \u00b7 xiaoming yuan\n\nreceived: 11 april 2013 / revised: 3 november 2014 / published online: 30 november 2014\n\u00a9 springer-verlag berlin heidelberg 2014\n\nabstract this note proposes a novel approach to derive a worst-case o(1/k) con-\nvergence rate measured by the iteration complexity in a non-ergodic sense for the\ndouglas\u2013rachford alternating direction method of multipliers proposed by glowin-\nski and marrocco.\nmathematics subject classi\ufb01cation 90c25 \u00b7 90c30\n\n1 introduction\n\nthere has been an impressive development on operator splitting methods in the area of\npartial differential equations, and among them are some alternating direction methods\nof multipliers (admms for short). in this note, we focus on the douglas\u2013rachford\nadmm scheme proposed by glowinski and marrocco in [7] (see also [5]) and we\nres", "behavioral and brain sciences (2011) 34, 169 \u2013231\ndoi:10.1017/s0140525x10003134\n\nbayesian fundamentalism\nor enlightenment? on the explanatory\nstatus and theoretical contributions of\nbayesian models of cognition\n\nmatt jones\ndepartment of psychology and neuroscience, university of colorado,\nboulder, co 80309\nmcj@colorado.edu\n\nhttp://matt.colorado.edu\n\nbradley c. love\ndepartment of psychology, university of texas, austin, tx 78712\nbrad_love@mail.utexas.edu\n\nhttp://love.psy.utexas.edu\n\nabstract: the prominence of bayesian modeling of cognition has increased recently largely because of mathematical advances in\nspecifying and deriving predictions from complex probabilistic models. much of this research aims to demonstrate that cognitive\nbehavior can be explained from rational principles alone, without recourse to psychological or neurological processes and\nrepresentations. we note commonalities between this rational approach and other movements in psychology \u2013 namely, behaviorism\nand evoluti", "neuron\n\nreview\n\na brief history of long-term potentiation\n\nroger a. nicoll1,*\n1department of cellular and molecular pharmacology, university of california at san francisco, san francisco, ca 94158, usa\n*correspondence: roger.nicoll@ucsf.edu\nhttp://dx.doi.org/10.1016/j.neuron.2016.12.015\n\nsince the discovery of long-term potentiation (ltp) in 1973, thousands of papers have been published on this\nintriguing phenomenon, which provides a compelling cellular model for learning and memory. although ltp\nhas suffered considerable growing pains over the years, ltp has \ufb01nally come of age. here the rich history of\nltp is reviewed. these are exciting times and the pace of discovery is remarkable.\n\nintroduction\nas the 45 year anniversary for long-term potentiation (ltp) is just\naround the corner, i thought it would be interesting to review the\nrich history of this \ufb01eld and where we are. some of the most\nfascinating questions of our time involve how we learn and\nhow our brain stores information. it ", "neuron\n\narticle\n\nmodel-based in\ufb02uences on humans\u2019 choices\nand striatal prediction errors\n\nnathaniel d. daw,1,* samuel j. gershman,2 ben seymour,3 peter dayan,4 and raymond j. dolan3\n1center for neural science and department of psychology, new york university, new york, ny 10012, usa\n2department of psychology and neuroscience institute, princeton university, princeton, nj 08540, usa\n3wellcome trust centre for neuroimaging, institute of neurology, university college london, wc1n 3bg london, uk\n4gatsby computational neuroscience unit, university college london, wc1n 3ar london, uk\n*correspondence: daw@cns.nyu.edu\ndoi 10.1016/j.neuron.2011.02.027\nopen access under cc by license.\n\nsummary\n\nthe mesostriatal dopamine system is prominently\nimplicated in model-free reinforcement\nlearning,\nwith fmri bold signals in ventral striatum notably\ncovarying with model-free prediction\nerrors.\nhowever,\nlatent learning and devaluation studies\nshow that behavior also shows hallmarks of\nmodel-based planning,", "psychological review1981, vol. 88, no. 2, 135-170copyright 1981 by the american psychological association, inc.0033-295x/8i/8802-oi35$00.75toward a modern theory of adaptive networks:expectation and predictionrichard s. sutton and andrew g. bartocomputer and information science departmentuniversity of massachusetts\u2014amherstmany adaptive neural network theories are based on neuronlike adaptive elementsthat can behave as single unit analogs of associative conditioning. in this articlewe develop a similar adaptive element, but one which is more closely in accordwith the facts of animal learning theory than elements commonly studied inadaptive network research. we suggest that an essential feature of classicalconditioning that has been largely overlooked by adaptive network theorists isits predictive nature. the adaptive element we present learns to increase itsresponse rate in anticipation of increased stimulation, producing a conditionedresponse before the occurrence of the unconditioned ", "9\n1\n0\n2\n\n \n\nb\ne\nf\n1\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n9\n4\n0\n9\n0\n\n.\n\n1\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nbiologically inspired alternatives to backpropagation through\n\ntime for learning in recurrent neural nets\n\nguillaume bellec*, franz scherr*, elias hajek , darjan salaj , robert legenstein ,\n\nand wolfgang maass\n\ninstitute for theoretical computer science, graz university of technology,\n\n*first authors\n\naustria\n\nfebruary 22, 2019\n\nabstract\n\nthe way how recurrently connected networks of spiking neurons in the brain acquire pow-\nerful information processing capabilities through learning has remained a mystery. this lack\nof understanding is linked to a lack of learning algorithms for recurrent networks of spiking\nneurons (rsnns) that are both functionally powerful and can be implemented by known bi-\nological mechanisms. since rsnns are simultaneously a primary target for implementations\nof brain-inspired circuits in neuromorphic hardware, this lack of algorithmic insight also hin-\nders technological prog", "magazine\nr709\n\nminute and 59 minute conditions and \nthen compared this average to the 5 \nminute condition; the impact of hub \nbaiting on long-term retention was \nlarger than in the 5 minute condition \n(t(7) = 2.80, p < 0.05). \n\nwe propose that hub placement \n\nprompted memory retrieval because \nrats expected a memory test after \nhub placement. our data suggest \nthat memory retrieval shortly after \nstudying promotes subsequent \nlong-term retention. importantly, the \nmemory test occurred early in the \nretention interval, thereby leaving a \nsubstantial amount of time before \nthe test (up to approximately an \nhour), during which it is unlikely \nthat the rats continued to maintain \nan active representation of the \nretrieved memory; the rats were \nphysically removed from the room \ncontaining the maze during these \nlong delays. importantly, the \nobserved improvement in accuracy \ncannot be attributed to memory of \nthe hub placement per se because \nsuch memory would not provide \ninformation abou", "\u0000\u0002\u0001\u0004\u0003\u0006\u0005\u0007\u0001\t\b\u000b\n\f\u0005\u0007\u0001\u000e\r\u0010\u000f\u0012\u0011\u0013\u000f\u0014\u0003\u0016\u0015\u0018\u0017\n\n\u0017\u001a\u0015\u001c\u001b\u000b\u0005\u0007\u0001\u001e\u001d\u001f\b\n\n \"!$#&%('*)+#-,/.\u001e021\f!*3\n\n4$57698\f:\f;<6>=@?bac5>dfe+g(hjikamlonp4\ta\tnrq@stnvu\nwo5>df=@gvh(xy5z:[hke@\\ki\u0018etxyd\f]^h(5>gj_`6a;b5>:f6a5\ncm:\f;bd\u00065zg9ef;ghihje@\\k4le+g(et:`h(e\nu>monj;b:\fp^qremi\u0018e+?b?b5zpt5sace\u0016=tt\n4$etgvet:`h(e\fu*ik=7:f=tt^=\nv2wx_\"u>yms\nz&n{x|=@;b?r}\u001cg~=ttx\\\u007fe+g9t\u0081\u0080\u00826>ea\u0083\u0084h(e+g(e+:[hve\f\u0083\u00845\u0085t[]\n\nusv2=@g96982uzqtq@s\n\u0086y\u0087\u0010\u0088\u008a\u00897\u008b7\u008c\f\u008dt\u0089\n\n\u008e\t=>h\u000657ef;<=@:2;g:[\\\u007f5>g(5z:f6\u008a5j\u008ff5>pt;g:fe\u0091\u0090\t;gh(8\u0092=yd\fg(;getgjt`;\u0093efhvg(;g\u008f\f]\fh(;get:2\\\u007fetg\u001axye`t[5z?&df=@g~=@xy5zh(5>g~emh(8f=7h\u0094;<e\nxy57=7:[h\u0095h(e\u00826>=@d^h(]\fgv5\u001ed\fgv;betg$\u008ff5>?g;b5\u008a\\re$=7\u008ffet]\fh&hv8\f5\u001egv5>?\u0093=7h(;get:fei8\f;bd\u001a\u008ff5>;b:^pmxye`t`5>?b57t\u0095\u0083$\u0096\fetg\u0010x\u0097]\f?gh(;g?\u0093=>ht5>g\ndf5>g96a5>d\fh(gvet:2:\f5zhf\u0090ketgv\u0098\feau\u0081\u0090\t8\f5zg(5\u0094hv8\f5jdf=7g9=@x\u00995>h(5>g~em=7g(5jhv8\f5j6\u008ae+:\f:\f576ah(;be+:\u009a\u0090k5>;bp+8[h~e\u008au*hv8\f5\u0094d\fgv;be+g\n?\u0093=+6\u009b\u0098^e\u0094=7:[h\u0014t[;bgv576\u008ah\u001axy57=7:\f;b:\fp\u0007\u009c\n\u0090\t8f=7h\u0097x|=@h(hv5>g9ej;<e\u0094h(8\f5yd^g(;be+gje7d\u00065>g\u0091\\\u007f]\f:\u009d6\u008ah(;get:fes6\u008aetxyd^]\fh(57t\n\u008f`h2h(8\f5o:\f5>hi\u0090\u001ee+g(\u0098\"h(8f=7h\u0094;\u0093ej;gxyd\f?b;g57ty\u008f[h2hv8\f;", "diving into the shallows: a computational perspective on\n\nlarge-scale shallow learning\n\nsiyuan ma, mikhail belkin\n\ndepartment of computer science and engineering\n\nthe ohio state university\n\n{masi,mbelkin}@cse.ohio-state.edu\n\n7\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n7\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n2\n2\n6\n0\n1\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\njune 20, 2017\n\nabstract\n\nremarkable recent success of deep neural networks has not been easy to analyze\ntheoretically. it has been particularly hard to disentangle relative signi\ufb01cance of archi-\ntecture and optimization in achieving accurate classi\ufb01cation on large datasets. on the\nother hand, shallow methods (such as kernel methods) have encountered obstacles in\nscaling to large data, despite excellent performance on smaller datasets, and extensive\ntheoretical analysis. practical large-scale optimization methods, such as variants of\ngradient descent, used so successfully in deep learning, seem to produce below par\nresults when applied to kernel methods.\n\nin this paper we \ufb01r", "the journal of neuroscience, may 29, 2013 \u2022 33(22):9353\u20139363 \u2022 9353\n\ndevelopment/plasticity/repair\n\ngabaergic circuits control spike-timing-dependent\nplasticity\n\nvincent paille,1,2* elodie fino,1,2* kai du,3,4 teresa morera-herreras,1,2 sylvie perez,1,2 jeanette hellgren kotaleski,3,4,5\nand laurent venance1,2\n1team dynamic and pathophysiology of neuronal networks, center for interdisciplinary research in biology, centre national de la recherche scientifique,\nunite\u00b4 mixte de recherche 7241/inserm u1050, college de france, 75005 paris, france, 2university pierre et marie curie, ecole doctorale 158, 75005 paris,\nfrance, 3stockholm brain institute and 4department of neuroscience, karolinska institute, 171 77 stockholm, sweden, and 5school of computer science\nand communication, kth royal institute of technology, 100 44 stockholm, sweden\n\nthe spike-timing-dependent plasticity (stdp), a synaptic learning rule for encoding learning and memory, relies on relative timing of\nneuronal activity on ", "enhanced deep residual networks for single image super-resolution\n\nbee lim\n\nsanghyun son\n\nheewon kim\n\nseungjun nah\n\nkyoung mu lee\n\ndepartment of ece, asri, seoul national university, 08826, seoul, korea\n\nforestrainee@gmail.com, thstkdgus35@snu.ac.kr, ghimhw@gmail.com\n\nseungjun.nah@gmail.com, kyoungmu@snu.ac.kr\n\n7\n1\n0\n2\n\n \nl\nu\nj\n \n\n0\n1\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n1\n2\n9\n2\n0\n\n.\n\n7\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nrecent research on super-resolution has progressed with\nthe development of deep convolutional neural networks\n(dcnn). in particular, residual learning techniques exhibit\nimproved performance.\nin this paper, we develop an en-\nhanced deep super-resolution network (edsr) with perfor-\nmance exceeding those of current state-of-the-art sr meth-\nods. the signi\ufb01cant performance improvement of our model\nis due to optimization by removing unnecessary modules in\nconventional residual networks. the performance is further\nimproved by expanding the model size while we stabilize\nthe training proce", "learning time-invariant representations for\nindividual neurons from population dynamics\n\nlu mi1,2\u2217, trung le2\u2217\n\n, tianxing he2, eli shlizerman2, uygar s\u00fcmb\u00fcl1\n\n1 allen institute for brain science\n\n2 university of washington\n\n{lu.mi,uygars}@alleninstitute.org\n\n{tle45, shlizee}@uw.edu\ngoosehe@cs.washington.edu\n\nabstract\n\nneurons can display highly variable dynamics. while such variability presumably\nsupports the wide range of behaviors generated by the organism, their gene expres-\nsions are relatively stable in the adult brain. this suggests that neuronal activity is\na combination of its time-invariant identity and the inputs the neuron receives from\nthe rest of the circuit. here, we propose a self-supervised learning based method to\nassign time-invariant representations to individual neurons based on permutation-,\nand population size-invariant summary of population recordings. we fit dynamical\nmodels to neuronal activity to learn a representation by considering the activity of\nboth the ", "appl. comput. harmon. anal. 21 (2006) 5\u201330\n\nwww.elsevier.com/locate/acha\n\ndiffusion maps\n\nronald r. coifman \u2217, st\u00e9phane lafon 1\n\nmathematics department, yale university, new haven, ct 06520, usa\n\nreceived 29 october 2004; revised 19 march 2006; accepted 2 april 2006\n\navailable online 19 june 2006\n\ncommunicated by the editors\n\nabstract\n\nin this paper, we provide a framework based upon diffusion processes for \ufb01nding meaningful geometric descriptions of data sets.\nwe show that eigenfunctions of markov matrices can be used to construct coordinates called diffusion maps that generate ef\ufb01cient\nrepresentations of complex geometric structures. the associated family of diffusion distances, obtained by iterating the markov\nmatrix, de\ufb01nes multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction.\nthe proposed framework relates the spectral properties of markov processes to their geometric counterparts and it uni\ufb01es ideas\narising in a variety", "equivariance through parameter-sharing\n\nsiamak ravanbakhsh 1 jeff schneider 1 barnab\u00b4as p\u00b4oczos 1\n\nabstract\n\nwe propose to study equivariance in deep neu-\nral networks through parameter symmetries. in\nparticular, given a group (cid:71) that acts discretely\non the input and output of a standard neural net-\n\nwork layer \u03c6w\u2236 (cid:82)m \u2192 (cid:82)n , we show that \u03c6w\n\nis equivariant with respect to (cid:71)-action iff (cid:71) ex-\nplains the symmetries of the network parameters\nw. inspired by this observation, we then pro-\npose two parameter-sharing schemes to induce\nthe desirable symmetry on w. our procedure\nfor tying the parameters achieves (cid:71)-equivariance\nand, under some conditions on the action of (cid:71),\nit guarantees sensitivity to all other permutation\ngroups outside (cid:71).\n\ngiven enough training data, a multi-layer perceptron would\neventually learn the domain invariances in a classi\ufb01cation\ntask. nevertheless, success of convolutional and recurrent\nnetworks suggests that enc", "\f", "journal of machine learning research 21 (2020) 1-34\n\nsubmitted 7/19; revised 3/20; published 6/20\n\na uni\ufb01ed framework of online learning algorithms for\n\ntraining recurrent neural networks\n\nowen marschall\ncenter for neural science\nnew york university\nnew york, ny 10003, usa\nkyunghyun cho\u2217\nnew york university\ncifar azrieli global scholar\n\ncristina savin\ncenter for neural science\ncenter for data science\nnew york university\n\neditor: yoshua bengio\n\noem214@nyu.edu\n\nkyunghyun.cho@nyu.edu\n\ncsavin@nyu.edu\n\nabstract\n\nwe present a framework for compactly summarizing many recent results in e\ufb03cient and/or\nbiologically plausible online training of recurrent neural networks (rnn). the framework\norganizes algorithms according to several criteria: (a) past vs. future facing, (b) tensor\nstructure, (c) stochastic vs. deterministic, and (d) closed form vs. numerical. these axes\nreveal latent conceptual connections among several recent advances in online learning.\nfurthermore, we provide novel mathematical", "multi-column deep neural networks for image classi\ufb01cation\n\ndan cires\u00b8an, ueli meier and j\u00a8urgen schmidhuber\n\nidsia-usi-supsi\n\ngalleria 2, 6928 manno-lugano, switzerland\n\n{dan,ueli,juergen}@idsia.ch\n\nabstract\n\ntraditional methods of computer vision and machine\nlearning cannot match human performance on tasks such\nas the recognition of handwritten digits or traf\ufb01c signs. our\nbiologically plausible, wide and deep arti\ufb01cial neural net-\nwork architectures can. small (often minimal) receptive\n\ufb01elds of convolutional winner-take-all neurons yield large\nnetwork depth, resulting in roughly as many sparsely con-\nnected neural layers as found in mammals between retina\nand visual cortex. only winner neurons are trained. sev-\neral deep neural columns become experts on inputs pre-\nprocessed in different ways; their predictions are averaged.\ngraphics cards allow for fast training. on the very com-\npetitive mnist handwriting benchmark, our method is the\n\ufb01rst to achieve near-human performance. on a traf", "article\nhigh-dimensional geometry of \npopulation responses in visual cortex\n\ncarsen stringer1,2,6*, marius pachitariu1,3,6*, nicholas steinmetz3,5, matteo carandini4,7 & kenneth d. harris3,7*\n\nhttps://doi.org/10.1038/s41586-019-1346-5\n\na neuronal population encodes information most efficiently when its stimulus responses are high-dimensional and \nuncorrelated, and most robustly when they are lower-dimensional and correlated. here we analysed the dimensionality of \nthe encoding of natural images by large populations of neurons in the visual cortex of awake mice. the evoked population \nactivity was high-dimensional, and correlations obeyed an unexpected power law: the nth principal component variance \nscaled as 1/n. this scaling was not inherited from the power law spectrum of natural images, because it persisted \nafter stimulus whitening. we proved mathematically that if the variance spectrum was to decay more slowly then the \npopulation code could not be smooth, allowing small changes ", "letter\n\ncommunicated by alexandre pouget\n\nmutual information, fisher information, and population\ncoding\n\nnicolas brunel\njean-pierre nadal\nlaboratoire de physique statistique de i\u2019e.n.s.,\u2020 ecole normale sup\u00b4erieure, 75231\nparis cedex 05, france\n\nin the context of parameter estimation and model selection, it is only quite\nrecently that a direct link between the fisher information and information-\ntheoretic quantities has been exhibited. we give an interpretation of this\nlink within the standard framework of information theory. we show that\nin the context of population coding, the mutual information between the\nactivity of a large array of neurons and a stimulus to which the neurons\nare tuned is naturally related to the fisher information. in the light of this\nresult, we consider the optimization of the tuning curves parameters in\nthe case of neurons responding to a stimulus represented by an angular\nvariable.\n\n1 introduction\n\na natural framework to study how neurons communicate, or trans", "biorxiv preprint \nthe copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is \n\nhttps://doi.org/10.1101/2020.06.15.148114\n; \n\nthis version posted june 17, 2020. \n\ndoi: \n\nmade available under a\n\ncc-by-nc-nd 4.0 international license\n.\n\nuntangling stability and gain modulation in cortical circuits with\n\nmultiple interneuron classes\n\nhannah bos1,2, anne-marie oswald2,3,4, and brent doiron1,2,4,5\n\n1department of mathematics, university of pittsburgh, pittsburgh, pa, usa\n\n2center for the neural basis of cognition, pittsburgh, pa, usa\n\n3department of neuroscience, university of pittsburgh, pittsburgh, pa, usa\n\n4department of neurobiology, university of chicago, chicago, il, usa\n\n5department of statistics, university of chicago, chicago, il, usa\n\nabstract\n\nsynaptic inhibition is the mechanistic backbone of a suite of cortical functions, not the least of which\nis maintaining", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/232105950\n\npsychological and neuroscienti\ufb01c connections with reinforcement learning\n\nchapter \u00b7 january 2012\n\ndoi: 10.1007/978-3-642-27645-3_16\n\ncitations\n11\n\n1 author:\n\nashvin shah\nthe university of sheffield\n\n16 publications\u00a0\u00a0\u00a0182 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n321\n\nall content following this page was uploaded by ashvin shah on 16 may 2014.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "4. w. j. gehring, m. g. h. coles, d. e. meyer, e. donchin,\n\npsychophysiology 27, s34 (1990).\n\n20. d. boussaoud, s. p. wise, exp. brain res. 95, 28 (1993).\n21. m. m. botvinick, t. s. braver, d. m. barch, c. s. carter,\n\n5. j. hohnsbein, m. falkenstein, j. hoorman, j. psycho-\n\nj. c. cohen, psychol. rev. 108, 624 (2001).\n\nphysiol. 3, 32 (1989).\n\n6. p. s. bernstein, m. k. scheffers, m. g. h. coles, j. exp.\n\npsychol. hum. percept. perform. 21, 1312 (1995).\n\n22. a. d. jones, r. cho, l. e. nystrom, j. d. cohen, t. s.\nbraver, cogn. affect. behav. neurosci. 2, 300 (2002).\n23. k. r. ridderinkhof, m. ullsperger, e. a. crone, s.\n\ngo activity was numerically greater than low/go in 288\nof 291 voxels (binomial test, p\n0.00001). high and\nlow activities also apparently diverged with training\nas expected, but this did not reach significance.\n\ng\n\n26. r. herrnstein, j. exp. anal. behav. 4, 267 (1961).\n27. l. p. sugrue, g. s. corrado, w. t. newsome, science\n\n7. s. ito, v. stuphorn, j. w. brown, j. d. schall", "article\n\ndoi:10.1038/nature10835\n\ngain control by layer six in cortical\ncircuits of vision\n\nshawn r. olsen1*, dante s. bortone1*, hillel adesnik1 & massimo scanziani1\n\nafter entering the cerebral cortex, sensory information spreads through six different horizontal neuronal layers that are\ninterconnected by vertical axonal projections. it is believed that through these projections layers can influence each\nother\u2019s response to sensory stimuli, but the specific role that each layer has in cortical processing is still poorly\nunderstood. here we show that layer six in the primary visual cortex of the mouse has a crucial role in controlling the\ngain of visually evoked activity in neurons of the upper layers without changing their tuning to orientation. this gain\nmodulation results from the coordinated action of layer six intracortical projections to superficial layers and deep\nprojections to the thalamus, with a substantial role of the intracortical circuit. this study establishes layer six ", "simultaneous selection by object-based attention in\nvisual and frontal cortex\n\narezoo pooresmaeilia,1, jasper poorta,2, and pieter r. roelfsemaa,b,c,3\n\nathe netherlands institute for neuroscience, royal netherlands academy of arts and sciences, 1105 ba, amsterdam, the netherlands; bdepartment of\nintegrative neurophysiology, centre for neurogenomics and cognitive research, vu university amsterdam, 1081 hv, amsterdam, the netherlands;\nand cpsychiatry department, academic medical centre, university of amsterdam, 1105 az, amsterdam, the netherlands\n\nedited by michael e. goldberg, columbia university college of physicians and surgeons, new york, ny, and approved march 12, 2014 (received for review\nnovember 15, 2013)\n\nmodels of visual attention hold that top-down signals from frontal\ncortex influence information processing in visual cortex.\nit is\nunknown whether situations exist in which visual cortex actively\nparticipates in attentional selection. to investigate this question,\nwe simultaneo", "7\n1\n0\n2\n\n \nr\na\n\nm\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n8\n0\n6\n8\n0\n\n.\n\n2\n0\n7\n1\n:\nv\ni\nx\nr\na\n\ntowards a rigorous science of interpretable machine learning\n\nfinale doshi-velez\u2217 and been kim\u2217\n\nfrom autonomous cars and adaptive email-\ufb01lters to predictive policing systems, machine learn-\ning (ml) systems are increasingly ubiquitous; they outperform humans on speci\ufb01c tasks [mnih\net al., 2013, silver et al., 2016, hamill, 2017] and often guide processes of human understanding\nand decisions [carton et al., 2016, doshi-velez et al., 2014]. the deployment of ml systems in\ncomplex applications has led to a surge of interest in systems optimized not only for expected\ntask performance but also other important criteria such as safety [otte, 2013, amodei et al., 2016,\nvarshney and alemzadeh, 2016], nondiscrimination [bostrom and yudkowsky, 2014, ruggieri et al.,\n2010, hardt et al., 2016], avoiding technical debt [sculley et al., 2015], or providing the right to\nexplanation [goodman and flaxman, 2016]. ", "an instrumental variable method for point processes:\n\ngeneralised wald estimation based on deconvolution\n\nzhichao jiang\u2217\u00a7\n\nshizhe chen\u2020\u00a7\n\npeng ding\u2021\n\njanuary 10, 2023\n\nabstract\n\npoint processes are probabilistic tools for modeling event data. while there exists a fast-\n\ngrowing literature studying the relationships between point processes, it remains unexplored how\n\nsuch relationships connect to causal e\ufb00ects. in the presence of unmeasured confounders, param-\n\neters from point process models do not necessarily have causal interpretations. we propose an\n\ninstrumental variable method for causal inference with point process treatment and outcome. we\n\nde\ufb01ne causal quantities based on potential outcomes and establish nonparametric identi\ufb01cation\n\nresults with a binary instrumental variable. we extend the traditional wald estimation to deal\n\nwith point process treatment and outcome, showing that it should be performed after a fourier\n\ntransform of the intention-to-treat e\ufb00ects on the treatmen", "neuron\n\nreview\n\nneuromodulation of spike-timing-dependent\nplasticity: past, present, and future\n\nzuzanna brzosko,1,2,3 susanna b. mierau,1,2 and ole paulsen1,*\n1department of physiology, development and neuroscience, university of cambridge, cambridge cb2 3eg, uk\n2these authors contributed equally\n3present address: sixfold bioscience ltd, translation and innovation hub, london w12 0bz, uk\n*correspondence: op210@cam.ac.uk\nhttps://doi.org/10.1016/j.neuron.2019.05.041\n\nspike-timing-dependent synaptic plasticity (stdp) is a leading cellular model for behavioral learning and\nmemory with rich computational properties. however, the relationship between the millisecond-precision\nspike timing required for stdp and the much slower timescales of behavioral learning is not well understood.\nneuromodulation offers an attractive mechanism to connect these different timescales, and there is now\nstrong experimental evidence that stdp is under neuromodulatory control by acetylcholine, monoamines,\nand ot", "1\n2\n0\n2\n\n \nr\na\n\nm\n \n6\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n4\nv\n5\n2\n5\n0\n0\n\n.\n\n0\n1\n0\n2\n:\nv\ni\nx\nr\na\n\na biologically plausible neural network for\n\nmulti-channel canonical correlation analysis\n\ndavid lipshutz\u22171, yanis bahroun\u22171, siavash golkar\u22171,\nanirvan m. sengupta1,2, and dmitri b. chklovskii1,3\n\n1center for computational neuroscience, flatiron institute\n2department of physics and astronomy, rutgers university\n\n3neuroscience institute, nyu medical center\n\nmarch 29, 2021\n\nabstract\n\ncortical pyramidal neurons receive inputs from multiple distinct neural\npopulations and integrate these inputs in separate dendritic compartments.\nwe explore the possibility that cortical microcircuits implement canonical\ncorrelation analysis (cca), an unsupervised learning method that projects\nthe inputs onto a common subspace so as to maximize the correlations between\nthe projections. to this end, we seek a multi-channel cca algorithm that\ncan be implemented in a biologically plausible neural network. for biologi", "7\n1\n0\n2\n\n \nr\na\n\nm\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n2\n1\n7\n0\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\nthe concrete distribution:\na continuous relaxation of\ndiscrete random variables\n\nchris j. maddison1,2, andriy mnih1, & yee whye teh1\n1deepmind, london, united kingdom\n2university of oxford, oxford, united kingdom\ncmaddis@stats.ox.ac.uk\n\nabstract\n\nthe reparameterization trick enables optimizing large scale stochastic computa-\ntion graphs via gradient descent. the essence of the trick is to refactor each\nstochastic node into a differentiable function of its parameters and a random vari-\nable with \ufb01xed distribution. after refactoring, the gradients of the loss propa-\ngated by the chain rule through the graph are low variance unbiased estimators\nof the gradients of the expected loss. while many continuous random variables\nhave such reparameterizations, discrete random variables lack useful reparame-\nterizations due to the discontinuous nature of discrete stat", "discovering neural wirings\n\nmitchell wortsman1,2, ali farhadi1,2,3, mohammad rastegari1,3\n\n1prior @ allen institute for ai, 2university of washington, 3xnor.ai\n\nmitchnw@cs.washington.edu, {ali, mohammad}@xnor.ai\n\n9\n1\n0\n2\n\n \n\nv\no\nn\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n6\n8\n5\n0\n0\n\n.\n\n6\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nthe success of neural networks has driven a shift in focus from feature engineering\nto architecture engineering. however, successful networks today are constructed\nusing a small and manually de\ufb01ned set of building blocks. even in methods of\nneural architecture search (nas) the network connectivity patterns are largely\nconstrained. in this work we propose a method for discovering neural wirings. we\nrelax the typical notion of layers and instead enable channels to form connections\nindependent of each other. this allows for a much larger space of possible networks.\nthe wiring of our network is not \ufb01xed during training \u2013 as we learn the network\nparameters we also learn the structure ", " \n\n \n\nadaptive optimal-control algorithms \n\nfor brainlike networks \n\n \n\n \n\n \n\nby \n\n \n\n \n\n \n\n \n\nlakshminarayan chinta venkateswararao \n\n \n\n \n\n \n\n \n\na thesis submitted in conformity with the requirements \n\nfor the degree of doctor of philosophy \n\ngraduate department of physiology \n\nuniversity of toronto \n\n\u00a9 copyright lakshminarayan chinta venkateswararao 2010 \n\n \n\n \n\n\f", "geometry of abstract learned knowledge in \nthe hippocampus\n\nhttps://doi.org/10.1038/s41586-021-03652-7\nreceived: 5 february 2020\naccepted: 18 may 2021\npublished online: 16 june 2021\n\n check for updates\n\nedward h. nieh1,6, manuel schottdorf1,6, nicolas w. freeman1, ryan j. low1, sam lewallen1, \nsue ann koay1, lucas pinto1,5, jeffrey l. gauthier1, carlos d. brody1,2,3,7\u2009\u2709 & david w. tank1,2,4,7\u2009\u2709\n\nhippocampal neurons encode physical variables1\u20137 such as space1 or auditory \nfrequency6 in cognitive maps8. in addition, functional magnetic resonance imaging \nstudies in humans have shown that the hippocampus can also encode more abstract, \nlearned variables9\u201311. however, their integration into existing neural representations \nof physical variables12,13 is unknown. here, using two-photon calcium imaging, we \nshow that individual neurons in the dorsal hippocampus jointly encode accumulated \nevidence with spatial position in mice performing a decision-making task in virtual \nreality14\u201316. nonlin", "available online at www.sciencedirect.com\n\nsciencedirect\n\noptogenetic approaches for dissecting\nneuromodulation and gpcr signaling in neural circuits\nskylar m spangler1,2,3,4 and michael r bruchas1,2,3,4\n\noptogenetics has revolutionized neuroscience by providing\nmeans to control cell signaling with spatiotemporal control in\ndiscrete cell types. in this review, we summarize four major\nclasses of optical tools to manipulate neuromodulatory gpcr\nsignaling: opsins (including engineered chimeric receptors);\nphotoactivatable proteins; photopharmacology through\ncaging \u2014 photoswitchable molecules; \ufb02uorescent protein based\nreporters and biosensors. additionally, we highlight technologies\nto utilize these tools in vitro and in vivo, including cre dependent\nviral vector expression and two-photon microscopy. these\nemerging techniques targeting speci\ufb01c members of the gpcr\nsignaling pathway offer an expansive base for investigating\ngpcr signaling in behavior and disease states, in addition to\npaving", "article\n\nhttps://doi.org/10.1038/s41467-021-21696-1\n\nopen\n\npredictive learning as a network mechanism for\nextracting low-dimensional latent space\nrepresentations\n\nstefano recanatesi\neric shea-brown1,2,7,8\n\n1\u2709, matthew farrell2, guillaume lajoie\n\n3,4, sophie deneve5, mattia rigotti\n\n6,8 &\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\narti\ufb01cial neural networks have recently achieved many successes in solving sequential\nprocessing and planning tasks. their success is often ascribed to the emergence of the task\u2019s\nlow-dimensional latent structure in the network activity \u2013 i.e., in the learned neural repre-\nsentations. here, we investigate the hypothesis that a means for generating representations\nlatent structure, possibly re\ufb02ecting an underlying\nwith easily accessed low-dimensional\nsemantic organization, is through learning to predict observations about the world. speci\ufb01-\ncally, we ask whether and when network mechanisms for sensory prediction coincide with\nthose for extracting the underlying latent var", "new neural activity patterns emerge with\nlong-term learning\n\nemily r. obya,b,c,d,e, matthew d. golubb,f,g,h, jay a. hennigb,i,j, alan d. degenharta,b,c,d, elizabeth c. tyler-kabaraa,k,l,m,\nbyron m. yub,f,i,n,1, steven m. chaseb,i,n,1, and aaron p. batistaa,b,c,d,1,2\n\nadepartment of bioengineering, university of pittsburgh, pittsburgh, pa 15213; bcenter for the neural basis of cognition, university of pittsburgh and\ncarnegie mellon university, pittsburgh, pa 15213; cuniversity of pittsburgh brain institute, pittsburgh, pa 15213; dsystems neuroscience center, university\nof pittsburgh, pittsburgh, pa 15213; edepartment of neurobiology, university of pittsburgh school of medicine, pittsburgh, pa 15213; fdepartment of\nelectrical and computer engineering, carnegie mellon university, pittsburgh, pa 15213; gdepartment of electrical engineering, stanford university,\nstanford, ca 94305; hwu tsai neurosciences institute, stanford university, stanford, ca 94305; icarnegie mellon neuroscience insti", "contrastive hebbian learning in the  continuous hopfield  model \n\njavier r.  movellan \n\ndepartment of psychology \ncarnegie mellon  university \n\npittsburgh,  pa 15213 \n\nemail:  jm2z+@andrew.cmu.edu \n\nabstract \n\nthis  pape.r  shows  that  contrastive hebbian, \nthe algorithm used in mean field learning, can \nbe applied to any continuous hopfield model. \nthis implies that non-logistic activation func \ntions  as well  as  self connections  are  allowed. \ncontrary  to previous  approaches,  the  learn \ning  algorithm  is  derived  without  consider \ning  it  a  mean field  approximation  to  boltz \nmann  machine learning.  the paper  includes \na  discussion  of the  conditions  under  which \nthe  function  that  contrastive  hebbian  mini~ \nmizes  can  be considered  a proper error func \ntion,  and  an  analysis  of five  different  train \ning regimes.  an  appendix provides  complete \ndemonstrations  and  specific  instructions  on \nhow to implement contrastive hebbian learn \ning  in  inter", "0\n2\n0\n2\n\n \nr\na\n\n \n\nm\n5\n \n \n]\nn\nn\n-\ns\ni\nd\n.\nt\na\nm\n-\nd\nn\no\nc\n[\n \n \n\n2\nv\n0\n9\n8\n0\n0\n\n.\n\n1\n1\n9\n1\n:\nv\ni\nx\nr\na\n\nmean-\ufb01eld inference methods for neural networks\n\nmarylou gabri\u00b4e1,2\n\n1center for data science, new york university\n\n2center for computational mathematics, flatiron institute\n\nabstract\n\nmachine learning algorithms relying on deep neural networks recently allowed a great leap\nforward in arti\ufb01cial intelligence. despite the popularity of their applications, the e\ufb03ciency of\nthese algorithms remains largely unexplained from a theoretical point of view. the mathe-\nmatical description of learning problems involves very large collections of interacting random\nvariables, di\ufb03cult to handle analytically as well as numerically. this complexity is precisely\nthe object of study of statistical physics. its mission, originally pointed towards natural sys-\ntems, is to understand how macroscopic behaviors arise from microscopic laws. mean-\ufb01eld\nmethods are one type of approximation strategy developed i", "0\n2\n0\n2\n\n \nc\ne\nd\n1\n1\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n8\n7\n8\n2\n1\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\ndirect feedback alignment scales to\n\nmodern deep learning tasks and architectures\n\njulien launay1,2\n1lighton\n\nfran\u00e7ois boniface1\n\niacopo poli1\n2lpens, \u00e9cole normale sup\u00e9rieure\n{firstname}@lighton.ai\n\nflorent krzakala1,2,3\n3 idephics, epfl\n\nlair.lighton.ai/dfa-scales\n\nabstract\n\ndespite being the workhorse of deep learning, the backpropagation algorithm is\nno panacea. it enforces sequential layer updates, thus preventing ef\ufb01cient paral-\nlelization of the training process. furthermore, its biological plausibility is being\nchallenged. alternative schemes have been devised; yet, under the constraint of\nsynaptic asymmetry, none have scaled to modern deep learning tasks and architec-\ntures. here, we challenge this perspective, and study the applicability of direct\nfeedback alignment (dfa) to neural view synthesis, recommender systems, geo-\nmetric learning, and natural language processing. in contrast w", "biorxiv preprint \n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\nhttps://doi.org/10.1101/2020.11.02.365072\n; \n\nthis version posted november 4, 2020. \n\nthe copyright holder for this preprint\n\ndoi: \n\navailable under a\n\ncc-by 4.0 international license\n.\n\nstrong coupling and local control of\ndimensionality across brain areas\n\ndavid dahmen1,*, stefano recanatesi2,*, gabriel k. ocker3,4, xiaoxuan jia3, moritz helias1,5,+, and eric shea-brown2,3,+\n\n1institute of neuroscience and medicine (inm-6 and inm-10) and institute for advanced simulation (ias-6), j\u00fclich research centre, j\u00fclich, germany\n\n2university of washington center for computational neuroscience and swartz center for theoretical neuroscience, seattle, wa, usa\n\n3allen institute, seattle, wa, usa\n\n4department of mathematics and statistics, boston university, ma, usa\n\n5department of physics, faculty 1, rwth aachen university, aachen, g", "i an update to this article is included at the end\n\nneuropeptides 47 (2013) 439\u2013450\n\ncontents lists available at sciencedirect\n\nneuropeptides\n\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / n p e p\n\nneuropeptides in learning and memory\n\n\u00e9va borb\u00e9ly, b\u00e1lint scheich, zsuzsanna helyes\n\n\u21d1\n\ndepartment of pharmacology and pharmacotherapy, faculty of medicine, university of p\u00e9cs, szigeti u. 12, h-7624 p\u00e9cs, hungary\nmolecular pharmacology research group, j\u00e1nos szent\u00e1gothai research center, university of p\u00e9cs, ifj\u00fas\u00e1g \u00fatja 20, h-7624 p\u00e9cs, hungary\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 2 september 2013\naccepted 14 october 2013\navailable online 24 october 2013\n\nkeywords:\ntachykinins\nsomatostatin\nopioid peptides\ncgrp\nvip/pacap\nnpy\ngalanin\nanimal models\n\ncontents\n\ndementia conditions and memory de\ufb01cits of different origins (vascular, metabolic and primary neurode-\ngenerative such as alzheimer\u2019s and parkinson\u2019s diseases) are getting more", "neuroimage 80 (2013) 80\u2013104\n\ncontents lists available at sciverse sciencedirect\n\nneuroimage\n\nj o u r n a l h o m e p a ge : w ww . e l s e v i e r . c o m/ l o c a t e / y n i m g\n\npushing spatial and temporal resolution for functional and diffusion mri\nin the human connectome project\nkamil u\u011furbil a,\u204e, junqian xu a,b, edward j. auerbach a, steen moeller a, an t. vu a, julio m. duarte-carvajalino a,\nchristophe lenglet a, xiaoping wu a, sebastian schmitter a, pierre francois van de moortele a, john strupp a,\nguillermo sapiro a,c, federico de martino a,d, dingxin wang a,e, noam harel a, michael garwood a,\nliyong chen f,g, david a. feinberg f,g, stephen m. smith h, karla l. miller h, stamatios n. sotiropoulos h,\nsaad jbabdi h, jesper l.r. andersson h, timothy e.j. behrens h,i, matthew f. glasser j,\ndavid c. van essen j, essa yacoub a\nfor the wu-minn hcp consortium\na center for magnetic resonance research (cmrr), university of minnesota, minneapolis, mn, usa\nb translational and molecular i", "learning continuous control policies by\n\nstochastic value gradients\n\nnicolas heess\u21e4, greg wayne\u21e4, david silver, timothy lillicrap, yuval tassa, tom erez\n\ngoogle deepmind\n\n{heess, gregwayne, davidsilver, countzero, tassa, etom}@google.com\n\n\u21e4these authors contributed equally.\n\nabstract\n\nwe present a uni\ufb01ed framework for learning continuous control policies using\nbackpropagation.\nit supports stochastic control by treating stochasticity in the\nbellman equation as a deterministic function of exogenous noise. the product\nis a spectrum of general policy gradient algorithms that range from model-free\nmethods with value functions to model-based methods without value functions.\nwe use learned models but only require observations from the environment in-\nstead of observations from model-predicted trajectories, minimizing the impact\nof compounded model errors. we apply these algorithms \ufb01rst to a toy stochastic\ncontrol problem and then to several physics-based control problems in simulation.\none of", "journal of machine learning research 15 (2014) 1929-1958\n\nsubmitted 11/13; published 6/14\n\ndropout: a simple way to prevent neural networks from\n\nover\ufb01tting\n\nnitish srivastava\ngeo\ufb00rey hinton\nalex krizhevsky\nilya sutskever\nruslan salakhutdinov\ndepartment of computer science\nuniversity of toronto\n10 kings college road, rm 3302\ntoronto, ontario, m5s 3g4, canada.\n\neditor: yoshua bengio\n\nnitish@cs.toronto.edu\nhinton@cs.toronto.edu\nkriz@cs.toronto.edu\nilya@cs.toronto.edu\nrsalakhu@cs.toronto.edu\n\nabstract\n\ndeep neural nets with a large number of parameters are very powerful machine learning\nsystems. however, over\ufb01tting is a serious problem in such networks. large networks are also\nslow to use, making it di\ufb03cult to deal with over\ufb01tting by combining the predictions of many\ndi\ufb00erent large neural nets at test time. dropout is a technique for addressing this problem.\nthe key idea is to randomly drop units (along with their connections) from the neural\nnetwork during training. this prevents units f", "supporting information\nyamins et al. 10.1073/pnas.1403112111\nsi text\ndata collection. we collected neural data, assessed human be-\nhavior, and tested models on a common image set. in this section,\nwe discuss this image set and the data collection methods used.\narray electrophysiology. neural data were collected in the visual\ncortex of two awake behaving rhesus macaques (macaca mulatta,\n7 and 9 kg) using parallel multielectrode array electrophysiology\nrecording systems (cerebus system; blackrock microsystems).\nall procedures were done in accordance with national institute\nof health guidelines and approved by the massachusetts institute\nof technology (mit) committee on animal care guidelines. six\n96-electrode arrays (3 arrays each in two monkeys) were surgically\nimplanted in anatomically determined v4, posterior inferior tem-\nporal (it), central it, and anterior it regions (1). of these, 296\nneural sites (168 in it and 128 in v4) were selected as being vi-\nsually driven with a separate i", "learning to represent continuous variables in\nheterogeneous neural networks\n\narticle\n\ngraphical abstract\n\nauthors\n\nran darshan, alexander rivkind\n\ncorrespondence\ndarshanr@hhmi.org (r.d.),\nalexander.rivkind@weizmann.ac.il (a.r.)\n\nin brief\ncontinuous features are ubiquitous in an\nanimal\u2019s life. they are hypothesized to be\nrepresented by neural manifold\nattractors. darshan and rivkind propose\na theory of manifold attractors with\nbiologically plausible asymmetric\nconnectome and neural representation.\nthey predict whether continuous memory\nsurvives asymmetry and what the\nfunctional implications of asymmetries\nare.\n\nhighlights\nd a learned manifold attractor theory predicts their stability and\n\nfunctional properties\n\nd neural activity is not restricted by symmetry; connectivity can\n\nbe heterogeneous\n\nd approximate continuity emerges quickly from a small number\n\nof samples\n\nd both asymmetry and heterogeneity affect functionality but do\n\nnot ruin an attractor\n\ndarshan & rivkind, 2022, cell repo", "biologically-plausible backpropagation through\narbitrary timespans via local neuromodulators\n\nyuhan helena liu1,2,3,*, stephen smith2,4, stefan mihalas1,2,3, eric shea-brown1,2,3, and uygar\n\ns\u00fcmb\u00fcl2,*\n\n1department of applied mathematics, university of washington, seattle, wa, usa\n\n2allen institute for brain science, 615 westlake ave n, seattle wa, usa\n\n3computational neuroscience center, university of washington, seattle, wa, usa\n\n4department of molecular and cellular physiology, stanford university, stanford ca, usa\n\n*correspondence: hyliu24@uw.edu, uygars@alleninstitute.org\n\nabstract\n\nthe spectacular successes of recurrent neural network models where key parame-\nters are adjusted via backpropagation-based gradient descent have inspired much\nthought as to how biological neuronal networks might solve the corresponding\nsynaptic credit assignment problem [1\u20133]. there is so far little agreement, however,\nas to how biological networks could implement the necessary backpropagation\nthrough t", "j. physiol. (1980), 302, pp. 463-482\nwith 8 text-figure8\nprinted in great britain\n\n463\n\npossible mechanisms for long-lasting potentiation of\n\nsynaptic transmission in hippocampal slices from\n\nguinea-pigs\n\nby p. andersen*, s. h. sundbergt, 0. sveent, j. w. swann\u00a7\n\nand h. wigstromii\n\nfrom the *institute of neurophysiology, university of oslo, norway and\n\nthe ildepartment of physiology, university of goteborg, sweden\n\n(received 28 june 1979)\n\nsummary\n\n1. long-lasting potentiation of synaptic transmission was studied in the cai\n\nregion of guinea-pig hippocampal slices maintained in vitro.\n\n2. stimulating pulses were delivered alternately to two independent afferent\npathways, stratum radiatum and stratum oriens. the presynaptic volleys and field\ne.p.s.p.s. were recorded from the same two layers, while an electrode in the pyramidal\ncell body layer recorded the population spike or in other experiments the extra- or\nintracellular potentials from a single pyramidal cell.\n\n3. a short tetanus to ", "siam j. control optim.\nvol. 40, no. 3, pp. 681\u2013698\n\nc(cid:2) 2001 society for industrial and applied mathematics\n\nlearning algorithms for markov decision processes\n\nwith average cost\u2217\n\nj. abounadi\u2020 , d. bertsekas\u2020 , and v. s. borkar\u2021\n\nabstract. this paper gives the \ufb01rst rigorous convergence analysis of analogues of watkins\u2019s\nq-learning algorithm, applied to average cost control of \ufb01nite-state markov chains. we discuss two\nalgorithms which may be viewed as stochastic approximation counterparts of two existing algorithms\nfor recursively computing the value function of the average cost problem\u2014the traditional relative\nvalue iteration (rvi) algorithm and a recent algorithm of bertsekas based on the stochastic shortest\npath (ssp) formulation of the problem. both synchronous and asynchronous implementations are\nconsidered and analyzed using the ode method. this involves establishing asymptotic stability of\nassociated ode limits. the ssp algorithm also uses ideas from two-time-scale stochasti", "credit  assignment through time: \nalternatives  to backpropagation \n\nyoshua bengio * \ndept.  informatique et \n\nrecherche  operationnelle \nuniversite de  montreal \nmontreal,  qc  h3c-3j7 \n\npaolo frasconi \n\ndip.  di sistemi e  informatica \n\nuniversita di  firenze \n50139  firenze  (italy) \n\nabstract \n\nlearning  to  recognize  or  predict  sequences  using  long-term  con \ntext  has  many  applications.  however,  practical  and  theoretical \nproblems  are  found  in  training  recurrent  neural  networks  to  per \nform tasks in which input/output dependencies span long intervals. \nstarting from a mathematical analysis of the  problem, we  consider \nand compare alternative algorithms and  architectures  on  tasks for \nwhich the span of the input/output dependencies can be controlled. \nresults  on the new  algorithms show  performance qualitatively su \nperior  to that obtained with backpropagation. \n\n1 \n\nintroduction \n\nrecurrent neural networks have been considered to learn to map input seq", "5\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n2\n2\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n9\n7\n5\n6\n0\n\n.\n\n6\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nunderstanding neural networks through deep visualization\n\njason yosinski\ncornell university\njeff clune\nanh nguyen\nuniversity of wyoming\nthomas fuchs\njet propulsion laboratory, california institute of technology\nhod lipson\ncornell university\n\nyosinski@cs.cornell.edu\n\njeffclune@uwyo.edu\nanguyen8@uwyo.edu\n\nfuchs@caltech.edu\n\nhod.lipson@cornell.edu\n\nabstract\n\nrecent years have produced great advances in\ntraining large, deep neural networks (dnns), in-\ncluding notable successes in training convolu-\ntional neural networks (convnets) to recognize\nnatural images. however, our understanding of\nhow these models work, especially what compu-\ntations they perform at intermediate layers, has\nlagged behind. progress in the \ufb01eld will be\nfurther accelerated by the development of bet-\nter tools for visualizing and interpreting neural\nnets. we introduce two such tools here. the\n\ufb01rst is a tool that visualizes the acti", "deep unsupervised learning using\nnonequilibrium thermodynamics\n\n5\n1\n0\n2\n\n \n\nv\no\nn\n8\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n8\nv\n5\n8\n5\n3\n0\n\n.\n\n3\n0\n5\n1\n:\nv\ni\nx\nr\na\n\njascha sohl-dickstein\nstanford university\neric a. weiss\nuniversity of california, berkeley\nniru maheswaranathan\nstanford university\nsurya ganguli\nstanford university\n\njascha@stanford.edu\n\neaweiss@berkeley.edu\n\nnirum@stanford.edu\n\nsganguli@stanford.edu\n\nabstract\n\na central problem in machine learning involves\nmodeling complex data-sets using highly \ufb02exi-\nble families of probability distributions in which\nlearning, sampling,\ninference, and evaluation\nare still analytically or computationally tractable.\nhere, we develop an approach that simultane-\nously achieves both \ufb02exibility and tractability.\nthe essential idea, inspired by non-equilibrium\nstatistical physics, is to systematically and slowly\ndestroy structure in a data distribution through\nan iterative forward diffusion process. we then\nlearn a reverse diffusion process that restores\ns", "sergey zagoruyko and nikos komodakis: wide residual networks\n\n1\n\nwide residual networks\n\nsergey zagoruyko\nsergey.zagoruyko@enpc.fr\nnikos komodakis\nnikos.komodakis@enpc.fr\n\nuniversit\u00e9 paris-est, \u00e9cole des ponts\nparistech\nparis, france\n\nabstract\n\ndeep residual networks were shown to be able to scale up to thousands of layers\nand still have improving performance. however, each fraction of a percent of improved\naccuracy costs nearly doubling the number of layers, and so training very deep resid-\nual networks has a problem of diminishing feature reuse, which makes these networks\nvery slow to train. to tackle these problems, in this paper we conduct a detailed exper-\nimental study on the architecture of resnet blocks, based on which we propose a novel\narchitecture where we decrease depth and increase width of residual networks. we call\nthe resulting network structures wide residual networks (wrns) and show that these are\nfar superior over their commonly used thin and very deep counterparts. ", "graph networks as learnable physics engines for inference and control\n\nalvaro sanchez-gonzalez 1 nicolas heess 1 jost tobias springenberg 1 josh merel 1 martin riedmiller 1\n\nraia hadsell 1 peter battaglia 1\n\n8\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n4\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n4\n2\n1\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nunderstanding and interacting with everyday\nphysical scenes requires rich knowledge about\nthe structure of the world, represented either im-\nplicitly in a value or policy function, or explic-\nitly in a transition model. here we introduce a\nnew class of learnable models\u2014based on graph\nnetworks\u2014which implement an inductive bias\nfor object- and relation-centric representations of\ncomplex, dynamical systems. our results show\nthat as a forward model, our approach supports\naccurate predictions from real and simulated data,\nand surprisingly strong and ef\ufb01cient generaliza-\ntion, across eight distinct physical systems which\nwe varied parametrically and structurally. we\nalso found that our infere", "biorxiv preprint \nthe copyright holder for this preprint (which was\nnot certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available \n\nthis version posted december 18, 2019. \n\nhttps://doi.org/10.1101/564476\n; \n\ndoi: \n\nunder a\n\ncc-by-nc-nd 4.0 international license\n.\n\nrecurrent neural networks learn robust representations by\n\ndynamically balancing compression and expansion\n\nmatthew farrell,1, 2 stefano recanatesi,1, 3 timothy\nmoore,1 guillaume lajoie,4, 5 and eric shea-brown1, 2\n\n1computational neuroscience center, university of washington\n2department of applied mathematics, university of washington\n\n3department of physiology and biophysics, university of washington\n4department of mathematics and statistics, universit\u00b4e de montr\u00b4eal\n\n5mila\u2014qu\u00b4ebec ai institute\n\nabstract\n\nrecordings of neural circuits in the brain reveal extraordinary dynamical richness and high vari-\nability. at the same time, dimensionalit", "article\nrapid signalling in distinct dopaminergic \naxons during locomotion and reward\n\ndoi:10.1038/nature18942\n\nm. w. howe1 & d. a. dombeck1\n\ndopaminergic projection axons from the midbrain to the striatum are crucial for motor control, as their degeneration \nin parkinson disease results in profound movement deficits. paradoxically, most recording methods report rapid phasic \ndopamine signalling (~100-ms bursts) in response to unpredicted rewards, with little evidence for movement-related \nsignalling. the leading model posits that phasic signalling in striatum-targeting dopamine neurons drives reward-based \nlearning, whereas slow variations in firing (tens of seconds to minutes) in these same neurons bias animals towards \nor away from movement. however, current methods have provided little evidence to support or refute this model. \nhere, using new optical recording methods, we report the discovery of rapid phasic signalling in striatum-targeting \ndopaminergic axons that is associated w", "research articles\n\nstrates are processed in mfas by individual full\nsets of active sites, according to the path of acp\ndescribed above. however, these studies have\nalso shown that a minority of substrates can be\nshuttled between the two sets of active sites,\neither by acp serving both mat domains or by\ndirect interaction of acp with both ks domains\n(6, 60\u201362). in light of the large 135 \u00e5 distance\nbetween the acp anchor point located in one\ncatalytic cleft and the mat in the other, the most\nplausible explanation for the minor mode-of-\ndomain interaction is a large-scale rotation of the\nupper portion of mfas, relative to the lower\nportion (fig. s4).\n\nthe molecular description of active sites in\nmfas should stimulate the development of\nimproved inhibitors as anticancer drug candi-\ndates. as demonstrated by structural homology,\nthis structure is also a good template for the\norganization of pks modules; it agrees with and\nextends present\ntheoretical models of pks\narchitecture (19, 22). furt", "learning to detect salient objects with image-level supervision\n\nlijun wang1, huchuan lu1, yifan wang1, mengyang feng1\n\ndong wang1, baocai yin1, and xiang ruan2\n\n1 dalian university of technology, china 2 tiwaki co., ltd\n\ne-mails: wlj@mail.dlut.edu.cn lhchuan@dlut.edu.cn\n\nabstract\n\ndeep neural networks (dnns) have substantially im-\nproved the state-of-the-art in salient object detection. how-\never, training dnns requires costly pixel-level annotations.\nin this paper, we leverage the observation that image-\nlevel tags provide important cues of foreground salient ob-\njects, and develop a weakly supervised learning method for\nsaliency detection using image-level tags only. the fore-\nground inference network (fin) is introduced for this chal-\nlenging task. in the \ufb01rst stage of our training method, fin is\njointly trained with a fully convolutional network (fcn) for\nimage-level tag prediction. a global smooth pooling layer\nis proposed, enabling fcn to assign object category tags to\ncorrespon", "the journal of neuroscience, october 24, 2007 \u2022 27(43):11573\u201311586 \u2022 11573\n\nbehavioral/systems/cognitive\n\nefferent association pathways from the rostral prefrontal\ncortex in the macaque monkey\n\nmichael petrides1,2 and deepak n. pandya3,4,5\n1montreal neurological institute, mcgill university, montreal, quebec, canada h3a 2b4, 2department of psychology, mcgill university, montreal, quebec,\ncanada h3a 1b1, 3departments of anatomy and neurology, boston university school of medicine, boston, massachusetts 02118, 4department of\nneurology, beth israel deaconess medical center, boston, massachusetts 02215, and 5edith nourse rogers memorial veterans hospital, bedford,\nmassachusetts 01730\n\nthe different prefrontal cortical regions exert executive control over processing occurring in posterior cortical regions. we examined\nwith the autoradiographic method, in the macaque monkey, the course and terminations of the efferent corticocortical connections of the\nrostral prefrontal region, the function ", "neuron\n\narticle\n\nintracellular determinants of hippocampal\nca1 place and silent cell activity\nin a novel environment\n\nje\u00b4 ro\u02c6 me epsztein,1,2,3,4 michael brecht,1 and albert k. lee1,5,*\n1bernstein center for computational neuroscience, humboldt university, berlin 10115, germany\n2institut de neurobiologie de la me\u00b4 diterrane\u00b4 e, marseille 13273, france\n3institut national de la sante\u00b4 et de la recherche me\u00b4 dicale u901, marseille 13273, france\n4universite\u00b4 de la me\u00b4 diterrane\u00b4 e aix-marseille ii, umr s901, marseille 13273, france\n5howard hughes medical institute, janelia farm research campus, ashburn, va 20147, usa\n*correspondence: leea@janelia.hhmi.org\ndoi 10.1016/j.neuron.2011.03.006\n\nsummary\n\nfor each environment a rodent has explored, its\nhippocampus contains a map consisting of a unique\nsubset of neurons, called place cells, that have\nspatially tuned spiking there, with the remaining\nneurons being essentially silent. using whole-cell\nrecording in freely moving rats exploring a novel", "the intrinsic attractor manifold and population \ndynamics of a canonical cognitive circuit across \nwaking and sleep\n\nrishidev chaudhuri\u200a\n\n\u200a1,2,3,4*, berk ger\u00e7ek\u200a\n\n\u200a1,5,9, biraj pandey1,6,9, adrien peyrache\u200a\n\n\u200a7 and ila fiete\u200a\n\n\u200a1,8*\n\nneural circuits construct distributed representations of key variables\u2014external stimuli or internal constructs of quantities rel-\nevant for survival, such as an estimate of one\u2019s location in the world\u2014as vectors of population activity. although population \nactivity vectors may have thousands of entries (dimensions), we consider that they trace out a low-dimensional manifold whose \ndimension and topology match the represented variable. this manifold perspective enables blind discovery and decoding of the \nrepresented variable using only neural population activity (without knowledge of the input, output, behavior or topography). \nwe characterize and directly visualize manifold structure in the mammalian head direction circuit, revealing that the states \nform", "generalized multidimensional scaling: a framework\nfor isometry-invariant partial surface matching\n\nalexander m. bronstein, michael m. bronstein, and ron kimmel\u2020\n\ndepartment of computer science, technion israel institute of technology, haifa 32000, israel\n\nedited by alexandre j. chorin, university of california, berkeley, ca, and approved december 5, 2005 (received for review october 3, 2005)\n\nan ef\ufb01cient algorithm for isometry-invariant matching of surfaces\nis presented. the key idea is computing the minimum-distortion\nmapping between two surfaces. for this purpose, we introduce the\ngeneralized multidimensional scaling, a computationally ef\ufb01cient\ncontinuous optimization algorithm for \ufb01nding the least distortion\nembedding of one surface into another. the generalized multidi-\nmensional scaling algorithm allows for both full and partial surface\nmatching. as an example, it is applied to the problem of expres-\nsion-invariant three-dimensional face recognition.\n\ngromov\u2013hausdorff distance \u5169 i", "t\u0006ai\u0002i\u0002g \b\u0006\u0003d\tc\b\u0007 \u0003f ex\u0004e\u0006\b\u0007 by \u0005i\u0002i\u0001izi\u0002g c\u0003\u0002\b\u0006a\u0007\bive\n\ndive\u0006ge\u0002ce\n\ngc\u0006u tr 2000\t004\n\nge\u0003(cid:11)\u0006ey e. \u0000i\u0002\b\u0003\u0002\n\nga\b\u0007by c\u0003\u0001\u0004\t\ba\bi\u0003\u0002a\u0000 \u0006e\t\u0006\u0003\u0007cie\u0002ce u\u0002i\b\n\nu\u0002ive\u0006\u0007i\by c\u0003\u0000\u0000ege \u0004\u0003\u0002d\u0003\u0002\n\n17 \t\tee\u0002 s\u0005\ta\u0006e\b \u0004\u0003\u0002d\u0003\u0002 wc1\u0006 3ar\b u.\u0003.\n\nh\b\b\u0004://www.ga\b\u0007by.\tc\u0000.ac.\tk/\n\nab\u0007\b\u0006ac\b\n\n\u0001\b i\u0007 \u0004\u0003\u0007\u0007ib\u0000e \b\u0003 c\u0003\u0001bi\u0002e \u0001\t\u0000\bi\u0004\u0000e \u0004\u0006\u0003babi\u0000i\u0007\bic \u0001\u0003de\u0000\u0007 \u0003f \bhe \u0007a\u0001e da\ba by \u0001\t\u0000\bi\u0004\u0000yi\u0002g\n\bhei\u0006 \u0004\u0006\u0003babi\u0000i\by di\u0007\b\u0006ib\t\bi\u0003\u0002\u0007 \b\u0003ge\bhe\u0006 a\u0002d \bhe\u0002 \u0006e\u0002\u0003\u0006\u0001a\u0000izi\u0002g. thi\u0007 i\u0007 a ve\u0006y e\u00e6cie\u0002\b\nway \b\u0003 \u0001\u0003de\u0000 high\tdi\u0001e\u0002\u0007i\u0003\u0002a\u0000 da\ba which \u0007i\u0001\t\u0000\ba\u0002e\u0003\t\u0007\u0000y \u0007a\bi\u0007(cid:12)e\u0007 \u0001a\u0002y di(cid:11)e\u0006e\u0002\b \u0000\u0003w\t\ndi\u0001e\u0002\u0007i\u0003\u0002a\u0000 c\u0003\u0002\u0007\b\u0006ai\u0002\b\u0007 beca\t\u0007e each i\u0002divid\ta\u0000 ex\u0004e\u0006\b \u0001\u0003de\u0000 ca\u0002 f\u0003c\t\u0007 \u0003\u0002 givi\u0002g high\n\u0004\u0006\u0003babi\u0000i\by \b\u0003 da\ba vec\b\u0003\u0006\u0007 \bha\b \u0007a\bi\u0007fy j\t\u0007\b \u0003\u0002e \u0003f \bhe c\u0003\u0002\u0007\b\u0006ai\u0002\b\u0007. da\ba vec\b\u0003\u0006\u0007 \bha\b \u0007a\bi\u0007fy\n\bhi\u0007 \u0003\u0002e c\u0003\u0002\u0007\b\u0006ai\u0002\b b\t\b vi\u0003\u0000a\be \u0003\bhe\u0006 c\u0003\u0002\u0007\b\u0006ai\u0002\b\u0007 wi\u0000\u0000 be \u0006\t\u0000ed \u0003\t\b by \bhei\u0006 \u0000\u0003w \u0004\u0006\u0003babi\u0000i\by\n\t\u0002de\u0006 \bhe \u0003\bhe\u0006 ex\u0004e\u0006\b\u0007. t\u0006ai\u0002i\u0002g a \u0004\u0006\u0003d\tc\b \u0003f ex\u0004e\u0006\b\u0007 a\u0004\u0004ea\u0006\u0007 di\u00e6c\t\u0000\b beca\t\u0007e\b i\u0002 addi\bi\u0003\u0002\n\b\u0003 \u0001axi\u0001izi\u0002g \bhe \u0004\u0006\u0003babi\u0000i\by \bha\b each i\u0002divid\ta\u0000 ex\u0004e\u0006\b a\u0007\u0007ig\u0002\u0007 \b\u0003 \bhe \u0003b\u0007e\u0006ved da\ba\b i\b\ni\u0007 \u0002ece\u0007\u0007", "l\n\ni\n\ny\ng\no\no\nn\nh\nc\ne\nt\no\nb\ne\nr\nu\nt\na\nn\nm\no\nc\n.\ne\nr\nu\nt\na\nn\nw\nw\nw\n\n/\n\n.\n\n/\n/\n:\np\nt\nt\nh\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n8\n0\n0\n2\n\u00a9\n\n \n\np r i m e r\n\nwhat is the expectation maximization \nalgorithm?\n\nchuong b do & serafim batzoglou\n\nthe expectation maximization algorithm arises in many computational biology applications that involve probabilistic \nmodels. what is it good for, and how does it work?\n\nprobabilistic models, such as hidden markov \n\nmodels  or  bayesian  networks,  are  com-\nmonly  used  to  model  biological  data.  much \nof  their  popularity  can  be  attributed  to  the \nexistence  of  efficient  and  robust  procedures \nfor  learning  parameters  from  observations. \noften,  however,  the  only  data  available  for \ntraining a probabilistic model are incomplete. \nmissing values can occur, for example, in medi-\ncal diagnosis, where patient histories generally \ninclude results from a limited battery of tests. \nalternatively,  in  gene  expression  clu", "5\n1\n0\n2\n\n \n\nv\no\nn\n9\n2\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\n1\nv\n6\n1\n9\n3\n\n.\n\n0\n1\n4\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2015\n\nmemory networks\n\njason weston, sumit chopra & antoine bordes\nfacebook ai research\n770 broadway\nnew york, usa\n{jase,spchopra,abordes}@fb.com\n\nabstract\n\nwe describe a new class of learning models called memory networks. memory\nnetworks reason with inference components combined with a long-term memory\ncomponent; they learn how to use these jointly. the long-term memory can be\nread and written to, with the goal of using it for prediction. we investigate these\nmodels in the context of question answering (qa) where the long-term mem-\nory effectively acts as a (dynamic) knowledge base, and the output is a textual\nresponse. we evaluate them on a large-scale qa task, and a smaller, but more\ncomplex, toy task generated from a simulated world. in the latter, we show the\nreasoning power of such models by chaining multiple supporting sentences to an-\nswer questions ", "3\n2\n0\n2\n\n \nr\np\na\n0\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n8\n6\n8\n0\n1\n\n.\n\n5\n0\n2\n2\n:\nv\ni\nx\nr\na\n\npublished in transactions on machine learning research (04/2023)\n\nmemory-e\ufb03cient reinforcement learning with value-based\nknowledge consolidation\n\nqingfeng lan \u2217\ndepartment of computing science\nuniversity of alberta\nyangchen pan \u2020\nuniversity of oxford\n\njun luo\nhuawei noah\u2019s ark lab\n\na. rupam mahmood\ndepartment of computing science\nuniversity of alberta\ncifar ai chair, amii\n\nqlan3@ualberta.ca\n\nyangchen.pan@eng.ox.ac.uk\n\njun.luo1@huawei.com\n\narmahmood@ualberta.ca\n\nreviewed on openreview: https: // openreview. net/ forum? id= zsdcvlavbn\n\nabstract\n\narti\ufb01cial neural networks are promising for general function approximation but challenging\nto train on non-independent or non-identically distributed data due to catastrophic forgetting.\nthe experience replay bu\ufb00er, a standard component in deep reinforcement learning, is often\nused to reduce forgetting and improve sample e\ufb03ciency by storing experiences in a la", "0\n2\n0\n2\n\n \nt\nc\no\n2\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n2\n6\n3\n1\n1\n\n.\n\n5\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nstable and expressive recurrent vision models\n\ndrew linsley\u2217, alekh k ashok\u2217, lakshmi n govindarajan\u2217, rex liu, thomas serre\n\ncarney institute for brain science\n\ndepartment of cognitive linguistic & psychological sciences\n\nbrown university\n\nprovidence, ri 02912\n\n{drew_linsley,alekh_ashok,lakshmi_govindarajan,\n\nrex_liu,thomas_serre}@brown.edu\n\nabstract\n\nprimate vision depends on recurrent processing for reliable perception [1\u20133]. a\ngrowing body of literature also suggests that recurrent connections improve the\nlearning ef\ufb01ciency and generalization of vision models on classic computer vision\nchallenges. why then, are current large-scale challenges dominated by feedforward\nnetworks? we posit that the effectiveness of recurrent vision models is bottlenecked\nby the standard algorithm used for training them, \u201cback-propagation through time\u201d\n(bptt), which has o(n ) memory-complexity for training an n step m", "70 \n\nieee  transactions  on  audio  and electroacoustics,  vol.  au-15, no.  2, june  1967 \n\nthe use of  fast fourier  transform for the estimation \nof  power spectra: a method based  on  time  aver. \n\naging over  short, modified periodograms \n\npeter  d.  welch \n\nabstract-the  use of  the  fast fourier  transform  in power spec- \ntrum analysis is described.  principal advantages of  this  method  are a \nof  computations  and  in  required  core \nreduction  in  the  number \nstorage,  and  convenient  application  in  nonstationarity  tests.  the \nmethod  involves  sectioning  the  record  and  averaging  modified \nperiodograms of  the sections. \n\nt \n\nintrodlction \n\nhis paper outlines  a  method for the  application \nof  the  fast  fourier  transform  algorithm  to  the \nestimation  of  power  spectra,  which  involves  sec- \ntioning  the  record,  taking  modified  periodograms  of \nthese  sections,  and  averaging  these \nmodified  periodo- \ngrams.  in  many  instances  this  method  i", "burst-dependent synaptic plasticity can \ncoordinate learning in hierarchical circuits\n\nalexandre payeur\u200a\nand richard naud\u200a\n\n\u200a1,2,3,12,13, jordan guerguiev4,5,13, friedemann zenke\u200a\n\u200a1,2,3,11,14\u2009\u2709\n\n\u200a6, blake a. richards\u200a\n\n\u200a7,8,9,10,14\u2009\u2709 \n\nsynaptic plasticity is believed to be a key physiological mechanism for learning. it is well established that it depends on pre- and \npostsynaptic activity. however, models that rely solely on pre- and postsynaptic activity for synaptic changes have, so far, not \nbeen able to account for learning complex tasks that demand credit assignment in hierarchical networks. here we show that \nif synaptic plasticity is regulated by high-frequency bursts of spikes, then pyramidal neurons higher in a hierarchical circuit \ncan coordinate the plasticity of lower-level connections. using simulations and mathematical analyses, we demonstrate that, \nwhen paired with short-term synaptic dynamics, regenerative activity in the apical dendrites and synaptic plasticity in fe", "a r t i c l e s\n\na causal link between prediction errors, dopamine \nneurons and learning\nelizabeth e steinberg1,2,11, ronald keiflin1,11, josiah r boivin1,2, ilana b witten3,4, karl deisseroth5\u20138 &  \npatricia h janak1,2,9,10\n\nsituations in which rewards are unexpectedly obtained or withheld represent opportunities for new learning. often, this learning \nincludes identifying cues that predict reward availability. unexpected rewards strongly activate midbrain dopamine neurons.  \nthis phasic signal is proposed to support learning about antecedent cues by signaling discrepancies between actual and expected \noutcomes, termed a reward prediction error. however, it is unknown whether dopamine neuron prediction error signaling and  \ncue-reward learning are causally linked. to test this hypothesis, we manipulated dopamine neuron activity in rats in two \nbehavioral procedures, associative blocking and extinction, that illustrate the essential function of prediction errors in learning. \nwe observ", "the journal of neuroscience, november 20, 2013 \u2022 33(47):18531\u201318539 \u2022 18531\n\nsystems/circuits\n\ncontrol of basal ganglia output by direct and indirect\npathway projection neurons\n\nbenjamin s. freeze,1,2,3 alexxai v. kravitz,1 nora hammack,1 joshua d. berke,4 and anatol c. kreitzer1,2,3,5\n1gladstone institute of neurological disease, san francisco, california 94158, 2biomedical sciences program and 3medical scientist training program,\nuniversity of california, san francisco, california 94117-1049, 4department of psychology, and neuroscience program, university of michigan, ann arbor,\nmichigan 48109-1340, and 5departments of physiology and neurology, university of california, san francisco, california 94117-1049\n\nthe direct and indirect efferent pathways from striatum ultimately reconverge to influence basal ganglia output nuclei, which in turn\nregulate behavior via thalamocortical and brainstem motor circuits. however, the distinct contributions of these two efferent pathways\nin shaping b", "research article\n\nthe evolutionary origins of hierarchy\nhenok mengistu1, joost huizinga1, jean-baptiste mouret2,3,4, jeff clune1*\n\n1 evolving ai lab, department of computer science, university of wyoming, laramie, wyoming, united\nstates of america, 2 larsen, inria, villers-l\u00e8s-nancy, france, 3 umr 7503 (loria), cnrs, vand\u0153uvre-\nl\u00e8s-nancy, france, 4 loria (umr 7503), universit\u00e9 de lorraine, vand\u0153uvre-l\u00e8s-nancy, france\n\n* jeffclune@uwyo.edu\n\nabstract\n\na11111\n\nopen access\n\ncitation: mengistu h, huizinga j, mouret j-b, clune\nj (2016) the evolutionary origins of hierarchy. plos\ncomput biol 12(6): e1004829. doi:10.1371/journal.\npcbi.1004829\n\neditor: olaf sporns, indiana university, united\nstates\n\nreceived: december 29, 2015\n\naccepted: february 25, 2016\n\npublished: june 9, 2016\n\ncopyright: \u00a9 2016 mengistu et al. this is an open\naccess article distributed under the terms of the\ncreative commons attribution license, which permits\nunrestricted use, distribution, and reproduction in any\nmedium, p", "available online at www.sciencedirect.com\n\nthe ubiquity of model-based reinforcement learning\nbradley b doll1,2, dylan a simon3 and nathaniel d daw2,3\n\nthe reward prediction error (rpe) theory of dopamine (da)\nfunction has enjoyed great success in the neuroscience of\nlearning and decision-making. this theory is derived from\nmodel-free reinforcement learning (rl), in which choices are\nmade simply on the basis of previously realized rewards.\nrecently, attention has turned to correlates of more \ufb02exible,\nalbeit computationally complex, model-based methods in the\nbrain. these methods are distinguished from model-free\nlearning by their evaluation of candidate actions using\nexpected future outcomes according to a world model.\npuzzlingly, signatures from these computations seem to be\npervasive in the very same regions previously thought to\nsupport model-free learning. here, we review recent behavioral\nand neural evidence about these two systems, in attempt to\nreconcile their enigmatic cohabita", "backpropagation and the brain\n\ntimothy\u00a0p.\u00a0lillicrap \ngeoffrey\u00a0hinton\n\n , adam\u00a0santoro, luke\u00a0marris, colin\u00a0j.\u00a0akerman and \n\nabstract | during learning, the brain modifies synapses to improve behaviour. in the \ncortex, synapses are embedded within multilayered networks, making it difficult \nto determine the effect of an individual synaptic modification on the behaviour of \nthe system. the backpropagation algorithm solves this problem in deep artificial \nneural networks, but historically it has been viewed as biologically problematic. \nnonetheless, recent developments in neuroscience and the successes of artificial \nneural networks have reinvigorated interest in whether backpropagation offers \ninsights for understanding learning in the cortex. the backpropagation algorithm \nlearns quickly by computing synaptic updates using feedback connections to \ndeliver error signals. although feedback connections are ubiquitous in the cortex, \nit is difficult to see how they could deliver the error si", "research article\n\nreinforcement learning of linking and\ntracing contours in recurrent\nneural networks\ntobias brosch1, heiko neumann1*, pieter r. roelfsema2,3,4\n\na11111\n\n1 university of ulm, institute of neural information processing, ulm, germany, 2 department of vision &\ncognition, netherlands institute for neuroscience (knaw), amsterdam, the netherlands, 3 department of\nintegrative neurophysiology, center for neurogenomics and cognitive research, vu university,\namsterdam, the netherlands, 4 psychiatry department, academic medical center, amsterdam, the\nnetherlands\n\n* heiko.neumann@uni-ulm.de\n\nabstract\n\nthe processing of a visual stimulus can be subdivided into a number of stages. upon stimu-\nlus presentation there is an early phase of feedforward processing where the visual informa-\ntion is propagated from lower to higher visual areas for the extraction of basic and complex\nstimulus features. this is followed by a later phase where horizontal connections within\nareas and feedback con", "understanding synthetic gradients and decoupled neural interfaces\n\nwojciech marian czarnecki 1 grzegorz \u00b4swirszcz 1 max jaderberg 1 simon osindero 1 oriol vinyals 1\n\nkoray kavukcuoglu 1\n\nabstract\n\nwhen training neural networks, the use of syn-\nthetic gradients (sg) allows layers or modules\nto be trained without update locking \u2013 without\nwaiting for a true error gradient to be backprop-\nagated \u2013 resulting in decoupled neural inter-\nfaces (dnis). this unlocked ability of being\nable to update parts of a neural network asyn-\nchronously and with only local information was\ndemonstrated to work empirically in jaderberg\net al. (2016). however, there has been very lit-\ntle demonstration of what changes dnis and sgs\nimpose from a functional, representational, and\nlearning dynamics point of view. in this paper,\nwe study dnis through the use of synthetic gra-\ndients on feed-forward networks to better under-\nstand their behaviour and elucidate their effect\non optimisation. we show that the incorpora", "neuron\n\nperspective\n\nneuronal cell types and connectivity:\nlessons from the retina\n\nh. sebastian seung1,* and uygar su\u00a8 mbu\u00a8 l2\n1princeton neuroscience institute and computer science department, princeton university, princeton, nj 08544, usa\n2grossman center for the statistics of mind and department of statistics, columbia university, new york, ny 10027, usa\n*correspondence: sseung@princeton.edu\nhttp://dx.doi.org/10.1016/j.neuron.2014.08.054\n\nwe describe recent progress toward de\ufb01ning neuronal cell types in the mouse retina and attempt to extract\nlessons that may be generally useful in the mammalian brain. achieving a comprehensive catalog of retinal\ncell types now appears within reach, because researchers have achieved consensus concerning two funda-\nmental challenges. the \ufb01rst is accuracy\u2014de\ufb01ning pure cell types rather than settling for neuronal classes that\nare mixtures of types. the second is completeness\u2014developing methods guaranteed to eventually identify\nall cell types, as well ", "learning both weights and connections for ef\ufb01cient\n\nneural networks\n\nsong han\n\nstanford university\n\nsonghan@stanford.edu\n\njeff pool\nnvidia\n\njpool@nvidia.com\n\njohn tran\nnvidia\n\njohntran@nvidia.com\n\nwilliam j. dally\nstanford university\n\nnvidia\n\ndally@stanford.edu\n\nabstract\n\nneural networks are both computationally intensive and memory intensive, making\nthem dif\ufb01cult to deploy on embedded systems. also, conventional networks \ufb01x\nthe architecture before training starts; as a result, training cannot improve the\narchitecture. to address these limitations, we describe a method to reduce the\nstorage and computation required by neural networks by an order of magnitude\nwithout affecting their accuracy by learning only the important connections. our\nmethod prunes redundant connections using a three-step method. first, we train\nthe network to learn which connections are important. next, we prune the unim-\nportant connections. finally, we retrain the network to \ufb01ne tune the weights of the\nremaining ", "int j comput vis (2009) 81: 317\u2013330\ndoi 10.1007/s11263-008-0178-9\n\nspectral curvature clustering (scc)\nguangliang chen \u00b7 gilad lerman\n\nreceived: 15 november 2007 / accepted: 12 september 2008 / published online: 10 december 2008\n\u00a9 the author(s) 2008. this article is published with open access at springerlink.com\n\nabstract this paper presents novel\ntechniques for im-\nproving the performance of a multi-way spectral cluster-\ning framework (govindu in proceedings of the 2005 ieee\ncomputer society conference on computer vision and pat-\ntern recognition (cvpr\u201905), vol. 1, pp. 1150\u20131157, 2005;\nchen and lerman, 2007, preprint in the supplementary web-\npage) for segmenting af\ufb01ne subspaces. speci\ufb01cally, it sug-\ngests an iterative sampling procedure to improve the uni-\nform sampling strategy, an automatic scheme of inferring\nthe tuning parameter from the data, a precise initialization\nprocedure for k-means, as well as a simple strategy for iso-\nlating outliers. the resulting algorithm, spectral c", "neuron\n\nreport\n\nnmda receptor-dependent multidendrite ca2+ spikes\nrequired for hippocampal burst firing in vivo\n\nchristine grienberger,1 xiaowei chen,1,2 and arthur konnerth1,*\n1institute of neuroscience, center for integrated protein science and synergy cluster, technical university munich, biedersteiner stra\u00dfe 29,\nmunich 80802, germany\n2brain research center, third military medical university, chongqing 400038, china\n*correspondence: arthur.konnerth@lrz.tum.de\nhttp://dx.doi.org/10.1016/j.neuron.2014.01.014\n\nsummary\n\nhigh-frequency bursts of action potentials (aps) are\na distinctive form of signaling in various types of\nmammalian central neurons. in ca1 hippocampal\npyramidal neurons in vivo, such complex spike\nbursts (css) are detected during various behaviors\nand are considered to be particularly important for\nlearning- and memory-related synaptic plasticity.\nhere, we combined whole-cell recordings and two-\nphoton imaging in mouse ca1 pyramidal neurons\nto investigate the cellular mec", "comparing dynamics: deep neural networks versus glassy systems\n\nmarco baity-jesi 1 levent sagun 2 3 mario geiger 3 stefano spigler 3 2 g\u00b4erard ben arous 4\n\nchiara cammarota 5 yann lecun 4 6 7 matthieu wyart 3 giulio biroli 2 8\n\nabstract\n\nwe analyze numerically the training dynamics of\ndeep neural networks (dnn) by using methods\ndeveloped in statistical physics of glassy systems.\nthe two main issues we address are (1) the com-\nplexity of the loss landscape and of the dynam-\nics within it, and (2) to what extent dnns share\nsimilarities with glassy systems. our \ufb01ndings, ob-\ntained for different architectures and datasets, sug-\ngest that during the training process the dynamics\nslows down because of an increasingly large num-\nber of \ufb02at directions. at large times, when the\nloss is approaching zero, the system diffuses at\nthe bottom of the landscape. despite some sim-\nilarities with the dynamics of mean-\ufb01eld glassy\nsystems, in particular, the absence of barrier cross-\ning, we \ufb01nd distinctiv", "submitted to iconip\u2019\t\b\n\neffect of batch learning in multilayer\n\nneural networks\n\nkenji fukumizu\n\nemail:fuku@brain.riken.go.jp\n\nlab. for information synthesis, riken brain science institute\n\nhirosawa \u0002-\u0001, wako, saitama, \u0003\u0005\u0001-\u0000\u0001\t\b, japan\n\nabstract\n\na (cid:12)rst step of the analysis of learning in multilayer networks.\n\nthis paper discusses batch gradient descent learning in mul-\ntilayer networks with a large number of statistical training\ndata. we emphasize on the di(cid:11)erence between regular cases,\nwhere the prepared model has the same size as the true func-\ntion, and overrealizable cases, where the model has surplus\nhidden units to realize the true function. first, experimental\nstudy on multilayer perceptrons and linear neural networks\n(lnn) shows that batch learning induces strong overtrain-\ning on both models in overrealizable cases, which means the\ndegrade of generalization error by surplus units can be alle-\nviated. we theoretically analyze the dynamics in lnn, and\nshow that thi", "associative long short-term memory\n\n6\n1\n0\n2\n\n \n\ny\na\nm\n9\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n2\n3\n0\n3\n0\n\n.\n\n2\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nivo danihelka\ngreg wayne\nbenigno uria\nnal kalchbrenner\nalex graves\ngoogle deepmind\n\ndanihelka@google.com\ngregwayne@google.com\nburia@google.com\nnalk@google.com\ngravesa@google.com\n\nabstract\n\nwe investigate a new method to augment recur-\nrent neural networks with extra memory without\nincreasing the number of network parameters.\nthe system has an associative memory based on\ncomplex-valued vectors and is closely related to\nholographic reduced representations and long\nshort-term memory networks. holographic re-\nduced representations have limited capacity: as\nthey store more information, each retrieval be-\ncomes noisier due to interference. our system\nin contrast creates redundant copies of stored in-\nformation, which enables retrieval with reduced\nnoise. experiments demonstrate faster learning\non multiple memorization tasks.\n\n1. introduction\nwe aim to enhance lstm (hoch", "the neural race reduction: dynamics of abstraction in gated networks\n\nandrew m. saxe* 1 2 3 shagun sodhani* 2 sam lewallen 1\n\nabstract\n\nour theoretical understanding of deep learning\nhas not kept pace with its empirical success.\nwhile network architecture is known to be criti-\ncal, we do not yet understand its effect on learned\nrepresentations and network behavior, or how this\narchitecture should reflect task structure.in this\nwork, we begin to address this gap by introduc-\ning the gated deep linear network framework\nthat schematizes how pathways of information\nflow impact learning dynamics within an architec-\nture. crucially, because of the gating, these net-\nworks can compute nonlinear functions of their in-\nput. we derive an exact reduction and, for certain\ncases, exact solutions to the dynamics of learn-\ning. our analysis demonstrates that the learning\ndynamics in structured networks can be concep-\ntualized as a neural race with an implicit bias to-\nwards shared representations, wh", "central cholinergic neurons are rapidly recruited\nby reinforcement feedback\n\narticle\n\ngraphical abstract\n\nauthors\nbala\u00b4 zs hangya, sachin p. ranade, maja\nlorenc, adam kepecs\n\ncorrespondence\nhangya.balazs@koki.mta.hu (b.h.),\nkepecs@cshl.edu (a.k.)\n\nin brief\nrecordings in basal forebrain cholinergic\nneurons during behavior show\nunexpectedly fast and precisely timed\nresponses to reward and punishment that\nare modulated by outcome expectations,\nsuggesting that the central cholinergic\nsystem may also convey cognitive\ninformation.\n\nhighlights\nd basal forebrain cholinergic neurons respond to reward and\n\npunishment\n\nd cholinergic responses are scaled by reinforcement surprise\n\nd neural correlates of attention are found in a non-cholinergic\n\npopulation\n\nhangya et al., 2015, cell 162, 1155\u20131168\naugust 27, 2015 \u00aa2015 elsevier inc.\nhttp://dx.doi.org/10.1016/j.cell.2015.07.057\n\n\f", "no free lunch from deep learning in neuroscience:\n\na case study through models of the\n\nentorhinal-hippocampal circuit\n\nrylan schaeffer\ncomputer science\n\nstanford\n\nmikail khona\n\nphysics\n\nmit\n\nrschaef@cs.stanford.edu\n\nmikail@mit.edu\n\nila rani fiete\n\nbrain and cognitive sciences\n\nmit\n\nfiete@mit.edu\n\nabstract\n\nresearch in neuroscience, as in many scientific disciplines, is undergoing a re-\nnaissance based on deep learning. unique to neuroscience, deep learning models\ncan be used not only as a tool but interpreted as models of the brain. the cen-\ntral claims of recent deep learning-based models of brain circuits are that they\nmake novel predictions about neural phenomena or shed light on the fundamental\nfunctions being optimized. we show, through the case-study of grid cells in the\nentorhinal-hippocampal circuit, that one may get neither. we begin by reviewing\nthe principles of grid cell mechanism and function obtained from first-principles\nmodeling efforts, then rigorously examine the clai", "674\n\nieee transactions on automatic control, vol. 42, no. 5, may 1997\n\nan analysis of temporal-difference learning\n\nwith function approximation\n\njohn n. tsitsiklis, member, ieee, and benjamin van roy\n\nabstract\u2014 we discuss the temporal-difference learning algo-\nrithm, as applied to approximating the cost-to-go function of\nan in\ufb01nite-horizon discounted markov chain. the algorithm we\nanalyze updates parameters of a linear function approximator on-\nline during a single endless trajectory of an irreducible aperiodic\nmarkov chain with a \ufb01nite or in\ufb01nite state space. we present a\nproof of convergence (with probability one), a characterization of\nthe limit of convergence, and a bound on the resulting approxi-\nmation error. furthermore, our analysis is based on a new line\nof reasoning that provides new intuition about the dynamics of\ntemporal-difference learning.\n\nin addition to proving new and stronger positive results than\nthose previously available, we identify the signi\ufb01cance of on-\nline up", "opinion\n\ntrends in cognitive sciences vol.8 no.10 october 2004\n\nthe reverse hierarchy theory of visual\nperceptual learning\nmerav ahissar1 and shaul hochstein2\n\n1department of psychology and interdisciplinary center for neural computation, hebrew university, jerusalem, 91905, israel\n2department of neurobiology and interdisciplinary center for neural computation, hebrew university, jerusalem, 91905, israel\n\ntypically referred to as perceptual learning, whereas the\nlatter are termed priming. recent evidence suggests that,\nat least when governed by top-down control, single\nexposures (priming) can induce strong and long-lasting\neffects that clearly change our perception (see box 1),\nsuggesting that these might be the initial processes of\nperceptual learning, as described below (the eureka effect).\n\nbox 1. the eureka effect\n\nthe picture (figure i) generally appears simply as a set of gray and\nblack regions, without further meaning; the object represented is\nhard to categorize without further", "progress in brain research, vol. 149\nissn 0079-6123\ncopyright \u00df 2005 elsevier bv. all rights reserved\n\nchapter 11\n\ndrivers and modulators from push-pull and\n\nbalanced synaptic input\n\nl.f. abbott1,* and frances s. chance2\n\n1volen center and department of biology, brandeis university, waltham, ma 02454-9110, usa\n\n2department of neurobiology and behavior, university of california at irvine,\n\nirvine, ca 92697-4550, usa\n\nabstract: in 1998, sherman and guillery proposed that there are two types of inputs to cortical neurons; drivers and\nmodulators. these two forms of input are required to explain how, for example, sensory driven responses are controlled\nand modi\ufb01ed by attention and other internally generated gating signals. one might imagine that driver signals are\ncarried by fast ionotropic receptors, whereas modulators correspond to slower metabotropic receptors. instead, we\nhave proposed a novel mechanism by which both driver and modulator inputs could be carried by transmission through\nt", "ne36ch15-shenoy\n\nari\n\n10 june 2013\n\n15:31\n\nannu. rev. neurosci. 2013. 36:337\u201359\n\nfirst published online as a review in advance on\nmay 29, 2013\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev-neuro-062111-150509\ncopyright c(cid:2) 2013 by annual reviews.\nall rights reserved\n\ncortical control of arm\nmovements: a dynamical\nsystems perspective\n\nkrishna v. shenoy,1,2 maneesh sahani,1,3\nand mark m. churchland4\n1departments of electrical engineering, 2bioengineering, and neurobiology, bio-x and\nneurosciences programs, stanford institute for neuro-innovation and translational\nneuroscience, stanford university, stanford, california 94305; email: shenoy@stanford.edu\n3gatsby computational neuroscience unit, university college london, london wc1n\n3ar, united kingdom; email: maneesh@gatsby.ucl.ac.uk\n4department of neuroscience, grossman center for the statistics of mind, david mahoney\ncenter for brain and behavior research, kavli institut", "research article\n\nlocal online learning in recurrent\nnetworks with random feedback\njames m murray*\n\nzuckerman mind, brain and behavior institute, columbia university, new york,\nunited states\n\nabstract recurrent neural networks (rnns) enable the production and processing of time-\ndependent signals such as those involved in movement or working memory. classic gradient-based\nalgorithms for training rnns have been available for decades, but are inconsistent with biological\nfeatures of the brain, such as causality and locality. we derive an approximation to gradient-based\nlearning that comports with these constraints by requiring synaptic weight updates to depend only\non local information about pre- and postsynaptic activities, in addition to a random feedback\nprojection of the rnn output error. in addition to providing mathematical arguments for the\neffectiveness of the new learning rule, we show through simulations that it can be used to train an\nrnn to perform a variety of tasks. finally", "review\n\nspecial  issue:  hippocampus  and  memory\n\nthe  hippocampal\u2013striatal  axis  in\nlearning,  prediction  and  goal-directed\nbehavior\nc.m.a.  pennartz1,  r.  ito2,3,  p.f.m.j.  verschure4,5,  f.p.  battaglia1 and  t.w.  robbins6\n\n1 cognitive  and  systems  neuroscience  group,  swammerdam  institute  for  life  sciences,  center  for  neuroscience,  sciencepark  904,\n1098  xh,  amsterdam,  the  netherlands\n2 department  of  experimental  psychology,  university  of  oxford,  south  parks  road,  oxford,  ox1  3ud,  uk\n3 department  of  psychology,  university  of  toronto  scarborough,  1265  military  trail,  toronto,  on,  m1c  1a4,  canada\n4 laboratory  of  synthetic  perceptive  emotive  and  cognitive  systems  (specs),  department  of  technology,  universitat  pompeu\nfabra,  roc  boronat  138,  08018  barcelona,  spain\n5 institucio\u00b4  catalana  de  recerca  i  estudis  avanc\u00b8ats  (icrea),  barcelona,  spain\n6 behavioral  and  clinical  neuroscience  institute  and  department", "global brain dynamics embed the motor command\nsequence of caenorhabditis elegans\n\narticle\n\ngraphical abstract\n\nauthors\nsaul kato, harris s. kaplan, tina\nschro\u00a8 del, ..., eviatar yemini, shawn\nlockery, manuel zimmer\n\ncorrespondence\nzimmer@imp.ac.at\n\nin brief\nsimultaneously recording the activity of\nnearly all neurons in the c. elegans brain\nreveals that most active neurons share\ninformation by engaging in coordinated,\ndynamical network activity that\ncorresponds to the sequential assembly\nof motor commands.\n\nhighlights\nd most active neurons in the brain participate in coordinated\n\ndynamical activity\n\nd smooth, cyclical dynamics continuously represent action\n\nsequences and decisions\n\nd internal representation of behavior persists when de-\n\ncoupled from its execution\n\nd brain dynamics provide a robust scaffold for sensory-driven\n\naction selection\n\nkato et al., 2015, cell 163, 656\u2013669\noctober 22, 2015 \u00aa2015 elsevier inc.\nhttp://dx.doi.org/10.1016/j.cell.2015.09.034\n\n\f", "a r t i c l e s\n\nmetamers of the ventral stream\njeremy freeman1 & eero p simoncelli1\u20133\n\nthe human capacity to recognize complex visual patterns emerges in a sequence of brain areas known as the ventral stream, beginning \nwith primary visual cortex (v1). we developed a population model for mid-ventral processing, in which nonlinear combinations of v1 \nresponses are averaged in receptive fields that grow with eccentricity. to test the model, we generated novel forms of visual metamers, \nstimuli that differ physically but look the same. we developed a behavioral protocol that uses metameric stimuli to estimate the \nreceptive field sizes in which the model features are represented. because receptive field sizes change along the ventral stream, our \nbehavioral results can identify the visual area corresponding to the representation. measurements in human observers implicate visual \narea v2, providing a new functional account of neurons in this area. the model also explains deficits of perip", "9\n1\n0\n2\n\n \n\ny\na\nm\n1\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n0\n1\n6\n4\n0\n\n.\n\n5\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nexplainable ai for trees: from local explanations to global\n\nunderstanding\n\nscott m. lundberg1, gabriel erion1,2, hugh chen1, alex degrave1,2, jordan m. prutkin3,\n\nbala nair4,5, ronit katz6, jonathan himmelfarb6, nisha bansal6, and su-in lee1,*\n\n1paul g. allen school of computer science and engineering, university of washington\n\n2medical scientist training program, university of washington\n\n3division of cardiology, department of medicine, university of washington\n4department of anesthesiology and pain medicine, university of washington\n5harborview injury prevention and research center, university of washington\n\n6kidney research institute, division of nephrology, department of medicine, university of washington\n\n*corresponding: suinlee@cs.washington.edu\n\none sentence summary: explanations for ensemble tree-based predictions; a unique exact solution that guarantees\ndesirable explanation properties.\n\n", "parametric return density estimation for reinforcement learning\n\ntetsuro morimura\nibm research - tokyo\ntetsuro@jp.ibm.com\n\nmasashi sugiyama\n\nhisashi kashima\n\ntokyo institute of technology\n\nthe university of tokyo\n\nsugi@cs.titech.ac.jp\n\nkashima@mist.i.u-tokyo.ac.jp\n\nhirotaka hachiya\n\ntokyo institute of technology\nhachiya@sg.cs.titech.ac.jp\n\ntoshiyuki tanaka\n\nkyoto university\n\ntt@i.kyoto-u.ac.jp\n\nabstract\n\nmost conventional reinforcement learning\n(rl) algorithms aim to optimize decision-\nmaking rules in terms of the expected re-\nturns. however, especially for risk man-\nagement purposes, other risk-sensitive crite-\nria such as the value-at-risk or the expected\nshortfall are sometimes preferred in real ap-\nplications. here, we describe a parametric\nmethod for estimating density of the returns,\nwhich allows us to handle various criteria in a\nuni(cid:12)ed manner. we (cid:12)rst extend the bellman\nequation for the conditional expected return\nto cover a conditional probability density of\nthe ", "the journal of neuroscience, may 1, 2001, 21(9):3251\u20133260\n\nthe role of the nucleus accumbens in instrumental conditioning:\nevidence of a functional dissociation between accumbens\ncore and shell\n\nlaura h. corbit,1 janice l. muir,2 and bernard w. balleine1\n1department of psychology, university of california los angeles, los angeles, california 90095, and 2school of\npsychology, cardiff university, cardiff, cf10 3yg, united kingdom\n\nin three experiments we examined the effect of bilateral exci-\ntotoxic lesions of the nucleus accumbens core or shell subre-\ngions on instrumental performance, outcome devaluation,\ndegradation of the instrumental contingency, pavlovian condi-\ntioning, and pavlovian-instrumental transfer. rats were food\ndeprived and trained to press two levers, one delivering food\npellets and the other a sucrose solution. all animals acquired\nthe lever-press response although the rate of acquisition and\noverall response rates in core-lesioned animals were depressed\nrelative to t", "the neural autoregressive distribution estimator\n\nhugo larochelle\n\ndepartment of computer science\n\nuniversity of toronto\n\ntoronto, canada\n\niain murray\n\nschool of informatics\n\nuniversity of edinburgh\n\nedinburgh, scotland\n\nabstract\n\nwe describe a new approach for modeling the\ndistribution of high-dimensional vectors of dis-\ncrete variables. this model is inspired by the\nrestricted boltzmann machine (rbm), which\nhas been shown to be a powerful model of\nsuch distributions. however, an rbm typi-\ncally does not provide a tractable distribution\nestimator, since evaluating the probability it\nassigns to some given observation requires the\ncomputation of the so-called partition func-\ntion, which itself is intractable for rbms of\neven moderate size. our model circumvents\nthis di\ufb03culty by decomposing the joint dis-\ntribution of observations into tractable condi-\ntional distributions and modeling each condi-\ntional using a non-linear function similar to a\nconditional of an rbm. our model can also\nb", "3\n2\n0\n2\n\n \nl\nu\nj\n \n\n5\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n6\n8\n5\n3\n1\n\n.\n\n7\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nsettling the sample complexity of\n\nonline reinforcement learning\n\nzihan zhang*\n\nyuxin chen\u2020\n\njason d. lee*\n\nsimon s. du\u2021\n\nprinceton\n\nupenn\n\nprinceton\n\nuniv. of washington\n\njuly 26, 2023\n\nabstract\n\na central issue lying at the heart of online reinforcement learning (rl) is data\nef\ufb01ciency. while a number of recent works achieved asymptotically minimal regret\nin online rl, the optimality of these results is only guaranteed in a \u201clarge-sample\u201d\nregime, imposing enormous burn-in cost in order for their algorithms to operate opti-\nmally. how to achieve minimax-optimal regret without incurring any burn-in cost has\nbeen an open problem in rl theory.\n\nwe settle this problem for the context of \ufb01nite-horizon inhomogeneous markov\ndecision processes. speci\ufb01cally, we prove that a modi\ufb01ed version of monotonic\nvalue propagation (mvp), a model-based algorithm proposed by zhang et al. (2021),\nachieves a regret on the ", "2\n2\n0\n2\n\n \n\ny\na\nm\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n6\n6\n6\n1\n0\n\n.\n\n1\n0\n2\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2022\n\nsample efficient deep reinforcement learn-\ning via uncertainty estimation\n\nvincent mai, kaustubh mani and liam paull \u2217\nrobotics and embodied ai lab\nmila - quebec institute of arti\ufb01cial intelligence\nuniversit\u00b4e de montr\u00b4eal, quebec, canada\n{vincent.mai,kaustubh.mani,liam.paull}@umontreal.ca\n\nabstract\n\nin model-free deep reinforcement learning (rl) algorithms, using noisy value es-\ntimates to supervise policy evaluation and optimization is detrimental to the sam-\nple ef\ufb01ciency. as this noise is heteroscedastic, its effects can be mitigated using\nuncertainty-based weights in the optimization process. previous methods rely on\nsampled ensembles, which do not capture all aspects of uncertainty. we provide\na systematic analysis of the sources of uncertainty in the noisy supervision that\noccurs in rl, and introduce inverse-variance rl, a bayesian framework which\n", "an empirical evaluation of generic convolutional and recurrent networks\n\nfor sequence modeling\n\nshaojie bai 1 j. zico kolter 2 vladlen koltun 3\n\n8\n1\n0\n2\n\n \nr\np\na\n9\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n1\n7\n2\n1\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nfor most deep learning practitioners, sequence\nmodeling is synonymous with recurrent networks.\nyet recent results indicate that convolutional ar-\nchitectures can outperform recurrent networks on\ntasks such as audio synthesis and machine trans-\nlation. given a new sequence modeling task or\ndataset, which architecture should one use? we\nconduct a systematic evaluation of generic convo-\nlutional and recurrent architectures for sequence\nmodeling. the models are evaluated across a\nbroad range of standard tasks that are commonly\nused to benchmark recurrent networks. our re-\nsults indicate that a simple convolutional archi-\ntecture outperforms canonical recurrent networks\nsuch as lstms across a diverse range of tasks\nand datasets, while demonstrating lon", "0\n2\n0\n2\n\n \n\nn\na\nj\n \n\n7\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n6\n8\n7\n1\n1\n\n.\n\n5\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nputting an end to end-to-end:\n\ngradient-isolated learning of representations\n\nsindy l\u00f6we\u2217\n\npeter o\u2019connor\n\nbastiaan s. veeling\u2217\n\namlab\n\nuniversity of amsterdam\n\nloewe.sindy@gmail.com, basveeling@gmail.com\n\nabstract\n\nwe propose a novel deep learning method for local self-supervised representation\nlearning that does not require labels nor end-to-end backpropagation but exploits\nthe natural order in data instead. inspired by the observation that biological neural\nnetworks appear to learn without backpropagating a global error signal, we split\na deep neural network into a stack of gradient-isolated modules. each module\nis trained to maximally preserve the information of its inputs using the infonce\nbound from oord et al. [2018]. despite this greedy training, we demonstrate that\neach module improves upon the output of its predecessor, and that the representa-\ntions created by the top module yield highly", "optimization theory of hebbian/anti-hebbian networks for pca and\n\nwhitening\n\ncengiz pehlevan1 and dmitri b. chklovskii1\n\n5\n1\n0\n2\n\n \n\nv\no\nn\n \n0\n3\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n8\n6\n4\n9\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014 in analyzing information streamed by sensory\norgans, our brains face challenges similar to those solved\nin statistical signal processing. this suggests that biologically\nplausible implementations of online signal processing algo-\nrithms may model neural computation. here, we focus on\nsuch workhorses of signal processing as principal component\nanalysis (pca) and whitening which maximize information\ntransmission in the presence of noise. we adopt the similarity\nmatching framework, recently developed for principal subspace\nextraction, but modify the existing objective functions by adding\na decorrelating term. from the modi\ufb01ed objective functions,\nwe derive online pca and whitening algorithms which are\nimplementable by neural networks with local learning rules,\ni.e. s", "decoupled neural interfaces using synthetic gradients\n\n\u02c6\u0000a\u0000b\n\n\u0000a\u0000b\n\nha\u2192b\n\nb\n\nsb\n\nma\u2192b\n\na\n\n\u02c6\u0000a\u0000b\n\n\u0000a\u0000b\n\n\u02c6\u0000a\u0000b\n\nb\nb\n\nsb\n\nma\u2192b\n\n\u02c6\u0000a\u0000b\n\nha\u2192b\n\na\n\n\u2026\u2026\n\nfi+2\n\n\u0000i+1\n\nfi+1\n\n\u0000i\n\nfi\n\nf n\n\ni+1\n\nf i\n\n1\n\n\u2026\u2026\n\n(b)\n\nmax jaderberg 1 wojciech marian czarnecki 1 simon osindero 1 oriol vinyals 1 alex graves 1 david silver 1\n\nkoray kavukcuoglu 1\n\nabstract\n\ntraining directed neural networks typically re-\nquires forward-propagating data through a com-\nputation graph, followed by backpropagating er-\nror signal, to produce weight updates. all lay-\ners, or more generally, modules, of the network\nare therefore locked, in the sense that they must\nwait for the remainder of the network to execute\nforwards and propagate error backwards before\nthey can be updated. in this work we break this\nconstraint by decoupling modules by introduc-\ning a model of the future computation of the net-\nwork graph. these models predict what the re-\nsult of the modelled subgraph will produce using\nonly local information. in particular we ", "published as a conference paper at iclr 2017\n\non large-batch training for deep learning:\ngeneralization gap and sharp minima\n\nnitish shirish keskar\u2217\nnorthwestern university\nevanston, il 60208\nkeskar.nitish@u.northwestern.edu\n\ndheevatsa mudigere\nintel corporation\nbangalore, india\ndheevatsa.mudigere@intel.com\n\njorge nocedal\nnorthwestern university\nevanston, il 60208\nj-nocedal@northwestern.edu\n\nmikhail smelyanskiy\nintel corporation\nsanta clara, ca 95054\nmikhail.smelyanskiy@intel.com\n\nping tak peter tang\nintel corporation\nsanta clara, ca 95054\npeter.tang@intel.com\n\nabstract\n\nthe stochastic gradient descent (sgd) method and its variants are algorithms of\nchoice for many deep learning tasks. these methods operate in a small-batch\nregime wherein a fraction of the training data, say 32\u2013512 data points, is sampled\nto compute an approximation to the gradient. it has been observed in practice that\nwhen using a larger batch there is a degradation in the quality of the model, as\nmeasured by its abi", "a r t i c l e s\n\nmodel-based choices involve prospective neural activity\nbradley b doll1,2, katherine d duncan2, dylan a simon3, daphna shohamy2,4 & nathaniel d daw1,3\n\ndecisions may arise via \u2018model-free\u2019 repetition of previously reinforced actions or by \u2018model-based\u2019 evaluation, which is  \nwidely thought to follow from prospective anticipation of action consequences using a learned map or model. while choices  \nand neural correlates of decision variables sometimes reflect knowledge of their consequences, it remains unclear whether  \nthis actually arises from prospective evaluation. using functional magnetic resonance imaging and a sequential reward-learning \ntask in which paths contained decodable object categories, we found that humans\u2019 model-based choices were associated with \nneural signatures of future paths observed at decision time, suggesting a prospective mechanism for choice. prospection also \ncovaried with the degree of model-based influences on neural correlates of decisio", "siam j. math. data sci.\nvol. 3, no. 3, pp. 930\u2013958\n\n\u00a9 2021 siam. published by siam under the terms\nof the creative commons 4.0 license\n\nlaurent el ghaoui\u2020 , fangda gu\u2020 , bertrand travacca\u2021 , armin askari\u2020 , and alicia tsai\u2020\n\nimplicit deep learning\u2217\n\nabstract. implicit deep learning prediction rules generalize the recursive rules of feedforward neural networks.\nsuch rules are based on the solution of a fixed-point equation involving a single vector of hidden\nfeatures, which is thus only implicitly defined. the implicit framework greatly simplifies the notation\nof deep learning, and opens up many new possibilities in terms of novel architectures and algorithms,\nrobustness analysis and design, interpretability, sparsity, and network architecture optimization.\n\nkey words. deep learning, deep equilibrium models, perron\u2013frobenius theory, fixed-point equations, robust-\n\nness, adversarial attacks\n\nams subject classifications. 690c26, 49m99, 65k10, 62m45, 26b10\n\ndoi. 10.1137/20m1358517\n\n1. intr", "neuron\n\narticle\n\noptimal control of transient dynamics\nin balanced networks supports generation\nof complex movements\n\nguillaume hennequin,1,2,* tim p. vogels,1,3,4 and wulfram gerstner1,4\n1school of computer and communication sciences and brain mind institute, school of life sciences, ecole polytechnique fe\u00b4 de\u00b4 rale de\nlausanne (epfl), 1015 lausanne, switzerland\n2department of engineering, university of cambridge, cambridge cb2 1pz, uk\n3centre for neural circuits and behaviour, university of oxford, oxford ox1 3sr, uk\n4co-senior author\n*correspondence: gjeh2@cam.ac.uk\nhttp://dx.doi.org/10.1016/j.neuron.2014.04.045\n\nsummary\n\npopulations of neurons in motor cortex engage in\ncomplex transient dynamics of large amplitude dur-\ning the execution of limb movements. traditional\nnetwork models with stochastically assigned synap-\nses cannot reproduce this behavior. here we intro-\nduce a class of cortical architectures with strong\nand random excitatory recurrence that is stabilized\nby intricate,", "c(cid:13)copyright 2020\nmatthew s farrell\n\n\f", "9\n1\n0\n2\n\n \nt\nc\no\n7\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n3\n4\n4\n0\n0\n\n.\n\n6\n0\n9\n1\n:\nv\ni\nx\nr\na\n\ndimensionality compression and expansion in deep\n\nneural networks\n\ncenter for computational neuroscience\n\ncenter for computational neuroscience\n\nstefano recanatesi\u2217\n\nuniversity of washington\n\nseattle, wa\n\nstefanor@uw.edu\n\nmatthew farrell\u2217\n\nuniversity of washington\n\nseattle, wa\nmsf9@uw.edu\n\nmadhu advani\n\ncenter for brain science\n\nharvard university\ncambridge, ma\n\nmadvani@fas.harvard.edu\n\ntimothy moore\n\ncenter for computational neuroscience\n\nuniversity of washington\n\nseattle, wa\n\ntjm36@uw.edu\n\ndept. of mathematics and statistics\n\ncenter for computational neuroscience\n\nguillaume lajoie\n\nuniversit\u00e9 de montr\u00e9al\n\nmontr\u00e9al, qu\u00e9bec, canada\n\nlajoie@dms.umontreal.ca\n\neric shea-brown\n\nuniversity of washington\n\nseattle, wa\netsb@uw.edu\n\nabstract\n\ndatasets such as images, text, or movies are embedded in high-dimensional spaces.\nhowever, in important cases such as images of objects, the statistical structure in\nth", "letter\nemergence of reproducible spatiotemporal activity\nduring motor learning\n\ndoi:10.1038/nature13235\n\nandrew j. peters1, simon x. chen1 & takaki komiyama1,2\n\nthe motor cortex is capable of reliably driving complex movements1,2\nyet exhibits considerable plasticity during motor learning3\u201310. these\nobservations suggest that the fundamental relationship between\nmotor cortex activity and movement may not be fixed but is instead\nshaped by learning; however, to what extent and how motor learning\nshapes this relationship are not fully understood. here we addressed\nthis issue by usinginvivotwo-photon calcium imaging11 to monitor\nthe activity of the same population of hundreds of layer 2/3 neurons\nwhile mice learned a forelimb lever-press task over two weeks. excit-\natory and inhibitory neurons were identified by transgenic labelling12,13.\ninhibitory neuron activity was relatively stable and balanced local\nexcitatory neuron activity on a movement-by-movement basis, whereas\nexcitatory neuron a", "journal of machine learning research 17 (2016) 1-36\n\nsubmitted 8/14; revised 12/15; published 3/16\n\nlearning the variance of the reward-to-go\n\naviv tamar\ndepartment of electrical engineering and computer sciences\nuniversity of california, berkeley\nberkeley, ca 94709, usa\n\ndotan di castro\nyahoo! research labs\nmatam, haifa 31905, israel\n\nshie mannor\ndepartment of electrical engineering\nthe technion - israel institute of technology\nhaifa 32000, israel\n\neditor: peter auer\n\navivt@berkeley.edu\n\ndot@yahoo-inc.com\n\nshie@ee.technion.ac.il\n\nabstract\n\nin markov decision processes (mdps), the variance of the reward-to-go is a natural measure\nof uncertainty about the long term performance of a policy, and is important in domains\nsuch as \ufb01nance, resource allocation, and process control. currently however, there is no\ntractable procedure for calculating it in large scale mdps. this is in contrast to the case of\nthe expected reward-to-go, also known as the value function, for which e\ufb00ective simulation", "the journal of neuroscience, september 9, 2015 \u2022 35(36):12477\u201312487 \u2022 12477\n\nsystems/circuits\n\ntraveling theta waves in the human hippocampus\n\nhonghui zhang1 and joshua jacobs2\n1school of biomedical engineering, sciences, and health systems, drexel university, philadelphia, pennsylvania 19104, and 2department of biomedical\nengineering, columbia university, new york, new york 10027\n\nthe hippocampal theta oscillation is strongly correlated with behaviors such as memory and spatial navigation, but we do not understand\nits specific functional role. one hint of theta\u2019s function came from the discovery in rodents that theta oscillations are traveling waves that\nallow parts of the hippocampus to simultaneously exhibit separate oscillatory phases. because hippocampal theta oscillations in humans\nhave different properties compared with rodents, we examined these signals directly using multielectrode recordings from neurosurgical\npatients. our findings confirm that human hippocampal theta oscill", "supervised contrastive learning\n\nprannay khosla \u2217\ngoogle research\nyonglong tian \u2020\n\nmit\n\npiotr teterwak \u2217\u2020\nboston university\nphillip isola \u2020\n\nmit\n\nchen wang \u2020\nsnap inc.\naaron maschinot\ngoogle research\n\naaron sarna \u2021\ngoogle research\n\nce liu\n\ngoogle research\n\ndilip krishnan\ngoogle research\n\nabstract\n\ncontrastive learning applied to self-supervised representation learning has seen\na resurgence in recent years, leading to state of the art performance in the unsu-\npervised training of deep image models. modern batch contrastive approaches\nsubsume or signi\ufb01cantly outperform traditional contrastive losses such as triplet,\nmax-margin and the n-pairs loss.\nin this work, we extend the self-supervised\nbatch contrastive approach to the fully-supervised setting, allowing us to effec-\ntively leverage label information. clusters of points belonging to the same class\nare pulled together in embedding space, while simultaneously pushing apart clus-\nters of samples from different classes. we analyze two p", "article\n\ndoi: 10.1038/s41467-017-01827-3\n\nopen\n\nsupervised learning in spiking neural networks with\nforce training\nwilten nicola1 & claudia clopath1\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\npopulations of neurons display an extraordinary diversity in the behaviors they affect and\ndisplay. machine learning techniques have recently emerged that allow us to create networks\nof model neurons that display behaviors of similar complexity. here we demonstrate the\ndirect applicability of one such technique, the force method, to spiking neural networks. we\ntrain these networks to mimic dynamical systems, classify inputs, and store discrete\nsequences that correspond to the notes of a song. finally, we use force training to create\ntwo biologically motivated model circuits. one is inspired by the zebra \ufb01nch and successfully\nreproduces songbird singing. the second network is motivated by the hippocampus and is\ntrained to store and replay a movie scene. force trained networks reproduce behaviors\ncomparable in complexit", "neuron\n\narticle\n\npreference distributions of primary\nmotor cortex neurons re\ufb02ect control solutions\noptimized for limb biomechanics\n\ntimothy p. lillicrap1,2,* and stephen h. scott2,3,*\n1department of pharmacology, university of oxford, oxford ox1 3qt, uk\n2centre for neuroscience studies\n3department of biomedical and molecular sciences\nqueen\u2019s university, kingston, on k7l 3n6, canada\n*correspondence: timothy.lillicrap@pharm.ox.ac.uk (t.p.l.), steve.scott@queensu.ca (s.h.s.)\nhttp://dx.doi.org/10.1016/j.neuron.2012.10.041\n\nsummary\n\nneurons in monkey primary motor cortex (m1) tend\nto be most active for certain directions of hand move-\nment and joint-torque loads applied to the limb. the\norigin and function of these biases in preference\ndistribution are unclear but may be key to under-\nstanding the causal role of m1 in limb control. we\ndemonstrate that these distributions arise naturally\nin a network model that commands muscle activity\nand is optimized to control movements and counter\napplie", "three types of incremental learning\n\nhttps://doi.org/10.1038/s42256-022-00568-3\n\nreceived: 1 october 2021\n\naccepted: 18 october 2022\n\npublished online: 5 december 2022\n\n check for updates\n\ngido m. van de ven\u2009\n\n \u20091,2,3 \n\n, tinne tuytelaars3 & andreas s. tolias\u2009\n\n \u20091,4\n\nincrementally learning new information from a non-stationary stream \nof data, referred to as \u2018continual learning\u2019, is a key feature of natural \nintelligence, but a challenging problem for deep neural networks. in \nrecent years, numerous deep learning methods for continual learning \nhave been proposed, but comparing their performances is difficult due to \nthe lack of a common framework. to help address this, we describe three \nfundamental types, or \u2018scenarios\u2019, of continual learning: task-incremental, \ndomain-incremental and class-incremental learning. each of these \nscenarios has its own set of challenges. to illustrate this, we provide a \ncomprehensive empirical comparison of currently used continual learning \nstrategies", "attractor dynamics gate cortical information flow \nduring decision-making\n\narseny finkelstein\u200a\n\u200a1\u2009\u2709\nkarel svoboda\u200a\n\n\u200a1,3, lorenzo fontolan\u200a\n\n\u200a1,3, michael n. economo1, nuo li1,2, sandro romani\u200a\n\n\u200a1\u2009\u2709 and \n\ndecisions are held in memory until enacted, which makes them potentially vulnerable to distracting sensory input. gating of \ninformation flow from sensory to motor areas could protect memory from interference during decision-making, but the underly-\ning network mechanisms are not understood. here, we trained mice to detect optogenetic stimulation of the somatosensory cor-\ntex, with a delay separating sensation and action. during the delay, distracting stimuli lost influence on behavior over time, even \nthough distractor-evoked neural activity percolated through the cortex without attenuation. instead, choice-encoding activity \nin the motor cortex became progressively less sensitive to the impact of distractors. reverse engineering of neural networks \ntrained to reproduce motor cortex", "large-scale neural recordings call for\nnew insights to link brain and behavior\nanne e. urai1,2, brent doiron3, andrew m. leifer4 and anne k. churchland1,5\n\n1 cold spring harbor laboratory, cold spring harbor, ny, usa;\n\n2 leiden university, leiden, the netherlands; 3 university of chicago, chicago, il, usa;\n4 princeton university, princeton, nj, usa; 5 university of california los angeles, ca, usa\n\n2021-07-23\n\nabstract\nneuroscientists today can measure activity from more neurons than ever before, and are facing the\nchallenge of connecting these brain-wide neural recordings to computation and behavior. here, we first\ndescribe emerging tools and technologies being used to probe large-scale brain activity and new\napproaches to characterize behavior in the context of such measurements. we next highlight insights\nobtained from large-scale neural recordings in diverse model systems, and argue that some of these\npose a challenge to traditional theoretical frameworks. finally, we elaborate on e", "2\n2\n0\n2\n\n \n\ng\nu\na\n4\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n6\n2\n9\n4\n0\n\n.\n\n7\n0\n7\n1\n:\nv\ni\nx\nr\na\n\ntheoretical insights into the optimization landscape of\n\nover-parameterized shallow neural networks\n\nmahdi soltanolkotabi\u2217 adel javanmard\u2020 jason d. lee\u2020\n\njuly 15, 2017; revised july 2022\n\nabstract\n\nin this paper we study the problem of learning a shallow arti\ufb01cial neural network that best\n\ufb01ts a training data set. we study this problem in the over-parameterized regime where the\nnumber of observations are fewer than the number of parameters in the model. we show that\nwith quadratic activations the optimization landscape of training such shallow neural networks\nhas certain favorable characteristics that allow globally optimal models to be found e\ufb03ciently\nusing a variety of local search heuristics. this result holds for an arbitrary training data of\ninput/output pairs. for di\ufb00erentiable activation functions we also show that gradient descent,\nwhen suitably initialized, converges at a linear rate to a g", "axiomatic attribution for deep networks\n\nmukund sundararajan * 1 ankur taly * 1 qiqi yan * 1\n\n7\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n3\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n6\n3\n1\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe study the problem of attributing the pre-\ndiction of a deep network to its input features,\na problem previously studied by several other\nworks. we identify two fundamental axioms\u2014\nsensitivity and implementation invariance that\nattribution methods ought to satisfy. we show\nthat they are not satis\ufb01ed by most known attri-\nbution methods, which we consider to be a fun-\ndamental weakness of those methods. we use\nthe axioms to guide the design of a new attri-\nbution method called integrated gradients. our\nmethod requires no modi\ufb01cation to the original\nnetwork and is extremely simple to implement;\nit just needs a few calls to the standard gradi-\nent operator. we apply this method to a couple\nof image models, a couple of text models and a\nchemistry model, demonstrating its ability to de-\nbug networks,", "global versus local methods\n\nin nonlinear dimensionality reduction\n\nvin de silva\n\ndepartment of mathematics,\n\nstanford university,\nstanford. ca 94305\n\nsilva@math.stanford.edu\n\njoshua b. tenenbaum\n\ndepartment of brain and cognitive sciences,\n\nmassachusetts institute of technology,\n\ncambridge. ma 02139\njbt@ai.mit.edu\n\nabstract\n\nrecently proposed algorithms for nonlinear dimensionality reduction fall\nbroadly into two categories which have different advantages and disad-\nvantages: global (isomap [1]), and local (locally linear embedding [2],\nlaplacian eigenmaps [3]). we present two variants of isomap which\ncombine the advantages of the global approach with what have previ-\nously been exclusive advantages of local methods: computational spar-\nsity and the ability to invert conformal maps.\n\n1 introduction\n\nin this paper we discuss the problem of nonlinear dimensionality reduction (nldr): the\ntask of recovering meaningful low-dimensional structures hidden in high-dimensional data.\nan example ", "frontal cortex neuron types categorically \nencode single decision variables\n\nhttps://doi.org/10.1038/s41586-019-1816-9\nreceived: 8 may 2017\naccepted: 15 october 2019\npublished online: 4 december 2019\n\njunya hirokawa1,2,5, alexander vaughan1,5, paul masset1,3,4, torben ott1 & adam kepecs1*\n\nindividual neurons in many cortical regions have been found to encode specific, \nidentifiable features of the environment or body that pertain to the function of the \nregion1\u20133. however, in frontal cortex, which is involved in cognition, neural responses \ndisplay baffling complexity, carrying seemingly disordered mixtures of sensory, \nmotor and other task-related variables4\u201313. this complexity has led to the suggestion \nthat representations in individual frontal neurons are randomly mixed and can only \nbe understood at the neural population level14,15. here we show that neural activity in \nrat orbitofrontal cortex (ofc) is instead highly structured: single neuron activity  \nco-varies with individual ", "a sparse coding model with synaptically local plasticity\nand spiking neurons can account for the diverse shapes\nof v1 simple cell receptive fields\n\njoel zylberberg1,2*, jason timothy murphy3, michael robert deweese1,2,3\n\n1 department of physics, university of california, berkeley, california, united states of america, 2 redwood center for theoretical neuroscience, university of california,\nberkeley, california, united states of america, 3 helen wills neuroscience institute, university of california, berkeley, california, united states of america\n\nabstract\n\nsparse coding algorithms trained on natural images can accurately predict the features that excite visual cortical neurons,\nbut it is not known whether such codes can be learned using biologically realistic plasticity rules. we have developed a\nbiophysically motivated spiking network, relying solely on synaptically local information, that can predict the full diversity of\nv1 simple cell receptive field shapes when trained on natural ", "somatostatin-expressing neurons  \nin cortical networks\n\njoanna urban-ciecko and alison l.\u00a0barth\n\nabstract | somatostatin-expressing gabaergic neurons constitute a major class of \ninhibitory neurons in the mammalian cortex and are characterized by dense wiring \ninto the local network and high basal firing activity that persists in the absence of \nsynaptic input. this firing provides both gaba type a receptor (gabaar)- and \ngababr-mediated inhibition that operates at fast and slow timescales. the activity \nof somatostatin-expressing neurons is regulated by brain state, during learning and \nin rewarded behaviour. here, we review recent advances in our understanding of \nhow this class of cells can control network activity, with specific reference to how \nthis is constrained by their anatomical and electrophysiological properties.\n\ninhibitory neurons in the cerebral \ncortex can be categorized into multiple \nmolecularly and anatomically distinct \nclasses that have very different and highly \n", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nlearning  with  three  factors:  modulating  hebbian\nplasticity  with  errors\n\u0142ukasz  ku\u0013smierz,  takuya  isomura  and  taro  toyoizumi\n\nsynaptic  plasticity  is  a  central  theme  in  neuroscience.  a\nframework  of  three-factor  learning  rules  provides  a  powerful\nabstraction,  helping  to  navigate  through  the  abundance  of\nmodels  of  synaptic  plasticity.  it  is  well-known  that  the\ndopamine  modulation  of  learning  is  related  to  reward,  but\ntheoretical  models  predict  other  functional  roles  of  the\nmodulatory  third  factor;  it  may  encode  errors  for  supervised\nlearning,  summary  statistics  of  the  population  activity  for\nunsupervised  learning  or  attentional  feedback.  specialized\nstructures  may  be  needed  in  order  to  generate  and  propagate\nthird  factors  in  the  neural  network.\n\naddress\nriken  brain  science  institute,  2-1  hirosawa,  wako,  saitama  351-0198,\njap", "unsupervised cross-lingual representation learning at scale\n\nalexis conneau\u2217 kartikay khandelwal\u2217\n\nnaman goyal vishrav chaudhary guillaume wenzek francisco guzm\u00b4an\n\nedouard grave myle ott luke zettlemoyer veselin stoyanov\n\nfacebook ai\n\nabstract\n\nthis paper shows that pretraining multilingual\nlanguage models at scale leads to signi\ufb01cant\nperformance gains for a wide range of cross-\nlingual transfer tasks. we train a transformer-\nbased masked language model on one hundred\nlanguages, using more than two terabytes of \ufb01l-\ntered commoncrawl data. our model, dubbed\nxlm-r, signi\ufb01cantly outperforms multilingual\nbert (mbert) on a variety of cross-lingual\nbenchmarks, including +14.6% average accu-\nracy on xnli, +13% average f1 score on\nmlqa, and +2.4% f1 score on ner. xlm-r\nperforms particularly well on low-resource lan-\nguages, improving 15.7% in xnli accuracy\nfor swahili and 11.4% for urdu over previ-\nous xlm models. we also present a detailed\nempirical analysis of the key factors that are\nrequi", "published as a conference paper at iclr 2022\n\nneural networks as kernel learners: the\nsilent alignment effect\n\nalexander atanasov\u2217 , blake bordelon\u2217 & cengiz pehlevan\nharvard university\ncambridge, ma 02138, usa\n{atanasov,blake bordelon,cpehlevan}@g.harvard.edu\n\nabstract\n\nneural networks in the lazy training regime converge to kernel machines. can\nneural networks in the rich feature learning regime learn a kernel machine with\na data-dependent kernel? we demonstrate that this can indeed happen due to a\nphenomenon we term silent alignment, which requires that the tangent kernel of\na network evolves in eigenstructure while small and before the loss appreciably\ndecreases, and grows only in overall scale afterwards. we empirically show that\nsuch an effect takes place in homogenous neural networks with small initialization\nand whitened data. we provide an analytical treatment of this effect in the fully\nconnected linear network case.\nin general, we \ufb01nd that the kernel develops a\nlow-rank cont", "research article\n\nbiologically plausible learning in recurrent\nneural networks reproduces neural\ndynamics observed during cognitive tasks\nthomas miconi*\n\nthe neurosciences institute, california, united states\n\nabstract neural activity during cognitive tasks exhibits complex dynamics that flexibly encode\ntask-relevant variables. chaotic recurrent networks, which spontaneously generate rich dynamics,\nhave been proposed as a model of cortical computation during cognitive tasks. however, existing\nmethods for training these networks are either biologically implausible, and/or require a\ncontinuous, real-time error signal to guide learning. here we show that a biologically plausible\nlearning rule can train such recurrent networks, guided solely by delayed, phasic rewards at the\nend of each trial. networks endowed with this learning rule can successfully learn nontrivial tasks\nrequiring flexible (context-dependent) associations, memory maintenance, nonlinear mixed\nselectivities, and coordinati", "a unified approach to linking experimental, statistical\nand computational analysis of spike train data\n\nliang meng1, mark a. kramer1, steven j. middleton2, miles a. whittington2, uri t. eden1*\n\n1 department of mathematics and statistics, boston university, boston, massachusetts, united states of america, 2 hull york medical school, york university, york, united\nkingdom\n\nabstract\n\na fundamental issue in neuroscience is how to identify the multiple biophysical mechanisms through which neurons\ngenerate observed patterns of spiking activity. in previous work, we proposed a method for linking observed patterns of\nspiking activity to specific biophysical mechanisms based on a state space modeling framework and a sequential monte\ncarlo, or particle filter, estimation algorithm. we have shown, in simulation, that this approach is able to identify a space of\nsimple biophysical models that were consistent with observed spiking data (and included the model that generated the\ndata), but have yet t", "letters to nature\n\n(average directional preference difference: 138.3 ^ 26.28, range: 89.18). for cell pairs in\nwhich two directions of plaid motion ful\ufb01lled these selection criteria, neuronal correlation\ncoef\ufb01cients (nccs) were averaged.\n\n16. kreiter, a. k. & singer, w. stimulus-dependent synchronization of neuronal responses in the visual\n\ncortex of the awake macaque monkey. j. neurosci. 16, 2381\u20132396 (1996).\n\n17. thiele, a., distler, c. & hoffmann, k. p. decision-related activity in the macaque dorsal visual\n\nperceptual reports\none monkey reported the direction of moving plaid patterns (six-alternative forced-\nchoice task). during training, the animal viewed plaid patterns with a random-dot texture\n\u2018pasted\u2019 on the thin bar phase of the gratings (for demonstrations and details see\nsupplementary section 6). the texture on the two gratings either moved in the same\ndirection (unambiguous coherent motion) or in different directions (unambiguous non-\ncoherent motion). the monkey had one va", "psychological review\n2013, vol. 120, no. 1, 190 \u2013229\n\n\u00a9 2013 american psychological association\n0033-295x/13/$12.00 doi: 10.1037/a0030852\n\ncognitive control over learning: creating, clustering, and generalizing\n\ntask-set structure\n\nanne g. e. collins and michael j. frank\n\nbrown university\n\nlearning and executive functions such as task-switching share common neural substrates, notably\nprefrontal cortex and basal ganglia. understanding how they interact requires studying how cognitive\ncontrol facilitates learning but also how learning provides the (potentially hidden) structure, such as\nabstract rules or task-sets, needed for cognitive control. we investigate this question from 3 comple-\nmentary angles. first, we develop a new context-task-set (c-ts) model, inspired by nonparametric\nbayesian methods, specifying how the learner might infer hidden structure (hierarchical rules) and decide\nto reuse or create new structure in novel situations. second, we develop a neurobiologically explicit\n", "7\n1\n0\n2\n \nc\ne\nd\n5\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\nv\n5\n1\n8\n1\n0\n\n.\n\n2\n1\n7\n1\n:\nv\ni\nx\nr\na\n\nmastering chess and shogi by self-play with a\n\ngeneral reinforcement learning algorithm\n\ndavid silver,1\u2217 thomas hubert,1\u2217 julian schrittwieser,1\u2217\n\nioannis antonoglou,1 matthew lai,1 arthur guez,1 marc lanctot,1\n\nlaurent sifre,1 dharshan kumaran,1 thore graepel,1\ntimothy lillicrap,1 karen simonyan,1 demis hassabis1\n\n1deepmind, 6 pancras square, london n1c 4ag.\n\u2217these authors contributed equally to this work.\n\nabstract\n\nthe game of chess is the most widely-studied domain in the history of arti\ufb01cial intel-\nligence. the strongest programs are based on a combination of sophisticated search tech-\nniques, domain-speci\ufb01c adaptations, and handcrafted evaluation functions that have been\nre\ufb01ned by human experts over several decades. in contrast, the alphago zero program\nrecently achieved superhuman performance in the game of go, by tabula rasa reinforce-\nment learning from games of self-play.\nin this paper, we ge", "proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394,\n\nuppsala, sweden, 11-16 july 2010. c(cid:13)2010 association for computational linguistics\n\n384\n\nwordrepresentations:asimpleandgeneralmethodforsemi-supervisedlearningjosephturiand\u00b4epartementd\u2019informatiqueetrechercheop\u00b4erationnelle(diro)universit\u00b4edemontr\u00b4ealmontr\u00b4eal,qu\u00b4ebec,canada,h3t1j4lastname@iro.umontreal.calevratinovdepartmentofcomputerscienceuniversityofillinoisaturbana-champaignurbana,il61801ratinov2@uiuc.eduyoshuabengiod\u00b4epartementd\u2019informatiqueetrechercheop\u00b4erationnelle(diro)universit\u00b4edemontr\u00b4ealmontr\u00b4eal,qu\u00b4ebec,canada,h3t1j4bengioy@iro.umontreal.caabstractifwetakeanexistingsupervisednlpsys-tem,asimpleandgeneralwaytoimproveaccuracyistouseunsupervisedwordrepresentationsasextrawordfeatures.weevaluatebrownclusters,collobertandweston(2008)embeddings,andhlbl(mnih&hinton,2009)embeddingsofwordsonbothnerandchunking.weusenearstate-of-the-artsupervisedbaselines,and\ufb01ndthateachofthet", "0\n2\n0\n2\n\n \nr\np\na\n9\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n8\n6\n1\n4\n0\n\n.\n\n5\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nequilibrium propagation with\n\ncontinual weight updates\n\nmaxence ernoult1,2, julie grollier2, damien querlioz1, yoshua bengio3,4, benjamin scellier3\n\n1centre de nanosciences et de nanotechnologies, universit\u00e9 paris-saclay\n2unit\u00e9 mixte de physique, cnrs, thales, universit\u00e9 paris-saclay\n3mila, universit\u00e9 de montr\u00e9al\n4canadian institute for advanced research\n\nabstract\n\nequilibrium propagation (ep) is a learning algorithm that bridges machine learn-\ning and neuroscience, by computing gradients closely matching those of back-\npropagation through time (bptt), but with a learning rule local in space. given\nan input x and associated target y, ep proceeds in two phases: in the \ufb01rst phase\nneurons evolve freely towards a \ufb01rst steady state; in the second phase output neu-\nrons are nudged towards y until they reach a second steady state. however, in\nexisting implementations of ep, the learning rule is not local in", "deep recurrent q-learning for partially observable mdps\n\nmatthew hausknecht and peter stone\n\ndepartment of computer science\nthe university of texas at austin\n{mhauskn, pstone}@cs.utexas.edu\n\n7\n1\n0\n2\n\n \n\nn\na\nj\n \n\n1\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n7\n2\n5\n6\n0\n\n.\n\n7\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\ndeep reinforcement learning has yielded pro\ufb01cient\ncontrollers for complex tasks. however,\nthese con-\ntrollers have limited memory and rely on being able\nto perceive the complete game screen at each deci-\nsion point. to address these shortcomings, this arti-\ncle investigates the effects of adding recurrency to\na deep q-network (dqn) by replacing the \ufb01rst\npost-convolutional fully-connected layer with a recur-\nrent lstm. the resulting deep recurrent q-network\n(drqn), although capable of seeing only a single frame\nat each timestep, successfully integrates information\nthrough time and replicates dqn\u2019s performance on\nstandard atari games and partially observed equivalents\nfeaturing \ufb02ickering game screens. a", "e\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\n\nsynaptic homeostasis and input selectivity follow\nfrom a calcium-dependent plasticity model\n\nluk chong yeung\u2020\u2021\u00a7, harel z. shouval\u2020\u00b6, brian s. blais\u2020\u50a8, and leon n. cooper\u2020\u2021\u2020\u2020\n\n\u2020institute for brain and neural systems, departments of \u2021physics and \u2020\u2020neuroscience, brown university, providence, ri 02912; \u00b6department of neurobiology\nand anatomy, university of texas medical school, houston, tx 77030; and \u50a8department of science and technology, bryant university, smith\ufb01eld, ri 02917\n\ncontributed by leon n. cooper, september 3, 2004\n\nmodi\ufb01cations in the strengths of synapses are thought to underlie\nmemory,\nlearning, and development of cortical circuits. many\ncellular mechanisms of synaptic plasticity have been investigated\nin which differential elevations of postsynaptic calcium concentra-\ntions play a key role in determining the direction and magnitude\nof synaptic changes. we have previously described a model of\nplasticity that uses calcium currents mediated by n-methyl-", "node perturbation in vanilla deep networks\n\npeter latham\n\nfebruary 25, 2019\n\n1 setup\n\nour goal is to compare node perturbation to stochastic gradient descent (sgd). we\u2019ll do\nthis in the context of a vanilla feedforward network. we\u2019ll start with a network with added\nnoise, because we\u2019ll need it for node perturbation,\n\nxk+1 = \u03c6(hk + \u03bek)\n\nhk = wk \u00b7 xk .\n\n(1a)\n\n(1b)\n\nhere everything in bold is a vector or matrix, the nonlinearity \u03c6 is pointwise, k labels layer\n(note that wk is the weight from layer k to layer k + 1), and \u03bek is a zero mean, uncorrelated\ngaussian random variable with variance \u03c32,\n\n(cid:104)\u03bek\u03bel(cid:105) = \u03c32 ikl\n\nwhere ikl is the identity matrix if k = l and 0 otherwise,\n\n(cid:26) identity matrix with the dimension of layer k\n\nikl =\n\n0\n\n(2)\n\n(3)\n\nk = l\nk (cid:54)= l .\n\nwe\u2019ll take bias into account by setting xk\n0 to 1 and not updating that variable. with this\ni0 is the bias for unit i in layer l. we\u2019ll use l to denote the loss; it depends on\nconvention, wk\nthe weights, w \u2261 (", "neural networks 144 (2021) 271\u2013278\n\ncontents lists available at sciencedirect\n\nneural networks\n\njournal homepage: www.elsevier.com/locate/neunet\n\n2021 special issue on ai and brain science: brain-inspired ai\nbiologically motivated learning method for deep neural networks\nusing hierarchical competitive learning\ntakashi shinozaki \u2217\n\ncenter for information and neural networks (cinet), national institute of information and communications technology (nict), 1-4 yamadaoka,\nsuita, osaka 565-0871, japan\ngraduate school of information science and technology, osaka university, 1-5 yamadaoka, suita, osaka 565-0871, japan\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\navailable online 3 september 2021\n\nkeywords:\nsemisupervised learning\nunsupervised learning\ndeep neural network\ndeep learning\nfeature extraction\n\n1. introduction\n\nthis study proposes a novel biologically motivated learning method for deep convolutional neural\nnetworks (cnns). the combination of cnns and backpropagation lea", "research article\n\narea 2 of primary somatosensory cortex\nencodes kinematics of the whole arm\nraeed h chowdhury1,2*, joshua i glaser3,4,5, lee e miller1,6,7,8*\n\n1department of biomedical engineering, northwestern university, evanston, united\nstates; 2systems neuroscience institute, university of pittsburgh, pittsburgh, united\nstates; 3interdepartmental neuroscience program, northwestern university,\nchicago, united states; 4department of statistics, columbia university, new york,\nunited states; 5zuckerman mind brain behavior institute, columbia university, new\nyork, united states; 6department of physiology, northwestern university, chicago,\nunited states; 7department of physical medicine and rehabilitation, northwestern\nuniversity, chicago, united states; 8shirley ryan abilitylab, chicago, united states\n\nabstract proprioception, the sense of body position, movement, and associated forces, remains\npoorly understood, despite its critical role in movement. most studies of area 2, a proprioc", "deep bayesian active learning with image data\n\nyarin gal 1 2 riashat islam 1 zoubin ghahramani 1 3\n\nabstract\n\neven though active learning forms an important\npillar of machine learning, deep learning tools\nare not prevalent within it. deep learning poses\nseveral dif\ufb01culties when used in an active learn-\ning setting. first, active learning (al) methods\ngenerally rely on being able to learn and update\nmodels from small amounts of data. recent ad-\nvances in deep learning, on the other hand, are no-\ntorious for their dependence on large amounts of\ndata. second, many al acquisition functions rely\non model uncertainty, yet deep learning methods\nrarely represent such model uncertainty. in this\npaper we combine recent advances in bayesian\ndeep learning into the active learning framework\nin a practical way. we develop an active learn-\ning framework for high dimensional data, a task\nwhich has been extremely challenging so far, with\nvery sparse existing literature. taking advantage\nof specialised ", "article\n\nhuman orbitofrontal cortex represents a cognitive\nmap of state space\n\nhighlights\nd we tested a novel theory of ofc function directly in humans\n\nwith fmri\n\nauthors\n\nnicolas w. schuck, ming bo cai,\nrobert c. wilson, yael niv\n\nd multivariate pattern analysis showed evidence for state\n\nencoding in ofc\n\ncorrespondence\nnschuck@princeton.edu\n\nd performance within and across participants was related to\n\nstate encoding in ofc\n\nd the \ufb01ndings provide strong support for the state\n\nrepresentation theory of ofc\n\nin brief\nschuck et al. present evidence that\norbitofrontal cortex contains an up-to-\ndate representation of task-related\ninformation during decision making. this\n\u2018\u2018state\u2019\u2019 representation might provide\nimportant input for ef\ufb01cient\nreinforcement learning and decision\nmaking elsewhere in the brain.\n\nschuck et al., 2016, neuron 91, 1402\u20131412\nseptember 21, 2016 \u00aa 2016 elsevier inc.\nhttp://dx.doi.org/10.1016/j.neuron.2016.08.019\n\n\f", "alearningalgorithmforcontinuallyrunningfully\n\nrecurrentneuralnetworks\nronaldj.williams\ncollegeofcomputerscience\nnortheasternuniversity\nboston,massachusetts\u0000\u0002\u0001\u0001\u0005\nand\ndavidzipser\ninstituteforcognitivescience\nuniversityofcalifornia,sandiego\nlajolla,california\t\u0002\u0000\t\u0003\nappearsinneuralcomputation,\u0001,pp.\u0002\u0007\u0000-\u0002\b\u0000,\u0001\t\b\t.\nabstract\ntheexactformofagradient-followinglearningalgorithmforcompletelyrecurrentnet-\nworksrunningincontinuallysampledtimeisderivedandusedasthebasisforpractical\nalgorithmsfortemporalsupervisedlearningtasks.thesealgorithmshave:(\u0001)theadvantage\nthattheydonotrequireapreciselyde(cid:12)nedtraininginterval,operatingwhilethenetwork\nruns;and(\u0002)thedisadvantagethattheyrequirenonlocalcommunicationinthenetworkbe-\ningtrainedandarecomputationallyexpensive.thesealgorithmsareshowntoallownetworks\nhavingrecurrentconnectionstolearncomplextasksrequiringtheretentionofinformation\novertimeperiodshavingeither(cid:12)xedorinde(cid:12)nitelength.\n\u0001 introduction\namajorprobleminconnectionisttheoryistodeveloplea", "review\n\nthe neural basis of the speed\u2013accuracy\ntradeoff\nrafal bogacz1, eric-jan wagenmakers2, birte u. forstmann2 and\nsander nieuwenhuis3\n\n1 department of computer science, university of bristol, bristol bs8 1ub, uk\n2 department of psychology, university of amsterdam, roetersstraat 15, 1018 wb amsterdam, the netherlands\n3 institute of psychology and leiden institute for brain and cognition, wassenaarseweg 52, 2333 ak leiden, the netherlands\n\nin many situations, decision makers need to negotiate\nbetween the competing demands of response speed and\nresponse accuracy, a dilemma generally known as the\nspeed\u2013accuracy tradeoff (sat). despite the ubiquity of\nsat, the question of how neural decision circuits imple-\nment sat has received little attention up until a year\nago. we review recent studies that show sat is modu-\nlated in association and pre-motor areas rather than in\nsensory or primary motor areas. furthermore, the stu-\ndies suggest that emphasis on response speed increases\nthe baselin", "front. comput. sci., 2018, 12(6): 1140\u20131148\nhttps://doi.org/10.1007/s11704-016-6107-0\n\nconvolutional adaptive denoising autoencoders for hierarchical\n\nfeature extraction\n\nqianjun zhang, lei zhang\n\nmachine intelligence laboratory, college of computer science, sichuan university, chengdu 610065, china\n\nc(cid:2) higher education press and springer-verlag gmbh germany, part of springer nature 2018\n\nabstract convolutional neural networks (cnns) are typi-\ncal structures for deep learning and are widely used in im-\nage recognition and classi\ufb01cation. however, the random ini-\ntialization strategy tends to become stuck at local plateaus\nor even diverge, which results in rather unstable and ine\ufb00ec-\ntive solutions in real applications. to address this limitation,\nwe propose a hybrid deep learning cnn-adapdae model,\nwhich applies the features learned by the adapdae algo-\nrithm to initialize cnn \ufb01lters and then train the improved\ncnn for classi\ufb01cation tasks. in this model, adapdae is pro-\nposed as a", "the journal of neuroscience, june 5, 2013 \u2022 33(23):9565\u20139575 \u2022 9565\n\nsystems/circuits\n\nmatching recall and storage in sequence learning with\nspiking neural networks\n\njohanni brea, walter senn, and jean-pascal pfister\ndepartment of physiology, and center for cognition, learning, and memory, university of bern, ch-3012 bern, switzerland\n\nstoring and recalling spiking sequences is a general problem the brain needs to solve. it is, however, unclear what type of biologically\nplausible learning rule is suited to learn a wide class of spatiotemporal activity patterns in a robust way. here we consider a recurrent\nnetwork of stochastic spiking neurons composed of both visible and hidden neurons. we derive a generic learning rule that is matched to\nthe neural dynamics by minimizing an upper bound on the kullback\u2013leibler divergence from the target distribution to the model\ndistribution. the derived learning rule is consistent with spike-timing dependent plasticity in that a presynaptic spike prec", "p. cisek, t. drew & j.f. kalaska (eds.)\nprogress in brain research, vol. 165\nissn 0079-6123\ncopyright r 2007 elsevier b.v. all rights reserved\n\nchapter 34\n\nto recognize shapes, \ufb01rst learn to generate images\n\ngeoffrey e. hinton\u0003\n\ndepartment of computer science, university of toronto, 10 kings college road, toronto, m5s 3g4 canada\n\nabstract: the uniformity of the cortical architecture and the ability of functions to move to different areas\nof cortex following early damage strongly suggest that there is a single basic learning algorithm for\nextracting underlying structure from richly structured, high-dimensional sensory data. there have been\nmany attempts to design such an algorithm, but until recently they all suffered from serious computational\nweaknesses. this chapter describes several of the proposed algorithms and shows how they can be com-\nbined to produce hybrid methods that work ef\ufb01ciently in networks with many layers and millions of\nadaptive connections.\n\nkeywords: learning algor", "neuroscience\nreview article\n\nm. heilbron, m. chait / neuroscience 389 (2018) 54\u201373\n\ngreat expectations: is there evidence for predictive coding in\nauditory cortex?\n\nmicha heilbron a,b* and maria chait c\na de\u00b4partement de biologie, e\u00b4 cole normale supe\u00b4rieure, paris 75005, france\nb universite\u00b4 pierre et marie curie p6, paris 75005, france\nc ear institute, university college london, london wc1x 8ee, united kingdom\n\nabstract\u2014predictive coding is possibly one of the most in\ufb02uential, comprehensive, and controversial theories of\nneural function. while proponents praise its explanatory potential, critics object that key tenets of the theory are\nuntested or even untestable. the present article critically examines existing evidence for predictive coding in the\nauditory modality. speci\ufb01cally, we identify \ufb01ve key assumptions of the theory and evaluate each in the light of\nanimal, human and modeling studies of auditory pattern processing. for the \ufb01rst two assumptions \u2013 that neural\nresponses are sh", "9424 \u2022 the journal of neuroscience, july 14, 2010 \u2022 30(28):9424 \u20139430\n\nbehavioral/systems/cognitive\n\nneuronal population coding of parametric working\nmemory\n\nomri barak,1 misha tsodyks,1 and ranulfo romo2\n1department of neurobiology, weizmann institute of science, rehovot 76100, israel, and 2instituto de fisiolog\u00eda celular-neurociencias, universidad\nnacional auto\u00b4noma de me\u00b4xico, 04510 mexico, d.f., mexico\n\ncomparing two sequentially presented stimuli is a widely used experimental paradigm for studying working memory. the delay activity\nof many single neurons in the prefrontal cortex (pfc) of monkeys was found to be stimulus-specific, however, population dynamics of\nstimulus representation has not been elucidated. we analyzed the population state of a large number of pfc neurons during a somato-\nsensory discrimination task. using the tuning curves of the neurons, we derived a compact characterization of the population state.\nstimulus representation by the population was found to degrad", "off-policy learning with options and\n\nrecognizers\n\ndoina precup\n\nmcgill university\n\nmontreal, qc, canada\n\nrichard s. sutton\nuniversity of alberta\nedmonton, ab, canada\n\ncosmin paduraru\nuniversity of alberta\nedmonton, ab, canada\n\nanna koop\n\nuniversity of alberta\nedmonton, ab, canada\n\nsatinder singh\n\nuniversity of michigan\nann arbor, mi, usa\n\nabstract\n\nwe introduce a new algorithm for off-policy temporal-difference learn-\ning with function approximation that has lower variance and requires less\nknowledge of the behavior policy than prior methods. we develop the no-\ntion of a recognizer, a \ufb01lter on actions that distorts the behavior policy to\nproduce a related target policy with low-variance importance-sampling\ncorrections. we also consider target policies that are deviations from\nthe state distribution of the behavior policy, such as potential temporally\nabstract options, which further reduces variance. this paper introduces\nrecognizers and their potential advantages, then develops a full", "research article\n\ndeep neural networks as a computational\nmodel for human shape sensitivity\n\njonas kubilius*, stefania bracci, hans p. op de beeck*\n\nbrain and cognition, university of leuven (ku leuven), leuven, belgium\n\n* jonas.kubilius@ppw.kuleuven.be (jk); hans.opdebeeck@ppw.kuleuven.be (hpodb)\n\na11111\n\nabstract\n\ntheories of object recognition agree that shape is of primordial importance, but there is no\nconsensus about how shape might be represented, and so far attempts to implement a\nmodel of shape perception that would work with realistic stimuli have largely failed. recent\nstudies suggest that state-of-the-art convolutional \u2018deep\u2019 neural networks (dnns) capture\nimportant aspects of human object perception. we hypothesized that these successes\nmight be partially related to a human-like representation of object shape. here we demon-\nstrate that sensitivity for shape features, characteristic to human and primate vision,\nemerges in dnns when trained for generic object recognition fr", ".\n\nd\ne\nv\nr\ne\ns\ne\nr\n \ns\nt\nh\ng\ni\nr\n \nl\nl\n\na\n\n \n.\n\ne\nr\nu\nt\na\nn\n \nr\ne\ng\nn\ni\nr\np\ns\n\n \nf\no\n \nt\nr\na\np\n\n \n,\n.\n\nc\nn\n\ni\n \n,\n\na\nc\n\n \n\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n7\n1\n0\n2\n \n\u00a9\n\n \n\na r t i c l e s\n\nthe spatial structure of correlated neuronal variability\nrobert rosenbaum1,2, matthew a smith3\u20135, adam kohn6,7, jonathan e rubin5,8 & brent doiron5,8\n\nshared neural variability is ubiquitous in cortical populations. while this variability is presumed to arise from overlapping  \nsynaptic input, its precise relationship to local circuit architecture remains unclear. we combine computational models and  \nin vivo recordings to study the relationship between the spatial structure of connectivity and correlated variability in neural \ncircuits. extending the theory of networks with balanced excitation and inhibition, we find that spatially localized lateral \nprojections promote weakly correlated spiking, but broader lateral projections produce a distinctive spatial correlation structure: \nnearby neuron pairs are posit", "from spiking neuron models to linear-nonlinear models\n\nsrdjan ostojic1,2*, nicolas brunel3\n\n1 center for theoretical neuroscience, columbia university, new york, new york, united states of america, 2 laboratoire de physique statistique, cnrs, universite\u00b4 pierre\net marie curie, universite\u00b4 paris-diderot, ecole normale supe\u00b4rieure, paris, france, 3 laboratory of neurophysics and physiology, cnrs umr 8119, universite\u00b4 paris descartes,\nparis, france\n\nabstract\n\nneurons transform time-varying inputs into action potentials emitted stochastically at a time dependent rate. the mapping\nfrom current input to output firing rate is often represented with the help of phenomenological models such as the linear-\nnonlinear (ln) cascade, in which the output firing rate is estimated by applying to the input successively a linear temporal\nfilter and a static non-linear transformation. these simplified models leave out the biophysical details of action potential\ngeneration. it is not a priori clear to whic", "4\n1\n0\n2\n\n \n\nv\no\nn\n4\n1\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n7\n7\n0\n4\n\n.\n\n1\n1\n4\n1\n:\nv\ni\nx\nr\na\n\na framework for studying synaptic plasticity\n\nwith neural spike train data\n\nby scott w. linderman\u2217, christopher h. stock, and ryan p. adams\u2020\n\nharvard university\n\nlearning and memory in the brain are implemented by complex,\ntime-varying changes in neural circuitry. the computational rules ac-\ncording to which synaptic weights change over time are the subject\nof much research, and are not precisely understood. until recently,\nlimitations in experimental methods have made it challenging to test\nhypotheses about synaptic plasticity on a large scale. however, as\nsuch data become available and these barriers are lifted, it becomes\nnecessary to develop analysis techniques to validate plasticity mod-\nels. here, we present a highly extensible framework for modeling ar-\nbitrary synaptic plasticity rules on spike train data in populations of\ninterconnected neurons. we treat synaptic weights as a (potentiall", "interpreting neuronal population activity by reconstruction: uni\ufb01ed\nframework with application to hippocampal place cells\n\nkechen zhang, 1 iris ginzburg, 1 bruce l. mcnaughton, 2 and terrence j. sejnowski 1,3\n1computational neurobiology laboratory, howard hughes medical institute, the salk institute for biological studies,\nla jolla, 92037; 2division of neural systems, memory, and aging and department of psychology, arizona research\nlaboratories, university of arizona, tucson, arizona 85724; and 3department of biology, university of california, san\ndiego, la jolla, california 92093\n\ndecoding problems have been studied previously (abbott\n1994; bialek et al. 1991; optican and richmond 1987; sali-\nnas and abbott 1994; seung and sompolinsky 1993; snippe\n1996; zemel et al. 1997; zohary et al. 1994).\n\nzhang, kechen, iris ginzburg, bruce l. mcnaughton, and ter-\nrence j. sejnowski. interpreting neuronal population activity by re-\nconstruction: uni\ufb01ed framework with application to hippocampal\npl", "3\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n1\n2\n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n2\nv\n8\n0\n7\n4\n\n.\n\n7\n0\n2\n1\n:\nv\ni\nx\nr\na\n\njournal of arti\ufb01cial intelligence research 47 (2013) 253\u2013279\n\nsubmitted 02/13; published 06/13\n\nthe arcade learning environment:\n\nan evaluation platform for general agents\n\nmarc g. bellemare\nuniversity of alberta, edmonton, alberta, canada\n\nyavar naddaf\nempirical results inc., vancouver,\nbritish columbia, canada\n\njoel veness\nmichael bowling\nuniversity of alberta, edmonton, alberta, canada\n\nmg17@cs.ualberta.ca\n\nyavar@empiricalresults.ca\n\nveness@cs.ualberta.ca\nbowling@cs.ualberta.ca\n\nabstract\n\nin this article we introduce the arcade learning environment (ale): both a chal-\nlenge problem and a platform and methodology for evaluating the development of general,\ndomain-independent ai technology. ale provides an interface to hundreds of atari 2600\ngame environments, each one di\ufb00erent, interesting, and designed to be a challenge for\nhuman players. ale presents signi\ufb01cant research challenges for reinforcemen", "vol 462 | 17 december 2009 | doi:10.1038/nature08577\n\nletters\n\nstably maintained dendritic spines are associated\nwith lifelong memories\nguang yang1, feng pan1 & wen-biao gan1\n\nchanges in synaptic connections are considered essential for learn-\ning and memory formation1\u20136. however, it is unknown how neural\ncircuits undergo continuous synaptic changes during learning while\nmaintaining lifelong memories. here we show, by following post-\nsynaptic dendritic spines over time in the mouse cortex7,8, that\nlearning and novel sensory experience lead to spine formation and\nelimination by a protracted process. the extent of spine remodelling\ncorrelates with behavioural improvement after learning, suggesting\na crucial role of synaptic structural plasticity in memory formation.\nimportantly, a small fraction of new spines induced by novel experi-\nence, together with most spines formed early during development\nand surviving experience-dependent elimination, are preserved and\nprovide a structural basis", "j neurol (2012) 259:1062\u20131070\ndoi 10.1007/s00415-011-6299-z\n\no r i g i n a l c o m m u n i c a t i o n\n\nvivid visual mental imagery in the absence\nof the primary visual cortex\n\nholly bridge \u2022 stephen harrold \u2022 emily a. holmes \u2022\nmark stokes \u2022 christopher kennard\n\nreceived: 14 september 2011 / accepted: 18 october 2011 / published online: 8 november 2011\n\u00f3 the author(s) 2011. this article is published with open access at springerlink.com\n\nabstract the role of the primary visual cortex in visual\nmental\nimagery has provided signi\ufb01cant debate in the\nimagery literature. functional neuroimaging studies show\nconsiderable variation depending on task and technique.\npatient studies can be dif\ufb01cult\nto interpret due to the\ndiverse nature of cortical damage. the type of cortical\ndamage in patient sbr is exceedingly rare as it is restricted\nto the gray matter of the calcarine sulcus. in this study, we\nshow that in spite of his near-complete cortical blindness,\nsbr exhibits vivid visual mental imagery", "6\n1\n0\n2\n\n \n\nv\no\nn\n6\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n7\n9\n3\n5\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nreinforcement learning with unsupervised\nauxiliary tasks\n\nmax jaderberg\u2217, volodymyr mnih*, wojciech marian czarnecki*\ntom schaul, joel z leibo, david silver & koray kavukcuoglu\ndeepmind\nlondon, uk\n{jaderberg,vmnih,lejlot,schaul,jzl,davidsilver,korayk}@google.com\n\nabstract\n\ndeep reinforcement learning agents have achieved state-of-the-art results by di-\nrectly maximising cumulative reward. however, environments contain a much\nwider variety of possible training signals. in this paper, we introduce an agent\nthat also maximises many other pseudo-reward functions simultaneously by rein-\nforcement learning. all of these tasks share a common representation that, like\nunsupervised learning, continues to develop in the absence of extrinsic rewards.\nwe also introduce a novel mechanism for focusing this representation upon ex-\ntrinsic rewards, so that learning can rapidly adapt to the most relevant aspects\nof", "smoothing of, and parameter estimation from, noisy\nbiophysical recordings\n\nquentin j. m. huys1,2*, liam paninski2,3\n\n1 gatsby computational neuroscience unit, university college london, london, united kingdom, 2 center for theoretical neuroscience, columbia university, new york,\nnew york, united states of america, 3 statistics department, columbia university, new york, new york, united states of america\n\nabstract\n\nbiophysically detailed models of single cells are difficult to fit to real data. recent advances in imaging techniques allow\nsimultaneous access to various intracellular variables, and these data can be used to significantly facilitate the modelling\ntask. these data, however, are noisy, and current approaches to building biophysically detailed models are not designed to\ndeal with this. we extend previous techniques to take the noisy nature of the measurements into account. sequential\nmonte carlo (\u2018\u2018particle filtering\u2019\u2019) methods, in combination with a detailed biophysical desc", "revisiting fundamentals of experience replay\n\nwilliam fedus * 1 2 prajit ramachandran * 1 rishabh agarwal 1 yoshua bengio 2 3 hugo larochelle 1 4\n\nmark rowland 5 will dabney 5\n\n0\n2\n0\n2\n\n \nl\nu\nj\n \n\n3\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n0\n0\n7\n6\n0\n\n.\n\n7\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nexperience replay is central to off-policy algo-\nrithms in deep reinforcement learning (rl), but\nthere remain signi\ufb01cant gaps in our understanding.\nwe therefore present a systematic and extensive\nanalysis of experience replay in q-learning meth-\nods, focusing on two fundamental properties: the\nreplay capacity and the ratio of learning updates to\nexperience collected (replay ratio). our additive\nand ablative studies upend conventional wisdom\naround experience replay \u2014 greater capacity is\nfound to substantially increase the performance\nof certain algorithms, while leaving others unaf-\nfected. counterintuitively we show that theoreti-\ncally ungrounded, uncorrected n-step returns are\nuniquely bene\ufb01cial while other techn", "cerebral cortex march 2014;24:677\u2013690\ndoi:10.1093/cercor/bhs348\nadvance access publication november 11, 2012\n\nemergence of complex computational structures from chaotic neural networks\nthrough reward-modulated hebbian learning\n\ngregor m. hoerzer, robert legenstein and wolfgang maass\n\ninstitute for theoretical computer science, graz university of technology, graz, austria\n\naddress correspondence to wolfgang maass. email: maass@igi.tugraz.at\n\nthis paper addresses the question how generic microcircuits of\nneurons in different parts of the cortex can attain and maintain\ndifferent computational specializations. we show that if stochastic\nvariations in the dynamics of local microcircuits are correlated with\nsignals related to functional improvements of the brain (e.g. in the\ncontrol of behavior), the computational operation of these microcir-\ncuits can become optimized for speci\ufb01c tasks such as the gener-\nation of speci\ufb01c periodic signals and task-dependent routing of\ninformation. furthermor", "the journal of neuroscience, august 10, 2011 \u2022 31(32):11597\u201311616 \u2022 11597\n\nbehavioral/systems/cognitive\n\nmapping human cortical areas in vivo based on myelin\ncontent as revealed by t1- and t2-weighted mri\n\nmatthew f. glasser and david c. van essen\ndepartment of anatomy and neurobiology, washington university school of medicine, st. louis, missouri 63110\n\nnoninvasively mapping the layout of cortical areas in humans is a continuing challenge for neuroscience. we present a new method of\nmapping cortical areas based on myelin content as revealed by t1-weighted (t1w) and t2-weighted (t2w) mri. the method is general-\nizable across different 3t scanners and pulse sequences. we use the ratio of t1w/t2w image intensities to eliminate the mr-related image\nintensity bias and enhance the contrast to noise ratio for myelin. data from each subject were mapped to the cortical surface and aligned\nacross individuals using surface-based registration. the spatial gradient of the group average myelin map ", "sampling and bayes' inference in scientific modelling and robustness \nauthor(s): george e. p. box \nsource: journal of the royal statistical society. series a (general), 1980, vol. 143, no. 4 \n(1980), pp. 383-430\n \npublished by: wiley for the royal statistical society \n\nstable url: https://www.jstor.org/stable/2982063\n\njstor is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide \nrange of content in a trusted digital archive. we use information technology and tools to increase productivity and \nfacilitate new forms of scholarship. for more information about jstor, please contact support@jstor.org. \n \nyour use of the jstor archive indicates your acceptance of the terms & conditions of use, available at \nhttps://about.jstor.org/terms\n\nroyal statistical society and wiley are collaborating with jstor to digitize, preserve and \nextend access to journal of the royal statistical society. series a (general)\n\nthis content downloaded from \n", "impala: scalable distributed deep-rl with importance weighted\n\nactor-learner architectures\n\nlasse espeholt * 1 hubert soyer * 1 remi munos * 1 karen simonyan 1 volodymyr mnih 1 tom ward 1\n\nyotam doron 1 vlad firoiu 1 tim harley 1 iain dunning 1 shane legg 1 koray kavukcuoglu 1\n\n8\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n8\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n1\n6\n5\n1\n0\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nin this work we aim to solve a large collection\nof tasks using a single reinforcement learning\nagent with a single set of parameters. a key\nchallenge is to handle the increased amount of\ndata and extended training time. we have devel-\noped a new distributed agent impala (impor-\ntance weighted actor-learner architecture) that\nnot only uses resources more ef\ufb01ciently in single-\nmachine training but also scales to thousands of\nmachines without sacri\ufb01cing data ef\ufb01ciency or\nresource utilisation. we achieve stable learning at\nhigh throughput by combining decoupled acting\nand learning with a novel off-policy correction\nmet", "neuron, vol. 21, 1163\u20131175, november, 1998, copyright \u00aa1998 by cell press\n\ninvolvement of a postsynaptic\nprotein kinase a substrate in the expression\nof homosynaptic long-term depression\n\nkimihiko kameyama,*\u00a7 hey-kyoung lee,\u2020\u00a7\nmark f. bear,\u2020 and richard l. huganir*\u2021\n* howard hughes medical institute\ndepartment of neuroscience\nthe johns hopkins university\n\nschool of medicine\n\nbaltimore, maryland 21205\n\u2020 howard hughes medical institute\ndepartment of neuroscience\nbrown university\nprovidence, rhode island 02912\n\nsummary\n\nhippocampal n-methyl-d-aspartate (nmda) receptor\u2013\ndependent long-term synaptic depression (ltd) is as-\nsociated with a persistent dephosphorylation of the\nglur1 subunit of ampa receptors at a site (ser-845)\nphosphorylated by camp-dependent protein kinase\n(pka). in the present study, we show that dephosphor-\nylation of a postsynaptic pka substrate may be crucial\nfor ltd expression. pka activators inhibited both ampa\nreceptor dephosphorylation and ltd.\ninjection of\na camp an", "training spiking neural networks using lessons from\n\ndeep learning\n\n3\n2\n0\n2\n\n \n\ng\nu\na\n3\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n6\nv\n4\n9\n8\n2\n1\n\n.\n\n9\n0\n1\n2\n:\nv\ni\nx\nr\na\n\njason k. eshraghian*\n\nuc santa cruz\n\nuniversity of michigan\n\njeshragh@ucsc.edu\n\nmax ward\n\nuwa\n\nharvard university\n\nemre neftci\n\nforschungszentrum j\u00fclich\n\nrwth aachen\n\nxinxin wang\n\nuniversity of michigan\n\ngregor lenz\n\nsynsense\n\ngirish dwivedi\n\nuwa\n\nmohammed bennamoun\n\nuwa\n\ndoo seok jeong\nhanyang university\n\nwei d. lu*\n\nuniversity of michigan\n\nwluee@umich.edu\n\nabstract\n\nthe brain is the perfect place to look for inspiration to develop more efficient neural networks. the\ninner workings of our synapses and neurons provide a glimpse at what the future of deep learning\nmight look like. this paper serves as a tutorial and perspective showing how to apply the lessons\nlearnt from several decades of research in deep learning, gradient descent, backpropagation and\nneuroscience to biologically plausible spiking neural neural networks.\nwe also e", "6\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n8\n9\n4\n3\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nimproved techniques for training gans\n\ntim salimans\n\ntim@openai.com\n\nian goodfellow\n\nian@openai.com\n\nwojciech zaremba\nwoj@openai.com\n\nvicki cheung\n\nvicki@openai.com\n\nalec radford\n\nalec.radford@gmail.com\n\nxi chen\n\npeter@openai.com\n\nabstract\n\nwe present a variety of new architectural features and training procedures that we\napply to the generative adversarial networks (gans) framework. we focus on two\napplications of gans: semi-supervised learning, and the generation of images\nthat humans \ufb01nd visually realistic. unlike most work on generative models, our\nprimary goal is not to train a model that assigns high likelihood to test data, nor do\nwe require the model to be able to learn well without using any labels. using our\nnew techniques, we achieve state-of-the-art results in semi-supervised classi\ufb01ca-\ntion on mnist, cifar-10 and svhn. the generated images are of high quality\nas con\ufb01rmed by a visual tu", "the elephant in the room\n\namir rosenfeld1, richard zemel2, and john k. tsotsos1\n\n1\n\n1york university\n\n2university of toronto\n\n1,2toronto, canada\n\n8\n1\n0\n2\n\n \n\ng\nu\na\n9\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n5\n0\n3\n3\n0\n\n.\n\n8\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe showcase a family of common failures of state-of-the\nart object detectors. these are obtained by replacing image\nsub-regions by another sub-image that contains a trained\nobject. we call this \u201cobject transplanting\u201d. modifying an\nimage in this manner is shown to have a non-local impact\non object detection. slight changes in object position can\naffect its identity according to an object detector as well\nas that of other objects in the image. we provide some\nanalysis and suggest possible reasons for the reported\nphenomena.\n\nintroduction\n\nreliable systems for image understanding are crucial for\napplications such as autonomous driving, medical imaging,\netc.\n\nadversarial examples [12] have been suggested as small\ntargeted perturbations. we show anothe", "a computational model of birdsong learning by auditory \nexperience and auditory feedback \n\n* \n\nkenji d ~ y a ' . ~  and terrence j. sejnow~ki~.~ \n\n'atr human information processing research laboratories, seika, soraku, \nkyoto 619-02, japan \n2howard hughes medical institute, the salk institute for biological studies, \nla jolla, california 92037, u.s.a. \n3department of biology, university of california, san diego, la jolla, \ncalifornia 92093, u.s.a. \n\nintroduction \n\nin addition to the goal of acquiring a precise description of the acoustic environment, \ncentral auditory processing also provides useful information for animal behaviors, such as \nnavigation and communication. singing is a learned behavior of male songbirds for protect- \ning territories and attracting females (konishi, 1985; catchpole and slater, 1995). it has been \nexperimentally shown that singing behavior depends on auditory information in two ways. \nfirst, the phonetic features of a bird's song depends on the bird's audi", "ifac papersonline 54-9 (2021) 285\u2013290\n\ndesigning experiments for data-driven\ndesigning experiments for data-driven\ndesigning experiments for data-driven\ndesigning experiments for data-driven\ndesigning experiments for data-driven\ndesigning experiments for data-driven\ndesigning experiments for data-driven\ndesigning experiments for data-driven\n\ncontrol of nonlinear systems\ncontrol of nonlinear systems\ncontrol of nonlinear systems\ncontrol of nonlinear systems\ncontrol of nonlinear systems\ncontrol of nonlinear systems\ncontrol of nonlinear systems\ncontrol of nonlinear systems\n\nclaudio de persis \u2217 pietro tesi \u2217\u2217\nclaudio de persis \u2217 pietro tesi \u2217\u2217\nclaudio de persis \u2217 pietro tesi \u2217\u2217\nclaudio de persis \u2217 pietro tesi \u2217\u2217\nclaudio de persis \u2217 pietro tesi \u2217\u2217\nclaudio de persis \u2217 pietro tesi \u2217\u2217\nclaudio de persis \u2217 pietro tesi \u2217\u2217\nclaudio de persis \u2217 pietro tesi \u2217\u2217\n\n\u2217 enteg, university of groningen, nijenborgh 4, 9747ag groningen,\n\u2217 enteg, university of groningen, nijenborgh 4, 9747ag groningen,\n\u2217 enteg, u", "journal of machine learning research 13 (2012) 307-361\n\nsubmitted 12/10; revised 11/11; published 2/12\n\nnoise-contrastive estimation of unnormalized statistical models,\n\nwith applications to natural image statistics\n\nmichael u. gutmann\naapo hyv\u00a8arinen\ndepartment of computer science\ndepartment of mathematics and statistics\nhelsinki institute for information technology hiit\nuniversity of helsinki, finland\n\neditor: yoshua bengio\n\nmichael.gutmann@helsinki.fi\naapo.hyvarinen@helsinki.fi\n\nabstract\n\nwe consider the task of estimating, from observed data, a probabilistic model that is parameterized\nby a \ufb01nite number of parameters. in particular, we are considering the situation where the model\nprobability density function is unnormalized. that is, the model is only speci\ufb01ed up to the partition\nfunction. the partition function normalizes a model so that it integrates to one for any choice of\nthe parameters. however, it is often impossible to obtain it in closed form. gibbs distributions,\nmarkov ", "neural networks 16 (2003) 5\u20139\n\nneural networks letter\n\nwww.elsevier.com/locate/neunet\n\nmeta-learning in reinforcement learning\n\nnicolas schweighofera,*, kenji doyaa,b,1\n\nacrest, japan science and technology corporation, atr, human information science laboratories, 2-2-2 hikaridai, seika-cho,\n\nbatr human information science laboratories, 2-2-2 hikaridai, seika-cho, soraku-gun, kyoto 619-0288, japan\n\nsoraku-gun, kyoto 619-0288, japan\n\nreceived 6 september 2002; accepted 10 october 2002\n\nabstract\n\nmeta-parameters in reinforcement learning should be tuned to the environmental dynamics and the animal performance. here, we propose\na biologically plausible meta-reinforcement learning algorithm for tuning these meta-parameters in a dynamic, adaptive manner. we tested\nour algorithm in both a simulation of a markov decision task and in a non-linear control task. our results show that the algorithm robustly\n\ufb01nds appropriate meta-parameter values, and controls the meta-parameter time course, in bo", "perception & psychophysics\n2003, 65 (7), 1136-1144\n\na gradual spread of attention\nduring mental curve tracing\n\nnetherlands ophthalmic research institute, amsterdam, the netherlands\n\nr. houtkamp\n\nand amc, graduate school neurosciences amsterdam, amsterdam, the netherlands\n\namc, graduate school neurosciences amsterdam, amsterdam, the netherlands\n\nh. spekreijse\n\nnetherlands ophthalmic research institute, amsterdam, the netherlands\n\np. r. roelfsema\n\nand amc, graduate school neurosciences amsterdam, amsterdam, the netherlands\n\nand\n\nthe visual system has to segregate objects that are relevant to behavior from other objects and the\nbackground, if they are embedded in a visual scene. this segregation process can be time consuming,\nespecially if the relevant object is spatially extended and overlaps with other image components, but\nthe cause of the delays is presently not well understood. in the present study, we used a curve-tracing\ntask to investigate processing delays during the grouping of ", "13402 \u2022 the journal of neuroscience, september 30, 2015 \u2022 35(39):13402\u201313418\n\nsystems/circuits\n\nsimple learned weighted sums of inferior temporal\nneuronal firing rates accurately predict human core\nobject recognition performance\n\nx najib j. majaj,1,2* x ha hong,1,2,3* x ethan a. solomon,1,2 and x james j. dicarlo1,2\n1department of brain and cognitive sciences, 2mcgovern institute for brain research, and 3harvard\u2013massachusetts institute of technology division of\nhealth sciences and technology, massachusetts institute of technology, cambridge, massachusetts 02139\n\nto go beyond qualitative models of the biological substrate of object recognition, we ask: can a single ventral stream neuronal linking\nhypothesis quantitatively account for core object recognition performance over a broad range of tasks? we measured human perfor-\nmance in 64 object recognition tests using thousands of challenging images that explore shape similarity and identity preserving object\nvariation. we then used multie", "tools and resources\n\nreal-time classification of experience-\nrelated ensemble spiking patterns for\nclosed-loop applications\ndavide ciliberti1,2,3*, fre\u00b4 de\u00b4 ric michon1,2,3, fabian kloosterman1,2,3,4*\n\n1neuro-electronics research flanders, leuven, belgium; 2brain and cognition, ku\nleuven, leuven, belgium; 3vib, leuven, belgium; 4imec, leuven, belgium\n\nabstract communication in neural circuits across the cortex is thought to be mediated by\nspontaneous temporally organized patterns of population activity lasting ~50 \u2013200 ms. closed-loop\nmanipulations have the unique power to reveal direct and causal links between such patterns and\ntheir contribution to cognition. current brain\u2013computer interfaces, however, are not designed to\ninterpret multi-neuronal spiking patterns at the millisecond timescale. to bridge this gap, we\ndeveloped a system for classifying ensemble patterns in a closed-loop setting and demonstrated its\napplication in the online identification of hippocampal neuronal replay ", "atheoreticalframeworkfortargetpropagationalexandermeulemans1,francescos.carzaniga1,johana.k.suykens2,jo\u00e3osacramento1,benjaminf.grewe11instituteofneuroinformatics,universityofz\u00fcrichandethz\u00fcrich2esat-stadius,kuleuvenameulema@ethz.chabstractthesuccessofdeeplearning,abrain-inspiredformofai,hassparkedinterestinunderstandinghowthebraincouldsimilarlylearnacrossmultiplelayersofneurons.however,themajorityofbiologically-plausiblelearningalgorithmshavenotyetreachedtheperformanceofbackpropagation(bp),noraretheybuiltonstrongtheoreticalfoundations.here,weanalyzetargetpropagation(tp),apopularbutnotyetfullyunderstoodalternativetobp,fromthestandpointofmathematicaloptimization.ourtheoryshowsthattpiscloselyrelatedtogauss-newtonoptimizationandthussubstantiallydiffersfrombp.furthermore,ouranalysisrevealsafundamentallimitationofdifferencetargetpropagation(dtp),awell-knownvariantoftp,intherealisticscenarioofnon-invertibleneuralnetworks.weprovidea\ufb01rstsolutiontothisproblemthroughanovelreconstructionlossthatimp", "a\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\nt\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nhhs public access\nauthor manuscript\nscience. author manuscript; available in pmc 2016 march 20.\n\npublished in final edited form as:\n\nscience. 2015 july 10; 349(6244): 184\u2013187. doi:10.1126/science.aaa4056.\n\nsingle-trial spike trains in parietal cortex reveal discrete steps \nduring decision-making\n\nkenneth w. latimer1,2, jacob l. yates1,2, miriam l. r. meister2,3, alexander c. huk1,2,4,5, \nand jonathan w. pillow1,2,5,6,*\n1center for perceptual systems, the university of texas at austin, austin, tx 78712, usa\n\n2institute for neuroscience, the university of texas at austin, austin, tx 78712, usa\n\n3department of physiology and biophysics, university of washington, seattle, wa 98195, usa\n\n4department of neuroscience, the university of texas at austin, austin, tx 78712, usa\n\n5department of psychology, the university of texas at austin, austi", "generalization of equilibrium propagation to vector field dynamics\n\nbenjamin scellier1, anirudh goyal1, jonathan binas1, thomas mesnard2, yoshua bengio1\u2020\n\n1 mila, universit\u00e9 de montr\u00e9al\n\n2 \u00e9cole normale sup\u00e9rieure de paris\n\n\u2020cifar senior fellow\n\naugust 16, 2018\n\n8\n1\n0\n2\n\n \n\ng\nu\na\n4\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n3\n7\n8\n4\n0\n\n.\n\n8\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nthe biological plausibility of the backpropagation algo-\nrithm has long been doubted by neuroscientists. two major\nreasons are that neurons would need to send two different\ntypes of signal in the forward and backward phases, and\nthat pairs of neurons would need to communicate through\nsymmetric bidirectional connections. we present a simple\ntwo-phase learning procedure for \ufb01xed point recurrent net-\nworks that addresses both these issues. in our model, neu-\nrons perform leaky integration and synaptic weights are up-\ndated through a local mechanism. our learning method gen-\neralizes equilibrium propagation to vector \ufb01eld dynamics,\nre", "leading edge\n\nreview\n\nthe molecular and systems\nbiology of memory\n\neric r. kandel,1,2,3,4,* yadin dudai,5 and mark r. mayford6\n1kavli institute for brain science\n2zuckerman mind brain behavior institute\n3howard hughes medical institute\n4departments of neuroscience, biochemistry and molecular biophysics, and psychiatry\ncollege of physicians and surgeons of columbia university, new york state psychiatric institute, 1051 riverside drive, new york,\nny 10032, usa\n5department of neurobiology, weizmann institute of science, rehovot 76100, israel\n6department of molecular and cellular neuroscience, the scripps research institute, la jolla, ca, usa\n*correspondence: erk5@columbia.edu\nhttp://dx.doi.org/10.1016/j.cell.2014.03.001\n\nlearning and memory are two of the most magical capabilities of our mind. learning is the biolog-\nical process of acquiring new knowledge about the world, and memory is the process of retaining\nand reconstructing that knowledge over time. most of our knowledge of the worl", "cerebellar granule cell axons support \nhigh-dimensional representations\n\nfrederic lanore\u200a\n\n\u200a1,2,4, n. alex cayco-gajic1,3,4, harsha gurnani1, diccon coyle1 and r. angus silver\u200a\n\n\u200a1\u2009\u2709\n\nin classical theories of cerebellar cortex, high-dimensional sensorimotor representations are used to separate neuronal activity \npatterns, improving associative learning and motor performance. recent experimental studies suggest that cerebellar granule \ncell (grc) population activity is low-dimensional. to examine sensorimotor representations from the point of view of down-\nstream purkinje cell \u2018decoders\u2019, we used three-dimensional acousto-optic lens two-photon microscopy to record from hundreds \nof grc axons. here we show that grc axon population activity is high dimensional and distributed with little fine-scale spatial \nstructure during spontaneous behaviors. moreover, distinct behavioral states are represented along orthogonal dimensions in \nneuronal activity space. these results suggest that the cer", "neuron\n\nreview\n\ndopaminergic modulation of synaptic transmission\nin cortex and striatum\n\nnicolas x. tritsch1 and bernardo l. sabatini1,*\n1howard hughes medical institute, department of neurobiology, harvard medical school, 220 longwood avenue, boston, ma 02115, usa\n*correspondence: bsabatini@hms.harvard.edu\nhttp://dx.doi.org/10.1016/j.neuron.2012.09.023\n\namong the many neuromodulators used by the mammalian brain to regulate circuit function and plasticity,\ndopamine (da) stands out as one of the most behaviorally powerful. perturbations of da signaling are\nimplicated in the pathogenesis or exploited in the treatment of many neuropsychiatric diseases, including\nparkinson\u2019s disease (pd), addiction, schizophrenia, obsessive compulsive disorder, and tourette\u2019s\nsyndrome. although the precise mechanisms employed by da to exert its control over behavior are not fully\nunderstood, da is known to regulate many electrical and biochemical aspects of neuronal function including\nexcitability, synapti", "4052 \u2022 the journal of neuroscience, march 4, 2015 \u2022 35(9):4052\u2013 4064\n\nsystems/circuits\n\nrole of the indirect pathway of the basal ganglia in\nperceptual decision making\n\nwei wei,1,2 x jonathan e. rubin,3 and xiao-jing wang1,2,4\n1center for neural science, new york university, new york, new york 10003, 2department of neurobiology and kavli institute for neuroscience, yale\nuniversity school of medicine, new haven, connecticut 06520, 3department of mathematics and center for the neural basis of cognition, university of\npittsburgh, pittsburgh, pennsylvania 15260, and 4nyu-ecnu institute of brain and cognitive science, nyu shanghai, shanghai, china 200122\n\nthe basal ganglia (bg) play an important role in motor control, reinforcement learning, and perceptual decision making. modeling and\nexperimental evidence suggest that, in a speed\u2013accuracy tradeoff, the corticostriatal pathway can adaptively adjust a decision threshold\n(the amount of information needed to make a choice). in this study, we ", "perspective\n\nhttps://doi.org/10.1038/s41467-023-37180-x\n\ncatalyzing next-generation arti\ufb01cial\nintelligence through neuroai\n\nreceived: 11 september 2022\n\naccepted: 3 march 2023\n\ncheck for updates\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\n1,29\n\n, sean escola 2,29, blake richards3,4,5,6,7,\n\nanthony zador\nbence \u00f6lveczky8, yoshua bengio 3, kwabena boahen9, matthew botvinick 10,\ndmitri chklovskii11, anne churchland 12, claudia clopath 13,\njames dicarlo 14, surya ganguli15, jeff hawkins16, konrad k\u00f6rding17,\nalexei koulakov1, yann lecun18,19, timothy lillicrap10, adam marblestone20,\nbruno olshausen21, alexandre pouget22, cristina savin 23,\nterrence sejnowski\nandreas s. tolias\n\n25, sara solla 26, david sussillo18,27,\n\n24, eero simoncelli\n\n28 & doris tsao21\n\nneuroscience has long been an essential driver of progress in arti\ufb01cial intelli-\ngence (ai). we propose that to accelerate progress in ai, we must invest in\nfundamental research in neuroai. a core component of this is ", "a path towards autonomous machine intelligence\n\nversion 0.9.2, 2022-06-27\n\nyann lecun\n\ncourant institute of mathematical sciences, new york university yann@cs.nyu.edu\n\nmeta - fundamental ai research yann@fb.com\n\njune 27, 2022\n\nabstract\n\nhow could machines learn as e\ufb03ciently as humans and animals? how could ma-\nchines learn to reason and plan? how could machines learn representations of percepts\nand action plans at multiple levels of abstraction, enabling them to reason, predict,\nand plan at multiple time horizons? this position paper proposes an architecture and\ntraining paradigms with which to construct autonomous intelligent agents. it combines\nconcepts such as con\ufb01gurable predictive world model, behavior driven through intrinsic\nmotivation, and hierarchical joint embedding architectures trained with self-supervised\nlearning.\n\nkeywords: arti\ufb01cial intelligence, machine common sense, cognitive architecture, deep\nlearning, self-supervised learning, energy-based model, world models, join", "report\n\nmismatch receptive fields in mouse visual cortex\n\nhighlights\nd v1 layer 2/3 neurons signal visuomotor mismatch in local\n\nauthors\n\npawel zmarz, georg b. keller\n\nparts of the visual \ufb01eld\n\nd resolution of mismatch receptive \ufb01elds matches that of\n\nvisual receptive \ufb01elds\n\nd mismatch receptive \ufb01elds are aligned to the visual retinotopy\n\ncorrespondence\ngeorg.keller@fmi.ch\n\nin brief\nzmarz and keller show that in v1 neurons,\nsensorimotor mismatch responses, like\nvisual responses, are con\ufb01ned to speci\ufb01c\nregions of the visual \ufb01eld. thus the\nconcept of receptive \ufb01elds can be directly\nintegrated with the theory of predictive\ncoding.\n\nzmarz & keller, 2016, neuron 92, 766\u2013772\nnovember 23, 2016 \u00aa 2016 elsevier inc.\nhttp://dx.doi.org/10.1016/j.neuron.2016.09.057\n\n\f", "physics- informed machine learning\n\n 1\n\n 1,2\u2009\u2709, ioannis\u00a0g.\u00a0kevrekidis3,4, lu\u00a0lu \n\n 5, paris\u00a0perdikaris6, \n\ngeorge\u00a0em\u00a0karniadakis \nsifan\u00a0wang7 and liu\u00a0yang \nabstract | despite great progress in simulating multiphysics problems using the numerical \ndiscretization of partial differential equations (pdes), one still cannot seamlessly incorporate noisy \ndata into existing algorithms, mesh generation remains complex, and high- dimensional problems \ngoverned by parameterized pdes cannot be tackled. moreover, solving inverse problems with \nhidden physics is often prohibitively expensive and requires different formulations and elaborate \ncomputer codes. machine learning has emerged as a promising alternative, but training deep neural \nnetworks requires big data, not always available for scientific problems. instead, such networks can \nbe trained from additional information obtained by enforcing the physical laws (for example, at \nrandom points in the continuous space- time domain). such physics", "journal of machine learning research 15 (2014) 3743-3773\n\nsubmitted 6/13; published 11/14\n\nwhat regularized auto-encoders learn from the\n\ndata-generating distribution\n\nguillaume alain\nyoshua bengio\ndepartment of computer science and operations research\nuniversity of montreal\nmontreal, h3c 3j7, quebec, canada\n\nguillaume.alain@umontreal.ca\nyoshua.bengio@umontreal.ca\n\neditors: aaron courville, rob fergus, and christopher manning\n\nabstract\n\nwhat do auto-encoders learn about the underlying data-generating distribution? recent\nwork suggests that some auto-encoder variants do a good job of capturing the local manifold\nstructure of data. this paper clari\ufb01es some of these previous observations by showing that\nminimizing a particular form of regularized reconstruction error yields a reconstruction\nfunction that locally characterizes the shape of the data-generating density. we show that\nthe auto-encoder captures the score (derivative of the log-density with respect to the input).\nit contradicts ", "single cortical neurons as deep arti\ufb01cial neural\nnetworks\n\narticle\n\ngraphical abstract\n\nauthors\n\ndavid beniaguev, idan segev,\nmichael london\n\ncorrespondence\ndavid.beniaguev@gmail.com\n\nin brief\nusing a modern machine learning\napproach, we show that the i/o\ncharacteristics of cortical pyramidal\nneurons can be approximated, at the\nmillisecond resolution (single spike\nprecision), by a temporally convolutional\nneural network with \ufb01ve to eight layers.\nthis computational complexity stems\nmainly from the interplay between nmda\nreceptors and dendritic morphology.\n\nhighlights\nd cortical neurons are well approximated by a deep neural\n\nnetwork (dnn) with 5\u20138 layers\n\nd dnn\u2019s depth arises from the interaction between nmda\n\nreceptors and dendritic morphology\n\nd dendritic branches can be conceptualized as a set of\n\nspatiotemporal pattern detectors\n\nd we provide a uni\ufb01ed method to assess the computational\n\ncomplexity of any neuron type\n\nbeniaguev et al., 2021, neuron 109, 2727\u20132739\nseptember 1, 2021 \u00aa ", "behavioural brain research 206 (2010) 157\u2013165\n\ncontents lists available at sciencedirect\n\nbehavioural brain research\n\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / b b r\n\nreview\nstructure learning in action\ndaniel a. braun a,\u2217, carsten mehring b, daniel m. wolpert a\n\na computational and biological learning lab, department of engineering, university of cambridge, uk\nb bernstein center for computational neuroscience, freiburg, germany\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 28 july 2009\naccepted 24 august 2009\navailable online 29 august 2009\n\nkeywords:\nstructure learning\nadaptive motor control\nlearning-to-learn\nvisuomotor learning\ndimensionality reduction\nvariability\n\ncontents\n\n\u2018learning to learn\u2019 phenomena have been widely investigated in cognition, perception and more recently\nalso in action. during concept learning tasks, for example, it has been suggested that characteristic\nfeatures are abstracted from a set of examples w", "imagenet classi\ufb01cation with deep convolutional\n\nneural networks\n\nalex krizhevsky\n\nuniversity of toronto\n\nkriz@cs.utoronto.ca\n\nilya sutskever\n\nuniversity of toronto\n\nilya@cs.utoronto.ca\n\ngeoffrey e. hinton\nuniversity of toronto\n\nhinton@cs.utoronto.ca\n\nabstract\n\nwe trained a large, deep convolutional neural network to classify the 1.2 million\nhigh-resolution images in the imagenet lsvrc-2010 contest into the 1000 dif-\nferent classes. on the test data, we achieved top-1 and top-5 error rates of 37.5%\nand 17.0% which is considerably better than the previous state-of-the-art. the\nneural network, which has 60 million parameters and 650,000 neurons, consists\nof \ufb01ve convolutional layers, some of which are followed by max-pooling layers,\nand three fully-connected layers with a \ufb01nal 1000-way softmax. to make train-\ning faster, we used non-saturating neurons and a very ef\ufb01cient gpu implemen-\ntation of the convolution operation. to reduce over\ufb01tting in the fully-connected\nlayers we employed a rece", "neuron\n\narticle\n\na modeling framework for deriving\nthe structural and functional architecture\nof a short-term memory microcircuit\n\ndimitry fisher,1 itsaso olasagasti,2,6 david w. tank,3 emre r.f. aksay,4,* and mark s. goldman1,5,*\n1center for neuroscience, university of california, davis, ca 95618, usa\n2department of neurology, zurich university hospital, 8006 zurich, switzerland\n3princeton neuroscience institute and department of molecular biology, princeton university, princeton, nj 08544, usa\n4institute for computational biomedicine and department of physiology and biophysics, weill medical college of cornell university,\nnew york, ny 10021, usa\n5departments of neurobiology, physiology, and behavior, and ophthalmology and visual sciences, university of california, davis,\nca 95618, usa\n6present address: department of fundamental neurosciences, university of geneva, 1211 geneva, switzerland\n*correspondence: ema2004@med.cornell.edu (e.r.f.a.), msgoldman@ucdavis.edu (m.s.g.)\nhttp://dx.do", "7\n1\n0\n2\n\n \n\ny\na\nm\n2\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n3\n6\n1\n2\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\nunrolled generative adversarial networks\n\nluke metz\u2217\ngoogle brain\nlmetz@google.com\n\nben poole\u2020\nstanford university\npoole@cs.stanford.edu\n\ndavid pfau\ngoogle deepmind\npfau@google.com\n\njascha sohl-dickstein\ngoogle brain\njaschasd@google.com\n\nabstract\n\nwe introduce a method to stabilize generative adversarial networks (gans) by\nde\ufb01ning the generator objective with respect to an unrolled optimization of the\ndiscriminator. this allows training to be adjusted between using the optimal dis-\ncriminator in the generator\u2019s objective, which is ideal but infeasible in practice,\nand using the current value of the discriminator, which is often unstable and leads\nto poor solutions. we show how this technique solves the common problem of\nmode collapse, stabilizes training of gans with complex recurrent generators,\nand increases diversity and coverage of the data distribu", "reconciling modern machine learning practice\n\nand the bias-variance trade-o\ufb00\n\nmikhail belkina, daniel hsub, siyuan maa, and soumik mandala\n\nathe ohio state university, columbus, oh\n\nbcolumbia university, new york, ny\n\nseptember 12, 2019\n\nabstract\n\nbreakthroughs in machine learning are rapidly changing science and society, yet our fun-\ndamental understanding of this technology has lagged far behind. indeed, one of the central\ntenets of the \ufb01eld, the bias-variance trade-o\ufb00, appears to be at odds with the observed behavior\nof methods used in the modern machine learning practice. the bias-variance trade-o\ufb00 implies\nthat a model should balance under-\ufb01tting and over-\ufb01tting: rich enough to express underlying\nstructure in data, simple enough to avoid \ufb01tting spurious patterns. however, in the modern\npractice, very rich models such as neural networks are trained to exactly \ufb01t (i.e., interpolate)\nthe data. classically, such models would be considered over-\ufb01t, and yet they often obtain high\naccurac", "fundamental tradeoffs between invariance and\n\nsensitivity to adversarial perturbations\n\nflorian tram\u00e8r 1 jens behrmann 2 nicholas carlini 3 nicolas papernot 3 j\u00f6rn-henrik jacobsen 4\n\nabstract\n\nadversarial examples are malicious inputs crafted\nto induce misclassi\ufb01cation. commonly studied\nsensitivity-based adversarial examples introduce\nsemantically-small changes to an input that result\nin a different model prediction. this paper studies\na complementary failure mode, invariance-based\nadversarial examples, that introduce minimal se-\nmantic changes that modify an input\u2019s true label\nyet preserve the model\u2019s prediction. we demon-\nstrate fundamental tradeoffs between these two\ntypes of adversarial examples. we show that de-\nfenses against sensitivity-based attacks actively\nharm a model\u2019s accuracy on invariance-based at-\ntacks, and that new approaches are needed to re-\nsist both attack types. in particular, we break state-\nof-the-art adversarially-trained and certi\ufb01ably-\nrobust models by gener", "estimating information flow in deep neural networks\n\nziv goldfeld 1 2 ewout van den berg 2 3 kristjan greenewald 2 3 igor melnyk 2 3 nam nguyen 2 3\n\nbrian kingsbury 2 3 yury polyanskiy 1 2\n\n9\n1\n0\n2\n\n \n\ny\na\nm\n0\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n8\n2\n7\n5\n0\n\n.\n\n0\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe study the estimation of the mutual information\ni(x; t(cid:96)) between the input x to a deep neural\nnetwork (dnn) and the output vector t(cid:96) of its\n(cid:96)th hidden layer (an \u201cinternal representation\u201d).\nfocusing on feedforward networks with \ufb01xed\nweights and noisy internal representations, we\ndevelop a rigorous framework for accurate estima-\ntion of i(x; t(cid:96)). by relating i(x; t(cid:96)) to infor-\nmation transmission over additive white gaussian\nnoise channels, we reveal that compression, i.e.\nreduction in i(x; t(cid:96)) over the course of train-\ning, is driven by progressive geometric clustering\nof the representations of samples from the same\nclass. experimental results verify this conn", "long-term stability of cortical population \ndynamics underlying consistent behavior\n\njuan a. gallego\u200a\nand lee e. miller\u200a\n\n\u200a1,2,7,8*, matthew g. perich\u200a\n\u200a2,4,6,9*\n\n\u200a3,8, raeed h. chowdhury\u200a\n\n\u200a4, sara a. solla\u200a\n\n\u200a2,5,9  \n\nanimals readily execute learned behaviors in a consistent manner over long periods of time, and yet no equally stable neural \ncorrelate has been demonstrated. how does the cortex achieve this stable control? using the sensorimotor system as a model \nof cortical processing, we investigated the hypothesis that the dynamics of neural latent activity, which captures the dominant \nco-variation patterns within the neural population, must be preserved across time. we recorded from populations of neurons \nin premotor, primary motor and somatosensory cortices as monkeys performed a reaching task, for up to 2\u2009years. intriguingly, \ndespite a steady turnover in the recorded neurons, the low-dimensional latent dynamics remained stable. the stability allowed \nreliable decoding of beh", "7\n1\n0\n2\n\n \nr\na\n\nm\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n1\n6\n1\n4\n0\n\n.\n\n0\n1\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\nwhy deep neural networks for function ap-\nproximation?\n\nshiyu liang & r. srikant\ncoordinated science laboratory\nand\ndepartment of electrical and computer engineering\nuniversity of illinois at urbana-champaign\nurbana, il 61801, usa\n{sliang26,rsrikant}@illinois.edu\n\nabstract\n\nrecently there has been much interest in understanding why deep neural networks\nare preferred to shallow networks. we show that, for a large class of piecewise\nsmooth functions, the number of neurons needed by a shallow network to ap-\nproximate a function is exponentially larger than the corresponding number of\nneurons needed by a deep network for a given degree of function approximation.\nfirst, we consider univariate functions on a bounded interval and require a neural\nnetwork to achieve an approximation error of \u03b5 uniformly over the interval. we\nshow that shallow networks (i.e., netw", "b r i e f c o m m u n i c at i o n s\n\nexperience-dependent rescaling\nof entorhinal grids\ncaswell barry1\u20134, robin hayman3,4, neil burgess1,2 &\nkathryn j jeffery3,4\n\nthe \ufb01ring pattern of entorhinal \u2018grid cells\u2019 is thought to provide\nan intrinsic metric for space. we report a strong experience-\ndependent environmental in\ufb02uence: the spatial scales of the\ngrids (which are aligned and have \ufb01xed relative sizes within\neach animal) vary parametrically with changes to a familiar\nenvironment\u2019s size and shape. thus grid scale re\ufb02ects an\ninteraction between intrinsic, path-integrative calculation of\nlocation and learned associations to the external environment.\n\n\u2018grid cells\u2019 in dorsolateral medial entorhinal cortex (dlmec) of freely\nmoving rats show regular grid-like patterns of \ufb01ring across the\nenvironment1 (fig. 1). these are thought to provide an absolute metric\nwhereby an animal can update its own location using self-motion\ninformation (\u2018path integration\u2019)2\u20136. accordingly, grid cells may provid", "direct feedback alignment based convolutional neural network training for\n\nlow-power online learning processor\n\ndonghyeon han\n\nhoi-jun yoo\n\nschool of electrical engineering\n\nkaist, daejeon, republic of korea\n\nschool of electrical engineering\n\nkaist, daejeon, republic of korea\n\nhdh4797@kaist.ac.kr\n\nhjyoo@kaist.ac.kr\n\nabstract\n\nthere were many algorithms to substitute the back-\npropagation (bp) in the deep neural network (dnn) train-\ning. however, they could not become popular because their\ntraining accuracy and the computational ef\ufb01ciency were\nworse than bp. one of them was direct feedback alignment\n(dfa), but it showed low training performance especially\nfor the convolutional neural network (cnn). in this pa-\nper, we overcome the limitation of the dfa algorithm by\ncombining with the conventional bp during the cnn train-\ning. to improve the training stability, we also suggest the\nfeedback weight initialization method by analyzing the pat-\nterns of the \ufb01xed random matrices in the dfa. fi", "5\n1\n0\n2\n\n \nr\na\n\nm\n6\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n1\n0\n1\n2\n0\n\n.\n\n3\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nescaping from saddle points \u2013\n\nonline stochastic gradient for tensor decomposition\n\nrong ge\u2217\n\nfurong huang \u2020\n\nchi jin \u2021\n\nyang yuan \u00a7\n\nabstract\n\nwe analyze stochastic gradient descent for optimizing non-convex functions. in many cases for non-\nconvex functions the goal is to \ufb01nd a reasonable local minimum, and the main concern is that gradient\nupdates are trapped in saddle points. in this paper we identify strict saddle property for non-convex\nproblem that allows for ef\ufb01cient optimization. using this property we show that stochastic gradient\ndescent converges to a local minimum in a polynomial number of iterations. to the best of our knowledge\nthis is the \ufb01rst work that gives global convergence guarantees for stochastic gradient descent on non-\nconvex functions with exponentially many local minima and saddle points.\n\nour analysis can be applied to orthogonal tensor decomposition, which is widely used ", "statistical physics of inference: thresholds and algorithms\n\nlenka zdeborov\u00b4a1,\u2217, and florent krzakala2,\u2217\n\n1 institut de physique th\u00b4eorique,\n\ncnrs, cea, universit\u00b4e paris-saclay,\n\nf-91191, gif-sur-yvette, france\n\n2 laboratoire de physique statistique, cnrs, psl universit\u00b4es\n\nuniversit\u00b4e pierre & marie curie, 75005, paris, france.\n\necole normale sup\u00b4erieure. sorbonne universit\u00b4es\n\u2217 lenka.zdeborova@cea.fr and \ufb02orent.krzakala@ens.fr\n\nmany questions of fundamental interest in today\u2019s science can be formulated as inference problems:\nsome partial, or noisy, observations are performed over a set of variables and the goal is to recover,\nor infer, the values of the variables based on the indirect information contained in the measurements.\nfor such problems, the central scienti\ufb01c questions are: under what conditions is the information\ncontained in the measurements su\ufb03cient for a satisfactory inference to be possible? what are the\nmost e\ufb03cient algorithms for this task? a growing body of work has", "early inference in energy-based models approximates\n\nback-propagation\n\nyoshua bengio, cifar senior fellow\n\nand asja fischer\n\nmontreal institute for learning algorithms, university of montreal\n\n6\n1\n0\n2\n\n \n\nb\ne\nf\n7\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n7\n7\n7\n2\n0\n\n.\n\n0\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe show that langevin mcmc inference in an\nenergy-based model with latent variables has the\nproperty that the early steps of inference, start-\ning from a stationary point, correspond to prop-\nagating error gradients into internal layers, simi-\nlarly to back-propagation. the error that is back-\npropagated is with respect to visible units that\nhave received an outside driving force pushing\nthem away from the stationary point. back-\npropagated error gradients correspond to tempo-\nral derivatives of the activation of hidden units.\nthis observation could be an element of a theory\nfor explaining how brains perform credit assign-\nment in deep hierarchies as ef\ufb01ciently as back-\npropagation does. in this theor", "neural networks 99 (2018) 56\u201367\n\ncontents lists available at sciencedirect\n\nneural networks\n\njournal homepage: www.elsevier.com/locate/neunet\n\nstdp-based spiking deep convolutional neural networks for object\nrecognition\nsaeed reza kheradpisheh a,b,*, mohammad ganjtabesh a, simon j. thorpe b,\ntimoth\u00e9e masquelier b\na department of computer science, school of mathematics, statistics, and computer science, university of tehran, tehran, iran\nb cerco umr 5549, cnrs \u2013universit\u00e9 toulouse 3, france\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 7 may 2017\nreceived in revised form 23 november\n2017\naccepted 8 december 2017\navailable online 23 december 2017\n\nkeywords:\nspiking neural network\nstdp\ndeep learning\nobject recognition\ntemporal coding\n\nprevious studies have shown that spike-timing-dependent plasticity (stdp) can be used in spiking neural\nnetworks (snn) to extract visual features of low or intermediate complexity in an unsupervised manner.\nthese studies, however, used ", "9\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n3\n\n \n \n]\n\n.\n\nr\np\nh\nt\na\nm\n\n[\n \n \n\n2\nv\n2\n7\n3\n9\n0\n\n.\n\n8\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nmean field analysis of neural networks: a central limit theorem\n\njustin sirignano\u2217 and konstantinos spiliopoulos\u2020\u2021\n\njune 4, 2019\n\nabstract\n\nwe rigorously prove a central limit theorem for neural network models with a single hidden layer. the\ncentral limit theorem is proven in the asymptotic regime of simultaneously (a) large numbers of hidden\nunits and (b) large numbers of stochastic gradient descent training iterations. our result describes the\nneural network\u2019s \ufb02uctuations around its mean-\ufb01eld limit. the \ufb02uctuations have a gaussian distribution\nand satisfy a stochastic partial di\ufb00erential equation. the proof relies upon weak convergence methods\nfrom stochastic analysis. in particular, we prove relative compactness for the sequence of processes and\nuniqueness of the limiting process in a suitable sobolev space.\n\n1\n\nintroduction\n\nneural network models have been used as computational tools in man", "self-attention with relative position representations\n\npeter shaw\n\ngoogle\n\npetershaw@google.com\n\njakob uszkoreit\n\ngoogle brain\n\nusz@google.com\n\nashish vaswani\ngoogle brain\n\navaswani@google.com\n\n8\n1\n0\n2\n\n \nr\np\na\n2\n1\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n2\nv\n5\n5\n1\n2\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nrelying entirely on an attention mechanism,\nthe transformer introduced by vaswani et\nal. (2017) achieves state-of-the-art results for\nin contrast to recurrent\nmachine translation.\nand convolutional neural networks,\nit does\nnot explicitly model relative or absolute po-\nsition information in its structure.\ninstead,\nit requires adding representations of abso-\nlute positions to its inputs.\nin this work\nwe present an alternative approach, extend-\ning the self-attention mechanism to ef\ufb01ciently\nconsider representations of the relative posi-\ntions, or distances between sequence elements.\non the wmt 2014 english-to-german and\nenglish-to-french translation tasks, this ap-\nproach yields improvements of 1.3 bleu", "5\n1\n0\n2\n\n \nr\np\na\n7\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n1\n4\n9\n0\n0\n\n.\n\n4\n0\n5\n1\n:\nv\ni\nx\nr\na\n\na simple way to initialize recurrent networks of\n\nrecti\ufb01ed linear units\n\nquoc v. le, navdeep jaitly, geoffrey e. hinton\n\ngoogle\n\nabstract\n\nlearning long term dependencies in recurrent networks is dif\ufb01cult due to van-\nishing and exploding gradients. to overcome this dif\ufb01culty, researchers have de-\nveloped sophisticated optimization techniques and network architectures. in this\npaper, we propose a simpler solution that use recurrent neural networks composed\nof recti\ufb01ed linear units. key to our solution is the use of the identity matrix or its\nscaled version to initialize the recurrent weight matrix. we \ufb01nd that our solution is\ncomparable to a standard implementation of lstms on our four benchmarks: two\ntoy problems involving long-range temporal structures, a large language modeling\nproblem and a benchmark speech recognition problem.\n\n1 introduction\n\nrecurrent neural networks (rnns) are very powerful dyna", "fast gradient-descent methods for temporal-difference learning\n\nwith linear function approximation\n\nrichard s. sutton,\u2217 hamid reza maei,\u2217 doina precup,\u2020 shalabh bhatnagar,\u2021 david silver,\u2217 csaba szepesv\u00b4ari,\u2217\neric wiewiora\u2217\n\u2217reinforcement learning and arti\ufb01cial intelligence laboratory, university of alberta, edmonton, canada\n\u2020school of computer science, mcgill university, montreal, canada\n\u2021department of computer science and automation, indian institute of science, bangalore, india\n\nabstract\n\nsutton, szepesv\u00b4ari and maei (2009) recently in-\ntroduced the \ufb01rst temporal-difference learning al-\ngorithm compatible with both linear function ap-\nproximation and off-policy training, and whose\ncomplexity scales only linearly in the size of\nthe function approximator. although their gra-\ndient temporal difference (gtd) algorithm con-\nverges reliably, it can be very slow compared to\nconventional linear td (on on-policy problems\nwhere td is convergent), calling into question its\npractical utility. in", "research article\n\nevolution of neural activity in circuits \nbridging sensory and abstract\u00a0knowledge\nfrancesca mastrogiuseppe1*\u2020, naoki hiratani2, peter latham1\n\n1gatsby computational neuroscience unit, university college london, london, \nunited kingdom; 2center for brain science, harvard university, harvard, united \nstates\n\nabstract the ability to associate sensory stimuli with abstract classes is critical for survival. how \nare these associations implemented in brain circuits? and what governs how neural activity evolves \nduring abstract knowledge acquisition? to investigate these questions, we consider a circuit model \nthat learns to map sensory input to abstract classes via gradient- descent synaptic plasticity. we focus \non typical neuroscience tasks (simple, and context- dependent, categorization), and study how both \nsynaptic connectivity and neural activity evolve during learning. to make contact with the current \ngeneration of experiments, we analyze activity via standard measu", "petreska et al. dynamical segmentation of single trials from population neural data\n\nnips 2011 pre-conference version\n\ndynamical segmentation of single trials\n\nfrom population neural data\n\nbiljana petreska\n\ngatsby computational neuroscience unit\n\nuniversity college london\n\nbiljana@gatsby.ucl.ac.uk\n\nbyron m. yu\nece and bme\n\ncarnegie mellon university\n\nbyronyu@cmu.edu\n\njohn p. cunningham\ndept of engineering\n\nuniversity of cambridge\njpc74@cam.ac.uk\n\ngopal santhanam, stephen i. ryu\u2020, krishna v. shenoy\u2021\n\u2021bioengineering, neurobiology and neurosciences program\n\nelectrical engineering\n\nstanford university\n\n\u2020dept of neurosurgery, palo alto medical foundation\n{gopals,seoulman,shenoy}@stanford.edu\n\nmaneesh sahani\n\ngatsby computational neuroscience unit\n\nuniversity college london\n\nmaneesh@gatsby.ucl.ac.uk\n\nabstract\n\nsimultaneous recordings of many neurons embedded within a recurrently-\nconnected cortical network may provide concurrent views into the dynamical pro-\ncesses of that network, and thus ", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/51769785\n\nentorhinal cortex layer iii input to the hippocampus is crucial for temporal\nassociation memory\n\nreads\n777\n\nalex rivest\nmassachusetts institute of technology\n\n7 publications\u00a0\u00a0\u00a01,381 citations\u00a0\u00a0\u00a0\n\nsee profile\n\narticle\u00a0\u00a0in\u00a0\u00a0science \u00b7 november 2011\n\ndoi: 10.1126/science.1210125\u00a0\u00b7\u00a0source: pubmed\n\ncitations\n279\n\n5 authors, including:\n\njunghyup suh\nharvard medical school\n\n23 publications\u00a0\u00a0\u00a02,979 citations\u00a0\u00a0\u00a0\n\nsee profile\n\ntakashi tominaga\ntokushima bunri university\n\n122 publications\u00a0\u00a0\u00a02,499 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nsome of the authors of this publication are also working on these related projects:\n\nconciousness view project\n\ncontractile vacuole function view project\n\nall content following this page was uploaded by junghyup suh on 08 may 2014.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/293648042\n\nbiological pattern formation: from basic mechanisms to complex structures\n\narticle\u00a0\u00a0in\u00a0\u00a0review of modern physics \u00b7 january 1994\n\ncitations\n166\n\n2 authors:\n\nandr\u00e9 koch\ndynamic phenomena s\u00e0rl\n\n16 publications\u00a0\u00a0\u00a01,238 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nsome of the authors of this publication are also working on these related projects:\n\nhypervelocity view project\n\nmorphogenesis view project\n\nreads\n180\n\nhans meinhardt\nmax planck institute for developmental biology\n\n130 publications\u00a0\u00a0\u00a013,240 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nall content following this page was uploaded by andr\u00e9 koch on 18 february 2020.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "0\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n8\n1\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n9\n3\n7\n0\n1\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nfourier features let networks learn\n\nhigh frequency functions in low dimensional domains\n\nmatthew tancik1\u2217\n\npratul p. srinivasan1,2\u2217\n\nben mildenhall1\u2217\n\nsara fridovich-keil1\n\nnithin raghavan1 utkarsh singhal1 ravi ramamoorthi3\n\njonathan t. barron2 ren ng1\n\n1university of california, berkeley\n\n2google research\n\n3university of california, san diego\n\nabstract\n\nwe show that passing input points through a simple fourier feature mapping\nenables a multilayer perceptron (mlp) to learn high-frequency functions in low-\ndimensional problem domains. these results shed light on recent advances in\ncomputer vision and graphics that achieve state-of-the-art results by using mlps\nto represent complex 3d objects and scenes. using tools from the neural tangent\nkernel (ntk) literature, we show that a standard mlp fails to learn high frequencies\nboth in theory and in practice. to overcome this spectral bias, we use a fo", "neuron\n\nreview\n\nre-evaluating circuit mechanisms\nunderlying pattern separation\n\nn. alex cayco-gajic1 and r. angus silver1,*\n1department of neuroscience, physiology and pharmacology, university college london, gower street, london wc1e 6bt, uk\n*correspondence: a.silver@ucl.ac.uk\nhttps://doi.org/10.1016/j.neuron.2019.01.044\n\nwhen animals interact with complex environments, their neural circuits must separate overlapping patterns\nof activity that represent sensory and motor information. pattern separation is thought to be a key function of\nseveral brain regions, including the cerebellar cortex, insect mushroom body, and dentate gyrus. however,\nrecent \ufb01ndings have questioned long-held ideas on how these circuits perform this fundamental computa-\ntion. here, we re-evaluate the functional and structural mechanisms underlying pattern separation. we argue\nthat the dimensionality of the space available for population codes representing sensory and motor informa-\ntion provides a common framework", "adversarial images for the primate brain\n\nli yuan,1,2,6, will xiao,1,3,6,* gabriel kreiman,4 francis e.h. tay,5 jiashi feng,2\n\nmargaret s. livingstone1,*\n\n1department of neurobiology, harvard medical school, boston, ma 02115, u.s.a.\n\n2department of electrical and computer engineering, national university of singapore, singapore 117583\n\n3department of molecular and cellular biology, harvard university, cambridge, ma 02134, u.s.a.\n\n4department of ophthalmology, boston children\u2019s hospital, boston, ma 02115, u.s.a.\n\n5department of mechanical engineering, national university of singapore, singapore 117583\n\n6these authors contributed equally: li yuan, will xiao\n\n*to whom correspondence should be addressed;\n\ne-mail: xiaow@fas.harvard.edu; margaret livingstone@hms.harvard.edu\n\nabstract\n\ndeep arti\ufb01cial neural networks have been proposed as a model of primate vision. however,\n\nthese networks are vulnerable to adversarial attacks, whereby introducing minimal noise can\n\nfool networks into misclass", "article\nrobust neuronal dynamics in premotor \ncortex during motor planning\n\ndoi:10.1038/nature17643\n\nnuo li1*, kayvon daie1*, karel svoboda1 & shaul druckmann1\n\nneural activity maintains representations that bridge past and future events, often over many seconds. network models \ncan produce persistent and ramping activity, but the positive feedback that is critical for these slow dynamics can cause \nsensitivity to perturbations. here we use electrophysiology and optogenetic perturbations in the mouse premotor cortex \nto probe the robustness of persistent neural representations during motor planning. we show that preparatory activity \nis remarkably robust to large-scale unilateral silencing: detailed neural dynamics that drive specific future movements \nwere quickly and selectively restored by the network. selectivity did not recover after bilateral silencing of the premotor \ncortex. perturbations to one hemisphere are thus corrected by information from the other hemisphere. corpus call", "markov chain monte carlo and variational inference:\n\nbridging the gap\n\n5\n1\n0\n2\n\n \n\ny\na\nm\n9\n1\n\n \n\n \n \n]\n\no\nc\n\n.\nt\na\nt\ns\n[\n \n \n\n4\nv\n0\n6\n4\n6\n\n.\n\n0\n1\n4\n1\n:\nv\ni\nx\nr\na\n\ntim salimans\nalgoritmica\n\ndiederik p. kingma and max welling\nuniversity of amsterdam\n\ntim@algoritmica.nl\n\n[d.p.kingma,m.welling]@uva.nl\n\nabstract\n\nrecent advances in stochastic gradient vari-\national\ninference have made it possible to\nperform variational bayesian inference with\nposterior approximations containing auxil-\niary random variables. this enables us to\nexplore a new synthesis of variational infer-\nence and monte carlo methods where we in-\ncorporate one or more steps of mcmc into\nour variational approximation. by doing so\nwe obtain a rich class of inference algorithms\nbridging the gap between variational meth-\nods and mcmc, and o\ufb00ering the best of\nboth worlds:\nfast posterior approximation\nthrough the maximization of an explicit ob-\njective, with the option of trading o\ufb00 addi-\ntional computation for additional accuracy", "algorithms for reinforcement learning\n\ndraft of the lecture published in the\n\nsynthesis lectures on arti\ufb01cial intelligence and machine learning\n\nseries\n\nby\n\nmorgan & claypool publishers\n\ncsaba szepesv\u00b4ari\njune 9, 2009\u2217\n\ncontents\n\n1 overview\n\n2 markov decision processes\n\n2.1 preliminaries\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2 markov decision processes . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3 value functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.4 dynamic programming algorithms for solving mdps . . . . . . . . . . . . . .\n\n3 value prediction problems\n\n3.1 temporal di\ufb00erence learning in \ufb01nite state spaces . . . . . . . . . . . . . . .\n3.1.1 tabular td(0)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.1.2 every-visit monte-carlo . . . . . . . . . . . . . . . . . . . . . . . . .\n3.1.3 td(\u03bb): unifying monte-carlo and td(0) . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . .", "2012 ieee/rsj international conference on\nintelligent robots and systems\noctober 7-12, 2012. vilamoura, algarve, portugal\n\n978-1-4673-1736-8/12/s31.00 \u00a92012 ieee\n\n4906\n\nauthorized licensed use limited to: university of washington libraries. downloaded on november 10,2023 at 21:05:34 utc from ieee xplore.  restrictions apply. \n\nsynthesisandstabilizationofcomplexbehaviorsthroughonlinetrajectoryoptimizationyuvaltassa,tomerezandemanueltodorovuniversityofwashingtonabstract\u2014wepresentanonlinetrajectoryoptimizationmethodandsoftwareplatformapplicabletocomplexhumanoidrobotsperformingchallengingtaskssuchasgettingupfromanarbitraryposeonthegroundandrecoveringfromlargedisturbancesusingdexterousacrobaticmaneuvers.theresult-ingbehaviors,illustratedintheattachedvideo,arecomputedonly7xslowerthanrealtime,onastandardpc.thevideoalsoshowsresultsontheacrobotproblem,planarswimmingandone-leggedhopping.thesesimplerproblemscanalreadybesolvedinrealtime,withoutpre-computinganything.i.introductiononlinetrajectoryop", "letter\nsynaptic amplification by dendritic spines enhances\ninput cooperativity\n\ndoi:10.1038/nature11554\n\nmark t. harnett1*, judit k. makara1,2*, nelson spruston1, william l. kath3 & jeffrey c. magee1\n\ndendritic spines are the nearly ubiquitous site of excitatory synaptic\ninput onto neurons1,2 and as such are critically positioned to influ-\nence diverse aspects of neuronal signalling. decades of theoretical\nstudies have proposed that spines may function as highly effective\nand modifiable chemical and electrical compartments that regulate\nsynaptic efficacy, integration and plasticity3\u20138. experimental studies\nhave confirmed activity-dependent structural dynamics and bio-\nchemical compartmentalization by spines9\u201312. however, there is a\nlongstanding debate over the influence of spines on the electrical\naspects of synaptic transmission and dendritic operation3\u20138,13\u201318.\nhere we measure the amplitude ratio of spine head to parent dendrite\nvoltage across a range of dendritic compartments and ca", "\u00a9 2019. published by the company of biologists ltd | journal of experimental biology (2019) 222, jeb188912. doi:10.1242/jeb.188912\n\nreview\n\norigin and role of path integration in the cognitive representations\nof the hippocampus: computational insights into open questions\nfrancesco savelli1,* and james j. knierim1,2,*\n\nabstract\npath integration is a straightforward concept with varied connotations\nthat are important to different disciplines concerned with navigation,\nsuch as ethology, cognitive science, robotics and neuroscience. in\nstudying the hippocampal formation, it is fruitful to think of path\nintegration as a computation that transforms a sense of motion into a\nsense of location, continuously integrated with landmark perception.\nhere, we review experimental evidence that path integration is\nintimately involved in fundamental properties of place cells and other\nspatial cells that are thought to support a cognitive abstraction of\nspace in this brain system. we discuss hypotheses ab", "o p t i m i z i n g e x p e c tat i o n s :\n\nf r o m d e e p r e i n f o r c e m e n t l e a r n i n g\nt o s t o c h a s t i c c o m p u tat i o n g r a p h s\n\njohn schulman\n\nsummer, 2016\n\na dissertation submitted in partial satisfaction of the requirements\n\nfor the degree of doctor of philosophy in computer science\n\nin the graduate division of the university of california, berkeley\n\ncommittee:\npieter abbeel, chair\njoan bruna\nmichael jordan\nstuart russell\n\n\f", "adversarially trained neural representations may already be as robust as\n\ncorresponding biological neural representations\n\n2\n2\n0\n2\n\n \n\nn\nu\nj\n \n9\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n8\n2\n2\n1\n1\n\n.\n\n6\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nchong guo 1 michael j. lee 1 2 3 guillaume leclerc 4 joel dapello 1 2 5 yug rao 6 aleksander madry 4 7\n\njames j. dicarlo 1 2 3\n\nabstract\n\nvisual systems of primates are the gold standard\nof robust perception. there is thus a general be-\nlief that mimicking the neural representations that\nunderlie those systems will yield arti\ufb01cial visual\nsystems that are adversarially robust. in this work,\nwe develop a method for performing adversar-\nial visual attacks directly on primate brain activ-\nity. we then leverage this method to demonstrate\nthat the above-mentioned belief might not be well\nfounded. speci\ufb01cally, we report that the biolog-\nical neurons that make up visual systems of pri-\nmates exhibit susceptibility to adversarial pertur-\nbations that is comparable in magnitude to ", "ne43ch19_mccormick\n\narjats.cls\n\njune 24, 2020\n\n9:45\n\nannual review of neuroscience\nneuromodulation of brain\nstate and behavior\n\ndavid a. mccormick,1 dennis b. nestvogel,1\nand biyu j. he2\n1institute of neuroscience, university of oregon, eugene, oregon 97403, usa;\nemail: davidmc@uoregon.edu\n2departments of neurology, neuroscience and physiology, and radiology, neuroscience\ninstitute, new york university school of medicine, new york, ny 10016, usa\n\nannu. rev. neurosci. 2020. 43:391\u2013415\n\nfirst published as a review in advance on\napril 6, 2020\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nhttps://doi.org/10.1146/annurev-neuro-100219-\n105424\n\ncopyright \u00a9 2020 by annual reviews.\nall rights reserved\n\nkeywords\nspontaneous activity, performance, variability, cerebral cortex, human,\nperception\n\nabstract\nneural activity and behavior are both notoriously variable, with responses\ndiffering widely between repeated presentation of identical stimuli or trials.\nrecent results", "3 august 2000 volume 406 issue no 6795 \n\nwhose scans are they, anyway?\n\nraw data are useful for researchers wishing to replicate the results of an experiment. care needs to be taken when, as\nwith brain-imaging measurements, such data can be misused or misinterpreted.\n\nlike motherhood and apple pie, the concept of sharing primary\n\ndata is widely recognized among scientists as a good thing. the\ndifficulty lies in putting this laudable aim into practice \u2014 and \nthe current controversy surrounding the national fmri data center,\na new repository for brain-imaging data at dartmouth college in\nhanover, new hampshire (see page 445), provides a case in point. \n\nmichael gazzaniga, director of the centre, is a respected cognitive\nneuroscientist, and an enthusiast for data sharing. but in his efforts to\nencourage  brain  mappers  to  submit  their  data  to  the  dartmouth \ncentre, gazzaniga has let his enthusiasm run away with him. as editor\nof the journal of cognitive neuroscience, gazzaniga had ", "continual learning through synaptic intelligence\n\nfriedemann zenke * 1 ben poole * 1 surya ganguli 1\n\nabstract\n\nwhile deep learning has led to remarkable ad-\nvances across diverse applications, it struggles\nin domains where the data distribution changes\nover the course of learning.\nin stark contrast,\nbiological neural networks continually adapt to\nchanging domains, possibly by leveraging com-\nplex molecular machinery to solve many tasks\nin this study, we introduce in-\nsimultaneously.\ntelligent synapses that bring some of this bio-\nlogical complexity into arti\ufb01cial neural networks.\neach synapse accumulates task relevant informa-\ntion over time, and exploits this information to\nrapidly store new memories without forgetting\nold ones. we evaluate our approach on continual\nlearning of classi\ufb01cation tasks, and show that it\ndramatically reduces forgetting while maintain-\ning computational ef\ufb01ciency.\n\n1. introduction\narti\ufb01cial neural networks (anns) have become an indis-\npensable asset for app", "report\n\nmedial entorhinal cortex lesions only partially\ndisrupt hippocampal place cells and hippocampus-\ndependent place memory\n\ngraphical abstract\n\nauthors\n\njena b. hales, magdalene i. schlesiger, ...,\nstefan leutgeb, robert e. clark\n\ncorrespondence\nsleutgeb@ucsd.edu (s.l.),\nreclark@ucsd.edu (r.e.c.)\n\nin brief\nto address whether the medial entorhinal\ncortex (mec) is necessary for spatial cod-\ning and hippocampus-dependent mem-\nory, hales et al. selectively removed the\nentire mec in rats. this led to only partial\ndeterioration of hippocampal spatial cod-\ning, and the mec was found to be neces-\nsary for only a limited number of hippo-\ncampus-dependent memory tasks.\n\nhighlights\nhippocampal place cells form without input from the medial en-\ntorhinal cortex\n\nmec lesions disrupt place cell precision and stability\n\nmec lesions impair a \ufb02exible type of place memory that involves\nnavigation\n\nmec lesions spare other hippocampus-dependent spatial\nmemory\n\nhales et al., 2014, cell reports 9, 893\u20139", "current biology 23, 2121\u20132129, november 4, 2013 \u00aa2013 elsevier ltd all rights reserved http://dx.doi.org/10.1016/j.cub.2013.09.013\n\ndistinct roles of the cortical layers\nof area v1 in figure-ground segregation\n\narticle\n\nmatthew w. self,1,* timo van kerkoerle,1 hans supe` r,2,3,4\nand pieter r. roelfsema1,5,6\n1department of vision & cognition, netherlands institute for\nneuroscience, meibergdreef 47, 1105 ba, amsterdam, the\nnetherlands\n2department of basic psychology, university of barcelona,\npasseig vall d\u2019hebron 171, 08035 barcelona, spain\n3institute for brain, cognition and behavior (ir3c), passeig\nvall d\u2019hebron 171, 08035 barcelona, spain\n4catalan institution for research and advanced studies\n(icrea), passeig vall d\u2019hebron 171, 08035 barcelona, spain\n5department of integrative neurophysiology, center for\nneurogenomics and cognitive research, vu university, de\nboelelaan 1085, 1081 hv amsterdam, the netherlands\n6psychiatry department, academic medical center, postbus\n22660, 1100 dd amst", "opinion\n\ntrends in neurosciences vol.27 no.12 december 2004\n\nthe bayesian brain: the role of\nuncertainty in neural coding and\ncomputation\n\ndavid c. knill and alexandre pouget\n\ncenter for visual science and the department of brain and cognitive science, university of rochester, ny 14627, usa\n\nto use sensory information ef\ufb01ciently to make judgments\nand guide action in the world, the brain must represent\nand use information about uncertainty in its computations\nfor perception and action. bayesian methods have proven\nsuccessful in building computational theories for percep-\ntion and sensorimotor control, and psychophysics is\nproviding a growing body of evidence that human\nperceptual computations are \u2018bayes\u2019 optimal\u2019. this leads\nto the \u2018bayesian coding hypothesis\u2019: that the brain\nrepresents sensory information probabilistically, in the\nform of probability distributions. several computational\nschemes have recently been proposed for how this might\nbe achieved in populations of neurons. neurop", "3\n2\n0\n2\n\n \n\ng\nu\na\n1\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n8\n9\n8\n6\n1\n\n.\n\n8\n0\n3\n2\n:\nv\ni\nx\nr\na\n\ntransformers as support vector machines\n\ndavoud ataee tarzanagh1\u22c6 yingcong li2\u22c6\n\nchristos thrampoulidis3\n\nsamet oymak4\u2020\n\nabstract\n\nsince its inception in \u201cattention is all you need\u201d, the transformer architecture has led to revolutionary advance-\nments in natural language processing. the attention layer within the transformer admits a sequence of input tokens\nx and makes them interact through pairwise similarities computed as softmax(xqk\u22a4x\u22a4), where (k, q) are the\ntrainable key-query parameters. in this work, we establish a formal equivalence between the optimization geometry of\nself-attention and a hard-margin svm problem that separates optimal input tokens from non-optimal tokens using linear\nconstraints on the outer-products of token pairs. this formalism builds on [tlzo23] and allows us to characterize the\nimplicit bias of 1-layer transformers optimized with gradient descent, as follows. (1) opti", "foundations and trends r(cid:13) in machine learning\nan introduction to\nvariational autoencoders\n\nsuggested citation: diederik p. kingma and max welling (2019), \u201can introduction to\nvariational autoencoders\u201d, foundations and trends r(cid:13) in machine learning: vol. xx, no.\nxx, pp 1\u201318. doi: 10.1561/xxxxxxxxx.\n\ndiederik p. kingma\ngoogle\ndurk@google.com\nmax welling\nuniversiteit van amsterdam, qualcomm\nmwelling@qti.qualcomm.com\n\n9\n1\n0\n2\n\n \nc\ne\nd\n1\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n1\n9\n6\n2\n0\n\n.\n\n6\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nthis article may be used only for the purpose of research, teaching,\nand/or private study. commercial use or systematic downloading\n(by robots or other automatic processes) is prohibited without ex-\nplicit publisher approval.\n\nboston \u2014 delft\n\n\f", "updates of equilibrium prop match gradients of\n\nbackprop through time in an rnn with static input\n\nmaxence ernoult1,2, julie grollier2, damien querlioz1, yoshua bengio3,4, benjamin scellier3\n\n1centre de nanosciences et de nanotechnologies, universit\u00e9 paris sud, universit\u00e9 paris-saclay\n\n2unit\u00e9 mixte de physique, cnrs, thales, universit\u00e9 paris-sud, universit\u00e9 paris-saclay\n\n3mila, universit\u00e9 de montr\u00e9al\n\n4canadian institute for advanced research\n\nabstract\n\nequilibrium propagation (ep) is a biologically inspired learning algorithm for\nconvergent recurrent neural networks, i.e. rnns that are fed by a static input x and\nsettle to a steady state. training convergent rnns consists in adjusting the weights\nuntil the steady state of output neurons coincides with a target y. convergent rnns\ncan also be trained with the more conventional backpropagation through time\n(bptt) algorithm. in its original formulation ep was described in the case of\nreal-time neuronal dynamics, which is computationally c", "letters to nature\n\nused are important. previous estimates of parasite diversity were\nrestricted by dna sequence availability and used a limited number\nof genes and/or sequences from genbank. databases frequently\ncontain erroneous dna sequences23, and it is dif\ufb01cult to verify the\naccuracy of the sequences and the identities of the parasites from\nwhich they were obtained. although synonymous sites and non-\ncoding sequences are assumed to be relatively neutral, the degree of\ndeviation from neutrality may not be the same for genes under\nvarious selection pressures. most of the genes used in previous\nstudies1,2 are immune/drug targets or housekeeping genes that\ncould be under strong selection.\n\nin contrast, we have used a large number of snps and parasites of\nhighly diverse genetic background to estimate the divergence time\nand effective population size. we have shown that the p. falciparum\npopulation is quite ancient and diverse, a result that will be\na\nimportant in the development of drug", "research article\ndeep convolutional models improve\npredictions of macaque v1 responses to\nnatural images\n\nsantiago a. cadenaid1,2,3*, george h. denfieldid3,4, edgar y. walkerid3,4, leon\na. gatys1,2, andreas s. toliasid2,3,4,5\u262f, matthias bethge1,2,3,6\u262f, alexander s. eckerid1,2,3\u262f\n\n1 centre for integrative neuroscience and institute for theoretical physics, university of tu\u00a8bingen,\ntu\u00a8bingen, germany, 2 bernstein center for computational neuroscience, tu\u00a8bingen, germany, 3 center\nfor neuroscience and artificial intelligence, baylor college of medicine, houston, texas, united states of\namerica, 4 department of neuroscience, baylor college of medicine, houston, houston, texas, united\nstates of america, 5 department of electrical and computer engineering, rice university, houston, houston,\ntexas, united states of america, 6 max planck institute for biological cybernetics, tu\u00a8bingen, germany\n\n\u262f these authors contributed equally to this work.\n* santiago.cadena@uni-tuebingen.de\n\nabstract\n\ndesp", "this article has been accepted for inclusion in a future issue of this journal. content is final as presented, with the exception of pagination.\n\nieee transactions on neural networks and learning systems\n\n1\n\nvariable binding for sparse distributed\nrepresentations: theory and applications\n\nedward paxon frady, denis kleyko , member, ieee, and friedrich t. sommer\n\nabstract\u2014 variable binding is a cornerstone of symbolic rea-\nsoning and cognition. but how binding can be implemented\nin connectionist models has puzzled neuroscientists, cognitive\npsychologists, and neural network researchers for many decades.\none type of connectionist model that naturally includes a binding\noperation is vector symbolic architectures (vsas). in contrast to\nother proposals for variable binding, the binding operation in\nvsas is dimensionality-preserving, which enables representing\ncomplex hierarchical data structures, such as trees, while avoid-\ning a combinatoric expansion of dimensionality. classical vsas\nencod", "hfsp journal\n\nissn: 1955-2068 (print) (online) journal homepage: https://www.tandfonline.com/loi/tfls19\n\nreinforcement learning: computational theory\nand biological mechanisms\n\nkenji doya\n\nto cite this article: kenji doya (2007) reinforcement learning: computational theory and\nbiological mechanisms, hfsp journal, 1:1, 30-40, doi: 10.2976/1.2732246/10.2976/1\n\nto link to this article:  https://doi.org/10.2976/1.2732246/10.2976/1\n\npublished online: 07 sep 2010.\n\nsubmit your article to this journal \n\narticle views: 2335\n\nview related articles \n\nciting articles: 14 view citing articles \n\nfull terms & conditions of access and use can be found at\n\nhttps://www.tandfonline.com/action/journalinformation?journalcode=tfls21\n\n\f", "elifesciences.org\n\nshort report\n\nretroactive modulation of spike timing-\ndependent plasticity by dopamine\nzuzanna brzosko, wolfram schultz, ole paulsen*\n\ndepartment of physiology, development and neuroscience, physiological\nlaboratory, university of cambridge, cambridge, united kingdom\n\nabstract most reinforcement learning models assume that the reward signal arrives after the\nactivity that led to the reward, placing constraints on the possible underlying cellular mechanisms.\nhere we show that dopamine, a positive reinforcement signal, can retroactively convert hippocampal\ntiming-dependent synaptic depression into potentiation. this effect requires functional nmda\nreceptors and is mediated in part through the activation of the camp/pka cascade. collectively, our\nresults support the idea that reward-related signaling can act on a pre-established synaptic eligibility\ntrace, thereby associating specific experiences with behaviorally distant, rewarding outcomes. this\nfinding identifies a b", "proc. natl acad. sci. usa\nvol. 79, pp. 2554-2558, april 1982\nbiophysics\n\nneural networks and physical systems with emergent collective\ncomputational abilities\n\n(associative memory/parallel processing/categorization/content-addressable memory/fail-soft devices)\n\nj. j. hopfield\ndivision of chemistry and biology, california institute of technology, pasadena, california 91125; and bell laboratories, murray hill, new jersey 07974\ncontributed by john j. hopfweld, january 15, 1982\n\ncomputational properties of use to biological or-\nabstract\nganisms or to the construction of computers can emerge as col-\nlective properties of systems -having a large number of simple\nequivalent components (or neurons). the physical meaning ofcon-\ntent-addressable memory is described by an appropriate phase\nspace flow of the state of a system. a model of such a system is\ngiven, based on aspects of neurobiology but readily adapted to in-\ntegrated circuits. the collective properties of this model produce\na content-a", "9\n1\n0\n2\n\n \n\nn\na\nj\n \n\n0\n2\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n4\nv\n0\n5\n1\n8\n0\n\n.\n\n4\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ndeep learning in spiking neural networks\n\namirhossein tavanaei\u2217, masoud ghodrati\u2020, saeed reza kheradpisheh\u2021,\n\ntimoth\u00b4ee masquelier\u00a7 and anthony maida\u2217\n\n\u2217center for advanced computer studies, university of louisiana at lafayette\n\n\u2020department of physiology, monash university, clayton, vic, australia\n\n\u2021department of computer science, faculty of mathematical sciences and computer,\n\nlafayette, louisiana, la 70504, usa\n\n\u00a7cerco umr 5549, cnrs-universit\u00b4e de toulouse 3, f-31300, france\n\nkharazmi university, tehran, iran\n\ntavanaei@louisiana.edu, masoud.ghodrati@monash.edu, kheradpisheh@ut.ac.ir,\n\ntimothee.masquelier@cnrs.fr, maida@louisiana.edu\n\nlearning,\n\nin recent\n\nabstract\u20141\n\nyears, deep learning has\nrevolutionized the \ufb01eld of machine\nfor\ncomputer vision in particular. in this approach, a deep\n(multilayer) arti\ufb01cial neural network (ann) is trained\nin a supervised manner using backpropagation. vast\namounts", "reinforcement learning using a continuous time\nactor-critic framework with spiking neurons\n\nnicolas fre\u00b4 maux1, henning sprekeler1,2, wulfram gerstner1*\n1 school of computer and communication sciences and school of life sciences, brain mind institute, e\u00b4cole polytechnique fe\u00b4de\u00b4rale de lausanne, 1015 lausanne epfl,\nswitzerland, 2 theoretical neuroscience lab, institute for theoretical biology, humboldt-universita\u00a8t zu berlin, berlin, germany\n\nabstract\n\nanimals repeat rewarded behaviors, but the physiological basis of reward-based learning has only been partially elucidated.\non one hand, experimental evidence shows that the neuromodulator dopamine carries information about rewards and\naffects synaptic plasticity. on the other hand, the theory of reinforcement learning provides a framework for reward-based\nlearning. recent models of reward-modulated spike-timing-dependent plasticity have made first steps towards bridging the\ngap between the two approaches, but faced two problems. first, ", "bert: pre-training of deep bidirectional transformers for\n\nlanguage understanding\n\njacob devlin ming-wei chang kenton lee kristina toutanova\n\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\ngoogle ai language\n\n9\n1\n0\n2\n\n \n\ny\na\nm\n4\n2\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n2\nv\n5\n0\n8\n4\n0\n\n.\n\n0\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe introduce a new language representa-\ntion model called bert, which stands for\nbidirectional encoder representations from\ntransformers. unlike recent language repre-\nsentation models (peters et al., 2018a; rad-\nford et al., 2018), bert is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. as a re-\nsult, the pre-trained bert model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\nbert is conceptually sim", "neurobiology of learning and memory 70, 119\u2013136 (1998)\narticle no. nl983843\n\nthe basal ganglia and chunking\n\nof action repertoires\n\nann m. graybiel\n\ndepartment of brain and cognitive sciences, massachusetts institute of technology,\n\ncambridge, massachusetts 02139\n\nthe basal ganglia have been shown to contribute to habit and stimulus\u2013response\n(s\u2013r) learning. these forms of learning have the property of slow acquisition and, in\nhumans, can occur without conscious awareness. this paper proposes that one aspect\nof basal ganglia-based learning is the recoding of cortically derived information within\nthe striatum. modular corticostriatal projection patterns, demonstrated experimen-\ntally, are viewed as producing recoded templates suitable for the gradual selection of\nnew input\u2013output relations in cortico-basal ganglia loops. recordings from striatal\nprojection neurons and interneurons show that activity patterns in the striatum are\nmodi\ufb01ed gradually during the course of s\u2013r learning. it is p", "neuroimage 48 (2009) 21\u201328\n\ncontents lists available at sciencedirect\n\nneuroimage\n\nj o u r n a l h o m e p a g e : w w w. e l s e v i e r. c o m / l o c a t e / y n i m g\n\nage-associated alterations in cortical gray and white matter signal intensity and gray\nto white matter contrast\nd.h. salat a,b,\u204e, s.y. lee a,c, a.j. van der kouwe a,b, d.n. greve a,b, b. fischl a,b,d, h.d. rosas a,c\na department of radiology, massachusetts general hospital, boston, ma, usa\nb athinoula a. martinos center for biomedical imaging, massachusetts general hospital and harvard medical school, boston, ma, usa\nc department of neurology, massachusetts general hospital, boston, ma, usa\nd computer science and arti\ufb01cial intelligence laboratory, massachusetts institute of technology, cambridge, ma, usa\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\nprior studies have focused on patterns of brain atrophy with aging and age-associated cognitive decline. it is\npossible that changes in neural tissue properties could provide", "rstb.royalsocietypublishing.org\n\nresearch\n\ncite this article: lu h, park h, poo m-m.\n2014 spike-timing-dependent bdnf secretion\nand synaptic plasticity. phil. trans. r. soc. b\n369: 20130132.\nhttp://dx.doi.org/10.1098/rstb.2013.0132\n\none contribution of 35 to a discussion meeting\nissue \u2018synaptic plasticity in health and disease\u2019.\n\nsubject areas:\nneuroscience\n\nkeywords:\nstdp, bdnf, synaptic plasticity, tltp\n\nauthor for correspondence:\nmu-ming poo\ne-mail: mpoo@berkeley.edu\n\n\u2020present address: department of neuroscience,\nbaylor college of medicine, houston, tx, usa.\n\nspike-timing-dependent bdnf secretion\nand synaptic plasticity\n\nhui lu\u2020, hyungju park and mu-ming poo\n\ndepartment of molecular and cell biology, helen wills neuroscience institute, university of california,\nberkeley, ca 94720, usa\n\nin acute hippocampal slices, we found that the presence of extracellular\nbrain-derived neurotrophic factor (bdnf) is essential for the induction\nof spike-timing-dependent long-term potentiation (tltp)", "vol 442|31 august 2006|doi:10.1038/nature05051\n\nletters\n\ndopamine-dependent prediction errors underpin\nreward-seeking behaviour in humans\nmathias pessiglione1, ben seymour1, guillaume flandin1, raymond j. dolan1 & chris d. frith1\n\ntheories of instrumental learning are centred on understanding\nhow success and failure are used to improve future decisions1.\nthese theories highlight a central role for reward prediction errors\nin updating the values associated with available actions2. in\nanimals, substantial evidence indicates that the neurotransmitter\ndopamine might have a key function in this type of learning,\nthrough its ability to modulate cortico-striatal synaptic ef\ufb01cacy3.\nhowever, no direct evidence links dopamine, striatal activity and\nbehavioural choice in humans. here we show that, during instru-\nmental learning, the magnitude of reward prediction error\nexpressed in the striatum is modulated by the administration of\ndrugs enhancing (3,4-dihydroxy-l-phenylalanine; l-dopa) or\nreduci", "network: computation in neural systems\n\nissn: 0954-898x (print) 1361-6536 (online) journal homepage: https://www.tandfonline.com/loi/inet20\n\nmaximum likelihood estimation of cascade point-\nprocess neural encoding models\n\nliam paninski\n\nto cite this article: liam paninski (2004) maximum likelihood estimation of cascade point-\nprocess neural encoding models, network: computation in neural systems, 15:4, 243-262,\ndoi: 10.1088/0954-898x_15_4_002\n\nto link to this article:  https://doi.org/10.1088/0954-898x_15_4_002\n\npublished online: 09 jul 2009.\n\nsubmit your article to this journal \n\narticle views: 806\n\nview related articles \n\nciting articles: 38 view citing articles \n\nfull terms & conditions of access and use can be found at\n\nhttps://www.tandfonline.com/action/journalinformation?journalcode=inet20\n\n\f", "sequence parallelism: long sequence training from\n\nsystem perspective\n\nshenggui li1,\n\nfuzhao xue1\u2020 chaitanya baranwal1 yongbin li1 yang you1\n\n1department of computer science, national university of singapore\n\n2\n2\n0\n2\n\n \n\ny\na\nm\n1\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n2\n1\n3\n1\n\n.\n\n5\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\ntransformer achieves promising results on various tasks. however, self-attention\nsuffers from quadratic memory requirements with respect to the sequence length.\nexisting work focuses on reducing time and space complexity from an algorithm\nperspective. in this work, we propose sequence parallelism, a memory-ef\ufb01cient\nparallelism method to help us break input sequence length limitation and train\nwith longer sequences on gpus ef\ufb01ciently. our approach is compatible with most\nexisting parallelisms (e.g., data parallelism, pipeline parallelism and tensor par-\nallelism), which means our sequence parallelism makes 4d parallelism possible.\nmore importantly, we no longer require a single device", "research article\nstimulus-driven population activity patterns\nin macaque primary visual cortex\n\nbenjamin r. cowley1,2, matthew a. smith2,3,4,5, adam kohn6,7, byron m. yu2,8,9*\n\n1 machine learning department, carnegie mellon university, pittsburgh, pennsylvania, united states of\namerica, 2 center for neural basis of cognition, carnegie mellon university, pittsburgh, pennsylvania,\nunited states of america, 3 department of ophthalmology, university of pittsburgh, pittsburgh,\npennsylvania, united states of america, 4 department of bioengineering, university of pittsburgh, pittsburgh,\npennsylvania, united states of america, 5 fox center for vision restoration, university of pittsburgh,\npittsburgh, pennsylvania, united states of america, 6 dominick purpura department of neuroscience, albert\neinstein college of medicine, bronx, new york, united states of america, 7 department of ophthalmology\nand vision sciences, albert einstein college of medicine, bronx, new york, united states of america,\n", "annu. rev. neurosci. 2000. 23:649\u2013711\ncopyright q 2000 by annual reviews. all rights reserved\n\nsynaptic plasticity and memory:\nan evaluation of the hypothesis\n\ns. j. martin, p. d. grimwood, and r. g. m. morris\ndepartment and centre for neuroscience, the university of edinburgh, crichton street,\nedinburgh, eh8 9le, united kingdom, e-mail: stephen.martin@ed.ac.uk,\npaulg@cfn.ed.ac.uk, r.g.m.morris@ed.ac.uk\n\nkey words ltp, ltd, depotentiation, learning, hippocampus, amygdala, cortex\nabstract changing the strength of connections between neurons is widely\nassumed to be the mechanism by which memory traces are encoded and stored in the\ncentral nervous system. in its most general form, the synaptic plasticity and memory\nhypothesis states that \u201cactivity-dependent synaptic plasticity is induced at appropriate\nsynapses during memory formation and is both necessary and suf\ufb01cient for the infor-\nmation storage underlying the type of memory mediated by the brain area in which\nthat plasticity is obser", "siam j. optim.\nvol. 19, no. 4, pp. 1574\u20131609\n\nc(cid:2) 2009 society for industrial and applied mathematics\n\nrobust stochastic approximation approach to\n\nstochastic programming\u2217\n\n\u2020\na. nemirovski\n\n, a. juditsky\n\n\u2021\n\n, g. lan\n\n\u2020\n\n, and a. shapiro\n\n\u2020\n\nabstract. in this paper we consider optimization problems where the objective function is given\nin a form of the expectation. a basic di\ufb03culty of solving such stochastic optimization problems is\nthat the involved multidimensional integrals (expectations) cannot be computed with high accuracy.\nthe aim of this paper is to compare two computational approaches based on monte carlo sampling\ntechniques, namely, the stochastic approximation (sa) and the sample average approximation (saa)\nmethods. both approaches, the sa and saa methods, have a long history. current opinion is that\nthe saa method can e\ufb03ciently use a speci\ufb01c (say, linear) structure of the considered problem, while\nthe sa approach is a crude subgradient method, which often performs poor", "m\na\nt\nh\n.\n \nc\no\nn\nt\nr\no\nl\n \ns\ni\ng\nn\na\nl\ns\n \ns\ny\ns\nt\ne\nm\ns\n \n(\n1\n9\n8\n9\n)\n \n2\n:\n3\n0\n3\n-\n3\n1\n4\n \nm\na\nt\nh\ne\nm\na\nt\ni\nc\ns\n \no\nf\n \nc\no\nn\nt\nr\no\nl\n,\n \ns\ni\ng\nn\na\nl\ns\n,\n \na\nn\nd\n \ns\ny\ns\nt\ne\nm\ns\n \n(cid:14)\n9\n \n1\n9\n8\n9\n \ns\np\nr\ni\nn\ng\ne\nr\n-\nv\ne\nr\nl\na\ng\n \nn\ne\nw\n \ny\no\nr\nk\n \ni\nn\nc\n.\n \na\np\np\nr\no\nx\ni\nm\na\nt\ni\no\nn\n \nb\ny\n \ns\nu\np\ne\nr\np\no\ns\ni\nt\ni\no\nn\ns\n \no\nf\n \na\n \ns\ni\ng\nm\no\ni\nd\na\nl\n \nf\nu\nn\nc\nt\ni\no\nn\n*\n \ng\n.\n \nc\ny\nb\ne\nn\nk\no\nt\n \na\nb\ns\nt\nr\n,\n,\nc\nt\n.\n \ni\nn\n \nt\nh\ni\ns\n \np\na\np\ne\nr\n \nw\ne\n \nd\ne\nm\no\nn\ns\nt\nr\na\nt\ne\n \nt\nh\na\nt\n \nf\ni\nn\ni\nt\ne\n \nl\ni\nn\ne\na\nr\n \nc\no\nm\nb\ni\nn\na\nt\ni\no\nn\ns\n \no\nf\n \nc\no\nm\n-\n \np\no\ns\ni\nt\ni\no\nn\ns\n \no\nf\n \na\n \nf\ni\nx\ne\nd\n,\n \nu\nn\ni\nv\na\nr\ni\na\nt\ne\n \nf\nu\nn\nc\nt\ni\no\nn\n \na\nn\nd\n \na\n \ns\ne\nt\n \no\nf\na\nf\nf\ni\nn\ne\n \nf\nu\nn\nc\nt\ni\no\nn\na\nl\ns\n \nc\na\nn\n \nu\nn\ni\nf\no\nr\nm\nl\ny\n \na\np\np\nr\no\nx\ni\nm\na\nt\ne\n \na\nn\ny\n \nc\no\nn\nt\ni\nn\nu\no\nu\ns\n \nf\nu\nn\nc\nt\ni\no\nn\n \no\nf\n \nn\n \nr\ne\na\nl\n \nv\na\nr\ni\na\nb\nl\ne\ns\n \nw\ni\nt\nh\n \ns\nu\np\np\no\nr\nt\n \ni\nn\n \nt\nh\ne\n \nu\nn\ni\nt\n \nh\ny\np\ne\nr\nc\nu\nb\ne\n;\n \no\nn\nl\ny\n \nm\ni\nl\nd\n \nc\no\nn\nd\ni\nt\ni\no\nn\ns\n \na\nr\ne\n \ni\nm\np\no\ns\ne\nd\n \no\nn\n \nt\nh\ne", "article\n\ndoi: 10.1038/s41467-017-01109-y\n\nopen\n\nsparse synaptic connectivity is required for\ndecorrelation and pattern separation in feedforward\nnetworks\nn. alex cayco-gajic1, claudia clopath2 & r. angus silver\n\n1\n\nfunction of the brain. the divergent feedforward\npattern separation is a fundamental\nnetworks thought to underlie this computation are widespread, yet exhibit remarkably similar\nsparse synaptic connectivity. marr-albus theory postulates that such networks separate\noverlapping activity patterns by mapping them onto larger numbers of sparsely active\nneurons. but spatial correlations in synaptic input and those introduced by network con-\nnectivity are likely to compromise performance. to investigate the structural and functional\ndeterminants of pattern separation we built models of the cerebellar input layer with spatially\ncorrelated input patterns, and systematically varied their synaptic connectivity. performance\nwas quanti\ufb01ed by the learning speed of a classi\ufb01er trained on e", "a simple connection from loss flatness to\n\ncompressed representations in neural networks\n\nshirui chen1,2, stefano recanatesi1,3,*, and eric shea-brown1,2,3,*\n\n1university of washington center for computational neuroscience and swartz center for theoretical neuroscience, seattle, wa, usa\n\n2university of washington, department of applied mathematics, seattle, wa, usa\n\n3allen institute, seattle, wa, usa\n\n*these authors share senior authorship\n\n3\n2\n0\n2\n\n \nt\nc\no\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n0\n7\n7\n1\n0\n\n.\n\n0\n1\n3\n2\n:\nv\ni\nx\nr\na\n\ndeep neural networks\u2019 generalization capacity has been\nstudied in a variety of ways, including at least two distinct cat-\negories of approach: one based on the shape of the loss land-\nscape in parameter space, and the other based on the structure\nof the representation manifold in feature space (that is, in the\nspace of unit activities). these two approaches are related, but\nthey are rarely studied together and explicitly connected. here,\nwe present a simple analysi", "a biologically plausible neural network for\n\nslow feature analysis\n\ndavid lipshutz\u21e41\n\ncharlie windolf\u21e41,2\n\nsiavash golkar 1\n\ndmitri b. chklovskii 1,3\n\n1 center for computational neuroscience, flatiron institute\n\n2 department of statistics, columbia university\n3 neuroscience institute, nyu medical center\n\n{dlipshutz,sgolkar,dchklovskii}@flatironinstitute.org\n\nc.windolf@columbia.edu\n\nabstract\n\nlearning latent features from time series data is an important problem in both\nmachine learning and brain function. one approach, called slow feature analy-\nsis (sfa), leverages the slowness of many salient features relative to the rapidly\nvarying input signals. furthermore, when trained on naturalistic stimuli, sfa repro-\nduces interesting properties of cells in the primary visual cortex and hippocampus,\nsuggesting that the brain uses temporal slowness as a computational principle\nfor learning latent features. however, despite the potential relevance of sfa for\nmodeling brain function, there is cu", "1314 \u2022 the journal of neuroscience, january 25, 2006 \u2022 26(4):1314 \u20131328\n\nbehavioral/systems/cognitive\n\na recurrent network mechanism of time integration in\nperceptual decisions\n\nkong-fatt wong and xiao-jing wang\nvolen center for complex systems, brandeis university, waltham, massachusetts 02454\n\nrecent physiological studies using behaving monkeys revealed that, in a two-alternative forced-choice visual motion discrimination task,\nreaction time was correlated with ramping of spike activity of lateral intraparietal cortical neurons. the ramping activity appears to\nreflect temporal accumulation, on a timescale of hundreds of milliseconds, of sensory evidence before a decision is reached. to elucidate\nthe cellular and circuit basis of such integration times, we developed and investigated a simplified two-variable version of a biophysically\nrealistic cortical network model of decision making. in this model, slow time integration can be achieved robustly if excitatory reverber-\nation is prim", "neuron\n\nreport\n\nsensorimotor mismatch signals\nin primary visual cortex of the behaving mouse\n\ngeorg b. keller,1,2,* tobias bonhoeffer,1 and mark hu\u00a8 bener1,*\n1max planck institute of neurobiology, 82152 munich-martinsried, germany\n2present address: friedrich miescher institute for biomedical research, maulbeerstrasse 66, ch-4058 basel, switzerland\n*correspondence: georg@neuro.mpg.de (g.b.k.), mark@neuro.mpg.de (m.h.)\ndoi 10.1016/j.neuron.2012.03.040\n\nsummary\n\nstudies in anesthetized animals have suggested that\nactivity in early visual cortex is mainly driven by visual\ninput and is well described by a feedforward pro-\ncessing hierarchy. however, evidence from experi-\nments on awake animals has shown that both eye\nmovements and behavioral state can strongly\nmodulate responses of neurons in visual cortex; the\nfunctional signi\ufb01cance of this modulation, however,\nremains elusive. using visual-\ufb02ow feedback manipu-\nlations during locomotion in a virtual reality environ-\nment, we found that res", "review\n\nj neurophysiol 106: 1068 \u20131077, 2011.\nfirst published june 15, 2011; doi:10.1152/jn.00429.2011.\n\ndistinct functions for direct and transthalamic corticocortical connections\n\ns. murray sherman1 and r. w. guillery2\n1department of neurobiology, the university of chicago, chicago, illinois; and 2medical research council anatomical\nneuropharmacology unit, oxford, united kingdom\n\nsubmitted 11 may 2011; accepted in \ufb01nal form 9 june 2011\n\nsherman sm, guillery rw. distinct functions for direct and transthalamic\ncorticocortical connections. j neurophysiol 106: 1068 \u20131077, 2011. first published\njune 15, 2011; doi:10.1152/jn.00429.2011.\u2014essentially all cortical areas receive\nthalamic inputs and send outputs to lower motor centers. cortical areas communi-\ncate with each other by means of direct corticocortical and corticothalamocortical\npathways, often organized in parallel. we distinguish these functionally, stressing\nthat the transthalamic pathways are class 1 (formerly known as \u201cdriver\u201d)", "8\n1\n0\n2\n\n \n\np\ne\ns\n1\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n4\n8\n1\n4\n0\n\n.\n\n9\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nsearching for ef\ufb01cient multi-scale\n\narchitectures for dense image prediction\n\nliang-chieh chen maxwell d. collins\n\nbarret zoph\n\nflorian schroff\n\nyukun zhu\n\nhartwig adam\n\ngeorge papandreou\n\njonathon shlens\n\ngoogle inc.\n\nabstract\n\nthe design of neural network architectures is an important component for achiev-\ning state-of-the-art performance with machine learning systems across a broad\narray of tasks. much work has endeavored to design and build architectures auto-\nmatically through clever construction of a search space paired with simple learning\nalgorithms. recent progress has demonstrated that such meta-learning methods\nmay exceed scalable human-invented architectures on image classi\ufb01cation tasks.\nan open question is the degree to which such methods may generalize to new\ndomains. in this work we explore the construction of meta-learning techniques\nfor dense image prediction focused on the tasks o", "2\n2\n0\n2\n\n \n\ng\nu\na\n2\n\n \n\n \n \n]\n\n.\n\nc\no\nh\nt\na\nm\n\n[\n \n \n\n2\nv\n3\n1\n3\n0\n0\n\n.\n\n5\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nrevisiting the polyak step size\n\nelad hazan *\u2020\n\nsham m. kakade *\u2021\n\nabstract\n\nthis note revisits the polyak step size schedule for convex\noptimization problems, proving that a simple variant of\nit simultaneously attains near optimal convergence rates\nfor the gradient descent algorithm, for all ranges of strong\nconvexity, smoothness, and lipschitz parameters, without\na priori knowledge of these parameters.\n\n1 introduction\n\nscaleable optimization for machine learning is based en-\ntirely on \ufb01rst order gradient methods. besides the age-old\nmethod of stochastic approximation [7], three accelerated\nmethods have proved their practical and theoretical signif-\nicance: nesterov acceleration [5], variance reduction [8]\nand adaptive learning-rate/regularization [4].\n\nadaptive choices of step sizes allow optimization algo-\nrithms to accelerate quickly according to the local curva-\nture and smoothness of the ", "q(\u03bb) with o\ufb00-policy corrections\n\nanna harutyunyan1(cid:63), marc g. bellemare2, tom stepleton2, and r\u00b4emi munos2\n\n1 vu brussel\n\n2 google deepmind\naharutyu@vub.ac.be\n\n{bellemare,stepleton,munos}@google.com\n\nabstract. we propose and analyze an alternate approach to o\ufb00-policy\nmulti-step temporal di\ufb00erence learning, in which o\ufb00-policy returns are\ncorrected with the current q-function in terms of rewards, rather than\nwith the target policy in terms of transition probabilities. we prove that\nsuch approximate corrections are su\ufb03cient for o\ufb00-policy convergence\nboth in policy evaluation and control, provided certain conditions. these\nconditions relate the distance between the target and behavior policies,\nthe eligibility trace parameter and the discount factor, and formalize\nan underlying tradeo\ufb00 in o\ufb00-policy td(\u03bb). we illustrate this theoretical\nrelationship empirically on a continuous-state control task.\n\n1\n\nintroduction\n\nin reinforcement learning (rl), learning is o\ufb00-policy when samples gene", "pergamon\n\nneural networks 13 (2000) 411\u2013430\n\ninvited article\n\nneural\n\nnetworks\n\nwww.elsevier.com/locate/neunet\n\nindependent component analysis: algorithms and applications\n\na. hyva\u00a8rinen, e. oja*\n\nneural networks research centre, helsinki university of technology, p.o. box 5400, fin-02015 hut, helsinki, finland\n\nreceived 28 march 2000; accepted 28 march 2000\n\nabstract\n\na fundamental problem in neural network research, as well as in many other disciplines, is \ufb01nding a suitable representation of multivariate\ndata, i.e. random vectors. for reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation\nof the original data. in other words, each component of the representation is a linear combination of the original variables. well-known linear\ntransformation methods include principal component analysis, factor analysis, and projection pursuit. independent component analysis (ica)\nis a recently developed method in which the goal is to \ufb01nd a", "3\n2\n0\n2\n\n \n\ny\na\nm\n1\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n0\n5\n0\n0\n2\n\n.\n\n5\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nlet\u2019s verify step by step\n\n\u2217\nhunter lightman\n\n\u2217\nvineet kosaraju\n\nyura burda\n\n\u2217\n\nharri edwards\n\nbowen baker\n\nteddy lee\n\njan leike\n\njohn schulman\n\nilya sutskever\n\n\u2217\nkarl cobbe\n\nopenai\n\nabstract\n\nin recent years, large language models have greatly improved in their\nability to perform complex multi-step reasoning. however, even state-\nof-the-art models still regularly produce logical mistakes. to train more\nreliable models, we can turn either to outcome supervision, which provides\nfeedback for a final result, or process supervision, which provides feedback\nfor each intermediate reasoning step. given the importance of training\nreliable models, and given the high cost of human feedback, it is impor-\ntant to carefully compare the both methods. recent work has already\nbegun this comparison, but many questions still remain. we conduct our\nown investigation, finding that process supervision significantly out", "published as a conference paper at iclr 2022\n\nassociated learning: a methodology to de-\ncompose end-to-end backpropagation on cnn,\nrnn, and transformer\n\ndennis y. wu1, di-nan lin2, vincent f. chen1, hung-hsuan chen2,\n1 institute of information science, academia sinica\n2 department of computer science, national central university\nhibb@iis.sinica.edu.tw, lindinan934301@gmail.com\nvincent0110@iis.sinica.edu.tw, hhchen@g.ncu.edu.tw\n\nabstract\n\nwe study associated learning (al), an alternative methodology to end-to-end\nbackpropagation (bp). we introduce the workflow to convert a neural network\ninto an al-form network such that al can be used to learn parameters for various\ntypes of neural networks. we compare al and bp on some of the most success-\nful neural networks: convolutional neural networks, recurrent neural networks,\nand transformers. experimental results show that al consistently outperforms\nbp on open datasets. we discuss possible reasons for al\u2019s success, its limita-\ntions, and al\u2019", "articles\n\nhttps://doi.org/10.1038/s41593-018-0147-8\n\nprefrontal cortex as a meta-reinforcement \nlearning system\n\njane x. wang!\njoel z. leibo1, demis hassabis1,4 and matthew botvinick!\n\n!1,5, zeb kurth-nelson1,2,5, dharshan kumaran1,3, dhruva tirumala1, hubert soyer1,  \n\n!1,4*\n\nover the past 20 years, neuroscience research on reward-based learning has converged on a canonical model, under which the \nneurotransmitter dopamine \u2018stamps in\u2019 associations between situations, actions and rewards by modulating the strength of \nsynaptic connections between neurons. however, a growing number of recent findings have placed this standard model under \nstrain. we now draw on recent advances in artificial intelligence to introduce a new theory of reward-based learning. here, the \ndopamine system trains another part of the brain, the prefrontal cortex, to operate as its own free-standing learning system. \nthis new perspective accommodates the findings that motivated the standard model, but also deals g", "version of record: https://www.sciencedirect.com/science/article/pii/s0959438817300910\nmanuscript_5171e60c85fbd8babd41ddcbde28ec6f\n\nthe temporal paradox of hebbian learning and homeostatic plasticity\n\nfriedemann zenkea,\u2217, wulfram gerstnerb, surya gangulia\n\nbbrain mind institute, school of life sciences and school of computer and communication sciences, ecole polytechnique\n\nadepartment of applied physics, stanford university, stanford, ca 94305, usa\n\nf\u00e9d\u00e9rale de lausanne, ch-1015 lausanne epfl, switzerland\n\nabstract\n\nhebbian plasticity, a synaptic mechanism which detects and ampli\ufb01es co-activity between neurons, is con-\nsidered a key ingredient underlying learning and memory in the brain. however, hebbian plasticity alone\nis unstable, leading to runaway neuronal activity, and therefore requires stabilization by additional compen-\nsatory processes. traditionally, a diversity of homeostatic plasticity phenomena found in neural circuits are\nthought to play this role. however, recent modell", "a r t i c l e s\n\nlimits on the memory storage capacity of\nbounded synapses\n\nstefano fusi & l f abbott\n\nmemories maintained in patterns of synaptic connectivity are rapidly overwritten and destroyed by ongoing plasticity related to the\nstorage of new memories. short memory lifetimes arise from the bounds that must be imposed on synaptic ef\ufb01cacy in any realistic\nmodel. we explored whether memory performance can be improved by allowing synapses to traverse a large number of states\nbefore reaching their bounds, or by changing the way these bounds are imposed. in the case of hard bounds, memory lifetimes\ngrow proportional to the square of the number of synaptic states, but only if potentiation and depression are precisely balanced.\nimproved performance can be obtained without \ufb01ne tuning by imposing soft bounds, but this improvement is only linear with\nrespect to the number of synaptic states. we explored several other possibilities and conclude that improving memory\nperformance requires a m", "understandingcatastrophicforgettingandrememberingincontinuallearningwithoptimalrelevancemappingprakharkaushik1alexgain1adamkortylewski1alanyuille1abstractcatastrophicforgettinginneuralnetworksisasigni\ufb01cantproblemforcontinuallearning.ama-jorityofthecurrentmethodsreplaypreviousdataduringtraining,whichviolatestheconstraintsofanidealcontinuallearningsystem.additionally,currentapproachesthatdealwithforgettingig-noretheproblemofcatastrophicremembering,i.e.theworseningabilitytodiscriminatebetweendatafromdifferenttasks.inourwork,weintroducerelevancemappingnetworks(rmns)whichareinspiredbytheoptimaloverlaphypothesis.themappingsre\ufb02ectstherelevanceoftheweightsforthetaskathandbyassigninglargeweightstoessentialparameters.weshowthatrmnslearnanoptimizedrepresentationaloverlapthatover-comesthetwinproblemofcatastrophicforgettingandremembering.ourapproachachievesstate-of-the-artperformanceacrossallcommoncontinuallearningdatasets,evensigni\ufb01cantlyoutperform-ingdatareplaymethodswhilenotviolatingtheconstrain", "learning identi\ufb01able and interpretable latent models\n\nof high-dimensional neural activity using pi-vae\n\nding zhou\n\ndepartment of statistics\n\ncolumbia university\n\ndz2336@columbia.edu\n\nxue-xin wei\n\ndepartment of neuroscience\n\nut austin\n\nweixx@utexas.edu\n\nabstract\n\nthe ability to record activities from hundreds of neurons simultaneously in the brain\nhas placed an increasing demand for developing appropriate statistical techniques\nto analyze such data. recently, deep generative models have been proposed to\n\ufb01t neural population responses. while these methods are \ufb02exible and expressive,\nthe downside is that they can be dif\ufb01cult to interpret and identify. to address\nthis problem, we propose a method that integrates key ingredients from latent\nmodels and traditional neural encoding models. our method, pi-vae, is inspired\nby recent progress on identi\ufb01able variational auto-encoder, which we adapt to make\nappropriate for neuroscience applications. speci\ufb01cally, we propose to construct\nlatent varia", "overcoming catastrophic forgetting with hard attention to the task\n\n8\n1\n0\n2\n\n \n\ny\na\nm\n9\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n3\n2\n4\n1\n0\n\n.\n\n1\n0\n8\n1\n:\nv\ni\nx\nr\na\n\njoan serr`a 1 d\u00b4\u0131dac sur\u00b4\u0131s 1 2 marius miron 1 3 alexandros karatzoglou 1\n\nabstract\n\ncatastrophic forgetting occurs when a neural net-\nwork loses the information learned in a previous\ntask after training on subsequent tasks. this prob-\nlem remains a hurdle for arti\ufb01cial intelligence\nsystems with sequential learning capabilities. in\nthis paper, we propose a task-based hard attention\nmechanism that preserves previous tasks\u2019 informa-\ntion without affecting the current task\u2019s learning.\na hard attention mask is learned concurrently to\nevery task, through stochastic gradient descent,\nand previous masks are exploited to condition\nsuch learning. we show that the proposed mecha-\nnism is effective for reducing catastrophic forget-\nting, cutting current rates by 45 to 80%. we also\nshow that it is robust to different hyperparameter\nchoices, an", "original research\npublished: 01 march 2021\ndoi: 10.3389/fncom.2021.640235\n\nbehavioral time scale plasticity of\nplace fields: mathematical analysis\n\nian cone 1,2 and harel z. shouval 1*\n\n1 department of neurobiology and anatomy, university of texas medical school, houston, tx, united states, 2 applied\nphysics program, rice university, houston, tx, united states\n\ntraditional synaptic plasticity experiments and models depend on tight\ntemporal\ncorrelations between pre- and postsynaptic activity. these tight temporal correlations,\non the order of tens of milliseconds, are incompatible with signi\ufb01cantly longer behavioral\ntime scales, and as such might not be able to account for plasticity induced by\nbehavior.\nindeed, recent \ufb01ndings in hippocampus suggest that rapid, bidirectional\nsynaptic plasticity which modi\ufb01es place \ufb01elds in ca1 operates at behavioral time scales.\nthese experimental results suggest that presynaptic activity generates synaptic eligibility\ntraces both for potentiation and d", "7\n1\n0\n2\n\n \n\ng\nu\na\n5\n\n \n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n6\n2\n0\n8\n0\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nlearning arbitrary dynamics in ef\ufb01cient, balanced\n\nspiking networks using local plasticity rules\n\naireza alemi\u2020\n\ngroup for neural theory, ens\n\n29 rue d\u2019ulm, paris\u201375005, france\n\n\u2020alireza.alemi@{ens.fr, gmail.com}\n\nchristian k. machens\n\nchampalimaud centre for the unknown\n\navenida brasilia, 1400-038 lisbon, portugal\n\nsophie den\u00e8ve\u2217\n\ngroup for neural theory, ens\n\n29 rue d\u2019ulm, paris\u201375005, france\n\njean-jacques slotine\u2217\n\nmassachusetts institute of technology\n\ncambridge, ma 02139, usa\n\nabstract\n\nunderstanding how recurrent neural circuits can learn to implement dynamical\nsystems is a fundamental challenge in neuroscience. the credit assignment problem,\ni.e. determining the local contribution of each synapse to the network\u2019s global\noutput error, is a major obstacle in deriving biologically plausible local learning\nrules. moreover, spiking recurrent networks implementing such tasks should not\nbe hug", "institute of mathematical statistics is collaborating with jstor to digitize, preserve, and extend access to\nthe annals of mathematical statistics.\n\nwww.jstor.org\n\n\u00ae\n\n\f", "1\n2\n0\n2\n\n \nr\na\n\nm\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n3\n4\n3\n1\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\ntowards understanding hierarchical learning:\n\nbene\ufb01ts of neural representations\n\nminshuo chen\u2217\n\nyu bai\u2020\n\njason d. lee\u2021\n\ntuo zhao\u00a7\n\nhuan wang\u00b6\n\ncaiming xiong\u00b6\n\nrichard socher\u00b6\n\nmarch 8, 2021\n\nabstract\n\ndeep neural networks can empirically perform e\ufb03cient hierarchical learning, in which the\nlayers learn useful representations of the data. however, how they make use of the intermediate\nrepresentations are not explained by recent theories that relate them to \u201cshallow learners\u201d such as\nkernels. in this work, we demonstrate that intermediate neural representations add more \ufb02exibility\nto neural networks and can be advantageous over raw inputs. we consider a \ufb01xed, randomly\ninitialized neural network as a representation function fed into another trainable network. when\nthe trainable network is the quadratic taylor model of a wide two-layer network, we show that\nneural representation can achieve improved sampl", "distance metric learning, with application\n\nto clustering with side-information\n\neric p. xing, andrew y. ng, michael i. jordan and stuart russell\n\nuniversity of california, berkeley\n\nberkeley, ca 94720\n\n\u0000 epxing,ang,jordan,russell\nabstract\n\n@cs.berkeley.edu\n\nmany algorithms rely critically on being given a good metric over their\ninputs. for instance, data can often be clustered in many \u201cplausible\u201d\nways, and if a clustering algorithm such as k-means initially fails to \ufb01nd\none that is meaningful to a user, the only recourse may be for the user to\nmanually tweak the metric until suf\ufb01ciently good clusters are found. for\nthese and other applications requiring good metrics, it is desirable that\nwe provide a more systematic way for users to indicate what they con-\nsider \u201csimilar.\u201d for instance, we may ask them to provide examples. in\nthis paper, we present an algorithm that, given examples of similar (and,\n, learns a distance metric over\nif desired, dissimilar) pairs of points in\nthat respect", "complex dynamics in simple neural networks:\nunderstanding gradient flow in phase retrieval\n\nstefano sarao mannelli1, giulio biroli2, chiara cammarota3,4,\nflorent krzakala5, pierfrancesco urbani1, and lenka zdeborov\u00e16\n\nabstract\n\ndespite the widespread use of gradient-based algorithms for optimizing high-\ndimensional non-convex functions, understanding their ability of \ufb01nding good\nminima instead of being trapped in spurious ones remains to a large extent an\nopen problem. here we focus on gradient \ufb02ow dynamics for phase retrieval from\nrandom measurements. when the ratio of the number of measurements over the\ninput dimension is small the dynamics remains trapped in spurious minima with\nlarge basins of attraction. we \ufb01nd analytically that above a critical ratio those\ncritical points become unstable developing a negative direction toward the signal.\nby numerical experiments we show that in this regime the gradient \ufb02ow algorithm\nis not trapped; it drifts away from the spurious critical points", "you only look once:\n\nuni\ufb01ed, real-time object detection\n\njoseph redmon\u2217, santosh divvala\u2217\u2020, ross girshick\u00b6, ali farhadi\u2217\u2020\n\nuniversity of washington\u2217, allen institute for ai\u2020, facebook ai research\u00b6\n\nhttp://pjreddie.com/yolo/\n\n6\n1\n0\n2\n\n \n\ny\na\nm\n9\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n5\nv\n0\n4\n6\n2\n0\n\n.\n\n6\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe present yolo, a new approach to object detection.\nprior work on object detection repurposes classi\ufb01ers to per-\nform detection. instead, we frame object detection as a re-\ngression problem to spatially separated bounding boxes and\nassociated class probabilities. a single neural network pre-\ndicts bounding boxes and class probabilities directly from\nfull images in one evaluation. since the whole detection\npipeline is a single network, it can be optimized end-to-end\ndirectly on detection performance.\n\nour uni\ufb01ed architecture is extremely fast. our base\nyolo model processes images in real-time at 45 frames\nper second. a smaller version of the network, fast yolo,\nprocesse", "visualizing the loss landscape of neural nets\n\nhao li1, zheng xu1, gavin taylor2, christoph studer3, tom goldstein1\n\n1university of maryland, college park 2united states naval academy 3cornell university\n\n{haoli,xuzh,tomg}@cs.umd.edu, taylor@usna.edu, studer@cornell.edu\n\nabstract\n\nneural network training relies on our ability to \ufb01nd \u201cgood\u201d minimizers of highly\nnon-convex loss functions. it is well-known that certain network architecture\ndesigns (e.g., skip connections) produce loss functions that train easier, and well-\nchosen training parameters (batch size, learning rate, optimizer) produce minimiz-\ners that generalize better. however, the reasons for these differences, and their\neffects on the underlying loss landscape, are not well understood. in this paper, we\nexplore the structure of neural loss functions, and the effect of loss landscapes on\ngeneralization, using a range of visualization methods. first, we introduce a simple\n\u201c\ufb01lter normalization\u201d method that helps us visualize l", "adversarial examples that fool both computer\n\nvision and time-limited humans\n\ngamaleldin f. elsayed\u2217\n\ngoogle brain\n\ngamaleldin.elsayed@gmail.com\n\nshreya shankar\nstanford university\n\nbrian cheung\nuc berkeley\n\n8\n1\n0\n2\n\n \n\ny\na\nm\n2\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n5\n9\n1\n8\n0\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nnicolas papernot\n\npennsylvania state university\n\nalex kurakin\ngoogle brain\n\nian goodfellow\ngoogle brain\n\njascha sohl-dickstein\n\ngoogle brain\n\njaschasd@google.com\n\nabstract\n\nmachine learning models are vulnerable to adversarial examples: small changes\nto images can cause computer vision models to make mistakes such as identifying\na school bus as an ostrich. however, it is still an open question whether humans\nare prone to similar mistakes. here, we address this question by leveraging recent\ntechniques that transfer adversarial examples from computer vision models with\nknown parameters and architecture to other models with unknown parameters and\narchitecture, and by matching the initial processin", "e\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\ne\nr\nu\nt\na\nn\nm\no\nc\n.\ne\nr\nu\nt\na\nn\nw\nw\nw\n\n/\n\n.\n\n/\n/\n:\np\nt\nt\nh\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n6\n0\n0\n2\n\u00a9\n\n \n\na r t i c l e s\n\nthe tempotron: a neuron that learns spike\ntiming\u2013based decisions\n\nrobert gu\u00a8tig1\u20134 & haim sompolinsky1,2,5\n\nthe timing of action potentials in sensory neurons contains substantial information about the eliciting stimuli. although the\ncomputational advantages of spike timing\u2013based neuronal codes have long been recognized, it is unclear whether, and if so\nhow, neurons can learn to read out such representations. we propose a new, biologically plausible supervised synaptic learning\nrule that enables neurons to ef\ufb01ciently learn a broad range of decision rules, even when information is embedded in the\nspatiotemporal structure of spike patterns rather than in mean \ufb01ring rates. the number of categorizations of random\nspatiotemporal patterns that a neuron can implement is several times larger than the number of its synapses. th", "the hsic bottleneck: deep learning without back-propagation\n\nwan-duo kurt ma\n\nj.p. lewis w. bastiaan kleijn\n\nvictoria university\n\nmawand@ecs.vuw.ac.nz, jplewis@google.com, bastiaan.kleijn@ecs.vuw.ac.nz\n\n9\n1\n0\n2\n\n \nc\ne\nd\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n8\n5\n1\n0\n\n.\n\n8\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe introduce the hsic (hilbert-schmidt independence crite-\nrion) bottleneck for training deep neural networks. the hsic\nbottleneck is an alternative to the conventional cross-entropy\nloss and backpropagation that has a number of distinct advan-\ntages. it mitigates exploding and vanishing gradients, resulting\nin the ability to learn very deep networks without skip con-\nnections. there is no requirement for symmetric feedback or\nupdate locking. we \ufb01nd that the hsic bottleneck provides per-\nformance on mnist/fashionmnist/cifar10 classi\ufb01cation\ncomparable to backpropagation with a cross-entropy target,\neven when the system is not encouraged to make the output\nresemble the classi\ufb01cation labels. app", "evolving images for visual neurons using a deep\ngenerative network reveals coding principles and\nneuronal preferences\n\narticle\n\ngraphical abstract\n\nauthors\ncarlos r. ponce, will xiao,\npeter f. schade, till s. hartmann,\ngabriel kreiman, margaret s. livingstone\n\ncorrespondence\ncrponce@wustl.edu (c.r.p.),\nmlivingstone@hms.harvard.edu (m.s.l.)\n\nin brief\nneurons guided the evolution of their own\nbest stimuli with a generative deep neural\nnetwork.\n\nhighlights\nd a generative deep neural network and a genetic algorithm\n\nevolved images guided by neuronal \ufb01ring\n\nd evolved images maximized neuronal \ufb01ring in alert macaque\n\nvisual cortex\n\nd evolved images activated neurons more than large numbers\n\nof natural images\n\nd similarity to evolved images predicts response of neurons to\n\nnovel images\n\nponce et al., 2019, cell 177, 999\u20131009\nmay 2, 2019 \u00aa 2019 elsevier inc.\nhttps://doi.org/10.1016/j.cell.2019.04.005\n\n\f", "article\n\ndoi:10.1038/nature11347\n\ndivision and subtraction by distinct\ncortical inhibitory networks in vivo\n\nnathan r. wilson1*, caroline a. runyan1*, forea l. wang1 & mriganka sur1\n\nbrain circuits process information through specialized neuronal subclasses interacting within a network. revealing their\ninterplay requires activating specific cells while monitoring others in a functioning circuit. here we use a new platform\nfor two-way light-based circuit interrogation in visual cortex in vivo to show the computational implications of\nmodulating different subclasses of inhibitory neurons during sensory processing. we find that soma-targeting,\nparvalbumin-expressing (pv) neurons principally divide responses but preserve stimulus selectivity, whereas\ndendrite-targeting, somatostatin-expressing (som) neurons principally subtract from excitatory responses and\nsharpen selectivity. visualized in vivo cell-attached recordings show that division by pv neurons alters response gain,\nwhereas subtra", "motor cortex embeds muscle-like commands in an\nuntangled population response\n\narticle\n\nhighlights\nd motor cortex displays a signature of a smooth dynamical\n\nsystem: low tangling\n\nd low tangling explains the previously puzzling dominant\n\nsignals in motor cortex\n\nd low tangling confers noise robustness and predicts\n\npopulation activity patterns\n\nd motor cortex embeds output commands in structure that\n\nreduces tangling\n\nauthors\n\nabigail a. russo, sean r. bittner,\nsean m. perkins, ...,\nlaurence f. abbott,\njohn p. cunningham,\nmark m. churchland\n\ncorrespondence\nmc3502@columbia.edu\n\nin brief\nusing a novel extended movement task,\nrusso et al. show that neural activity in\nmotor cortex is dominated by non-\nmuscle-like signals. a computational\napproach reveals that these dominant\nfeatures are expected and can be\npredicted given the constraint that neural\nactivity produces muscle commands\nwhile obeying a smooth \ufb02ow-\ufb01eld.\n\nrusso et al., 2018, neuron 97, 953\u2013966\nfebruary 21, 2018 \u00aa 2018 elsevier inc", "international journal of computer vision 40(1), 49\u201371, 2000\nc(cid:176) 2000 kluwer academic publishers. manufactured in the netherlands.\n\na parametric texture model based on joint statistics\n\nof complex wavelet coef\ufb01cients\n\ncenter for neural science, and courant institute of mathematical sciences, new york university,\n\njavier portilla and eero p. simoncelli\n\nnew york, ny 10003, usa\n\nreceived november 12, 1999; revised june 9, 2000\n\nabstract. we present a universal statistical model for texture images in the context of an overcomplete complex\nwavelet transform. the model is parameterized by a set of statistics computed on pairs of coef\ufb01cients corresponding\nto basis functions at adjacent spatial locations, orientations, and scales. we develop an ef\ufb01cient algorithm for\nsynthesizing random images subject to these constraints, by iteratively projecting onto the set of images satisfying\neach constraint, and we use this to test the perceptual validity of the model. in particular, we demonstra", "extracting and composing robust features with denoising\n\nautoencoders\n\npascal vincent\nhugo larochelle\nyoshua bengio\npierre-antoine manzagol\nuniversit\u00b4e de montr\u00b4eal, dept. iro, cp 6128, succ. centre-ville, montral, qubec, h3c 3j7, canada\n\nvincentp@iro.umontreal.ca\nlarocheh@iro.umontreal.ca\nbengioy@iro.umontreal.ca\nmanzagop@iro.umontreal.ca\n\nabstract\n\nprevious work has shown that the di\ufb03cul-\nties in learning deep generative or discrim-\ninative models can be overcome by an ini-\ntial unsupervised learning step that maps in-\nputs to useful intermediate representations.\nwe introduce and motivate a new training\nprinciple for unsupervised learning of a rep-\nresentation based on the idea of making the\nlearned representations robust to partial cor-\nruption of the input pattern. this approach\ncan be used to train autoencoders, and these\ndenoising autoencoders can be stacked to ini-\ntialize deep architectures. the algorithm can\nbe motivated from a manifold learning and\ninformation theoretic persp", "grid cells, place cells, and geodesic generalization for\nspatial reinforcement learning\n\nnicholas j. gustafson1*, nathaniel d. daw1,2\n\n1 center for neural science, new york university, new york, new york, united states of america, 2 department of psychology, new york university, new york, new york,\nunited states of america\n\nabstract\n\nreinforcement learning (rl) provides an influential characterization of the brain\u2019s mechanisms for learning to make\nadvantageous choices. an important problem, though, is how complex tasks can be represented in a way that enables\nefficient learning. we consider this problem through the lens of spatial navigation, examining how two of the brain\u2019s\nlocation representations\u2014hippocampal place cells and entorhinal grid cells\u2014are adapted to serve as basis functions for\napproximating value over space for rl. although much previous work has focused on these systems\u2019 roles in combining\nupstream sensory cues to track location, revisiting these representations with a ", "a r t i c l e s\n\nconnectivity reflects coding: a model of voltage-based \nstdp with homeostasis\n\nclaudia clopath1, lars b\u00fcsing1,2, eleni vasilaki1,2 & wulfram gerstner1\n\nelectrophysiological connectivity patterns in cortex often have a few strong connections, which are sometimes bidirectional, \namong a lot of weak connections. to explain these connectivity patterns, we created a model of spike timing\u2013dependent plasticity \n(stdp) in which synaptic changes depend on presynaptic spike arrival and the postsynaptic membrane potential, filtered with \ntwo different time constants. our model describes several nonlinear effects that are observed in stdp experiments, as well as \nthe voltage dependence of plasticity. we found that, in a simulated recurrent network of spiking neurons, our plasticity rule led \nnot only to development of localized receptive fields but also to connectivity patterns that reflect the neural code. for temporal \ncoding procedures with spatio-temporal input correlations, s", "8\n1\n0\n2\n\n \n\nn\na\nj\n \n\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n1\n4\n7\n8\n0\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\ntrain longer, generalize better: closing the\n\ngeneralization gap in large batch training of neural\n\nnetworks\n\nelad hoffer\u2217,\n\nitay hubara\u2217,\n\ndaniel soudry\n\ntechnion - israel institute of technology, haifa, israel\n\n{elad.hoffer, itayhubara, daniel.soudry}@gmail.com\n\nabstract\n\nbackground: deep learning models are typically trained using stochastic gradient\ndescent or one of its variants. these methods update the weights using their\ngradient, estimated from a small fraction of the training data. it has been observed\nthat when using large batch sizes there is a persistent degradation in generalization\nperformance - known as the \"generalization gap\" phenomenon. identifying the\norigin of this gap and closing it had remained an open problem.\ncontributions: we examine the initial high learning rate training phase. we\n\ufb01nd that the weight distance from its initialization grows logarithmically with the\nnumbe", "article\n\ncommunicated by yuri dabaghian\n\nquantifying information conveyed by large\nneuronal populations\n\njohn a. berkowitz\njaberkow@ucsd.edu\ndepartment of physics, university of california san diego, san diego,\nca 92093, u.s.a.\n\ntatyana o. sharpee\nsharpee@salk.edu\ncomputational neurobiology laboratory, salk institute for biological studies, la\njolla, ca 92037, and department of physics, university of california san diego,\nsan diego, ca 92093, u.s.a.\n\nquantifying mutual information between inputs and outputs of a large\nneural circuit is an important open problem in both machine learning\nand neuroscience. however, evaluation of the mutual information is\nknown to be generally intractable for large systems due to the exponen-\ntial growth in the number of terms that need to be evaluated. here we\nshow how information contained in the responses of large neural popu-\nlations can be effectively computed provided the input-output functions\nof individual neurons can be measured and approximated b", "proc. nati. acad. sci. usa\nvol. 88, pp. 4433-4437, may 1991\nneurobiology\n\na more biologically plausible learning rule for neural networks\n\n(reinforcement learning/coordinate transformation/posterior parietal cortex/sensorimotor integration/hebbian synapses)\n\npietro mazzonit*, richard a. andersent\u00a7, and michael i. jordant\ntdepartment of brain and cognitive sciences, and tharvard-mit division of health sciences and technology, massachusetts institute of technology,\ncambridge, ma 02139\n\ncommunicated by francis crick, february 21, 1991 (received for review october 19, 1990)\n\nabstract\nmany recent studies have used artificial neural\nnetwork algorithms to model how the brain might process\ninformation. however, back-propagation learning, the method\nthat is generally used to train these networks, is distinctly\n\"unbiological.\" we describe here a more biologically plausible\nlearning rule, using reinforcement learning, which we have\napplied to the problem of how area 7a in the posterior parietal\nc", "learning single-index models\nwith shallow neural networks\n\nalberto bietti\n\nnew york university\n\nmeta ai\n\njoan bruna\n\nnew york university\n\nclayton sanford\ncolumbia university\n\nmin jae song\n\nnew york university\n\nabstract\n\nsingle-index models are a class of functions given by an unknown univariate\n\u201clink\u201d function applied to an unknown one-dimensional projection of the input.\nthese models are particularly relevant in high dimension, when the data might\npresent low-dimensional structure that learning algorithms should adapt to. while\nseveral statistical aspects of this model, such as the sample complexity of recov-\nering the relevant (one-dimensional) subspace, are well-understood, they rely on\ntailored algorithms that exploit the speci\ufb01c structure of the target function. in this\nwork, we introduce a natural class of shallow neural networks and study its ability\nto learn single-index models via gradient \ufb02ow. more precisely, we consider shal-\nlow networks in which biases of the neurons are f", "\u0000\u0002\u0001\u0004\u0003\n\n\u0005\u0007\u0006\t\b\n\u0000\f\u000b\u000e\r\u000f\u000b\u0010\u0001\u0012\u0011\u0013\u000b\u0010\u0001\u0015\u0014\u0016\r\u0017\u0006\u0019\u0018\u001b\u001a\u001c\r\u0017\u001d\u001e\u0001\t\b \u001f!\u000b\u0013\"$#\u001e%&\u0006\u0004\u0006\t%\u0016'(\u0001\u0004\u001f*)\n\n+,\u0006\u0012)-%&\u001a\u0013\u0001.\u000b\u0010/0\u0003\n\n\u0011213%&\u001a\n\n45%&\u001f0\u001f6\b7\u00148\u000b\u000e\u0001\u0015%&\u001f\u001e\u0001\u0015\u00119\u000b;:<\b \u0001=\u001f>13%&\u001a?\u00147\b \u0003\n\n\b \u001f!\u000b(@a\b\u0016\r\u0017\u001a\u0013\u001f\u001e\u0001=\u001f>)\n\nb6c$d\u000ee$fhgji\u0010k\u0010l,mnfhfhmhe$o\u0019p\n\nc?fnfnrts?r\u0007cvu\nb\u001ec$]czedfrgevpcz\\rt]\\dih\u0002d\u0010mkj9rt]\\plmkznm\n\nc?oxw\u000ey[z\\r^]>_a`tmnr^d\u0010`tr\n\no-c?ppzqc$dsrutwv\n\nx\u0013yfz?zg{\n\n|\u0013}l}q~\u0080\u007f\u0004\u0081h\u0082?\u0083\u0085\u0084\u0017\u0086\u0088\u0087\u0004\u0089\u008b\u008ap\u008c\u008e\u008dq\u008f\u0010\u0090^\u008f\u008b\u0087\u0015\u0091\u0092\u008d\\\u008c\u008e\u008dp\u0093\u0015\u0094$\u0095\t\u0096l}l}t\u0097\u0099\u0098\u0004\u0098\t\u009a\t\u009b\u009c\u0098=\u009d\t\u009e\\\u0096e\u009f.\u009a\u0004\u009a=\u0098l\u0097\n\n\u00a0\u0017\u00a1$\u00a2\u0015\u00a3\u0004\u00a4n\u00a5\u00a7\u00a6c\u00a3\n\n\u00a8\u000e\u00a9\u00a7\u00aa\u00ac\u00abu\u00adp\u00ae\u00b0\u00af\u00b0\u00aa\u00ac\u00b1\u0015\u00b2\u008e\u00b3\u000f\u00b4\u00a7\u00ae\u00b5\u00b3\t\u00ab\u0092\u00b3\t\u00b6\\\u00af\u00b0\u00ab\u00b7\u00ad!\u00b8c\u00b3\t\u00b6\u00b9\u00b3\u0015\u00ae\u0080\u00adc\u00b2$\u00b1\u0015\u00b2\u00ba\u00adc\u00ab\u00b0\u00ab\u00b7\u00bbc\u00bca\u00adc\u00ab\u00b0\u00ab\u00b0\u00bbq\u00b1\u0015\u00aa\u00ac\u00adp\u00af\u00b0\u00aa\u00ac\u00bdc\u00b3\u00be\u00ae\u00b0\u00b3\t\u00aa\u00ac\u00b6\u00b9\u00bc\u00bf\u00bbc\u00ae\u00b5\u00b1\u0012\u00b3\t\u00e0\u00be\u00b3\t\u00b6e\u00afu\u00b2\u00ac\u00b3\t\u00adp\u00ae\u00b5\u00b6\u00a7\u00aa\u00ac\u00b6\u00b9\u00b8\u0017\u00adc\u00b2\u008e\u00b8c\u00bbc\u00ae\u00b5\u00aa\u008e\u00af\u00b0\u00a9t\u00e0\u00e1\u00abu\u00bc\u00e2\u00bbc\u00ae\n\u00b1\u0012\u00bbl\u00b6t\u00b6\u00b9\u00b3\t\u00b1\u0012\u00af\u00b0\u00aa\u008e\u00bbl\u00b6\u00a7\u00aa\u00ac\u00ab\u00b0\u00af\u00b7\u00b6\u00a7\u00b3\u0015\u00afk\u00e3[\u00bbc\u00ae\u00b0\u00e4q\u00ab\u0016\u00b1\u0012\u00bbl\u00b6\\\u00af\u00b0\u00adc\u00aa\u00ba\u00b6\u00a7\u00aa\u00ac\u00b6\u00b9\u00b8-\u00ab\u0092\u00af\u0092\u00bb3\u00b1\u00b5\u00a9\u00a7\u00adc\u00ab\u00b0\u00af\u00b0\u00aa\u00ac\u00b1\u00e1\u00e5\u00a7\u00b6\u00a7\u00aa\u00ac\u00af\u00b0\u00ab\u0015\u00e6\u00e7\u00a8\u00e8\u00a9\u00b9\u00b3\t\u00ab\u0092\u00b3\u00be\u00adc\u00b2\u008e\u00b8c\u00bbc\u00ae\u00b5\u00aa\u008e\u00af\u00b0\u00a9t\u00e0\u00e1\u00ab\u0015\u00e9\u0099\u00b1\u0015\u00adc\u00b2\u00ac\u00b2\u00ac\u00b3\t\u00ea*\u00eb\u00e8\u00ec\u0013\u00ed\u008b\u00eeu\u00efv\u00f0\u00b7\u00eb\u000e\u00f1f\u00ec\n\u00adc\u00b2\u00ac\u00b8c\u00bbc\u00ae\u00b5\u00aa\u008e\u00af\u00b0\u00a9\u00a7\u00e0\u00e1\u00ab\t\u00e9e\u00adp\u00ae\u00b0\u00b3\u00b7\u00ab\u00b0\u00a9\u00b9\u00bb=\u00e3\u00e8\u00b6\u0088\u00af\u0092\u00bb\u000f\u00e0\u00e1\u00adp\u00e4c\u00b3u\u00e3a\u00b3\t\u00aa\u00ac\u00b8l\u00a9\\\u00aff\u00adc\u00ean\u00f2\u00b0\u00e5\u00a7\u00ab\u0092\u00af\u00b0\u00e0\u00be\u00b3\t\u00b6e\u00af\u00b0\u00aba\u00aa\u00ba\u00b6\u00f3\u00ad\u0016\u00ea\u00a7\u00aa\u008e\u00ae\u00b0\u00b3\t\u00b1\u0012\u00af\u00b0\u00aa\u008e\u00bbl\u00b6\u00f3\u00af\u00b0\u00a9\u00a7\u00adp\u00aff\u00b2\u00ac\u00aa\u008e\u00b3\t\u00aba\u00adc\u00b2\u008e\u00bbl\u00b6\u00b9\u00b8 \u00af\u00b0\u00a9\u00a7\u00b38\u00b8c\u00ae\u00b5\u00adc\u00eat\u00aa\u008e\u00b3\t\u00b6\\\u00af\n\u00bbc\u00bct\u00b3\u0012\u00f4q\u00b4g\u00b3\t\u00b1\u0012\u00af\u0092\u00b3\t\u00ea\u00e7\u00ae\u00b0\u00b3\t\u00aa\u00ac\u00b6\u00b9\u00bc\u00bf\u00bbc\u00ae\u00b5\u00b1\u0012\u00b3\t\u00e0\u00be\u00b3\t\u00b6e\u00af?\u00aa\u00ac\u00b6s\u00f5g\u00bbc\u00af\u00b0\u00a9\u00e7\u00aa\u00ac\u00e0\u00e1\u00e0\u00be\u00b3\t\u00ea\u00a7\u00aa\u00ba\u00adp\u00af\u0092\u00b3\u0012\u00f6\u00d7\u00ae\u00b0\u00b3\t\u00aa\u00ac\u00b6\u00b9\u00bc\u00bf\u00bbc\u00ae\u00b5\u00b1\u0012\u00b3\t\u00e0\u00be\u00b3\t\u00b6e\u00af", "the geometry of abstraction in the hippocampus\nand prefrontal cortex\n\narticle\n\ngraphical abstract\n\nauthors\nsilvia bernardi, marcus k. benna,\nmattia rigotti, je\u00b4 ro\u02c6 me munuera,\nstefano fusi, c. daniel salzman\n\ncorrespondence\nsf2237@columbia.edu (s.f.),\ncds2005@columbia.edu (c.d.s.)\n\nin brief\ndifferent types of cognitive, emotional,\nand behavioral \ufb02exibility\u2014generalization\nin novel situations and the ability to\ngenerate many different responses to\ncomplex patterns of inputs\u2014place\ndifferent demands on neural\nrepresentations. this paper shows how\nthe geometry of neural representations\ncan be critical for elucidating how the\nbrain supports these forms of \ufb02exible\nbehavior.\n\nhighlights\nd the geometry of abstraction supports generalization\n\nd hippocampal and pfc representations are simultaneously\n\nabstract and high dimensional\n\nd multiple task-relevant variables are represented in an\n\nabstract format\n\nd representations in simulated neural networks are similar to\n\nrecorded ones\n\nbernardi et al", "r e v i e w s\n\ntop-down influences on visual \nprocessing\n\ncharles d.\u00a0gilbert1 and wu li2\n\nabstract | re-entrant or feedback pathways between cortical areas carry rich and varied \ninformation about behavioural context, including attention, expectation, perceptual \ntasks, working memory and motor commands. neurons receiving such inputs effectively \nfunction as adaptive processors that are able to assume different functional states \naccording to the task being executed. recent data suggest that the selection of \nparticular inputs, representing different components of an association field, enable \nneurons to take on different functional roles. in this review, we discuss the various \ntop-down influences exerted on the visual cortical pathways and highlight the dynamic \nnature of the receptive field, which allows neurons to carry information that is relevant \nto the current perceptual demands.\n\nthe functional properties of cortical neurons are not \nfixed. rather, they can be thought of as ad", "article\n\ndoi:10.1038/nature10918\n\nchoice-specific sequences in parietal\ncortex during a virtual-navigation\ndecision task\n\nchristopher d. harvey1,3,4{, philip coen1,4 & david w. tank1,2,3,4\n\nthe posterior parietal cortex (ppc) has an important role in many cognitive behaviours; however, the neural circuit\ndynamics underlying ppc function are not well understood. here we optically imaged the spatial and temporal activity\npatterns of neuronal populations in mice performing a ppc-dependent task that combined a perceptual decision and\nmemory-guided navigation in a virtual environment. individual neurons had transient activation staggered relative to\none another in time, forming a sequence of neuronal activation spanning the entire length of a task trial. distinct sequences\nof neurons were triggered on trials with opposite behavioural choices and defined divergent, choice-specific trajectories\nthrough a state space of neuronal population activity. cells participating in the different sequenc", "7\n1\n0\n2\n\n \n\nb\ne\nf\n1\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n4\nv\n3\n3\n5\n2\n0\n\n.\n\n7\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nworkshop track - iclr 2017\n\nadversarial examples in the physical world\n\nalexey kurakin\ngoogle brain\nkurakin@google.com\n\nian j. goodfellow\nopenai\nian@openai.com\n\nsamy bengio\ngoogle brain\nbengio@google.com\n\nabstract\n\nmost existing machine learning classi\ufb01ers are highly vulnerable to adversarial\nexamples. an adversarial example is a sample of input data which has been mod-\ni\ufb01ed very slightly in a way that is intended to cause a machine learning classi\ufb01er\nto misclassify it. in many cases, these modi\ufb01cations can be so subtle that a human\nobserver does not even notice the modi\ufb01cation at all, yet the classi\ufb01er still makes\na mistake. adversarial examples pose security concerns because they could be\nused to perform an attack on machine learning systems, even if the adversary has\nno access to the underlying model. up to now, all previous work has assumed a\nthreat model in which the adversary can feed data di", "6\n1\n0\n2\n\n \nc\ne\nd\n1\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n5\nv\n6\n9\n5\n1\n0\n\n.\n\n9\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ndirect feedback alignment provides learning in\n\ndeep neural networks\n\narild n\u00f8kland\n\ntrondheim, norway\n\narild.nokland@gmail.com\n\nabstract\n\narti\ufb01cial neural networks are most commonly trained with the back-propagation\nalgorithm, where the gradient for learning is provided by back-propagating the error,\nlayer by layer, from the output layer to the hidden layers. a recently discovered\nmethod called feedback-alignment shows that the weights used for propagating the\nerror backward don\u2019t have to be symmetric with the weights used for propagation\nthe activation forward. in fact, random feedback weights work evenly well, because\nthe network learns how to make the feedback useful. in this work, the feedback\nalignment principle is used for training hidden layers more independently from\nthe rest of the network, and from a zero initial condition. the error is propagated\nthrough \ufb01xed random feedback connecti", "24. l. vician et al., proc. natl. acad. sci. u.s.a. 92, 2164\n\n(1995).\n\n25. m. yoshihara et al., data not shown.\n26. w. li, j. t. ohlmeyer, m. e. lane, d. kalderon, cell 80,\n\n553 (1995).\n\n27. y. zhong, v. budnik, c.-f. wu, j. neurosci. 12, 644\n\n(1992).\n\n28. g. w. davis, c. m. schuster, c. s. goodman, neuron\n\n19, 561 (1997).\n\n29. d. o. hebb, the organization of behavior (wiley,\n\nnew york, 1949).\n\n30. u. frey, r. g. morris, nature 385, 533 (1997).\n\n31. k. c. martin et al., cell 91, 927 (1997).\n32. we are grateful to s. waddell for fruitful discussions,\nproviding drosophila strains, and critical reading of the\nmanuscript; m. sheng, m. bear, s. tonegawa, c. quinn,\nm. sur, and e. montana for critical reading of the\nmanuscript; k. ikeda for useful discussion; c.-f. wu for\ntechnical advice; g. miesenbock for providing phluorin\ndna; n. harden and e. buchner for antibodies; and a.\ndiantonio, d. kalderon, j. kiger, s. thor, k. schulze,\nj. phillips, g. davis, c. s. goodman, and the bloomington\nsto", "j\no\nu\nr\nn\na\nl\n \no\nf\n \nc\no\nm\np\nu\nt\na\nt\ni\no\nn\na\nl\n \nn\ne\nu\nr\no\ns\nc\ni\ne\nn\nc\ne\n,\n \n1\n,\n \n3\n1\n3\n-\n3\n2\n1\n \n(\n1\n9\n9\n4\n)\n \n(cid:14)\n9\n \n1\n9\n9\n4\n \nk\nl\nu\nw\ne\nr\n \na\nc\na\nd\ne\nm\ni\nc\n \np\nu\nb\nl\ni\ns\nh\ne\nr\ns\n,\n \nb\no\ns\nt\no\nn\n.\n \nm\na\nn\nu\nf\na\nc\nt\nu\nr\ne\nd\n \ni\nn\n \nt\nh\ne\n \nn\ne\nt\nh\ne\nr\nl\na\nn\nd\ns\n.\n \nw\nh\ne\nn\n \ni\nn\nh\ni\nb\ni\nt\ni\no\nn\n \nn\no\nt\n \ne\nx\nc\ni\nt\na\nt\ni\no\nn\n \ns\ny\nn\nc\nh\nr\no\nn\ni\nz\ne\ns\n \nn\ne\nu\nr\na\nl\n \nf\ni\nr\ni\nn\ng\n \nc\na\nr\nl\n \nv\na\nn\n \nv\nr\ne\ne\ns\nw\ni\nj\nk\n \na\nn\nd\n \nl\n.\nf\n.\n \na\nb\nb\no\nt\nt\n \nc\ne\nn\nt\ne\nr\n \nf\no\nr\n \nc\no\nm\np\nl\ne\nx\n \ns\ny\ns\nt\ne\nm\ns\n,\n \nb\nr\na\nn\nd\ne\ni\ns\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n,\n \nw\na\nl\nt\nh\na\nm\n,\n \nm\na\n \n0\n2\n2\n5\n4\n \ng\n.\n \nb\na\nr\nd\n \ne\nr\nm\ne\nn\nt\nr\no\nu\nt\n \nd\ne\np\na\nr\nt\nm\ne\nn\nt\n \no\nf\n \nm\na\nt\nh\ne\nm\na\nt\ni\nc\ns\n,\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n \no\nf\n \np\ni\nt\nt\ns\nb\nu\nr\ng\nh\n,\n \np\ni\nt\nt\ns\nb\nu\nr\ng\nh\n,\n \np\na\n \n1\n5\n2\n6\n0\n \nr\ne\nc\ne\ni\nv\ne\nd\n \na\np\nr\ni\nl\n \n1\n3\n,\n \n1\n9\n9\n4\n;\n \nr\ne\nv\ni\ns\ne\nd\n \nj\nu\nl\ny\n \n2\n2\n,\n \n1\n9\n9\n4\n;\n \na\nc\nc\ne\np\nt\ne\nd\n(\ni\nn\n \nr\ne\nv\ni\ns\ne\nd\n \nf\no\nr\nm\n)\nj\nu\nl\ny\n \n2\n7\n,\n \n1\n9\n9\n4\n.\n \na\nc\nt\ni\no\nn\n \ne\nd\ni\nt\no\nr\n:\n \nj\n.\n \nr\ni\nn\nz", "published as a conference paper at iclr 2020\n\nthe curious case of\nneural text degeneration\n\nari holtzman\u2020\u2021\n\njan buys\u00a7\u2020\n\nli du\u2020\n\nmaxwell forbes\u2020\u2021\n\nyejin choi\u2020\u2021\n\n\u2020paul g. allen school of computer science & engineering, university of washington\n\u2021allen institute for arti\ufb01cial intelligence\n\u00a7department of computer science, university of cape town\n{ahai,dul2,mbforbes,yejin}@cs.washington.edu, jbuys@cs.uct.ac.za\n\nabstract\n\ndespite considerable advances in neural language modeling, it remains an open\nquestion what the best decoding strategy is for text generation from a language\nmodel (e.g.\nto generate a story). the counter-intuitive empirical observation is\nthat even though the use of likelihood as training objective leads to high quality\nmodels for a broad range of language understanding tasks, maximization-based\ndecoding methods such as beam search lead to degeneration \u2014 output text that is\nbland, incoherent, or gets stuck in repetitive loops.\nto address this we propose nucleus sampling, a s", "task representations in neural networks trained to \nperform many cognitive tasks\n\nguangyu\u00a0robert\u00a0yang\u200a\nand xiao-jing\u00a0wang\u200a\n\n\u200a1,2, madhura\u00a0r.\u00a0joglekar1,6, h.\u00a0francis\u00a0song1,7, william\u00a0t.\u00a0newsome3,4  \n\u200a1,5*\n\nthe brain has the ability to flexibly perform many tasks, but the underlying mechanism cannot be elucidated in traditional \nexperimental and modeling studies designed for one task at a time. here, we trained single network models to perform 20 \ncognitive tasks that depend on working memory, decision making, categorization, and inhibitory control. we found that after \ntraining, recurrent units can develop into clusters that are functionally specialized for different cognitive processes, and we \nintroduce a simple yet effective measure to quantify relationships between single-unit neural representations of tasks. learning \noften gives rise to compositionality of task representations, a critical feature for cognitive flexibility, whereby one task can be \nperformed by recombining instruct", "rethinkingtheroleofdemonstrations:whatmakesin-contextlearningwork?sewonmin1,2xinxilyu1ariholtzman1mikelartetxe2mikelewis2hannanehhajishirzi1,3lukezettlemoyer1,21universityofwashington2metaai3alleninstituteforai{sewon,alrope,ahai,hannaneh,lsz}@cs.washington.edu{artetxe,mikelewis}@meta.comabstractlargelanguagemodels(lms)areabletoin-contextlearn\u2014performanewtaskviainfer-encealonebyconditioningonafewinput-labelpairs(demonstrations)andmakingpre-dictionsfornewinputs.however,therehasbeenlittleunderstandingofhowthemodellearnsandwhichaspectsofthedemonstra-tionscontributetoendtaskperformance.inthispaper,weshowthatgroundtruthdemon-strationsareinfactnotrequired\u2014randomlyreplacinglabelsinthedemonstrationsbarelyhurtsperformanceonarangeofclassi\ufb01cationandmulti-chocetasks,consistentlyover12dif-ferentmodelsincludinggpt-3.instead,we\ufb01ndthatotheraspectsofthedemonstrationsarethekeydriversofendtaskperformance,in-cludingthefactthattheyprovideafewexam-plesof(1)thelabelspace,(2)thedistributionoftheinputtext,and(3", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/2022.12.30.522267\n; \n\nthis version posted december 31, 2022. \n\nthe copyright holder for this\n\npreprint (which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in \n\nperpetuity. it is made available under a\n\ncc-by-nc 4.0 international license\n.\n\na sensory-motor theory of the neocortex based on\n\nactive predictive coding\n\nrajesh p. n. rao\n\ncenter for neurotechnology &\n\npaul g. allen school of computer science and engineering\n\nuniversity of washington, seattle, usa\n\nrao@cs.washington.edu\n\nabstract\n\nwe propose that the neocortex implements active predictive coding (apc), a form of predictive\ncoding that incorporates hierarchical dynamics and actions.\nin this model, each neocortical area\nestimates both sensory states and actions, and the cortex as whole learns to predict the sensory\nconsequences of actions at multiple hierarchical levels. \u201chigher\u201d cortical areas maintain more abs", "article\n\na neural population mechanism for rapid learning\n\ngraphical abstract\n\nauthors\n\ninputs\n\npremotor\ncortex\n\nmotor\ncortex\n\nbehavior\n\nmatthew g. perich, juan a. gallego,\nlee e. miller\n\ncorrespondence\nlm@northwestern.edu\n\nin brief\nbehavioral adaptation occurs rapidly,\neven after single errors. perich et al.\npropose that the premotor cortex can\nexploit an \u2018\u2018output-null\u2019\u2019 subspace to\nlearn to adjust its output to downstream\nregions in response to behavioral errors.\n\npremotor population\n\nactivity converges on m1\n\nvisuomotor rotation\nadaptation occurs\n\nbefore premotor cortex\n\npmd m1\n\n\u201coutput-null\u201d premotor\nplanning activity enables\n\nforce field adaptation\n\n30o\n\npotent\n\nnull\n\nseparate\n\n\u201coutput-null\u201d and\n\n\u201coutput-potent\u201d activity\n\nhighlights\nd macaque monkeys rapidly adapted to reaching movement\n\nperturbations\n\nd functional connectivity within motor and premotor cortex\n\nwas unchanged\n\nd the \u2018\u2018output-null\u2019\u2019 activity within pmd held a signature of\n\nforce-\ufb01eld learning\n\nd there was no corresp", "neuron\n\nreview\n\nacetylcholine as a neuromodulator:\ncholinergic signaling shapes\nnervous system function and behavior\n\nmarina r. picciotto,1,2,3,* michael j. higley,2,3 and yann s. mineur1\n1department of psychiatry\n2department of neurobiology\n3program in cellular neuroscience, neurodegeneration and repair\nyale university school of medicine, new haven, ct 06511, usa\n*correspondence: marina.picciotto@yale.edu\nhttp://dx.doi.org/10.1016/j.neuron.2012.08.036\n\nacetylcholine in the brain alters neuronal excitability, in\ufb02uences synaptic transmission, induces synaptic\nplasticity, and coordinates \ufb01ring of groups of neurons. as a result, it changes the state of neuronal networks\nthroughout the brain and modi\ufb01es their response to internal and external inputs: the classical role of a neuro-\nmodulator. here, we identify actions of cholinergic signaling on cellular and synaptic properties of neurons in\nseveral brain areas and discuss consequences of this signaling on behaviors related to drug abuse, a", "weighted importance sampling for off-policy learning\n\nwith linear function approximation\n\na. rupam mahmood, hado van hasselt, richard s. sutton\nreinforcement learning and arti\ufb01cial intelligence laboratory\n\nuniversity of alberta\n\nedmonton, alberta, canada t6g 1s2\n\n{ashique,vanhasse,sutton}@cs.ualberta.ca\n\nabstract\n\nimportance sampling is an essential component of off-policy model-free rein-\nforcement learning algorithms. however, its most effective variant, weighted im-\nportance sampling, does not carry over easily to function approximation and, be-\ncause of this, it is not utilized in existing off-policy learning algorithms. in this\npaper, we take two steps toward bridging this gap. first, we show that weighted\nimportance sampling can be viewed as a special case of weighting the error of\nindividual training samples, and that this weighting has theoretical and empiri-\ncal bene\ufb01ts similar to those of weighted importance sampling. second, we show\nthat these bene\ufb01ts extend to a new weighte", "cognition 152 (2016) 160\u2013169\n\ncontents lists available at sciencedirect\n\ncognition\n\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / c o g n i t\n\noriginal articles\nneural signature of hierarchically structured expectations predicts\nclustering and transfer of rule sets in reinforcement learning\nanne gabrielle eva collins a,b,\u21d1\n\n, michael joshua frank a\n\na department of cognitive, linguistic and psychological sciences, brown institute for brain science, brown university, providence, ri, usa\nb department of psychology, university of california berkeley, berkeley, ca, usa\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 19 may 2015\nrevised 9 november 2015\naccepted 2 april 2016\navailable online 12 april 2016\n\nkeywords:\nstructure-learning\neeg\ntransfer\nclustering\nprefrontal cortex\n\noften the world is structured such that distinct sensory contexts signify the same abstract rule set.\nlearning from feedback thus informs us not only about the valu", "prl 116, 038701 (2016)\n\np h y s i c a l r e v i e w l e t t e r s\n\nweek ending\n\n22 january 2016\n\nheterogeneous preference and local nonlinearity in consensus decision making\n\nandrew t. hartnett,1,2,* emmanuel schertzer,3,4 simon a. levin,2 and iain d. couzin2,5,6\n\n1department of physics, princeton university, princeton, new jersey 08544, usa\n\n2department of ecology and evolutionary biology, princeton university, princeton, new jersey 08544, usa\n\n3upmc universit\u00e9 paris 06, laboratoire de probabilit\u00e9s et mod\u00e8les al\u00e9atoires, cnrs umr 7599, 75005 paris, france\n\n4coll\u00e8ge de france, center for interdisciplinary research in biology, cnrs umr 7241, 75005 paris, france\n\n5department of collective behaviour, max planck institute for ornithology, d-78457 konstanz, germany\n\n6chair of biodiversity and collective behaviour, university of konstanz, d-78457 konstanz, germany\n\n(received 3 may 2015; published 22 january 2016)\n\nin recent years, a large body of research has focused on unveiling the fundame", "3\n2\n0\n2\n\n \n\nn\na\nj\n \n\n9\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n8\n2\n0\n8\n0\n\n.\n\n1\n0\n3\n2\n:\nv\ni\nx\nr\na\n\na survey of meta-reinforcement learning\n\njacob beck\u2217\n\njacob.beck@cs.ox.ac.uk\n\nuniversity of oxford\n\nristo vuorio\u2217\n\nristo.vuorio@cs.ox.ac.uk\n\nuniversity of oxford\n\nevan zheran liu\n\nzheng xiong\n\nluisa zintgraf\u2020\n\nevanliu@cs.stanford.edu\n\nzheng.xiong@cs.ox.ac.uk\n\nzintgraf@deepmind.com\n\nstanford university\n\nuniversity of oxford\n\nuniversity of oxford\n\nchelsea finn\n\ncbfinn@cs.stanford.edu\n\nstanford university\n\nshimon whiteson\n\nshimon.whiteson@cs.ox.ac.uk\n\nuniversity of oxford\n\nabstract\n\nwhile deep reinforcement learning (rl) has fueled multiple high-pro\ufb01le suc-\ncesses in machine learning, it is held back from more widespread adoption by\nits often poor data ef\ufb01ciency and the limited generality of the policies it produces.\na promising approach for alleviating these limitations is to cast the development\nof better rl algorithms as a machine learning problem itself in a process called\nmeta-rl. meta-rl is most", "research article\ninferring spikes from calcium imaging in\ndopamine neurons\n\nweston flemingid1\u262f, sean jewell2\u262f, ben engelhard1, daniela m. witten2*, ilana\nb. witten1*\n\n1 princeton neuroscience institute, princeton university, princeton, new jersey, united states of america,\n2 department of statistics & biostatistics, university of washington, seattle, washington, united states of\namerica\n\n\u262f these authors contributed equally to this work.\n* iwitten@princeton.edu (ibw); dwitten@uw.edu (dmw)\n\nabstract\n\ncalcium imaging has led to discoveries about neural correlates of behavior in subcortical\nneurons, including dopamine (da) neurons. however, spike inference methods have not\nbeen tested in most populations of subcortical neurons. to address this gap, we simulta-\nneously performed calcium imaging and electrophysiology in da neurons in brain slices and\napplied a recently developed spike inference algorithm to the gcamp fluorescence. this\nrevealed that individual spikes can be inferred accurate", "unitary evolution recurrent neural networks\n\nmartin arjovsky \u2217\namar shah \u2217\nyoshua bengio\nuniversidad de buenos aires, university of cambridge,\nuniversit\u00b4e de montr\u00b4eal. yoshua bengio is a cifar senior fellow.\n\u2217indicates \ufb01rst authors. ordering determined by coin \ufb02ip.\n\nmarjovsky@dc.uba.ar\nas793@cam.ac.uk\n\nabstract\n\nrecurrent neural networks (rnns) are notori-\nously dif\ufb01cult to train. when the eigenvalues\nof the hidden to hidden weight matrix deviate\nfrom absolute value 1, optimization becomes dif-\n\ufb01cult due to the well studied issue of vanish-\ning and exploding gradients, especially when try-\ning to learn long-term dependencies. to circum-\nvent this problem, we propose a new architecture\nthat learns a unitary weight matrix, with eigen-\nvalues of absolute value exactly 1. the chal-\nlenge we address is that of parametrizing uni-\ntary matrices in a way that does not require ex-\npensive computations (such as eigendecomposi-\ntion) after each weight update. we construct an\nexpressive unitary w", "e\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\ne\nr\nu\nt\na\nn\nm\no\nc\n.\ne\nr\nu\nt\na\nn\nw\nw\nw\n\n/\n\n.\n\n/\n/\n:\np\nt\nt\nh\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n4\n0\n0\n2\n\u00a9\n\n \n\na r t i c l e s\n\ncomputational subunits in thin dendrites of\npyramidal cells\n\nalon polsky1, bartlett w mel2 & jackie schiller1\n\nthe thin basal and oblique dendrites of cortical pyramidal neurons receive most of the synaptic inputs from other cells, but their\nintegrative properties remain uncertain. previous studies have most often reported global linear or sublinear summation. an\nalternative view, supported by biophysical modeling studies, holds that thin dendrites provide a layer of independent\ncomputational \u2018subunits\u2019 that sigmoidally modulate their inputs prior to global summation. to distinguish these possibilities, we\ncombined confocal imaging and dual-site focal synaptic stimulation of identified thin dendrites in rat neocortical pyramidal\nneurons. we found that nearby inputs on the same branch summed sigmoidally, whereas widely ", "cerebellar motor learning: when is cortical\nplasticity not enough?\n\njohn porrill, paul dean*\n\ndepartment of psychology, sheffield university, sheffield, united kingdom\n\nclassical marr-albus theories of cerebellar learning employ only cortical sites of plasticity. however, tests of these\ntheories using adaptive calibration of the vestibulo\u2013ocular reflex (vor) have indicated plasticity in both cerebellar\ncortex and the brainstem. to resolve this long-standing conflict, we attempted to identify the computational role of the\nbrainstem site, by using an adaptive filter version of the cerebellar microcircuit to model vor calibration for changes in\nthe oculomotor plant. with only cortical plasticity, introducing a realistic delay in the retinal-slip error signal of 100 ms\nprevented learning at frequencies higher than 2.5 hz, although the vor itself is accurate up to at least 25 hz. however,\nthe introduction of an additional brainstem site of plasticity, driven by the correlation between cereb", "9improvedtemporaldifferencemethodswithlinearfunctionapproximationdimitrip.bertsekasandangelianedichmassachusettsinstituteoftechnologyalphatech,inc.viveks.borkartatainstituteoffundamentalresearcheditor\u2019ssummary:thischapterconsiderstemporaldifferencealgorithmswithinthecontextofin\ufb01nite-horizon\ufb01nite-statedynamicprogrammingproblemswithdis-countedcostandlinearcostfunctionapproximation.thisproblemarisesasasubprobleminthepolicyiterationmethodofdynamicprogramming.additionaldiscussionsofsuchproblemscanbefoundinchapters12and6.theadvantageofthemethodpresentedhereisthatthisisthe\ufb01rstiterativetemporaldifferencemethodthatconvergeswithoutrequiringadiminishingstepsize.thechapterdiscussestheconnectionswithsutton\ufb02std(\u03bb)andwithvariousversionsofleast-squaresthatarebasedonvalue-iteration.itisshownusingbothanalysisandexperimentsthattheproposedmethodissubstantiallyfaster,simpler,andmorereliablethantd(\u03bb).comparisonsarealsomadewiththelstdmethodofboyanandbradtkeandbarto.9.1introductioninthispaper,weanalyzemethods", "8368 \u2022 the journal of neuroscience, august 9, 2006 \u2022 26(32):8368 \u2013 8376\n\nbehavioral/systems/cognitive\n\nreward-related cortical inputs define a large striatal\nregion in primates that interface with associative\ncortical connections, providing a substrate for\nincentive-based learning\n\nsuzanne n. haber,1 ki-sok kim,2 philippe mailly,3 and roberta calzavara1\n1department of pharmacology and physiology, university of rochester school of medicine, rochester, new york 14642, 2department of public health,\nkeimyung university, taegue 704-701, korea, and 3neurobiologie des signaux intercellulaires, centre national de la recherche scientifique, universite\npierre et marie curie, 75252 paris, france\n\nthe anterior cingulate and orbital cortices and the ventral striatum process different aspects of reward evaluation, whereas the dorso-\nlateral prefrontal cortex and the dorsal striatum are involved in cognitive function. collectively, these areas are critical to decision\nmaking. we mapped the striatal a", "article\n\nhttps://doi.org/10.1038/s41467-020-14578-5\n\nopen\n\nseparability and geometry of object manifolds\nin deep neural networks\n\nuri cohen\n\n1,6, sueyeon chung\n\n2,3,4,6, daniel d. lee5 & haim sompolinsky1,2*\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\nstimuli are represented in the brain by the collective population responses of sensory neu-\nrons, and an object presented under varying conditions gives rise to a collection of neural\npopulation responses called an \u2018object manifold\u2019. changes in the object representation along\na hierarchical sensory system are associated with changes in the geometry of those mani-\nfolds, and recent theoretical progress connects this geometry with \u2018classi\ufb01cation capacity\u2019, a\nquantitative measure of the ability to support object classi\ufb01cation. deep neural networks\ntrained on object classi\ufb01cation tasks are a natural testbed for the applicability of this relation.\nwe show how classi\ufb01cation capacity improves along the hierarchies of deep neural networks\nwith different arch", "open\n\na local learning rule for \nindependent component analysis\n\ntakuya isomura1,2,3 & taro toyoizumi1\n\nreceived: 16 february 2016\naccepted: 26 may 2016\npublished: 21 june 2016\n\nhumans can separately recognize independent sources when they sense their superposition. this \ndecomposition is mathematically formulated as independent component analysis (ica). while a few \nbiologically plausible learning rules, so-called local learning rules, have been proposed to achieve \nica, their performance varies depending on the parameters characterizing the mixed signals. here, \nwe propose a new learning rule that is both easy to implement and reliable. both mathematical and \nnumerical analyses confirm that the proposed rule outperforms other local learning rules over a wide \nrange of parameters. notably, unlike other rules, the proposed rule can separate independent sources \nwithout any preprocessing, even if the number of sources is unknown. the successful performance of \nthe proposed rule is then ", "spike-based reinforcement learning in continuous state\nand action space: when policy gradient methods fail\n\neleni vasilaki1,2*, nicolas fre\u00b4 maux1, robert urbanczik3, walter senn3, wulfram gerstner1\n\n1 laboratory of computational neuroscience, epfl, lausanne, switzerland, 2 department of computer science, university of sheffield, sheffield, united kingdom,\n3 department of physiology, university of bern, bern, switzerland\n\nabstract\n\nchanges of synaptic connections between neurons are thought to be the physiological basis of learning. these changes can\nbe gated by neuromodulators that encode the presence of reward. we study a family of reward-modulated synaptic\nlearning rules for spiking neurons on a learning task in continuous space inspired by the morris water maze. the synaptic\nupdate rule modifies the release probability of synaptic transmission and depends on the timing of presynaptic spike arrival,\npostsynaptic action potentials, as well as the membrane potential of the postsynapti", "journal of mathematical psychology 56 (2012) 1\u201312\n\ncontents lists available at sciverse sciencedirect\n\njournal of mathematical psychology\n\njournal homepage: www.elsevier.com/locate/jmp\n\ntutorial\na tutorial on bayesian nonparametric models\nsamuel j. gershman a,\u21e4, david m. blei b\na department of psychology and princeton neuroscience institute, princeton university, princeton nj 08540, usa\nb department of computer science, princeton university, princeton nj 08540, usa\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 25 january 2011\nreceived in revised form\n4 august 2011\navailable online 1 september 2011\n\nkeywords:\nbayesian methods\nchinese restaurant process\nindian buffet process\ncontents\n\na key problem in statistical modeling is model selection, that is, how to choose a model at an appropriate\nlevel of complexity. this problem appears in many settings, most prominently in choosing the number of\nclusters in mixture models or the number of factors in factor analysis. in t", "3\n2\n0\n2\n\n \n\nn\na\nj\n \n\n9\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n6\nv\n4\n9\n7\n9\n0\n\n.\n\n1\n1\n1\n2\n:\nv\ni\nx\nr\na\n\njournal of arti\ufb01cial intelligence research 76 (2023) 201-264\n\nsubmitted 08/2022; published 01/2023\n\na survey of zero-shot generalisation in deep\n\nreinforcement learning\n\nrobert kirk\nuniversity college london, gower st, london\nwc1e 6bt, united kingdom\namy zhang\nuniversity of california, berkeley, berkeley\nca, united states\nmeta ai research,\nedward grefenstette\nuniversity college london, gower st, london\nwc1e 6bt, united kingdom\ntim rockt\u00e4schel\nuniversity college london, gower st, london\nwc1e 6bt, united kingdom\n\nrobert.kirk.20@ucl.ac.uk\n\namyzhang@fb.com\n\ne.grefenstette@ucl.ac.uk\n\ntim.rocktaschel@ucl.ac.uk\n\nabstract\n\nthe study of zero-shot generalisation (zsg) in deep reinforcement learning (rl)\naims to produce rl algorithms whose policies generalise well to novel unseen situations at\ndeployment time, avoiding over\ufb01tting to their training environments. tackling this is vital\nif we are to deploy reinf", "article\n\ndoi:10.1038/nature15257\n\nlabelling and optical erasure of synaptic\nmemory traces in the motor cortex\n\nakiko hayashi-takagi1,2, sho yagishita1,3, mayumi nakamura1, fukutoshi shirai1, yi i. wu4, amanda l. loshbaugh5,6,\nbrian kuhlman5,6, klaus m. hahn5,7 & haruo kasai1,3\n\ndendritic spines are the major loci of synaptic plasticity and are considered as possible structural correlates of memory.\nnonetheless, systematic manipulation of specific subsets of spines in the cortex has been unattainable, and thus, the link\nbetween spines and memory has been correlational. we developed a novel synaptic optoprobe, as-parac1 (activated\nsynapse targeting photoactivatable rac1), that can label recently potentiated spines specifically, and induce the selective\nshrinkage of as-parac1-containing spines. in vivo imaging of as-parac1 revealed that a motor learning task induced\nsubstantial synaptic remodelling in a small subset of neurons. the acquired motor learning was disrupted by the optical\nshri", "r e p o r t s\n\n35. r. n. shepard, psychon. bull. rev. 1, 2 (1994).\n36. j. b. tenenbaum, adv. neural info. proc. syst. 10, 682\n\n(1998).\n\n37. t. martinetz, k. schulten, neural netw. 7, 507 (1994).\n38. v. kumar, a. grama, a. gupta, g. karypis, introduc-\ntion to parallel computing: design and analysis of\nalgorithms (benjamin/cummings, redwood city, ca,\n1994), pp. 257\u2014297.\n\n39. d. beymer, t. poggio, science 272, 1905 (1996).\n40. available at www.research.att.com/;yann/ocr/mnist.\n41. p. y. simard, y. lecun, j. denker, adv. neural info.\n\n42.\n\nproc. syst. 5, 50 (1993).\nin order to evaluate the (cid:222)ts of pca, mds, and isomap\non comparable grounds, we use the residual variance\n\n1 \u2014 r2(d(cid:246)\nm, dy). dy is the matrix of euclidean distanc-\nes in the low-dimensional embedding recovered by\neach algorithm. d(cid:246)\nm is each algorithm(cid:213)s best estimate\nof the intrinsic manifold distances: for isomap, this is\nthe graph distance matrix dg; for pca and mds, it is\nthe euclidean input-spa", "vol 443|7 september 2006|doi:10.1038/nature05078\n\nletters\n\nexperience-dependent representation of visual\ncategories in parietal cortex\ndavid j. freedman1 & john a. assad1\n\ncategorization is a process by which the brain assigns meaning to\nsensory stimuli. through experience, we learn to group stimuli\ninto categories, such as \u2018chair\u2019, \u2018table\u2019 and \u2018vehicle\u2019, which are\ncritical for rapidly and appropriately selecting behavioural\nresponses1,2. although much is known about the neural represen-\ntation of simple visual stimulus features (for example, orientation,\ndirection and colour), relatively little is known about how the\nbrain learns and encodes the meaning of stimuli. we trained\nmonkeys to classify 3608 of visual motion directions into two\ndiscrete categories, and compared neuronal activity in the lateral\nintraparietal (lip) and middle temporal (mt) areas, two inter-\nconnected brain regions3 known to be involved in visual motion\nprocessing4\u20136. here we show that neurons in lip\u2014an area kno", "a r t i c l e s\n\nrobust timing and motor patterns by taming chaos in \nrecurrent neural networks\nrodrigo laje1,5 & dean v buonomano1\u20134\nthe brain\u2019s ability to tell time and produce complex spatiotemporal motor patterns is critical for anticipating the next ring of a \ntelephone or playing a musical instrument. one class of models proposes that these abilities emerge from dynamically changing \npatterns of neural activity generated in recurrent neural networks. however, the relevant dynamic regimes of recurrent networks \nare highly sensitive to noise; that is, chaotic. we developed a firing rate model that tells time on the order of seconds and \ngenerates complex spatiotemporal patterns in the presence of high levels of noise. this is achieved through the tuning of the \nrecurrent connections. the network operates in a dynamic regime that exhibits coexisting chaotic and locally stable trajectories. \nthese stable patterns function as \u2018dynamic attractors\u2019 and provide a feature that is characte", "original research\npublished: 04 april 2019\ndoi: 10.3389/fncom.2019.00018\n\ndeep learning with asymmetric\nconnections and hebbian updates\n\nyali amit*\n\ndepartment of statistics, university of chicago, chicago, il, united states\n\nwe show that deep networks can be trained using hebbian updates yielding similar\nperformance to ordinary back-propagation on challenging image datasets. to overcome\nthe unrealistic symmetry in connections between layers, implicit in back-propagation,\nthe feedback weights are separate from the feedforward weights. the feedback weights\nare also updated with a local rule, the same as the feedforward weights\u2014a weight is\nupdated solely based on the product of activity of the units it connects. with \ufb01xed\nfeedback weights as proposed in lillicrap et al. (2016) performance degrades quickly\nas the depth of the network increases. if the feedforward and feedback weights are\ninitialized with the same values, as proposed in zipser and rumelhart (1990), they remain\nthe same thr", "anrv314-ne30-11\n\nari\n\n10 may 2007\n\n20:1\n\nmultiple dopamine\nfunctions at different\ntime courses\nwolfram schultz\ndepartment of physiology, development, and neuroscience, university of\ncambridge, cambridge cb2 3dy, united kingdom; email: ws234@cam.ac.uk\n\nannu. rev. neurosci. 2007. 30:259\u201388\n\nfirst published online as a review in advance on\nmarch 12, 2007\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev.neuro.28.061604.135722\ncopyright c(cid:2) 2007 by annual reviews.\nall rights reserved\n\n0147-006x/07/0721-0259$20.00\n\nkey words\nreward, uncertainty, punishment, movement\n\nabstract\nmany lesion studies report an amazing variety of de\ufb01cits in behav-\nioral functions that cannot possibly be encoded in great detail by the\nrelatively small number of midbrain dopamine neurons. although\nhoping to unravel a single dopamine function underlying these phe-\nnomena, electrophysiological and neurochemical studies still give a\nconfusing, mutually ex", "sanhueza and lisman molecular brain 2013, 6:10\nhttp://www.molecularbrain.com/content/6/1/10\n\nr ev i e w\nthe camkii/nmdar complex as a molecular\nmemory\nmagdalena sanhueza1 and john lisman2*\n\nopen access\n\nabstract\ncamkii is a major synaptic protein that is activated during the induction of long-term potentiation (ltp) by the ca2+\ninflux through nmdars. this activation is required for ltp induction, but the role of the kinase in the maintenance\nof ltp is less clear. elucidating the mechanisms of maintenance may provide insights into the molecular processes\nthat underlie the stability of stored memories. in this brief review, we will outline the criteria for evaluating an ltp\nmaintenance mechanism. the specific hypothesis evaluated is that ltp is maintained by the complex of activated\ncamkii with the nmdar. the evidence in support of this hypothesis is substantial, but further experiments are\nrequired, notably to determine the time course and persistence of complex after ltp induction. add", "journal of machine learning research 11 (2010) 2287-2322\n\nsubmitted 7/09; revised 4/10; published 8/10\n\nspectral regularization algorithms for learning large incomplete\n\nmatrices\n\nrahul mazumder\ntrevor hastie\u2217\ndepartment of statistics\nstanford university\nstanford, ca 94305\nrobert tibshirani\u2020\ndepartment of health, research and policy\nstanford university\nstanford, ca 94305\n\neditor: tommi jaakkola\n\nrahulm@stanford.edu\nhastie@stanford.edu\n\ntibs@stanford.edu\n\nabstract\n\nwe use convex relaxation techniques to provide a sequence of regularized low-rank solutions for\nlarge-scale matrix completion problems. using the nuclear norm as a regularizer, we provide a sim-\nple and very ef\ufb01cient convex algorithm for minimizing the reconstruction error subject to a bound\non the nuclear norm. our algorithm soft-impute iteratively replaces the missing elements with\nthose obtained from a soft-thresholded svd. with warm starts this allows us to ef\ufb01ciently compute\nan entire regularization path of solutions on ", "neuron, vol. 32, 899\u2013910, december 6, 2001, copyright \uf8e92001 by cell press\n\ntemporal precision and temporal drift in brain\nand behavior of zebra finch song\n\nzhiyi chi1,3 and daniel margoliash2\n1 department of statistics\n2 department of organismal biology and anatomy\nuniversity of chicago\nchicago, illinois 60637\n\nsummary\n\nin the zebra finch forebrain nucleus robustus archistri-\natalis (ra), neurons burst during singing. we showed\nthat the internal structure of spike bursts was regu-\nlated with a precision of circa 0.2 ms, and yielded\nalignment of acoustic features of song with a precision\nof circa 1 ms. in addition, interburst intervals and cor-\nresponding syllable durations displayed systematic\nvariation within song (average elongation 0.3 ms/s\nsong), and slower \u201cdrift\u201d across songs. systematic\nvariation on even a coarser time scale might be difficult\nto detect in other systems, but could affect the analysis\nof temporal patterning. the close relationship be-\ntween precise timing of indi", "an introduction to mds\n\nflorian wickelmaier\n\nsound quality research unit, aalborg university, denmark\n\nmay 4, 2003\n\nauthor\u2019s note\n\nthis manuscript was completed while the author was employed at the sound qual-\n\nity research unit at aalborg university. this unit receives \ufb01nancial support from\n\ndelta acoustics & vibration, br\u00a8uel & kj\u00e6r, and bang & olufsen, as well as from\n\nthe danish national agency for industry and trade (efs) and the danish technical\n\nresearch council (stvf). the author would like to thank karin zimmer and wolf-\n\ngang ellermeier for numerous helpful comments on earlier drafts of the manuscript.\n\ni am also indebted to jody ghani for her linguistic revision of the paper. cor-\n\nrespondence concerning this article should be addressed to florian wickelmaier,\n\ndepartment of acoustics, aalborg university, fredrik bajers vej 7 b5, 9220 aal-\n\nborg east, denmark (email: fw@acoustics.dk).\n\n1\n\n\f", "reinforcement learning with gaussian processes\n\nyaakov engel\ndept. of computing science, university of alberta, edmonton, canada\n\nyaki@cs.ualberta.ca\n\nshie mannor\ndept. of electrical and computer engineering, mcgill university, montreal, canada\n\nshie@ece.mcgill.ca\n\nron meir\ndept. of electrical engineering, technion institute of technology, haifa 32000, israel\n\nrmeir@ee.technion.ac.il\n\nabstract\n\ngaussian process temporal di\ufb00erence\n(gptd) learning o\ufb00ers a bayesian solution\nto the policy evaluation problem of reinforce-\nment learning. in this paper we extend the\ngptd framework by addressing two pressing\nissues, which were not adequately treated\nin the original gptd paper (engel et al.,\n2003). the \ufb01rst is the issue of stochasticity\nin the state transitions, and the second is\nconcerned with action selection and policy\nimprovement. we present a new generative\nmodel for the value function, deduced from\nits relation with the discounted return. we\nderive a corresponding on-line algorithm\nfor le", "research article\nmodeling functional cell types in spike train\ndata\n\ndaniel n. zdeblickid1*, eric t. shea-brownid2,3, daniela m. wittenid4, michael\na. buiceid2,3\n\n1 department of electrical and computer engineering, university of washington, seattle, washington, united\nstates of america, 2 department of applied math, university of washington, seattle, washington, united\nstates of america, 3 mindscope program, allen institute, seattle, washington, united states of america,\n4 department of statistics and biostatistics, university of washington, seattle, washington, united states of\namerica\n\n* zdeblick@uw.edu\n\nabstract\n\na major goal of computational neuroscience is to build accurate models of the activity of\nneurons that can be used to interpret their function in circuits. here, we explore using func-\ntional cell types to refine single-cell models by grouping them into functionally relevant clas-\nses. formally, we define a hierarchical generative model for cell types, single-cell\nparamete", "review\nfrom word embeddings to pre-trained language models:\na state-of-the-art walkthrough\nmourad mars 1,2\n\n1 college of computers and information systems, umm al-qura university, mecca 24382, saudi arabia;\n\nmsmars@uqu.edu.sa\n\n2 higher institute of computer sciences and mathematics, monastir university, monastir 5000, tunisia\n\nabstract: with the recent advances in deep learning, different approaches to improving pre-trained\nlanguage models (plms) have been proposed. plms have advanced state-of-the-art (sota) per-\nformance on various natural language processing (nlp) tasks such as machine translation, text\nclassi\ufb01cation, question answering, text summarization, information retrieval, recommendation sys-\ntems, named entity recognition, etc. in this paper, we provide a comprehensive review of prior\nembedding models as well as current breakthroughs in the \ufb01eld of plms. then, we analyse and\ncontrast the various models and provide an analysis of the way they have been built (number of\nparamet", "articles\n\nvol 459 | 28 may 2009 | doi:10.1038/nature08010\n\nhippocampal theta oscillations are\ntravelling waves\n\nevgueniy v. lubenov1 & athanassios g. siapas1\n\ntheta oscillations clock hippocampal activity during awake behaviour and rapid eye movement (rem) sleep. these\noscillations are prominent in the local field potential, and they also reflect the subthreshold membrane potential and strongly\nmodulate the spiking of hippocampal neurons. the prevailing view is that theta oscillations are synchronized throughout the\nhippocampus, despite the lack of conclusive experimental evidence. in contrast, here we show that in freely behaving rats,\ntheta oscillations in area ca1 are travelling waves that propagate roughly along the septotemporal axis of the hippocampus.\nfurthermore, we find that spiking in the ca1 pyramidal cell layer is modulated in a consistent travelling wave pattern. our\nresults demonstrate that theta oscillations pattern hippocampal activity not only in time, but also across ", "acute silencing of hippocampal ca3 reveals  \na dominant role in place field responses\n\nheydar\u00a0davoudi\u200a\n\n\u200a1,2,3,4 and david\u00a0j.\u00a0foster\u200a\n\n\u200a1,2,3*\n\nneurons  in  hippocampal  output  area  ca1  are  thought  to \nexhibit  redundancy  across  cortical  and  hippocampal  inputs. \nhere we show instead that acute silencing of ca3 terminals \ndrastically reduces place field responses for many ca1 neu-\nrons, while a smaller number are unaffected or have increased \nresponses. these results imply that ca3 is the predominant \ndriver of ca1 place cells under normal conditions, while also \nrevealing heterogeneity in input dominance across cells.\n\nthe hippocampus plays a critical role in memory, and two sys-\ntems-level mechanisms are thought to support this role: the spa-\ntially localized responses of individual hippocampal neurons during \nlocomotion, and the coordinated activity of large numbers of neu-\nrons during rest reflected in the local field potential (lfp) as high \nfrequency  (100\u2013250 hz)  sharp", "psychobiology \n1988.  vol.  16 (2).  85-125 \n\na neuronal model of classical conditioning \n\nair force  wright aeronautical laboratories,  wright-patterson  air force  base,  ohio \n\na.  harry  klopf \n\na neuronal model of classical conditioning is proposed. the model is most easily described by \ncontrasting it with a still influential neuronal model first analyzed by hebb (1949). it is proposed \nthat the hebbian model be modified in three ways to yield a model more in accordance with animal \nlearning phenomena. first, instead of correlating pre- and postsynaptic levels of activity, changes \nin pre- and postsynaptic levels of activity should be correlated to determine the changes in syn \naptic efficacy that represent learning. second, instead of correlating approximately simultaneous \npre- and postsynaptic signals, earlier changes in presynaptic signals should be correlated with \nlater changes in postsynaptic signals. third, a change in the efficacy of a synapse should be propor \ntional to", "3\n2\n0\n2\n\n \n\nv\no\nn\n3\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n8\n2\n9\n6\n0\n\n.\n\n1\n1\n3\n2\n:\nv\ni\nx\nr\na\n\nattention for causal relationship discovery from\n\nbiological neural dynamics\n\nziyu lu\u2217\n\nuniversity of washington\n\noak ridge national laboratory\n\noak ridge national laboratory\n\nseattle, wa 98195\nluziyu@uw.edu\n\noak ridge, tn 37830\ntabassuma@ornl.gov\n\noak ridge, tn 37830\nkulkarnisr@ornl.gov\n\nanika tabassum\n\nshruti kulkarni\n\nlu mi\n\nj. nathan kutz\n\neric shea-brown\n\nallen institute for brain science\n\nuniversity of washington\n\nuniversity of washington\n\nseattle, wa 98195\n\nkutz@uw.edu\n\nseattle, wa 98195\n\netsb@uw.edu\n\nseattle, wa 98109\n\nuniversity of washington\n\nseattle, wa 98195\n\nlu.mi@alleninstitute.org\n\nseung-hwan lim\n\noak ridge national laboratory\n\noak ridge, tn 37830\n\nlims1@ornl.gov\n\nabstract\n\nthis paper explores the potential of the transformer models for learning granger\ncausality in networks with complex nonlinear dynamics at every node, as in\nneurobiological and biophysical networks. our study prima", "neural wave machines: learning spatiotemporally structured representations\n\nwith locally coupled oscillatory recurrent neural networks\n\nt. anderson keller 1 max welling 1\n\nabstract\n\ntraveling waves have been measured at a diversity\nof regions and scales in the brain, however a con-\nsensus as to their computational purpose has yet to\nbe reached. an intriguing hypothesis is that travel-\ning waves serve to structure neural representations\nboth in space and time, thereby acting as an in-\nductive bias towards natural data. in this work,\nwe investigate this hypothesis by introducing the\nneural wave machine (nwm) \u2013 a locally coupled\noscillatory recurrent neural network capable of ex-\nhibiting traveling waves in its hidden state. after\ntraining on simple dynamic sequences, we show\nthat this model indeed learns static spatial struc-\nture such as topographic organization, and further\nuses complex spatiotemporal structure such as\ntraveling waves to encode observed transforma-\ntions. to measure th", "tools and resources\n\nmultiphoton imaging of neural structure \nand activity in drosophila through the \nintact\u00a0cuticle\nmax jameson aragon1\u2020\u2021, aaron t mok2\u2020, jamien shea1\u2020, mengran wang2\u2020, \nhaein kim1, nathan barkdull3, chris xu2, nilay yapici1*\n\n1department of neurobiology and behavior, cornell university, ithaca, united states; \n2school of applied and engineering physics, cornell university, ithaca, united states; \n3department of physics, university of florida, gainesville, united states\n\nabstract we developed a multiphoton imaging method to capture neural structure and \nactivity in behaving flies through the intact cuticle. our measurements showed that the fly head \ncuticle has surprisingly high transmission at wavelengths >900nm, and the difficulty of through- \ncuticle imaging is due to the air sacs and/or fat tissue underneath the head cuticle. by compressing \nor removing the air sacs, we performed multiphoton imaging of the fly brain through the intact \ncuticle. our anatomical and f", "gradient starvation:\n\na learning proclivity in neural networks\n\nmohammad pezeshki1,2\naaron courville1,2\n\ns\u00e9kou-oumar kaba1,3 yoshua bengio1,2\nguillaume lajoie1,2\ndoina precup1,3,4\n1mila\n4google deepmind\ncorresponding authors:{pezeshki, guillaume.lajoie}@mila.quebec\n\n2universit\u00e9 de montr\u00e9al\n\n3mcgill university\n\nabstract\n\nwe identify and formalize a fundamental gradient descent phenomenon leading to\na learning proclivity in over-parameterized neural networks. gradient starvation\narises when cross-entropy loss is minimized by capturing only a subset of features\nrelevant for the task, despite the presence of other predictive features that fail to\nbe discovered. this work provides a theoretical explanation for the emergence of\nsuch feature imbalances in neural networks. using tools from dynamical systems\ntheory, we identify simple properties of learning dynamics during gradient descent\nthat lead to this imbalance, and prove that such a situation can be expected given\ncertain statistical str", "available online at www.sciencedirect.com\n\nsciencedirect\n\ncurrent opinion in\n\nneurobiology\n\nfrom lazy to rich to exclusive task representations in\nneural networks and neural codes\nmatthew farrell1, stefano recanatesi2 and eric shea-brown2\n\nabstract\nneural circuits\u2014both in the brain and in \u201cartificial\u201d neural\nnetwork models\u2014learn to solve a remarkable variety of tasks,\nand there is a great current opportunity to use neural networks\nas models for brain function. key to this endeavor is the ability\nto characterize the representations formed by both artificial and\nbiological brains. here, we investigate this potential through the\nlens of recently developing theory that characterizes neural\nnetworks as \u201clazy\u201d or \u201crich\u201d depending on the approach they\nuse to solve tasks: lazy networks solve tasks by making small\nchanges in connectivity, while rich networks solve tasks by\nsignificantly modifying weights throughout the network\n(including \u201chidden layers\u201d). we further elucidate rich networks\nthro", "neuron\n\narticle\n\nthe excitatory neuronal network\nof the c2 barrel column\nin mouse primary somatosensory cortex\n\nsandrine lefort,1 christian tomm,2 j.-c. floyd sarria,3 and carl c.h. petersen1,*\n1laboratory of sensory processing, brain mind institute\n2laboratory of computational neuroscience, brain mind institute\n3bioimaging and optics platform\nfaculty of life sciences, ecole polytechnique federale de lausanne (epfl), ch1015, switzerland\n*correspondence: carl.petersen@ep\ufb02.ch\ndoi 10.1016/j.neuron.2008.12.020\n\nsummary\n\nlocal microcircuits within neocortical columns form\nkey determinants of sensory processing. here, we\ninvestigate the excitatory synaptic neuronal network\nof an anatomically de\ufb01ned cortical column, the c2\nbarrel column of mouse primary somatosensory\ncortex. this cortical column is known to process\ntactile information related to the c2 whisker. through\nmultiple simultaneous whole-cell\nrecordings, we\nquantify connectivity maps between individual excit-\natory neurons located ac", "the successor representation in human \nreinforcement learning\n\ni. momennejad\u200a\n\n\u200a1*, e. m. russek2, j. h. cheong3, m. m. botvinick\u200a\n\n\u200a4, n. d. daw1 and s. j. gershman\u200a\n\n\u200a5\n\ntheories of reward learning in neuroscience have focused on two families of algorithms thought to capture deliberative versus \nhabitual choice. \u2018model-based\u2019 algorithms compute the value of candidate actions from scratch, whereas \u2018model-free\u2019 algo-\nrithms make choice more efficient but less flexible by storing pre-computed action values. we examine an intermediate algorith-\nmic family, the successor representation, which balances flexibility and efficiency by storing partially computed action values: \npredictions about future events. these pre-computation strategies differ in how they update their choices following changes in a \ntask. the successor representation\u2019s reliance on stored predictions about future states predicts a unique signature of insensitiv-\nity to changes in the task\u2019s sequence of events, but flexibl", "neuron, vol. 33, 163\u2013175, january 17, 2002, copyright \uf8e92002 by cell press\n\nthalamic relay functions and\ntheir role in corticocortical communication:\ngeneralizations from the visual system\n\nreview\n\nr.w. guillery1 and s. murray sherman2,3\n1department of anatomy\nuniversity of wisconsin school of medicine\n1300 university avenue\nmadison, wisconsin 53706\n2 department of neurobiology\nstate university of new york\nstony brook, new york 11794\n\nsummary\n\nall neocortical areas receive thalamic inputs. some\nthalamocortical pathways relay information from as-\ncending pathways (first order thalamic relays) and oth-\ners relay information from other cortical areas (higher\norder thalamic relays), thus serving a role in cortico-\ncortical communication. most, possibly all, afferents\nreaching thalamus, ascending and cortical, are branches\nof axons that innervate lower (motor) centers, so that\nthalamocortical pathways can be viewed generally as\nmonitors of ongoing motor instructions. in terms of\nnumbers, the", "neuron\n\nperspective\n\ncanonical microcircuits for predictive coding\n\nandre m. bastos,1,2,6 w. martin usrey,1,3,4 rick a. adams,8 george r. mangun,2,3,5 pascal fries,6,7 and karl j. friston8,*\n1center for neuroscience\n2center for mind and brain\n3department of neurology\n4department of neurobiology, physiology and behavior\n5department of psychology\nuniversity of california, davis, davis, ca 95618 usa\n6ernst stru\u00a8 ngmann institute (esi) for neuroscience in cooperation with max planck society, deutschordenstra\u00dfe 46, 60528 frankfurt,\ngermany\n7donders institute for brain, cognition and behaviour, radboud university nijmegen, kapittelweg 29, 6525 en nijmegen, netherlands\n8the wellcome trust centre for neuroimaging, university college london, queen square, london wc1n 3bg, uk\n*correspondence: k.friston@ucl.ac.uk\nhttp://dx.doi.org/10.1016/j.neuron.2012.10.038\n\nthis perspective considers the in\ufb02uential notion of a canonical (cortical) microcircuit in light of recent theories\nabout neuronal process", "letter\nrepetitive motor learning induces coordinated\nformation of clustered dendritic spines in vivo\n\ndoi:10.1038/nature10844\n\nmin fu1, xinzhu yu1, ju lu2 & yi zuo1\n\nmany lines of evidence suggest that memory in the mammalian\nbrain is stored with distinct spatiotemporal patterns1,2. despite\nrecent progresses in identifying neuronal populations involved\nin memory coding3\u20135, the synapse-level mechanism is still poorly\nunderstood. computational models and electrophysiological data\nhave shown that functional clustering of synapses along dendritic\nbranches leads to nonlinear summation of synaptic inputs and\ngreatly expands the computing power of a neural network6\u201310.\nhowever, whether neighbouring synapses are involved in encoding\nsimilar memory and how task-specific cortical networks develop\nduring learning remain elusive. using transcranial two-photon\nmicroscopy11, we followed apical dendrites of layer 5 pyramidal\nneurons in the motor cortex while mice practised novel forelimb\nskills. here", "a r t i c l e s\n\nthalamic nuclei convey diverse contextual information \nto layer 1 of visual cortex\nmorgane m roth1,4, johannes c dahmen2\u20134, dylan r muir1,4, fabia imhof1, francisco j martini1 & sonja b hofer1,2\nsensory perception depends on the context in which a stimulus occurs. prevailing models emphasize cortical feedback as the \nsource of contextual modulation. however, higher order thalamic nuclei, such as the pulvinar, interconnect with many cortical \nand subcortical areas, suggesting a role for the thalamus in providing sensory and behavioral context. yet the nature of the \nsignals conveyed to cortex by higher order thalamus remains poorly understood. here we use axonal calcium imaging to measure \ninformation provided to visual cortex by the pulvinar equivalent in mice, the lateral posterior nucleus (lp), as well as the \ndorsolateral geniculate nucleus (dlgn). we found that dlgn conveys retinotopically precise visual signals, while lp provides \ndistributed information from the ", "neural networks 15 (2002) 665\u2013687\n\n2002 special issue\n\nwww.elsevier.com/locate/neunet\n\ncontrol of exploitation \u2013 exploration meta-parameter\n\nin reinforcement learning\n\nshin ishiia,b,*, wako yoshidaa,b, junichiro yoshimotoa,b\n\nanara institute of science and technology, 8916-5 takayama-cho, ikoma, nara 630-0101, japan\n\nbcrest, japan science and technology corporation, japan\n\nreceived 10 october 2001; accepted 16 april 2002\n\nabstract\n\nin reinforcement learning (rl), the duality between exploitation and exploration has long been an important issue. this paper presents a\nnew method that controls the balance between exploitation and exploration. our learning scheme is based on model-based rl, in which the\nbayes inference with forgetting effect estimates the state-transition probability of the environment. the balance parameter, which\ncorresponds to the randomness in action selection, is controlled based on variation of action results and perception of environmental change.\nwhen applied to ma", "acta numerica (2021), pp. 327\u2013444\ndoi:10.1017/s0962492921000052\n\nprinted in the united kingdom\n\nneural network approximation\n\nronald devore\n\ndepartment of mathematics, texas a&m university,\n\ncollege station, tx 77843, usa\ne-mail: rdevore@math.tamu.edu\n\ndepartment of operations research and financial engineering,\n\nboris hanin\n\nprinceton university, nj 08544, usa\n\ne-mail: bhanin@princeton.edu\n\nguergana petrova\n\ndepartment of mathematics, texas a&m university,\n\ncollege station, tx 77843, usa\ne-mail: gpetrova@math.tamu.edu\n\nneural networks (nns) are the method of choice for building learning algorithms.\nthey are now being investigated for other numerical tasks such as solving high-\ndimensional partial di\ufb00erential equations. their popularity stems from their empir-\nical success on several challenging learning problems (computer chess/go, autonom-\nous navigation, face recognition). however, most scholars agree that a convincing\ntheoretical explanation for this success is still lacking. since", "available  online  at  www.sciencedirect.com\n\nsequence  learning  and  the  role  of  the  hippocampus  in  rodent\nnavigation\ndavid  j  foster1 and  james  j  knierim1,2\n\nthe  hippocampus  has  long  been  associated  with  navigation\nand  spatial  representations,  but  it  has  been  dif\ufb01cult  to  link\ndirectly  the  neurophysiological  correlates  of  hippocampal\nplace  cells  with  navigational  planning  and  action.  in  recent\nyears,  large-scale  population  recordings  of  place  cells  have\nrevealed  that  spatial  sequences  are  stored  and  activated  in\nways  that  may  support  navigational  strategies.  plasticity\nmechanisms  allow  the  hippocampus  to  store  learned\nsequences  of  locations  that  may  allow  predictions  of  future\nlocations  based  on  past  experience.  these  sequences  can  also\nbe  activated  during  navigational  behavior  in  ways  that  may\nallow  the  animal  to  learn  trajectories  toward  goals.  task-\ndependent  alterations  in  place  ", "article\n\na top-down cortical circuit for accurate sensory\nperception\n\nhighlights\nd somatosensory (s1) and secondary motor (m2) cortices form\n\na top-down circuit\n\nd sensory stimulation induces sequential s1 to m2 and m2 to\n\ns1 input patterns\n\nd m2 evokes a dendritic spike and persistent \ufb01ring in s1 layer 5\n\n(l5) neurons\n\nd optogenetic inhibition of m2 to s1 axons degrades accurate\n\nsensory perception\n\nauthors\n\nsatoshi manita, takayuki suzuki, ...,\nmatthew e. larkum,\nmasanori murayama\n\ncorrespondence\nmasa_murayama@brain.riken.jp\n\nin brief\ntop-down input from higher brain areas\nto primary sensory areas is thought to\nmerely modulate perception. using a\nmultidisciplinary approach in mice,\nmanita et al. demonstrate that top-down\ninput is essential for accurate perception.\n\nmanita et al., 2015, neuron 86, 1304\u20131316\njune 3, 2015 \u00aa2015 elsevier inc.\nhttp://dx.doi.org/10.1016/j.neuron.2015.05.006\n\n\f", "\f", "6\n1\n0\n2\n\n \n\nv\no\nn\n6\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n1\n4\n1\n5\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\ntraining spiking deep networks\n\nfor neuromorphic hardware\n\ncentre for theoretical neuroscience\n\ncentre for theoretical neuroscience\n\neric hunsberger\n\nuniversity of waterloo\nwaterloo, on n2l 3g1\n\nchris eliasmith\n\nuniversity of waterloo\nwaterloo, on n2l 3g1\n\nehunsber@uwaterloo.ca\n\nceliasmith@uwaterloo.ca\n\nabstract\n\nwe describe a method to train spiking deep networks that can be run using leaky\nintegrate-and-\ufb01re (lif) neurons, achieving state-of-the-art results for spiking lif\nnetworks on \ufb01ve datasets, including the large imagenet ilsvrc-2012 bench-\nmark. our method for transforming deep arti\ufb01cial neural networks into spik-\ning networks is scalable and works with a wide range of neural nonlinearities.\nwe achieve these results by softening the neural response function, such that its\nderivative remains bounded, and by training the network with noise to provide\nrobustness against the variability introduce", "2\n2\n0\n2\n\n \nr\na\n\nm\n4\n\n \n\n \n \n]\np\ns\n.\ns\ns\ne\ne\n[\n \n \n\n2\nv\n2\n3\n6\n2\n1\n\n.\n\n0\n1\n0\n2\n:\nv\ni\nx\nr\na\n\nbiologically plausible single-layer networks for nonnegative\n\nindependent component analysis\n\ndavid lipshutz1, cengiz pehlevan2, and dmitri b. chklovskii1,3\n\n1center for computational neuroscience, flatiron institute\n\n2john a. paulson school of engineering and applied sciences, harvard university\n\n3neuroscience institute, nyu medical center\n\nmarch 8, 2022\n\nabstract\n\nan important problem in neuroscience is to understand how brains extract relevant signals\nfrom mixtures of unknown sources, i.e., perform blind source separation. to model how the brain\nperforms this task, we seek a biologically plausible single-layer neural network implementation\nof a blind source separation algorithm. for biological plausibility, we require the network to\nsatisfy the following three basic properties of neuronal circuits: (i) the network operates in the\nonline setting; (ii) synaptic learning rules are local; (iii) neu", "st02ch15-salakhutdinov\n\nari\n\n14 march 2015\n\n8:3\n\nlearning deep generative\nmodels\nruslan salakhutdinov\ndepartments of computer science and statistical sciences, university of toronto,\ntoronto m5s 3g4, canada; email: rsalakhu@cs.toronto.edu\n\nannu. rev. stat. appl. 2015. 2:361\u201385\n\nthe annual review of statistics and its application is\nonline at statistics.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev-statistics-010814-020120\ncopyright c(cid:2) 2015 by annual reviews.\nall rights reserved\n\nkeywords\ndeep learning, deep belief networks, deep boltzmann machines, graphical\nmodels\n\nabstract\nbuilding intelligent systems that are capable of extracting high-level rep-\nresentations from high-dimensional sensory data lies at the core of solv-\ning many arti\ufb01cial intelligence\u2013related tasks, including object recognition,\nspeech perception, and language understanding. theoretical and biological\narguments strongly suggest that building such systems requires models with\ndeep architectures that inv", "3\n2\n0\n2\n\n \nl\nu\nj\n \n\n7\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n3\n3\n7\n7\n0\n\n.\n\n1\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nlearning-rate-free learning by d-adaptation\n\naaron defazio\nmeta ai, fundamental ai research (fair) team\nkonstantin mishchenko\nsamsung ai center*\n\nabstract\n\nd-adaptation is an approach to automatically setting the learning rate which asymptotically achieves\nthe optimal rate of convergence for minimizing convex lipschitz functions, with no back-tracking\nor line searches, and no additional function value or gradient evaluations per step. our approach\nis the first hyper-parameter free method for this class without additional multiplicative log factors\nin the convergence rate. we present extensive experiments for sgd and adam variants of our\nmethod, where the method automatically matches hand-tuned learning rates across more than a\ndozen diverse machine learning problems, including large-scale vision and language problems.\n\nan open-source implementation is available1.\n\n1. introduction\nwe consider the probl", "the journal of neuroscience, july 27, 2016 \u2022 36(30):7817\u20137828 \u2022 7817\n\nbehavioral/cognitive\n\na probability distribution over latent causes, in the\norbitofrontal cortex\n\nx stephanie c. y. chan, x yael niv,* and x kenneth a. norman*\nprinceton neuroscience institute, princeton university, princeton, new jersey 08544\n\nthe orbitofrontal cortex (ofc) has been implicated in both the representation of \u201cstate,\u201d in studies of reinforcement learning and\ndecision making, and also in the representation of \u201cschemas,\u201d in studies of episodic memory. both of these cognitive constructs require\na similar inference about the underlying situation or \u201clatent cause\u201d that generates our observations at any given time. the statistically\noptimal solution to this inference problem is to use bayes\u2019 rule to compute a posterior probability distribution over latent causes. to test\nwhether such a posterior probability distribution is represented in the ofc, we tasked human participants with inferring a probability\ndist", "\f", "iffil\n\nemi\n\n1!\n\n'-0'-tmw*wmm\n\ns\n\n*\n\nscreen of the microscope. between 150 to 300\nnerve fibers were analyzed per cross section.\n\n13. c. f. eldridge, m. bartlett, r. p. bunge, p. m. wood,\nj. cell biol. 105, 1023 (1987); c. f. eldridge, m. b.\nbunge, r. p. bunge, j. neurosci. 9, 625 (1989).\n\n14. addition of progesterone (dissolved in ethanol; final\nconcentration of ethanol, 0.1 %) daily to a final con-\ncentration of 20 nm to culture medium for 2 weeks\ndid not increase the area occupied by the neurite\nnetwork extending around drg explants (control,\n20.6 + 1.4; progesterone, 19.7 + 1.2 mm2), the\ndensity of neurites (control, 422 + 24; progesterone,\n397 + 35 mm/mm2), or the number of schwann\ncells (control, 2500 + 193; progesterone, 2625 +\n104 cells per mm2), measured after staining the ex-\nplants with toluidine blue. measurements were made\nwith an imaging system (biocom, rag version 2).\nresults are means + sem of five culture dishes,\neach containing 10 drg explants.\n\n15. u. suter et al., nat", "neuron\n\nreport\n\nsynaptic integration gradients\nin single cortical pyramidal cell dendrites\n\ntiago branco1,2,* and michael ha\u00a8 usser1,2,*\n1wolfson institute for biomedical research\n2department of neuroscience, physiology and pharmacology, university college london, gower street, london wc1e 6bt, uk\n*correspondence: t.branco@ucl.ac.uk (t.b.), m.hausser@ucl.ac.uk (m.h.)\ndoi 10.1016/j.neuron.2011.02.006\n\nsummary\n\ncortical pyramidal neurons receive thousands of\nsynaptic inputs arriving at different dendritic loca-\ntions with varying degrees of temporal synchrony. it\nis not known if different\nlocations along single\ncortical dendrites integrate excitatory inputs in\ndifferent ways. here we have used two-photon gluta-\nmate uncaging and compartmental modeling to\nreveal a gradient of nonlinear synaptic integration in\nbasal and apical oblique dendrites of cortical pyra-\nmidal neurons. excitatory inputs to the proximal\ndendrite sum linearly and require precise temporal\ncoincidence for effective sum", "9\n1\n0\n2\n\n \n\nb\ne\nf\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n4\n5\n0\n2\n0\n\n.\n\n0\n1\n8\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2019\n\ngradient descent provably optimizes\nover-parameterized neural networks\n\nsimon s. du\u2217\nmachine learning department\ncarnegie mellon university\nssdu@cs.cmu.edu\n\nbarnab\u00b4as pocz\u00b4os\nmachine learning department\ncarnegie mellon university\nbapozos@cs.cmu.edu\n\nxiyu zhai\u2217\ndepartment of eecs\nmassachusetts institute of technology\nxiyuzhai@mit.edu\n\naarti singh\nmachine learning department\ncarnegie mellon university\naartisingh@cmu.edu\n\nabstract\n\none of the mysteries in the success of neural networks is randomly initialized \ufb01rst\norder methods like gradient descent can achieve zero training loss even though\nthe objective function is non-convex and non-smooth. this paper demysti\ufb01es this\nsurprising phenomenon for two-layer fully connected relu activated neural net-\nworks. for an m hidden node shallow neural network with relu activation and n\ntraining data, we show as long as ", "revealing the hidden networks of interaction in mobile\nanimal groups allows prediction of complex\nbehavioral contagion\n\nsara brin rosenthala,1, colin r. twomeyb,1, andrew t. hartnetta, hai shan wub, and iain d. couzinb,c,d,2\n\ndepartments of aphysics and becology and evolutionary biology, princeton university, princeton, nj 08544; cdepartment of collective behaviour, max planck\ninstitute for ornithology, d-78547 konstanz, germany; and dchair of biodiversity and collective behavior, department of biology, university of konstanz,\nd-78547 konstanz, germany\n\nedited by gene e. robinson, university of illinois at urbana\u2013champaign, urbana, il, and approved february 24, 2015 (received for review october 22, 2014)\n\ncoordination among social animals requires rapid and efficient\ntransfer of information among individuals, which may depend cru-\ncially on the underlying structure of the communication network.\nestablishing the decision-making circuits and networks that give\nrise to individual behavior", "9050 \u2022 the journal of neuroscience, july 15, 2009 \u2022 29(28):9050 \u20139058\n\nbehavioral/systems/cognitive\n\nthe foveal confluence in human visual cortex\n\nmark m. schira,1,2,3 christopher w. tyler,4 michael breakspear,2,3,5 and branka spehar1\nschools of 1psychology and 2psychiatry, university of new south wales, sydney, new south wales 2052, australia, 3the black dog institute, prince of\nwales hospital, randwick, new south wales 2031, australia, 4the smith-kettlewell eye research institute, san francisco, california 94115, and\n5queensland institute of medical research and the royal brisbane and woman\u2019s hospital, herston, queensland 4006, australia\n\nthe human visual system devotes a significant proportion of its resources to a very small part of the visual field, the fovea. foveal vision\nis crucial for natural behavior and many tasks in daily life such as reading or fine motor control. despite its significant size, this part of\ncortex is rarely investigated and the limited data have resulted in", "cellpose: a generalist algorithm for cellular \nsegmentation\n\ncarsen stringer, tim wang, michalis michaelos and marius pachitariu\u200a\n\n\u200a\u2009\u2709\n\nmany biological applications require the segmentation of cell bodies, membranes and nuclei from microscopy images. deep \nlearning has enabled great progress on this problem, but current methods are specialized for images that have large training \ndatasets. here we introduce a generalist, deep learning-based segmentation method called cellpose, which can precisely seg-\nment cells from a wide range of image types and does not require model retraining or parameter adjustments. cellpose was \ntrained on a new dataset of highly varied images of cells, containing over 70,000 segmented objects. we also demonstrate a \nthree-dimensional (3d) extension of cellpose that reuses the two-dimensional (2d) model and does not require 3d-labeled \ndata. to support community contributions to the training data, we developed software for manual labeling and for curation of \n", "8\n9\n9\n1\n\n \n\np\ne\ns\n4\n\n \n\n \n \n]\nh\np\n-\np\nm\no\nc\n.\ns\nc\ni\ns\ny\nh\np\n[\n \n \n\n2\nv\n8\n0\n0\n3\n0\n8\n9\n/\ns\nc\ni\ns\ny\nh\np\n:\nv\ni\nx\nr\na\n\ntechnical report no. 9805, department of statistics, university of toronto\n\nannealed importance sampling\n\nradford m. neal\n\ndepartment of statistics and department of computer science\n\nuniversity of toronto, toronto, ontario, canada\n\nhttp://www.cs.utoronto.ca/\u223cradford/\n\nradford@stat.utoronto.ca\n\nfirst version:\nrevised:\n\n18 february 1998\n1 september 1998\n\nabstract. simulated annealing \u2014 moving from a tractable distribution to a distribu-\ntion of interest via a sequence of intermediate distributions \u2014 has traditionally been\nused as an inexact method of handling isolated modes in markov chain samplers. here,\nit is shown how one can use the markov chain transitions for such an annealing sequence\nto de\ufb01ne an importance sampler. the markov chain aspect allows this method to per-\nform acceptably even for high-dimensional problems, where \ufb01nding good importance\nsampling distributions", "this is the accepted manuscript made available via chorus. the article has been\n\npublished as:\n\nmachine learning hidden symmetries\n\nziming liu and max tegmark\n\nphys. rev. lett. 128, 180201 \u2014 published  6 may 2022\n\ndoi: 10.1103/physrevlett.128.180201\n\n\f", "the journal of neuroscience, june 27, 2007 \u2022 27(26):6923\u2013 6930 \u2022 6923\n\nbehavioral/systems/cognitive\n\nlateral habenula stimulation inhibits rat midbrain dopamine\nneurons through a gabaa receptor-mediated mechanism\n\nhuifang ji and paul d. shepard\nmaryland psychiatric research center and department of psychiatry, university of maryland school of medicine, baltimore, maryland 21228\n\ntransient changes in the activity of midbrain dopamine neurons encode an error signal that contributes to associative learning. although\nconsiderable attention has been devoted to the mechanisms contributing to phasic increases in dopamine activity, less is known about the\norigin of the transient cessation in firing accompanying the unexpected loss of a predicted reward. recent studies suggesting that\nthe lateral habenula (lhb) may contribute to this type of signaling in humans prompted us to evaluate the effects of lhb stimulation on\nthe activity of dopamine and non-dopamine neurons of the anesthetized rat. si", "9\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n1\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n8\n8\n4\n6\n0\n\n.\n\n2\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nfeedback alignment in deep convolutional net-\nworks\n\ntheodore h. moskovitz1,2,*, ashok litwin-kumar1, and l.f. abbott1\n\n1mortimer b. zuckerman mind, brain and behavior institute, department of neuroscience, columbia\nuniversity, new york, ny\n2department of computer science, columbia university, new york, ny\n*t.moskovitz@columbia.edu\n\nabstract\n\nseveral recent studies have identi\ufb01ed similarities between neural representations\nin biological networks and in deep arti\ufb01cial neural networks. this has led to\nrenewed interest in developing analogies between the backpropagation learning al-\ngorithm used to train arti\ufb01cial networks and the synaptic plasticity rules operative\nin the brain. these e\ufb00orts are challenged by biologically implausible features of\nbackpropagation, one of which is a reliance on symmetric forward and backward\nsynaptic weights. a number of methods have been proposed that do not rely on\nwei", "research article\n\nneural dynamics at successive stages of\nthe ventral visual stream are consistent\nwith hierarchical error signals\nelias b issa\u2020*, charles f cadieu\u2021, james j dicarlo\n\ndepartment of brain and cognitive sciences, mcgovern institute for brain\nresearch, massachusetts institute of technology, cambridge, united states\n\nabstract ventral visual stream neural responses are dynamic, even for static image\npresentations. however, dynamical neural models of visual cortex are lacking as most progress has\nbeen made modeling static, time-averaged responses. here, we studied population neural\ndynamics during face detection across three cortical processing stages. remarkably,~30\nmilliseconds after the initially evoked response, we found that neurons in intermediate level areas\ndecreased their responses to typical configurations of their preferred face parts relative to their\nresponse for atypical configurations even while neurons in higher areas achieved and maintained a\npreference for t", "article\n\nbayesian computation through cortical latent\ndynamics\n\ngraphical abstract\n\nauthors\n\ntwo-prior time interval reproduction task\n\nhansem sohn, devika narain,\nnicolas meirhaeghe, mehrdad jazayeri\n\nin-vivo\n\nestimation\n\nproduction\n\nready\n\nset\n\ngo\n\nmonkey\nfrontal \ncortex\n\nshort prior\nlong prior\n\nsample interval\n\nin-silico\n\nrecurrent\n\nneural\nnetwork\nmodel\n\ncurved manifold hypothesis\n\ncorrespondence\nmjaz@mit.edu\n\nin brief\nsohn et al. found that prior beliefs warp\nneural representations in the frontal\ncortex. this warping provides a substrate\nfor the optimal integration of prior beliefs\nwith sensory evidence during\nsensorimotor behavior.\n\ncurved neural\n\ntrajectory\n\ni\n\ns\nx\na\n\n \n\ni\n\ng\nn\nd\no\nc\nn\ne\n\nbias\n\nbayesian\nestimate\n\nmeasurement\n\nhighlights\nd monkeys estimate time by integrating sensory evidence with\n\nprior beliefs\n\nd prior beliefs warp neural representations in the frontal cortex\n\nd warped representations provide an optimal substrate for\n\nintegrating beliefs\n\nd recurrent neural netw", "3\n1\n0\n2\n\n \nc\ne\nd\n9\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n0\n6\n5\n\n.\n\n2\n1\n3\n1\n:\nv\ni\nx\nr\na\n\nplaying atari with deep reinforcement learning\n\nvolodymyr mnih koray kavukcuoglu david silver alex graves\n\nioannis antonoglou\n\ndaan wierstra martin riedmiller\n\ndeepmind technologies\n\n{vlad,koray,david,alex.graves,ioannis,daan,martin.riedmiller} @ deepmind.com\n\nabstract\n\nwe present the \ufb01rst deep learning model to successfully learn control policies di-\nrectly from high-dimensional sensory input using reinforcement learning. the\nmodel is a convolutional neural network, trained with a variant of q-learning,\nwhose input is raw pixels and whose output is a value function estimating future\nrewards. we apply our method to seven atari 2600 games from the arcade learn-\ning environment, with no adjustment of the architecture or learning algorithm. we\n\ufb01nd that it outperforms all previous approaches on six of the games and surpasses\na human expert on three of them.\n\n1\n\nintroduction\n\nlearning to control agents dir", "9\n1\n0\n2\n\n \n\nb\ne\nf\n6\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n5\n0\n4\n2\n0\n\n.\n\n2\n0\n9\n1\n:\nv\ni\nx\nr\na\n\non the variance of unbiased online recurrent optimization\n\ntim cooijmans\u2217\nmila, universit\u00e9 de montr\u00e9al\n6666 st-urbain, #200\nmontreal, qc h2s 3h1, canada\njames martens\ndeepmind\n6 pancras square\nlondon, n1c 4ag, united kingdom\n\nabstract\n\ncooijmat@mila.quebec\n\njamesmartens@google.com\n\nthe recently proposed unbiased online recurrent optimization (uoro) algorithm (tallec\nand ollivier, 2018) uses an unbiased approximation of rtrl to achieve fully online gradient-\nbased learning in rnns. in this work we analyze the variance of the gradient estimate\ncomputed by uoro, and propose several possible changes to the method which reduce this\nvariance both in theory and practice. we also contribute signi\ufb01cantly to the theoretical\nand intuitive understanding of uoro (and its existing variance reduction technique), and\ndemonstrate a fundamental connection between its gradient estimate and the one that would\nbe compute", "a r t i c l e s\n\nexperience-dependent spatial expectations in mouse \nvisual cortex\naris fiser1,2,4, david mahringer1,2,4, hassana k oyibo1,2,4, anders v petersen3, marcus leinweber1 &  \ngeorg b keller1,2\n\nin generative models of brain function, internal representations are used to generate predictions of sensory input, yet little is \nknown about how internal models influence sensory processing. here we show that, with experience in a virtual environment, the \nactivity of neurons in layer 2/3 of mouse primary visual cortex (v1) becomes increasingly informative of spatial location. we found \nthat a subset of v1 neurons exhibited responses that were predictive of the upcoming visual stimulus in a spatially dependent \nmanner and that the omission of an expected stimulus drove strong responses in v1. stimulus-predictive responses also emerged \nin v1-projecting anterior cingulate cortex axons, suggesting that anterior cingulate cortex serves as a source of predictions of \nvisual input to v1.", "review\npublished: 25 october 2018\ndoi: 10.3389/fnins.2018.00774\n\ndeep learning with spiking neurons:\nopportunities and challenges\n\nmichael pfeiffer* and thomas pfeil\n\nbosch center for arti\ufb01cial intelligence, robert bosch gmbh, renningen, germany\n\nspiking neural networks (snns) are inspired by information processing in biology, where\nsparse and asynchronous binary signals are communicated and processed in a massively\nparallel fashion. snns on neuromorphic hardware exhibit favorable properties such\nas low power consumption, fast inference, and event-driven information processing.\nthis makes them interesting candidates for the ef\ufb01cient implementation of deep neural\nnetworks, the method of choice for many machine learning tasks. in this review, we\naddress the opportunities that deep spiking networks offer and investigate in detail the\nchallenges associated with training snns in a way that makes them competitive with\nconventional deep learning, but simultaneously allows for ef\ufb01cient mapping", "exascale deep learning for climate analytics\nthorsten kurth\u2217\nmayur mudigonda\u2217\ntkurth@lbl.gov\nmudigonda@berkeley.edu\nnathan luehr\u2020\nmichael matheson\u2021\nmathesonma@ornl.gov\nnluehr@nvidia.com\njack deslippe\u2217\nmichael houston\u2020\njrdeslippe@lbl.gov\nmhouston@nvidia.com\n\nsean treichler\u2020\nsean@nvidia.com\neverett phillips\u2020\nephillips@nvidia.com\nmassimiliano fatica\u2020\nmfatica@nvidia.com\n\njoshua romero\u2020\njoshr@nvidia.com\nankur mahesh\u2217\namahesh@lbl.gov\n\nprabhat@lbl.gov\n\nprabhat\u2217\n\n8\n1\n0\n2\n\n \nt\nc\no\n3\n\n \n\n \n \n]\n\nc\nd\n.\ns\nc\n[\n \n \n\n1\nv\n3\n9\n9\n1\n0\n\n.\n\n0\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014we extract pixel-level masks of extreme weather\npatterns using variants of tiramisu and deeplabv3+ neural net-\nworks. we describe improvements to the software frameworks,\ninput pipeline, and the network training algorithms necessary\nto ef\ufb01ciently scale deep learning on the piz daint and summit\nsystems. the tiramisu network scales to 5300 p100 gpus with\na sustained throughput of 21.0 pf/s and parallel ef\ufb01ciency of\n79.0%. deeplabv3+ scales up ", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/2021.12.21.473757\n; \n\nthis version posted june 25, 2022. \n\nthe copyright holder for this preprint\n\n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\navailable under a\n\ncc-by-nc-nd 4.0 international license\n.\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\nisolating salient variations of interest in single-cell data with\n\ncontrastivevi\n\nethan weinberger1,*, chris lin1,*, and su-in lee1,\f\n\n1paul g. allen school of computer science & engineering, university of washington,\n\nseattle\n\n* these authors contributed equally to this work.\n\n\f corresponding: suinlee@cs.washington.edu\n\nabstract\n\nsingle-cell datasets are routinely collected to investigate changes in cellular state between control\n\ncells and corresponding cells in a treatment condition, such as exposure to a drug or infection by ", "0\n2\n0\n2\n\n \n\np\ne\ns\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n8\n8\n6\n0\n1\n\n.\n\n1\n1\n9\n1\n:\nv\ni\nx\nr\na\n\nrethinking softmax with cross-entropy:\n\nneural network classifier as\nmutual information estimator\n\na preprint\n\nzhenyue qin1,*, dongwoo kim1,2,*, and tom gedeon1\n\n1australian national university, australia\n\n2pohang university of science and technology, republic of korea\n\nzhenyue.qin@anu.edu.au, dongwookim@postech.ac.kr, tom@cs.anu.edu.au\n\n*equal contribution and correspondence\n\nabstract\n\ncross-entropy loss with softmax output is a standard choice to train neural network classi\ufb01ers. while\nit is reasonable to reduce the cross-entropy between outputs of a neural network and labels, the\nimplication of cross-entropy with softmax on the relation between inputs and labels remains to be\nbetter explained. we show that training a neural network with cross-entropy maximises the mutual\ninformation between inputs and labels through a variational form of mutual information. our result\nprovides an alternative view", "9\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n1\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n4\n5\n5\n4\n0\n\n.\n\n6\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nprincipled training of neural networks\n\nwith direct feedback alignment\n\njulien launay & iacopo poli\n\nlighton\n\njulien@lighton.ai\niacopo@lighton.ai\n\nflorent krzakala\n\nlighton, laboratoire de physique de l\u2019ens,\n\nuniversit\u00e9 psl, cnrs, sorbonne universit\u00e9, universit\u00e9\n\nparis-diderot, sorbonne paris cit\u00e9, paris, france\n\nflorent.krzakala@ens.fr, florent@lighton.ai\n\nabstract\n\nthe backpropagation algorithm has long been the canonical training method for\nneural networks. modern paradigms are implicitly optimized for it, and numerous\nguidelines exist to ensure its proper use. recently, synthetic gradients methods \u2013\nwhere the error gradient is only roughly approximated \u2013 have garnered interest.\nthese methods not only better portray how biological brains are learning, but also\nopen new computational possibilities, such as updating layers asynchronously.\neven so, they have failed to scale past simple tasks li", "research article\n\nattention stabilizes the shared gain of v4\npopulations\nneil c rabinowitz1*, robbe l goris1, marlene cohen2, eero p simoncelli1*\n\n1center for neural science, howard hughes medical institute, new york university,\nnew york, united states; 2department of neuroscience and center for the neural\nbasis of cognition, university of pittsburgh, pittsburgh, united states\n\nabstract responses of sensory neurons represent stimulus information, but are also influenced\nby internal state. for example, when monkeys direct their attention to a visual stimulus, the\nresponse gain of specific subsets of neurons in visual cortex changes. here, we develop a\nfunctional model of population activity to investigate the structure of this effect. we fit the model\nto the spiking activity of bilateral neural populations in area v4, recorded while the animal\nperformed a stimulus discrimination task under spatial attention. the model reveals four separate\ntime-varying shared modulatory signals, the dom", "research article\n\non pixel-wise explanations for non-linear\nclassifier decisions by layer-wise relevance\npropagation\nsebastian bach1,2\u262f*, alexander binder2,5\u262f, gr\u00e9goire montavon2, frederick klauschen3,\nklaus-robert m\u00fcller2,4*, wojciech samek1,2*\n\n1 machine learning group, fraunhofer heinrich hertz institute, berlin, germany, 2 machine learning group,\ntechnische universit\u00e4t berlin, berlin, germany, 3 charit\u00e9 university hospital, berlin, germany,\n4 department of brain and cognitive engineering, korea university, seoul, korea, 5 istd pillar, singapore\nuniversity of technology and design (sutd), singapore\n\n\u262f these authors contributed equally to this work.\n* sebastian.bach@hhi.fraunhofer.de (sb), klaus-robert.mueller@tu-berlin.de (km), wojciech.samek@hhi.\nfraunhofer.de (ws)\n\nabstract\n\nunderstanding and interpreting classification decisions of automated image classification\nsystems is of high value in many applications, as it allows to verify the reasoning of the sys-\ntem and provides additi", "13522 \u2022 the journal of neuroscience, december 10, 2008 \u2022 28(50):13522\u201313531\n\nbehavioral/systems/cognitive\n\nthe neural basis for combinatorial coding in a cortical\npopulation response\n\nleslie c. osborne,1* stephanie e. palmer,3* stephen g. lisberger,1,2 and william bialek3\n1sloan-swartz center for theoretical neurobiology, w. m. keck foundation center for integrative neuroscience, and department of physiology, and\n2howard hughes medical institute, university of california at san francisco, san francisco, california 94143, and 3joseph henry laboratories of physics\nand lewis-sigler institute for integrative genomics, princeton university, princeton, new jersey 08544\n\nwe have used a combination of theory and experiment to assess how information is represented in a realistic cortical population\nresponse, examining how motion direction and timing is encoded in groups of neurons in cortical area mt. combining data from several\nsingle-unit experiments, we constructed model population responses", "research article\ndopamine neurons do not constitute an\nobligatory stage in the final common path for\nthe evaluation and pursuit of brain\nstimulation reward\n\nivan trujillo-pisanty\u00a4, kent conover, pavel solis, daniel palacios, peter shizgalid*\n\ncentre for studies in behavioural neurobiology, concordia university, montreal, que\u00b4bec, canada\n\n\u00a4 current address: center for the neurobiology of addiction, pain, and emotion, department of\nanesthesiology and pain medicine, department of pharmacology, university of washington, seattle, wa,\nunited states of america\n* peter.shizgal@concordia.ca\n\nabstract\n\nthe neurobiological study of reward was launched by the discovery of intracranial self-\nstimulation (icss). subsequent investigation of this phenomenon provided the initial link\nbetween reward-seeking behavior and dopaminergic neurotransmission. we re-evaluated\nthis relationship by psychophysical, pharmacological, optogenetic, and computational\nmeans. in rats working for direct, optical activation", "deepfool: a simple and accurate method to fool deep neural networks\n\nseyed-mohsen moosavi-dezfooli, alhussein fawzi, pascal frossard\n\n\u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\n{seyed.moosavi,alhussein.fawzi,pascal.frossard} at epfl.ch\n\n6\n1\n0\n2\n\n \nl\nu\nj\n \n\n4\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n9\n9\n5\n4\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nstate-of-the-art deep neural networks have achieved im-\npressive results on many image classi\ufb01cation tasks. how-\never, these same architectures have been shown to be un-\nstable to small, well sought, perturbations of the images.\ndespite the importance of this phenomenon, no effective\nmethods have been proposed to accurately compute the ro-\nbustness of state-of-the-art deep classi\ufb01ers to such pertur-\nbations on large-scale datasets. in this paper, we \ufb01ll this\ngap and propose the deepfool algorithm to ef\ufb01ciently com-\npute perturbations that fool deep networks, and thus reli-\nably quantify the robustness of these classi\ufb01ers. extensive\nexperimental results show", "2\n1\n0\n2\n\n \nl\nu\nj\n \n\n2\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n9\n0\n2\n6\n\n.\n\n2\n1\n1\n1\n:\nv\ni\nx\nr\na\n\nbuilding high-level features\n\nusing large scale unsupervised learning\n\nquoc v. le\nmarc\u2019aurelio ranzato\nrajat monga\nmatthieu devin\nkai chen\ngreg s. corrado\nje\ufb00 dean\nandrew y. ng\n\nquocle@cs.stanford.edu\nranzato@google.com\nrajatmonga@google.com\nmdevin@google.com\nkaichen@google.com\ngcorrado@google.com\njeff@google.com\nang@cs.stanford.edu\n\nabstract\n\n1. introduction\n\nwe consider the problem of building high-\nlevel, class-speci\ufb01c feature detectors from\nonly unlabeled data. for example, is it pos-\nsible to learn a face detector using only unla-\nbeled images? to answer this, we train a 9-\nlayered locally connected sparse autoencoder\nwith pooling and local contrast normalization\non a large dataset of images (the model has\n1 billion connections, the dataset has 10 mil-\nlion 200x200 pixel images downloaded from\nthe internet). we train this network using\nmodel parallelism and asynchronous sgd on\na cluster with 1,00", "9\n1\n0\n2\n\n \nr\na\n\nm\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n9\n6\n3\n0\n1\n\n.\n\n5\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nstable recurrent models\n\njohn miller\u2217\n\nmoritz hardt\u2020\n\nmarch 5, 2019\n\nabstract\n\nstability is a fundamental property of dynamical systems, yet to this date it has had little\nbearing on the practice of recurrent neural networks.\nin this work, we conduct a thorough\ninvestigation of stable recurrent models. theoretically, we prove stable recurrent neural networks\nare well approximated by feed-forward networks for the purpose of both inference and training\nby gradient descent. empirically, we demonstrate stable recurrent models often perform as well\nas their unstable counterparts on benchmark sequence tasks. taken together, these \ufb01ndings\nshed light on the e\ufb00ective power of recurrent networks and suggest much of sequence learning\nhappens, or can be made to happen, in the stable regime. moreover, our results help to explain\nwhy in many cases practitioners succeed in replacing recurrent models by feed-forward", "under review as a conference paper at iclr 2019\n\nthe effectiveness of layer-by-layer training\nusing the information bottleneck principle\n\nanonymous authors\npaper under double-blind review\n\nabstract\n\nthe recently proposed information bottleneck (ib) theory of deep nets suggests\nthat during training, each layer attempts to maximize its mutual information (mi)\nwith the target labels (so as to allow good prediction accuracy), while minimizing\nits mi with the input (leading to effective compression and thus good generaliza-\ntion). to date, evidence of this phenomenon has been indirect and aroused con-\ntroversy due to theoretical and practical complications. in particular, it has been\npointed out that the mi with the input is theoretically in\ufb01nite in many cases of in-\nterest, and that the mi with the target is fundamentally dif\ufb01cult to estimate in high\ndimensions. as a consequence, the validity of this theory has been questioned.\nin this paper, we overcome these obstacles by two means. first", "opinion\n\na  cellular  mechanism  for  cortical\nassociations:  an  organizing  principle\nfor  the  cerebral  cortex\n\nmatthew  larkum\n\nneurocure  cluster  of  excellence,  department  of  biology,  humboldt  university,  charite\u00b4  platz  1,  10117,  berlin,  germany\n\na  basic  feature  of  intelligent  systems  such  as  the  cere-\nbral  cortex  is  the  ability  to  freely  associate  aspects  of\nperceived  experience  with  an  internal  representation  of\nthe  world  and  make  predictions  about  the  future.  here,  a\nhypothesis  is  presented  that  the  extraordinary  perfor-\nmance  of  the  cortex  derives  from  an  associative  mecha-\nnism  built  in  at  the  cellular  level  to  the  basic  cortical\nneuronal  unit:  the  pyramidal  cell.  the  mechanism  is\nrobustly  triggered  by  coincident  input  to  opposite  poles\nof  the  neuron,  is  exquisitely  matched  to  the  large-  and\n\ufb01ne-scale  architecture  of  the  cortex,  and  is  tightly  con-\ntrolled  by  local  microci", "eligibility traces provide a data-inspired alternative\n\nto backpropagation through time\n\nguillaume bellec*, franz scherr*, elias hajek, darjan salaj, anand subramoney,\n\nrobert legenstein & wolfgang maass\ninstitute of theoretical computer science\ngraz university of technology, austria\n\n{bellec,scherr,salaj,hajek,legenstein,maass}@igi.tugraz.at\n\n* equal contributions\n\nabstract\n\nlearning in recurrent neural networks (rnns) is most often implemented by\ngradient descent using backpropagation through time (bptt), but bptt does\nnot model accurately how the brain learns. instead, many experimental results\non synaptic plasticity can be summarized as three-factor learning rules involving\neligibility traces of the local neural activity and a third factor. we present here\neligibility propagation (e-prop), a new factorization of the loss gradients in rnns\nthat \ufb01ts the framework of three factor learning rules when derived for biophysical\nspiking neuron models. when tested on the timit speech recogni", "improving generalisation for temporal\n\ndifference learning:\n\nthe successor representation\n\npeter dayan\n\ncomputational neurobiology laboratory\n\nthe salk institute\n\npo box 85800, san diego ca 92186-5800\n\nabstract\n\nestimation of returns over time, the focus of temporal difference (td) algorithms,\nimposes particular constraints on good function approximators or representations.\nappropriate generalisation between states is determined by how similar their succes-\nsors are, and representations should follow suit. this paper shows how td machinery\ncan be used to learn such representations, and illustrates, using a navigation task, the\nappropriately distributed nature of the result.\n\n1 introduction\n\nthe method of temporal differences (td, samuel 1959; sutton, 1984; 1988) is a way of esti-\nmating future outcomes in problems whose temporal structure is paramount. a paradig-\nmatic example is predicting the long term discounted value of executing a particular pol-\nicy in a \ufb01nite markovian decision ", "deep residual learning for image recognition\n\nkaiming he\n\nxiangyu zhang\n\nshaoqing ren\n\njian sun\n\nmicrosoft research\n\n{kahe, v-xiangz, v-shren, jiansun}@microsoft.com\n\nabstract\n\ndeeper neural networks are more dif\ufb01cult to train. we\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. we explicitly reformulate the layers as learn-\ning residual functions with reference to the layer inputs, in-\nstead of learning unreferenced functions. we provide com-\nprehensive empirical evidence showing that these residual\nnetworks are easier to optimize, and can gain accuracy from\nconsiderably increased depth. on the imagenet dataset we\nevaluate residual nets with a depth of up to 152 layers\u20148\u00d7\ndeeper than vgg nets [40] but still having lower complex-\nity. an ensemble of these residual nets achieves 3.57% error\non the imagenet test set. this result won the 1st place on the\nilsvrc 2015 classi\ufb01cation task. we also present analysis", "neuroscience  letters  680  (2018)  88\u201393\n\ncontents  lists  available  at  sciencedirect\n\nneuroscience\n\n \n\nletters\n\nj o  u r  n a l  h o  m e   p  a g e :  w w w . e l s e v i e r . c o m / l o c a t e / n e u l e t\n\nreview   article\nbarlow   versus   hebb:   when   is   it   time   to   abandon   the   notion   of   feature\ndetectors   and   adopt   the   cell   assembly   as   the   unit   of   cognition?\n\nhoward   eichenbaum\nboston\ncenter\n\nmemory\n\nbrain,\n\nand\n\nfor\n\n \n\n \n\n \n\n \n\n \n\n \n\nuniversity,\n\n \n\nboston,\n\n \n\nma\n\n \n\n02215,\n\n \n\nunited\n\n \n\nstates\n\nh   i  g   h   l   i   g   h   t   s\n\u2022 in   higher   order   areas   and   complex   tasks,   neurons   exhibit   mixed   selectivity   and   dependence   on   cognitive   demands.\n\u2022 these   properties   are   observed   in   inferotemporal   cortex,   prefrontal   cortex,   and   the   hippocampus.\n\u2022 new   population   analyses   reveal   how   cell   assemblies   in   these   brain   areas   organize   task   relevant   knowledge.\n\na   r ", "distinctive image features\n\nfrom scale-invariant keypoints\n\ndavid g. lowe\n\ncomputer science department\nuniversity of british columbia\n\nvancouver, b.c., canada\n\nlowe@cs.ubc.ca\n\njanuary 5, 2004\n\nabstract\n\nthis paper presents a method for extracting distinctive invariant features from\nimagesthatcanbeusedtoperformreliablematchingbetweendifferentviewsof\nan object or scene. the features are invariant to image scale and rotation, and\nareshowntoprovide robust matching across aasubstantial range ofaf\ufb01nedis-\ntortion, change in 3d viewpoint, addition of noise, and change inillumination.\nthefeatures arehighly distinctive, inthesensethatasinglefeature canbecor-\nrectly matched with high probability against a large database of features from\nmany images. this paper also describes an approach to using these features\nfor object recognition. the recognition proceeds by matching individual fea-\nturestoadatabaseoffeaturesfromknownobjectsusingafastnearest-neighbor\nalgorithm,followedbyahoughtransformtoidenti", "7\n1\n0\n2\n\n \nr\na\n\nm\n \n6\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n7\n7\n7\n3\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nlearning to represent signals spike by spike\nwieland brendel,1,2,3,\u2217 ralph bourdoukan,2,\u2217 pietro vertechi,1,2,\u2217\n\nchristian k. machens,1,\u2020, sophie den`eve2,\u2020\n\n1champalimaud neuroscience programme, champalimaud foundation, lisbon, portugal\n\n2group for neural theory, inserm u960, d\u00b4epartement d\u2019etudes cognitives,\n\necole normale sup\u00b4erieure, paris, france\n\n3werner reichardt centre for integrative neuroscience,\n\nuniversity of t\u00a8ubingen, germany\n\u2217these authors contributed equally\n\n\u2020to whom correspondence should be addressed;\n\ne-mail: sophie.deneve@ens.fr or christian.machens@neuro.fchampalimaud.org\n\na key question in neuroscience is at which level functional meaning emerges\nfrom biophysical phenomena. in most vertebrate systems, precise functions\nare assigned at the level of neural populations, while single-neurons are deemed\nunreliable and redundant. here we challenge this view and show that many\n", "pergamorr\n\n@\n\nprogress in neurobiology, vol. 50, pp.381 to 425, 1996\ncopyright~ 1996elsevierscienceltd. all rightsreserved\nprinted in great britain\n\n0301-0082/96/$32.00\n\npii: s0301-0082(96)00042-1\n\nthe basal ganglia: focused selection and\ninhibition of competing motor programs\n\ndepartment\n\nof neurology, box 8111, washington university school of medicine,\n\n660 s. euclid ave.,\n\nst. louis, mo 63110, u.s.a.\n\njonathan w. mink\n\n(received\n\n17 january\n\n1996; accepted\n\n23 june 1996)\n\nabstract-thebasalgangliacompriseseveralnucleiintheforebrain,diencephalon,andmidbrainthought\nto playa significantrolein thecontrolof postureandmovement.it is wellrecognizedthat peoplewith\ndegenerativediseasesof the basalgangliasufferfromrigidlyheldabnormalbodypostures,slowingof\nmovement,involuntarymovements,or a combinationof theseabnormalities.however,it has not been\nagreedjust whatthebasalgangliacontributeto normalmovement.recentadvancesin knowledgeofthe\nbasalgangliacircuitry,activityofbasalganglianeuronsduringmov", "doi: 10.21105/joss.01003\nsoftware\n\n\u2022 review\n\u2022 repository\n\u2022 archive\n\nsubmitted: 17 september 2018\npublished: 01 november 2018\nlicense\nauthors of papers retain copy-\nright and release the work un-\nder a creative commons attri-\nbution 4.0 international license\n(cc-by).\n\nfixedpointfinder: a tensorflow toolbox for identifying\nand characterizing fixed points in recurrent neural\nnetworks\nmatthew d. golub1,2 and david sussillo1,2,3,4\n1 department of electrical engineering, stanford university 2 stanford neurosciences institute,\nstanford university 3 google brain 4 work done while at stanford university\n\nsummary\n\nrecurrent neural networks (rnns) are powerful function approximators that can be\ndesigned or trained to solve a variety of computational tasks. such tasks require the\ntransformation of a set of time-varying input signals into a set of time-varying output\nsignals. neuroscientists are increasingly interested in using rnns to explain complex\nrelationships present in recorded neural activi", "letter\n\ncommunicated by nathaniel daw\n\nattention-gated reinforcement learning of internal\nrepresentations for classi\ufb01cation\n\npieter r. roelfsema\np.roelfsema@ioi.knaw.nl\nnetherlands ophthalmic research institute, 1105 ba amsterdam, netherlands, and\ncenter for neurogenomics and cognitive research, department of experimental\nneurophysiology, vrije universiteit, 1081 hv amsterdam, netherlands\n\narjen van ooyen\narjen.van.ooyen@falw.vu.nl\nnetherlands institute for brain research, 1105 az amsterdam, netherlands, and\ncenter for neurogenomics and cognitive research, department of experimental\nneurophysiology, vrije universiteit, 1081 hv amsterdam, netherlands\n\nanimal learning is associated with changes in the ef\ufb01cacy of connections\nbetween neurons. the rules that govern this plasticity can be tested in\nneural networks. rules that train neural networks to map stimuli onto\noutputs are given by supervised learning and reinforcement learning\ntheories. supervised learning is ef\ufb01cient but biologically", "neuroscience  and  biobehavioral  reviews  68  (2016)  862\u2013879\n\ncontents  lists  available  at  sciencedirect\n\nneuroscience\n\n \n\nand\n\n \n\nbiobehavioral\n\n \n\nreviews\n\nj o u r n a l  h  o m  e p a  g e :  w w w . e l s e v i e r . c o m / l o c a t e / n e u b i o r e v\n\nactive   inference   and   learning\nkarl   friston a,\u2217,   thomas   fitzgerald a,b,   francesco   rigoli a,   philipp   schwartenbeck a,b,c,d,\njohn   o\u2019doherty e,   giovanni   pezzulo f\na the  wellcome  trust  centre  for  neuroimaging,  ucl,  12  queen  square,  london,  united  kingdom\nb max-planck\u2014ucl  centre  for  computational  psychiatry  and  ageing  research,  london,  united  kingdom\nc centre  for  neurocognitive  research,  university  of  salzburg,  salzburg,  austria\nd neuroscience  institute,  christian-doppler-klinik,  paracelsus  medical  university  salzburg,  salzburg,  austria\ne caltech  brain  imaging  center,  california  institute  of  technology,  pasadena,  usa\nf institute  of  cognitive  sciences  and", "phil. trans. r. soc. b (2008) 363, 3845\u20133857\ndoi:10.1098/rstb.2008.0158\npublished online 1 october 2008\n\ncortical mechanisms for reinforcement learning\n\nin competitive games\nhyojung seo and daeyeol lee*\n\ndepartment of neurobiology, yale university school of medicine, 333 cedar street,\n\nshm b404, new haven, ct 06510, usa\n\ngame theory analyses optimal strategies for multiple decision makers interacting in a social group.\nhowever, the behaviours of individual humans and animals often deviate systematically from the\noptimal strategies described by game theory. the behaviours of rhesus monkeys (macaca mulatta)\nin simple zero-sum games showed similar patterns, but their departures from the optimal strategies\nwere well accounted for by a simple reinforcement-learning algorithm. during a computer-simulated\nzero-sum game, neurons in the dorsolateral prefrontal cortex often encoded the previous choices of\nthe animal and its opponent as well as the animal\u2019s reward history. by contrast, the neuron", "understanding dopamine and reinforcement learning:\nthe dopamine reward prediction error hypothesis\n\npaul w. glimcher1\n\ncenter for neuroeconomics, new york university, new york, ny 10003\n\nedited by donald w. pfaff, the rockefeller university, new york, ny, and approved december 9, 2010 (received for review october 14, 2010)\n\na number of recent advances have been achieved in the study of\nmidbrain dopaminergic neurons. understanding these advances\nand how they relate to one another requires a deep understand-\ning of the computational models that serve as an explanatory\nframework and guide ongoing experimental inquiry. this intertwin-\ning of theory and experiment now suggests very clearly that the\nphasic activity of the midbrain dopamine neurons provides a global\nmechanism for synaptic modi\ufb01cation. these synaptic modi\ufb01cations,\nin turn, provide the mechanistic underpinning for a speci\ufb01c class of\nreinforcement learning mechanisms that now seem to underlie much\nof human and animal behavior. t", "r e v i e w s\n\npath integration and the neural basis \nof the \u2018cognitive map\u2019\n\nbruce l. mcnaughton*\u00b6, francesco p. battaglia\u00a7, ole jensen||, edvard i. moser\u00b6 \nand may-britt moser\u00b6\nabstract | the hippocampal formation can encode relative spatial location, without \nreference to external cues, by the integration of linear and angular self-motion (path \nintegration). theoretical studies, in conjunction with recent empirical discoveries, suggest \nthat the medial entorhinal cortex (mec) might perform some of the essential underlying \ncomputations by means of a unique, periodic synaptic matrix that could be self-organized in \nearly development through a simple, symmetry-breaking operation. the scale at which \nspace is represented increases systematically along the dorsoventral axis in both the \nhippocampus and the mec, apparently because of systematic variation in the gain of a \nmovement-speed signal. convergence of spatially periodic input at multiple scales, from \nso-called grid cells in the", "orthogonal representations for robust context-\ndependent task performance in brains and neural\nnetworks\n\narticle\n\nhighlights\nd we trained arti\ufb01cial neural networks and humans on context-\n\ndependent decision tasks\n\nd initial weight variance determined the network\u2019s\n\nrepresentational geometry\n\nd human fronto-parietal representations were similar to those\n\nof low-variance networks\n\nd theory of nonlinear gating explains how these are formed in\n\nneural networks and brains\n\nauthors\n\ntimo flesch, keno juechems,\ntsvetomira dumbalska, andrew saxe,\nchristopher summer\ufb01eld\n\ncorrespondence\ntimo.\ufb02esch@psy.ox.ac.uk (t.f.),\na.saxe@ucl.ac.uk (a.s.),\nchristopher.summer\ufb01eld@\npsy.ox.ac.uk (c.s.)\n\nin brief\nhow do brains represent multiple tasks?\nthe authors use arti\ufb01cial neural networks\nto derive a computational theory of neural\ncoding and validate its predictions in\nrecordings of human and macaque\nbrains. the \ufb01ndings suggest that task-\nspeci\ufb01c information is represented along\northogonal coding axes, which", "the journal of neuroscience, november 12, 2014 \u2022 34(46):15497\u201315504 \u2022 15497\n\nsymposium\n\nattention, reward, and information seeking\n\nx jacqueline gottlieb,1,2 mary hayhoe,3 okihide hikosaka,6 and antonio rangel4,5\n1department of neuroscience and 2kavli institute for brain science, columbia university, new york, new york 10032, 3university of texas at austin,\naustin, texas 78712, 4division of humanities and social sciences and 5computational and neural systems, california institute of technology, pasadena,\ncalifornia 91125, and 6laboratory of sensorimotor research, national eye institute, national institutes of health, bethesda, maryland 20892\n\ndecision making is thought to be guided by the values of alternative options and involve the accumulation of evidence to an internal\nbound. however, in natural behavior, evidence accumulation is an active process whereby subjects decide when and which sensory\nstimulus to sample. these sampling decisions are naturally served by attention and rapid ", "neurocomputing 48 (2002) 17\u201337\n\nwww.elsevier.com/locate/neucom\n\nerror-backpropagation in temporally encoded networks of\n\nspiking neurons\n\nsander m. bohtea;\u2217, joost n. koka;c , han la poutr\u0003ea;b\n\nacwi, kruislaan 413, 1098 sj amsterdam, the netherlands\n\nbschool of technology management, eindhoven university of technology, the netherlands\n\ncliacs, leiden university, p.o. box 9512, 2300 ra leiden, the netherlands\n\nreceived 27 october 2000; accepted 6 june 2001\n\nabstract\n\nfor a network of spiking neurons that encodes information in the timing of individual\nspike times, we derive a supervised learning rule, spikeprop, akin to traditional error-\nbackpropagation. with this algorithm, we demonstrate how networks of spiking neurons\nwith biologically reasonable action potentials can perform complex non-linear classi7cation\nin fast temporal coding just as well as rate-coded networks. we perform experiments for\nthe classical xor problem, when posed in a temporal setting, as well as for a number\nof ", "journal of mathematical psychology 76 (2017) 198\u2013211\n\ncontents lists available at sciencedirect\n\njournal of mathematical psychology\n\njournal homepage: www.elsevier.com/locate/jmp\n\na tutorial on the free-energy framework for modelling perception\nand learning\nrafal bogacz\u2217\n\nmrc unit for brain network dynamics, university of oxford, mansfield road, oxford, ox1 3th, uk\nnuffield department of clinical neurosciences, university of oxford, john radcliffe hospital, oxford, ox3 9du, uk\n\nh i g h l i g h t s\n\u2022 bayesian inference about stimulus properties can be performed by networks of neurons.\n\u2022 learning about statistics of stimuli can be achieved by hebbian synaptic plasticity.\n\u2022 structure of the model resembles the hierarchical organization of the neocortex.\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\navailable online 14 december 2015\n\nthis paper provides an easy to follow tutorial on the free-energy framework for modelling perception\ndeveloped by friston, which extends the pred", "recurrent networks endowed with structural priors\nexplain suboptimal animal behavior\n\narticle\n\nhighlights\nd rats waive across-trial accumulated evidence after an error\n\nin a 2afc task\n\nd this suboptimal behavior may re\ufb02ect the in\ufb02uence of an\n\nadaptive structural prior\n\nd rnns pre-trained in a more naturalistic environment mimic\n\nrat\u2019s suboptimal behavior\n\nd after errors, pre-trained rnns decouple trial accumulated\n\nevidence from the decision\n\nauthors\nmanuel molano-mazo\u00b4 n, yuxiu shao,\ndaniel duque, guangyu robert yang,\nsrdjan ostojic, jaime de la rocha\n\ncorrespondence\nmanuelmolanomazon@gmail.com\n\nin brief\nmolano-mazon et al. show that pre-\ntrained recurrent networks replicate the\nrobust, suboptimal reset behavior\ndisplayed by rats in a 2afc task\npresenting serial correlations. they then\nuse population analyses to understand\nthe network mechanisms implemented\nby the pre-trained rnns and provide\nspeci\ufb01c predictions to be tested\nexperimentally.\n\nmolano-mazo\u00b4 n et al., 2023, current biolog", "neural networks 15 (2002) 507\u2013521\n\n2002 special issue\n\nwww.elsevier.com/locate/neunet\n\ndopamine-dependent plasticity of corticostriatal synapses\n\njohn n.j. reynolds, jeffery r. wickens*\n\nthe neuroscience research centre and department of anatomy and structural biology, school of medical sciences, university of otago, p.o. box 913,\n\nreceived 6 november 2001; revised 25 february 2002; accepted 25 february 2002\n\ndunedin, new zealand\n\nabstract\n\nknowledge of the effect of dopamine on corticostriatal synaptic plasticity has advanced rapidly over the last 5 years. we consider this new\nknowledge in relation to three factors proposed earlier to describe the rules for synaptic plasticity in the corticostriatal pathway. these factors\nare a phasic increase in dopamine release, presynaptic activity and postsynaptic depolarisation. a function is proposed which relates the\namount of dopamine release in the striatum to the modulation of corticostriatal synaptic ef\ufb01cacy. it is argued that this function", "a scale-dependent measure of system\ndimensionality\n\narticle\n\nhighlights\nd the scale-dependent dimensionality uni\ufb01es widely used\n\nmeasures of dimensionality\n\nd dynamical systems show distinct dimensionality properties\n\nat different scales\n\nd the scale-dependent dimensionality allows us to identify\n\ncritical scales of the system\n\nd fundamental trends in dimensionality of neural activity\n\ndepend on the brain state\n\nauthors\n\nstefano recanatesi, serena bradde,\nvijay balasubramanian,\nnicholas a. steinmetz,\neric shea-brown\n\ncorrespondence\nstefano.recanatesi@gmail.com (s.r.),\nserena.bradde@gmail.com (s.b.),\netsb@uw.edu (e.s.-b.)\n\nin brief\nwe present a scale-dependent\ndimensionality analysis that reveals the\neffective degrees of freedom in a complex\nsystem at all scales. we illustrate how,\nwhen applied to various types of data, the\nscale-dependent participation ratio\nexposes interesting geometrical patterns.\nour method incorporates widely known\ndimensionality measures and is\napplicable to multi", "perspectives\n\no p i n i o n\n\nthe short-latency dopamine signal: \na role in discovering novel actions?\n\npeter redgrave and kevin gurney\n\nabstract | an influential concept in contemporary computational neuroscience is \nthe reward prediction error hypothesis of phasic dopaminergic function. it \nmaintains that midbrain dopaminergic neurons signal the occurrence of \nunpredicted reward, which is used in appetitive learning to reinforce existing \nactions that most often lead to reward. however, the availability of limited afferent \nsensory processing and the precise timing of dopaminergic signals suggest that \nthey might instead have a central role in identifying which aspects of context and \nbehavioural output are crucial in causing unpredicted events.\n\nin his famous experiment on reinforcement \nlearning, thorndike placed a hungry cat in a \ncage, the door of which was held closed by \na pin1. a peddle in the cage was connected \nto the pin, such that if the cat pressed the \npeddle, the pin was", "0\n2\n0\n2\n\n \nt\nc\no\n0\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n5\nv\n9\n5\n3\n5\n0\n\n.\n\n9\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nactivation relaxation: a local dynamical\n\napproximation to backpropagation in the brain\n\nberen millidge\n\nschool of informatics\nuniversity of edinburgh\nberen@millidge.name\n\nalexander tschantz\n\nsackler center for consciousness science\n\nschool of engineering and informatics\n\nuniversity of sussex\n\ntschantz.alec@gmail.com\n\nanil k seth\n\nchristopher l buckley\n\nsackler center for consciousness science\n\nevolutionary and adaptive systems research group\n\nevolutionary and adaptive systems research group\n\nschool of engineering and informatics\n\nschool of engineering and informatics\n\nuniversity of sussex\n\na.k.seth@sussex.ac.uk\n\nuniversity of sussex\n\nc.l.buckley@sussex.ac.uk\n\noctober 13, 2020\n\nabstract\n\nthe backpropagation of error algorithm (backprop) has been instrumental in the recent success of\ndeep learning. however, a key question remains as to whether backprop can be formulated in a\nmanner suitable for implem", "when networks do and don\u2019t learn to represent task features \u2013\n\nimplications for neural codes\n\nmatthew farrell, stefano recanatesi, and eric shea-brown\n\nmarch 2023\n\nabstract\n\nneural circuits \u2013 both in the brain and in \u201cartifical\u201d neural network models \u2013 learn to solve a remarkable\nvariety of tasks, and there is great potential in using neural networks as models for brain function. to better\ncharacterize this potential, we survey the literature to gain insight on a fundamental question: what are\nthe mechanisms behind neural network behavior? a natural way to explore this is by quantifying how the\ncircuits change over the course of learning, especially in terms of the activity patterns, or representations,\nthat they produce. here, we focus on three questions that have attracted recent research: when do net-\nworks change their internal representations over the course of learning, vs repurposing pre-existing ones?\nwhen do the representations that emerge isolate the task at hand, giving comp", "the neuronal code for number\n\nandreas nieder\nabstract | humans and non-human primates share an elemental quantification system that \nresides in a dedicated neural network in the parietal and frontal lobes. in this cortical network, \n\u2018number neurons\u2019 encode the number of elements in a set, its cardinality or numerosity, \nirrespective of stimulus appearance across sensory motor systems, and from both spatial and \ntemporal presentation arrays. after numbers have been extracted from sensory input, they need \nto be processed to support goal-directed behaviour. studying number neurons provides insights \ninto how information is maintained in working memory and transformed in tasks that require \nrule-based decisions. beyond an understanding of how cardinal numbers are encoded, number \nprocessing provides a window into the neuronal mechanisms of high-level brain functions.\n\nordinal numbers\nnumbers that relate to the \nempirical property of \u2018rank\u2019 \nin\u00a0a\u00a0sequence (for example, \n\u2018fifth place\u2019).\n\nno", "www.nature.com/cr\nwww.cell-research.com\n\nopen\n\narticle\na novel somatosensory spatial navigation system outside the\nhippocampal formation\nxiaoyang long1 and sheng-jia zhang1\n\nspatially selective \ufb01ring of place cells, grid cells, boundary vector/border cells and head direction cells constitutes the basic building\nblocks of a canonical spatial navigation system centered on the hippocampal-entorhinal complex. while head direction cells can be\nfound throughout the brain, spatial tuning outside the hippocampal formation is often non-speci\ufb01c or conjunctive to other\nrepresentations such as a reward. although the precise mechanism of spatially selective \ufb01ring activity is not understood, various\nstudies show sensory inputs, particularly vision, heavily modulate spatial representation in the hippocampal-entorhinal circuit. to\nbetter understand the contribution of other sensory inputs in shaping spatial representation in the brain, we performed recording\nfrom the primary somatosensory cortex in fo", "diverse feature visualizations reveal invariances\n\nin early layers of deep neural networks\n\nsantiago a. cadena, marissa a. weis, leon a. gatys,\n\nmatthias bethge, and alexander s. ecker\n\ncenter for integrative neuroscience and institute for theoretical physics\n\nbernstein center for computational neuroscience\n\nuniversity of t\u00fcbingen, germany\n\n{first.last}@bethgelab.org\n\nabstract. visualizing features in deep neural networks (dnns) can\nhelp understanding their computations. many previous studies aimed to\nvisualize the selectivity of individual units by inding meaningful images\nthat maximize their activation. however, comparably little attention has\nbeen paid to visualizing to what image transformations units in dnns\nare invariant. here we propose a method to discover invariances in the\nresponses of hidden layer units of deep neural networks. our approach\nis based on simultaneously searching for a batch of images that strongly\nactivate a unit while at the same time being as distinct from e", "7\n1\n0\n2\n\n \nc\ne\nd\n \n0\n3\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n2\n6\n0\n0\n0\n\n.\n\n1\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ndendritic error backpropagation\n\nin deep cortical microcircuits\n\njo\u02dcao sacramento1*, rui ponte costa1, yoshua bengio2, walter senn1*\n\n1department of physiology\n\nuniversity of bern, switzerland\n\n2montreal institute for learning algorithms\nuniversit\u00b4e de montr\u00b4eal, quebec, canada\n\nabstract\n\nanimal behaviour depends on learning to associate sensory stimuli with the desired motor com-\nmand. understanding how the brain orchestrates the necessary synaptic modi\ufb01cations across\ndifferent brain areas has remained a longstanding puzzle. here, we introduce a multi-area neu-\nronal network model in which synaptic plasticity continuously adapts the network towards a global\ndesired output. in this model synaptic learning is driven by a local dendritic prediction error that\narises from a failure to predict the top-down input given the bottom-up activities. such errors oc-\ncur at apical dendrites of pyramidal n", "r e v i e w s\n\nthe free-energy principle:  \na unified brain theory?\n\nkarl friston\n\nabstract | a free-energy principle has been proposed recently that accounts for action, \nperception and learning. this review looks at some key brain theories in the biological (for \nexample, neural darwinism) and physical (for example, information theory and optimal \ncontrol theory) sciences from the free-energy perspective. crucially, one key theme runs \nthrough each of these theories \u2014 optimization. furthermore, if we look closely at what is \noptimized, the same quantity keeps emerging, namely value (expected reward, expected \nutility) or its complement, surprise (prediction error, expected cost). this is the quantity that \nis optimized under the free-energy principle, which suggests that several global brain \ntheories might be unified within a free-energy framework.\n\nfree energy\nan information theory measure \nthat bounds or limits (by being \ngreater than) the surprise on \nsampling some data, given a ", "5\n1\n0\n2\n\n \nr\np\na\n0\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n9\n6\n2\n3\n\n.\n\n6\n0\n4\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2015\n\nscheduled denoising autoencoders\n\nkrzysztof j. geras\nschool of informatics\nuniversity of edinburgh\nk.j.geras@sms.ed.ac.uk\n\ncharles sutton\nschool of informatics\nuniversity of edinburgh\ncsutton@inf.ed.ac.uk\n\nabstract\n\nwe present a representation learning method that learns features at multiple dif-\nferent levels of scale. working within the unsupervised framework of denoising\nautoencoders, we observe that when the input is heavily corrupted during train-\ning, the network tends to learn coarse-grained features, whereas when the input\nis only slightly corrupted, the network tends to learn \ufb01ne-grained features. this\nmotivates the scheduled denoising autoencoder, which starts with a high level of\nnoise that lowers as training progresses. we \ufb01nd that the resulting representation\nyields a signi\ufb01cant boost on a later supervised task compared to the original in-\nput", "a r t i c l e s\n\nspecific evidence of low-dimensional continuous \nattractor dynamics in grid cells\nkijung yoon1, michael a buice1, caswell barry2\u20134, robin hayman4, neil burgess2,3 & ila r fiete1\nwe examined simultaneously recorded spikes from multiple rat grid cells, to explain mechanisms underlying their activity.  \namong grid cells with similar spatial periods, the population activity was confined to lie close to a two-dimensional (2d) manifold: \ngrid cells differed only along two dimensions of their responses and otherwise were nearly identical. relationships between \ncell pairs were conserved despite extensive deformations of single-neuron responses. results from novel environments suggest \nsuch structure is not inherited from hippocampal or external sensory inputs. across conditions, cell-cell relationships are better \nconserved than responses of single cells. finally, the system is continually subject to perturbations that, were the 2d manifold not \nattractive, would drive the sy", "a distributional perspective on reinforcement learning\n\nmarc g. bellemare * 1 will dabney * 1 r\u00b4emi munos 1\n\n7\n1\n0\n2\n\n \nl\nu\nj\n \n\n1\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n7\n8\n8\n6\n0\n\n.\n\n7\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nin this paper we argue for the fundamental impor-\ntance of the value distribution: the distribution\nof the random return received by a reinforcement\nlearning agent. this is in contrast to the com-\nmon approach to reinforcement learning which\nmodels the expectation of this return, or value.\nalthough there is an established body of liter-\nature studying the value distribution, thus far it\nhas always been used for a speci\ufb01c purpose such\nas implementing risk-aware behaviour. we begin\nwith theoretical results in both the policy eval-\nuation and control settings, exposing a signi\ufb01-\ncant distributional instability in the latter. we\nthen use the distributional perspective to design\na new algorithm which applies bellman\u2019s equa-\ntion to the learning of approximate value distri-\nbutions. we ev", "topics in cognitive science 7 (2015) 217\u2013229\ncopyright \u00a9 2015 cognitive science society, inc. all rights reserved.\nissn:1756-8757 print / 1756-8765 online\ndoi: 10.1111/tops.12142\n\nrational use of cognitive resources: levels of analysis\n\nbetween the computational and the algorithmic\n\nthomas l. grif\ufb01ths,a falk lieder,a noah d. goodmanb\n\nadepartment of psychology, university of california, berkeley\n\nbdepartment of psychology, stanford university\n\nreceived 22 july 2013; received in revised form 24 february 2014; accepted 18 june 2014\n\nabstract\n\nmarr\u2019s levels of analysis\u2014computational, algorithmic, and implementation\u2014have served cogni-\ntive science well over the last 30 years. but the recent increase in the popularity of the computa-\ntional level raises a new challenge: how do we begin to relate models at different levels of\nanalysis? we propose that it is possible to de\ufb01ne levels of analysis that lie between the computa-\ntional and the algorithmic, providing a way to build a bridge between", "3\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n1\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n7\n8\n4\n0\n0\n\n.\n\n2\n0\n3\n2\n:\nv\ni\nx\nr\na\n\njournal of latex class files, vol. 14, no. 8, august 2015\n\n1\n\na comprehensive survey of continual learning:\n\ntheory, method and application\n\nliyuan wang, xingxing zhang, hang su, jun zhu, fellow, ieee\n\nabstract\u2014to cope with real-world dynamics, an intelligent agent needs to incrementally acquire, update, accumulate, and exploit\nknowledge throughout its lifetime. this ability, known as continual learning, provides a foundation for ai systems to develop themselves\nadaptively. in a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually\nresults in a dramatic performance degradation of the old tasks. beyond this, increasingly numerous advances have emerged in recent\nyears that largely extend the understanding and application of continual learning. the growing and widespread interest in this direction\ndemonstrates its realistic significance a", "advanced review\n\npredictive coding\nyanping huang and rajesh p. n. rao\u2217\n\npredictive coding is a unifying framework for understanding redundancy reduction\nand ef\ufb01cient coding in the nervous system. by transmitting only the unpredicted\nportions of an incoming sensory signal, predictive coding allows the nervous\nsystem to reduce redundancy and make full use of the limited dynamic range of\nneurons. starting with the hypothesis of ef\ufb01cient coding as a design principle in the\nsensory system, predictive coding provides a functional explanation for a range of\nneural responses and many aspects of brain organization. the lateral and temporal\nantagonism in receptive \ufb01elds in the retina and lateral geniculate nucleus occur nat-\nurally as a consequence of predictive coding of natural images. in the higher visual\nsystem, predictive coding provides an explanation for oriented receptive \ufb01elds and\ncontextual effects as well as the hierarchical reciprocally connected organization of\nthe cortex. predictiv", "communicated by fernando pineda \n\nan  efficient gradient-based algorithm for on-line \ntraining of  recurrent network trajectories \n\nronald j. williams \njing peng \ncollege of  computer  science, northeastern  university, boston, m a  02225 usa \n\na novel variant of  the familiar backpropagation-through-time approach \nto training recurrent networks is described.  this algorithm is intended \nto be used on arbitrary recurrent networks that run continually without \never being  reset to  an initial state,  and  it  is specifically designed for \ncomputationally  efficient computer  implementation.  this  algorithm \ncan be viewed as a cross between epochwise backpropagation through time, \nwhich  is not  appropriate  for continually running  networks,  and  the \nwidely used on-line gradient approximation technique of  truncated back- \npropagation  through time. \n\n1 introduction \n\nartificial neural networks having feedback connections can implement a \nwide variety of  dynamic systems.  the proble", "reviews\n\nmaking memories last: the synaptic \ntagging and capture hypothesis\n\nroger l. redondo*\u2021 and richard g. m. morris*\n\nabstract | the synaptic tagging and capture hypothesis of protein synthesis-dependent \nlong-term potentiation asserts that the induction of synaptic potentiation creates only  \nthe potential for a lasting change in synaptic efficacy, but not the commitment to such a \nchange. other neural activity, before or after induction, can also determine whether \npersistent change occurs. recent findings, leading us to revise the original hypothesis, \nindicate that the induction of a local, synapse-specific \u2018tagged\u2019 state and the expression  \nof long-term potentiation are dissociable. additional observations suggest that there are \nmajor differences in the mechanisms of functional and structural plasticity. these advances \ncall for a revised theory that incorporates the specific molecular and structural processes \ninvolved. addressing the physiological relevance of previous in", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/2021.05.17.444526\n; \n\nthis version posted december 1, 2021. \n\nthe copyright holder for this\n\npreprint (which was not certified by peer review) is the author/funder. all rights reserved. no reuse allowed without permission. \n\nnetwork-level modeling of stimulation-induced functional connectivity\n\nchange: an optogenetic study in non-human primate cortex\n\njulien bloch1,6,7, alexander greaves-tunnell2, eric shea-brown3, 6, 7, zaid harchaoui2, ali\n\nshojaie4, and azadeh yazdan-shahmorad1, 5, 6, 7, 8, *\n\n1department of bioengineering, university of washington, seattle\n\n2department of statistics, university of washington, seattle\n\n3department of applied mathematics, university of washington, seattle\n\n4department of biostatistics, university of washington, seattle\n\n5department of electrical and computer engineering, university of washington, seattle\n\n6center for neurotechnology, university of washington, seattle\n\n7computational neuroscience cente", "letters\n\nvol 441|15 june 2006|doi:10.1038/nature04766\n\ncortical substrates for exploratory decisions in\nhumans\nnathaniel d. daw1*, john p. o\u2019doherty2*\u2020, peter dayan1, ben seymour2 & raymond j. dolan2\n\ndecision making in an uncertain environment poses a con\ufb02ict\nbetween the opposing demands of gathering and exploiting infor-\nmation. in a classic illustration of this \u2018exploration\u2013exploitation\u2019\ndilemma1, a gambler choosing between multiple slot machines\nbalances the desire to select what seems, on the basis of accumu-\nlated experience, the richest option, against the desire to choose a\nless familiar option that might turn out more advantageous (and\nthereby provide information for improving future decisions). far\nfrom representing idle curiosity, such exploration is often critical\nfor organisms to discover how best to harvest resources such as\nfood and water. in appetitive choice, substantial experimental\nevidence, underpinned by computational reinforcement learning2\n(rl) theory, indicates ", "research article\ndimensionality in recurrent spiking networks:\nglobal trends in activity and local origins in\nconnectivity\n\nstefano recanatesiid1*, gabriel koch ockerid2, michael a. buice1,2,3, eric shea-\nbrown1,2,3\n\n1 center for computational neuroscience, university of washington, seattle, washington, united states of\namerica, 2 allen institute for brain science, seattle, washington, united states of america, 3 department of\napplied mathematics, university of washington, seattle, washington, united states of america\n\n* stefanor@uw.edu\n\nabstract\n\nthe dimensionality of a network\u2019s collective activity is of increasing interest in neuroscience.\nthis is because dimensionality provides a compact measure of how coordinated network-\nwide activity is, in terms of the number of modes (or degrees of freedom) that it can indepen-\ndently explore. a low number of modes suggests a compressed low dimensional neural\ncode and reveals interpretable dynamics [1], while findings of high dimension may sug", "r e v i e w\n\nfocus on big data\n\ndimensionality reduction for large-scale \nneural recordings\n\njohn p cunningham1 & byron m yu2\u20134\n\nmost sensory, cognitive and motor functions depend on the interactions of many neurons. in recent years, there has been \nrapid development and increasing use of technologies for recording from large numbers of neurons, either sequentially or \nsimultaneously. a key question is what scientific insight can be gained by studying a population of recorded neurons beyond \nstudying each neuron individually. here, we examine three important motivations for population studies: single-trial hypotheses \nrequiring statistical power, hypotheses of population response structure and exploratory analyses of large data sets. many \nrecent studies have adopted dimensionality reduction to analyze these populations and to find features that are not apparent \nat the level of individual neurons. we describe the dimensionality reduction methods commonly applied to population activity", "8\n1\n0\n2\n\n \nt\nc\no\n4\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n0\n4\n4\n6\n0\n\n.\n\n4\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nequivalence between policy gradients and soft q-learning\n\njohn schulman1, xi chen1,2, and pieter abbeel1,2\n\n1openai\n\n2uc berkeley, eecs dept.\n\n{joschu, peter, pieter}@openai.com\n\nabstract\n\ntwo of the leading approaches for model-free reinforcement learning are policy gradient methods\nand q-learning methods. q-learning methods can be e\ufb00ective and sample-e\ufb03cient when they work,\nhowever, it is not well-understood why they work, since empirically, the q-values they estimate are very\ninaccurate. a partial explanation may be that q-learning methods are secretly implementing policy\ngradient updates: we show that there is a precise equivalence between q-learning and policy gradient\nmethods in the setting of entropy-regularized reinforcement learning, that \u201csoft\u201d (entropy-regularized)\nq-learning is exactly equivalent to a policy gradient method. we also point out a connection between\nq-learning methods and n", "nonlinear ica using auxiliary variables\n\nand generalized contrastive learning\n\n9\n1\n0\n2\n\n \n\nb\ne\nf\n4\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n1\n5\n6\n8\n0\n\n.\n\n5\n0\n8\n1\n:\nv\ni\nx\nr\na\n\naapo hyv\u00a8arinen 1,2\n\nhiroaki sasaki 3,1\n\nrichard e. turner 4\n\n1 the gatsby unit\n\nucl, uk\n\n2 dept. of cs and hiit\nuniv. helsinki, finland\n\n3div. of info. sci.\n\nnaist, japan\n\n4 univ. cambridge &\nmicrosoft research, uk\n\nabstract\n\nnonlinear ica is a fundamental problem\nfor unsupervised representation learning, em-\nphasizing the capacity to recover the underly-\ning latent variables generating the data (i.e.,\nidenti\ufb01ability). recently, the very \ufb01rst iden-\nti\ufb01ability proofs for nonlinear ica have been\nproposed, leveraging the temporal structure\nof the independent components. here, we\npropose a general framework for nonlinear\nica, which, as a special case, can make use of\ntemporal structure. it is based on augment-\ning the data by an auxiliary variable, such\nas the time index, the history of the time se-\nries, or any other ava", "partition functions from\n\nrao-blackwellized tempered sampling\n\n6\n1\n0\n2\n\n \n\ny\na\nm\n5\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n2\n1\n9\n1\n0\n\n.\n\n3\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ndavid e. carlson\u22171,2\npatrick stinson\u22172\nari pakman\u22171,2\nliam paninski1,2\n1 department of statistics\n2 grossman center for the statistics of mind\ncolumbia university, new york, ny, 10027\n\nabstract\n\npartition functions of probability distributions\nare important quantities for model evaluation\nand comparisons. we present a new method\nto compute partition functions of complex and\nmultimodal distributions. such distributions\nare often sampled using simulated tempering,\nwhich augments the target space with an auxil-\niary inverse temperature variable. our method\nexploits the multinomial probability law of the\ninverse temperatures, and provides estimates\nof the partition function in terms of a simple\nquotient of rao-blackwellized marginal in-\nverse temperature probability estimates, which\nare updated while sampling. we show that\nthe method ha", "generative modeling by estimating gradients of the\n\ndata distribution\n\nyang song\n\nstanford university\n\nyangsong@cs.stanford.edu\n\nstefano ermon\n\nstanford university\n\nermon@cs.stanford.edu\n\n0\n2\n0\n2\n\n \nt\nc\no\n0\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n0\n6\n5\n0\n\n.\n\n7\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe introduce a new generative model where samples are produced via langevin\ndynamics using gradients of the data distribution estimated with score matching.\nbecause gradients can be ill-de\ufb01ned and hard to estimate when the data resides on\nlow-dimensional manifolds, we perturb the data with different levels of gaussian\nnoise, and jointly estimate the corresponding scores, i.e., the vector \ufb01elds of\ngradients of the perturbed data distribution for all noise levels. for sampling, we\npropose an annealed langevin dynamics where we use gradients corresponding to\ngradually decreasing noise levels as the sampling process gets closer to the data\nmanifold. our framework allows \ufb02exible model architectures, requires n", "6\n1\n0\n2\n\n \nl\nu\nj\n \n\n8\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n2\n8\n3\n9\n0\n\n.\n\n3\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ndeep networks with stochastic depth\n\ngao huang*, yu sun*, zhuang liu\u2020, daniel sedra, kilian q. weinberger\n\n{gh349, ys646, dms422, kqw4}@cornell.edu, cornell university\n\n* authors contribute equally\n\nliuzhuang13@mails.tsinghua.edu.cn, tsinghua university\n\n\u2020\n\nabstract. very deep convolutional networks with hundreds of layers\nhave led to signi\ufb01cant reductions in error on competitive benchmarks.\nalthough the unmatched expressiveness of the many layers can be highly\ndesirable at test time, training very deep networks comes with its own\nset of challenges. the gradients can vanish, the forward \ufb02ow often di-\nminishes, and the training time can be painfully slow. to address these\nproblems, we propose stochastic depth, a training procedure that enables\nthe seemingly contradictory setup to train short networks and use deep\nnetworks at test time. we start with very deep networks but during train-\ning, for each ", "knowledge-based systems 199 (2020) 105972\n\ncontents lists available at sciencedirect\n\nknowledge-based systems\n\njournal homepage: www.elsevier.com/locate/knosys\n\ninterpretable neural networks based on continuous-valued logic and\nmulticriteria decision operators\norsolya csisz\u00e1r a,b,\u2217, g\u00e1bor csisz\u00e1r c, j\u00f3zsef dombi d\n\na faculty of basic sciences, university of applied sciences esslingen, esslingen, germany\nb institute of applied mathematics, \u00f3buda university, budapest, hungary\nc institute of materials physics, university of stuttgart, stuttgart, germany\nd institute of informatics, university of szeged, szeged, hungary\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 9 february 2020\nreceived in revised form 20 april 2020\naccepted 23 april 2020\navailable online 28 april 2020\n\nkeywords:\nexplainable artificial intelligence\ncontinuous logic\nnilpotent logic\nneural network\nadversarial problems\n\n1. introduction\n\ncombining neural networks with continuous logic and multicriteria ", "neuron, vol. 36, 241\u2013263, october 10, 2002, copyright \uf8e92002 by cell press\n\ngetting formal with\ndopamine and reward\n\nreview\n\nwolfram schultz1,2,3\n1institute of physiology\nuniversity of fribourg\nch-1700 fribourg\nswitzerland\n2 department of anatomy\nuniversity of cambridge\ncambridge cb2 3dy\nunited kingdom\n\nrecent neurophysiological studies reveal that neurons\nin certain brain structures carry specific signals about\npast and future rewards. dopamine neurons display\na short-latency, phasic reward signal indicating the\ndifference between actual and predicted rewards. the\nsignal is useful for enhancing neuronal processing and\nlearning behavioral reactions. it is distinctly different\nfrom dopamine\u2019s tonic enabling of numerous behav-\nioral processes. neurons in the striatum, frontal cor-\ntex, and amygdala also process reward information\nbut provide more differentiated information for identi-\nfying and anticipating rewards and organizing goal-\ndirected behavior. the different reward signals have\n", "j neurophysiol 98: 3648 \u20133665, 2007.\nfirst published october 10, 2007; doi:10.1152/jn.00364.2007.\n\nreinforcement learning with modulated spike timing\u2013dependent\nsynaptic plasticity\n\nmichael a. farries1 and adrienne l. fairhall2\n1department of biology, university of texas at san antonio, san antonio, texas; and 2department of physiology and biophysics,\nuniversity of washington, seattle, washington\n\nsubmitted 2 april 2007; accepted in \ufb01nal form 9 october 2007\n\nfarries ma, fairhall al. reinforcement learning with modulated spike\ntiming\u2013dependent synaptic plasticity. j neurophysiol 98: 3648\u20133665,\n2007. first published october 10, 2007; doi:10.1152/jn.00364.2007.\nspike timing\u2013 dependent synaptic plasticity (stdp) has emerged as\nthe preferred framework linking patterns of pre- and postsynaptic\nactivity to changes in synaptic strength. although synaptic plasticity\nis widely believed to be a major component of learning, it is unclear\nhow stdp itself could serve as a mechanism for general purpos", "bombesin-like peptide recruits disinhibitory cortical\ncircuits and enhances fear memories\n\narticle\n\ngraphical abstract\n\nauthors\nsarah melzer, elena r. newmark,\ngrace or mizuno, ..., james levasseur,\nlin tian, bernardo l. sabatini\n\ncorrespondence\nbsabatini@hms.harvard.edu\n\nin brief\ncritical function of neuropeptides in\ncortex-dependent behaviors is\ndemonstrated by local and long-range\nsources of the neuropeptide, grp,\nselectively recruiting disinhibitory cortical\nmicrocircuits in auditory cortex to\nregulate fear memories in mice.\n\nhighlights\nd gastrin-releasing peptide (grp) receptors are expressed in\n\ncortical vip cells\n\nd grp modulates cortical disinhibitory vip cell activity and\n\ngene expression\n\nd cortical vip cells activate to novel sounds and shocks during\n\nfear conditioning\n\nd ablation of grp receptor in auditory cortex results in\n\nimpaired fear memory\n\nmelzer et al., 2021, cell 184, 5622\u20135634\noctober 28, 2021 \u00aa 2021 elsevier inc.\nhttps://doi.org/10.1016/j.cell.2021.09.013\n\nll\n\n\f", "j neurophysiol 108: 624 \u2013 644, 2012.\nfirst published april 11, 2012; doi:10.1152/jn.00371.2011.\n\nbehavioral and neural correlates of visuomotor adaptation observed through\na brain-computer interface in primary motor cortex\n\nsteven m. chase,1,3 robert e. kass,2,3 and andrew b. schwartz1,3\n1department of neurobiology, university of pittsburgh, pittsburgh, pennsylvania; 2department of statistics, carnegie mellon\nuniversity, pittsburgh, pennsylvania; and 3center for the neural basis of cognition, carnegie mellon university and\nuniversity of pittsburgh, pittsburgh, pennsylvania\n\nsubmitted 22 april 2011; accepted in \ufb01nal form 9 april 2012\n\nchase sm, kass re, schwartz ab. behavioral and neural cor-\nrelates of visuomotor adaptation observed through a brain-computer\ninterface in primary motor cortex. j neurophysiol 108: 624 \u2013 644,\n2012. first published april 11, 2012; doi:10.1152/jn.00371.2011.\u2014\nbrain-computer interfaces (bcis) provide a de\ufb01ned link between\nneural activity and devices, allowing", "circuit mechanisms for the maintenance and \nmanipulation of information in working memory\n\nnicolas y. masse\u200a\nand david j. freedman\u200a\n\n\u200a1,3*\n\n\u200a1*, guangyu r. yang2,4, h. francis song2,5, xiao-jing wang\u200a\n\n\u200a2  \n\nrecently it has been proposed that information in working memory (wm) may not always be stored in persistent neuronal \nactivity but can be maintained in \u2018activity-silent\u2019 hidden states, such as synaptic efficacies endowed with short-term synap-\ntic plasticity. to test this idea computationally, we investigated recurrent neural network models trained to perform several \nwm-dependent tasks, in which wm representation emerges from learning and is not a\u00a0priori assumed to depend on self-sus-\ntained persistent activity. we found that short-term synaptic plasticity can support the short-term maintenance of information, \nprovided that the memory delay period is sufficiently short. however, in tasks that require actively manipulating information, \npersistent activity naturally emerges from ", "13326 \u2022 the journal of neuroscience, october 6, 2010 \u2022 30(40):13326 \u201313337\n\ndevelopment/plasticity/repair\n\nfunctional requirements for reward-modulated\nspike-timing-dependent plasticity\n\nnicolas fre\u00b4maux,* henning sprekeler,* and wulfram gerstner\nschool of computer and communication sciences and brain-mind institute, ecole polytechnique fe\u00b4de\u00b4rale de lausanne, ch-1015 lausanne, switzerland\n\nrecent experiments have shown that spike-timing-dependent plasticity is influenced by neuromodulation. we derive theoretical\nconditions for successful learning of reward-related behavior for a large class of learning rules where hebbian synaptic plasticity\nis conditioned on a global modulatory factor signaling reward. we show that all learning rules in this class can be separated into a\nterm that captures the covariance of neuronal firing and reward and a second term that presents the influence of unsupervised\nlearning. the unsupervised term, which is, in general, detrimental for reward-based learni", "math. program., ser. b (2015) 151:249\u2013281\ndoi 10.1007/s10107-015-0893-2\n\nfull length paper\n\nrecent advances in trust region algorithms\n\nya-xiang yuan1\n\nreceived: 29 september 2014 / accepted: 17 february 2015 / published online: 15 march 2015\n\u00a9 springer-verlag berlin heidelberg and mathematical optimization society 2015\n\nabstract trust region methods are a class of numerical methods for optimization.\nunlike line search type methods where a line search is carried out in each iteration,\ntrust region methods compute a trial step by solving a trust region subproblem where\na model function is minimized within a trust region. due to the trust region con-\nstraint, nonconvex models can be used in trust region subproblems, and trust region\nalgorithms can be applied to nonconvex and ill-conditioned problems. normally it is\neasier to establish the global convergence of a trust region algorithm than that of its\nline search counterpart. in the paper, we review recent results on trust region meth-\no", "vol 442|13 july 2006|doi:10.1038/nature04968\n\nletters\n\na high-performance brain\u2013computer interface\ngopal santhanam1*, stephen i. ryu1,2*, byron m. yu1, afsheen afshar1,3 & krishna v. shenoy1,4\n\nrecent studies have demonstrated that monkeys1\u20134 and humans5\u20139\ncan use signals from the brain to guide computer cursors. brain\u2013\ncomputer interfaces (bcis) may one day assist patients suffering\nfrom neurological injury or disease, but relatively low system\nperformance remains a major obstacle. in fact, the speed and\naccuracy with which keys can be selected using bcis is still far\nlower than for systems relying on eye movements. this is true\nwhether bcis use recordings from populations of individual\nneurons using invasive electrode techniques1\u20135,7,8 or electro-\nencephalogram recordings using less-6 or non-invasive9 tech-\nniques. here we present the design and demonstration, using\nelectrode arrays implanted in monkey dorsal premotor cortex, of a\nmanyfold higher performance bci than previously repor", "high-precision coding in visual cortex\n\narticle\n\ngraphical abstract\n\nauthors\ncarsen stringer, michalis michaelos,\ndmitri tsyboulski, sarah e. lindo,\nmarius pachitariu\n\ncorrespondence\npachitarium@janelia.hhmi.org\n\nin brief\nlarge-scale recordings in mouse primary\nvisual cortex and higher order visual\nareas uncover neural representations\nmore precise than behavioral\ndiscrimination thresholds, suggesting\nvisual perception is limited by non-\nsensory brain networks.\n\nhighlights\nd large-scale recordings in visual cortex reveal a highly\n\nprecise encoding of stimuli\n\nd the neural precision far surpasses behavioral precision in\n\nmice and even in humans\n\nd primary visual cortex (v1) does not contribute to behavioral\n\nvariability in a task\n\nd behavioral variability may be explained by temporal\n\nintegration of noise outside v1\n\nstringer et al., 2021, cell 184, 2767\u20132778\nmay 13, 2021 \u00aa 2021 elsevier inc.\nhttps://doi.org/10.1016/j.cell.2021.03.042\n\nll\n\n\f", "a simple neural network module\n\nfor relational reasoning\n\nadam santoro\u2217\n\ndavid raposo\u2217\n\ndavid g.t. barrett\n\nadamsantoro@google.com\n\ndraposo@google.com\n\nbarrettdavid@google.com\n\nmateusz malinowski\nmateuszm@google.com\n\nrazvan pascanu\nrazp@google.com\n\npeter battaglia\n\npeterbattaglia@google.com\n\ntimothy lillicrap\n\ndeepmind\n\nlondon, united kingdom\ncountzero@google.com\n\nabstract\n\nrelational reasoning is a central component of generally intelligent behavior, but\nhas proven dif\ufb01cult for neural networks to learn. in this paper we describe how to\nuse relation networks (rns) as a simple plug-and-play module to solve problems\nthat fundamentally hinge on relational reasoning. we tested rn-augmented net-\nworks on three tasks: visual question answering using a challenging dataset called\nclevr, on which we achieve state-of-the-art, super-human performance; text-\nbased question answering using the babi suite of tasks; and complex reasoning\nabout dynamic physical systems. then, using a curated dataset c", "learning discrete representations via information maximizing\n\nself-augmented training\n\nweihua hu 1 2 takeru miyato 3 4 seiya tokui 3 1 eiichi matsumoto 3 1 masashi sugiyama 2 1\n\n7\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n4\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n0\n2\n7\n8\n0\n\n.\n\n2\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nlearning discrete representations of data is a cen-\ntral machine learning task because of the com-\npactness of the representations and ease of in-\nterpretation. the task includes clustering and\nhash learning as special cases. deep neural net-\nworks are promising to be used because they can\nmodel the non-linearity of data and scale to large\ndatasets. however, their model complexity is\nhuge, and therefore, we need to carefully regu-\nlarize the networks in order to learn useful rep-\nresentations that exhibit intended invariance for\napplications of interest. to this end, we pro-\npose a method called information maximizing\nself-augmented training (imsat). in imsat,\nwe use data augmentation to impose the invari-\nanc", "letter\n\ncommunicated by peter dayan\n\nthe successor representation and temporal context\n\nsamuel j. gershman\nsjgershm@princeton.edu\nchristopher d. moore\nchrmoore@gmail.com\nmichael t. todd\nmttodd@princeton.edu\nkenneth a. norman\nknorman@princeton.edu\ndepartment of psychology and princeton neuroscience institute,\nprinceton university, princeton, nj 08540\n\nper b. sederberg\nsederberg.1@osu.edu\ndepartment of psychology, the ohio state university, columbus oh 43210, u.s.a.\n\nthe successor representation was introduced into reinforcement learning\nby dayan (1993) as a means of facilitating generalization between states\nwith similar successors. although reinforcement learning in general has\nbeen used extensively as a model of psychological and neural processes,\nthe psychological validity of the successor representation has yet to be\nexplored. an interesting possibility is that the successor representation\ncan be used not only for reinforcement learning but for episodic learning\nas well. our main co", "annu. rev. neurosci. 2000. 23:473\u2013500\ncopyright q 2000 by annual reviews. all rights reserved\n\nneuronal coding\nof prediction errors\n\nwolfram schultz1 and anthony dickinson2\n1institute of physiology and program in neuroscience, university of fribourg, ch-1700\nfribourg, switzerland; e-mail: wolfram.schultz@unifr.ch\n2department of experimental psychology, university of cambridge, cambridge cb2\n3eb, united kingdom; e-mail: ad15@cus.cam.ac.uk\n\nkey words\n\nlearning, plasticity, dopamine, reward, attention\n\nabstract associative learning enables animals to anticipate the occurrence of\nimportant outcomes. learning occurs when the actual outcome differs from the pre-\ndicted outcome, resulting in a prediction error. neurons in several brain structures\nappear to code prediction errors in relation to rewards, punishments, external stimuli,\nand behavioral reactions. in one form, dopamine neurons, norepinephrine neurons,\nand nucleus basalis neurons broadcast prediction errors as global reinforcement o", "perspectives\n\nand her attitude would remain negative. but\nalice might join bob if he wants to go to the\nrestaurant. by visiting the restaurant again,\nalice gets a chance to change her opinion.\nalice\u2019s attitude will depend on bob\u2019s, but\nonly because he influenced the probability\nof her revisiting the restaurant. \n\nfinally, the number of your friends who\nengage in some activity can also influence\nyour estimate of the value of this activity. if\nyou have many friends who start firms, for\nexample, your estimate of the chances of suc-\ncess will be based on a large sample size. a\nlarge  sample  size  may  lead  you  to  have  a\nhigher estimate of the success rate than you\nwould  if  the  sample  size  were  small.\nexperiments show that a large sample size\nleads to a more optimistic view when the out-\ncome distribution is skewed (8). if only 10%\nsucceed, you may only observe failures in a\nsmall sample, and will then underestimate the\nsuccess rate. \n\nthese  mechanisms  produce  behavior\nthat  l", "learning curves for stochastic gradient descent\n\nin linear feedforward networks\n\njustin werfel\ndept. of eecs\n\nmit\n\ncambridge, ma 02139\njkwerfel@mit.edu\n\nxiaohui xie\n\nh. sebastian seung\n\nhhmi\n\nmit\n\ndept. of brain & cog. sci.\n\ncambridge, ma 02139\n\nseung@mit.edu\n\ndept. of molecular biology\n\nprinceton university\nprinceton, nj 08544\n\nxhx@princeton.edu\n\nabstract\n\ngradient-following learning methods can encounter problems of imple-\nmentation in many applications, and stochastic variants are frequently\nused to overcome these dif\ufb01culties. we derive quantitative learning\ncurves for three online training methods used with a linear perceptron:\ndirect gradient descent, node perturbation, and weight perturbation. the\nmaximum learning rate for the stochastic methods scales inversely with\nthe \ufb01rst power of the dimensionality of the noise injected into the sys-\ntem; with suf\ufb01ciently small learning rate, all three methods give iden-\ntical learning curves. these results suggest guidelines for when these\n", "1\n\n9\n1\n0\n2\n\n \n\ng\nu\na\n2\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n4\n7\n7\n3\n0\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\non kernel method-based connectionist models\nand supervised deep learning without backprop-\nagation\n\nshiyu duan1\nshujian yu1\nyunmei chen2\njose c. principe1\n\n1department of electrical and computer engineering, university of florida\n\n2department of mathematics, university of florida\n\nkeywords: kernel method, connectionist model, supervised learning, deep learning\n\nabstract\n\nwe propose a novel family of connectionist models based on kernel machines and con-\n\nsider the problem of learning layer-by-layer a compositional hypothesis class, i.e., a\n\nfeedforward, multilayer architecture, in a supervised setting. in terms of the models, we\n\npresent a principled method to \u201ckernelize\u201d (partly or completely) any neural network\n\n\f", "research article\n\nexplaining data-driven document classifications1\n\ndepartment of engineering management, faculty of applied economics, university of antwerp, prinsstraat 13,\n\n2018 antwerp, belgium  {david.martens@ua.ac.be}\n\ndavid martens\n\ndepartment of information, operations and management sciences, stern school of business, new york university,\n\n44 west 4th street, new york, ny  10012-1126  u.s.a.  {fprovost@stern.nyu.edu}\n\nfoster provost\n\nmany  document  classification  applications  require  human  understanding  of  the  reasons  for  data-driven\nclassification decisions by managers, client-facing employees, and the technical team.  predictive models treat\ndocuments as data to be classified, and document data are characterized by very high dimensionality, often\nwith tens of thousands to millions of variables (words).  unfortunately, due to the high dimensionality, under-\nstanding the decisions made by document classifiers is very difficult.  this paper begins by extending the mos", "0\n2\n0\n2\n\n \nt\nc\no\n3\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n3\n2\n1\n7\n0\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nkernelized information bottleneck leads to\n\nbiologically plausible 3-factor hebbian learning in\n\ndeep networks\n\ngatsby computational neuroscience unit\n\ngatsby computational neuroscience unit\n\nroman pogodin\n\nuniversity college london\n\nlondon, w1t 4jg\n\nroman.pogodin.17@ucl.ac.uk\n\npeter e. latham\n\nuniversity college london\n\nlondon, w1t 4jg\n\npel@gatsby.ucl.ac.uk\n\nabstract\n\nthe state-of-the art machine learning approach to training deep neural networks,\nbackpropagation, is implausible for real neural networks: neurons need to know\ntheir outgoing weights; training alternates between a bottom-up forward pass\n(computation) and a top-down backward pass (learning); and the algorithm often\nneeds precise labels of many data points. biologically plausible approximations to\nbackpropagation, such as feedback alignment, solve the weight transport problem,\nbut not the other two. thus, fully biologically plausible le", "neural networks 16 (2003) 1429\u20131451\n\nwww.elsevier.com/locate/neunet\n\nthe general inef\ufb01ciency of batch training for gradient\n\ndescent learning\n\nd. randall wilsona,*, tony r. martinezb,1\n\nafonix corporation, 180 west election road suite 200, draper, ut, usa\n\nbcomputer science department, 3361 tmcb, brigham young university, provo, ut 84602, usa\n\nreceived 10 july 2001; revised 8 april 2003; accepted 8 april 2003\n\nabstract\n\ngradient descent training of neural networks can be done in either a batch or on-line manner. a widely held myth in the neural network\ncommunity is that batch training is as fast or faster and/or more \u2018correct\u2019 than on-line training because it supposedly uses a better\napproximation of the true gradient for its weight updates. this paper explains why batch training is almost always slower than on-line\ntraining\u2014often orders of magnitude slower\u2014especially on large training sets. the main reason is due to the ability of on-line training to\nfollow curves in the error surface", "p\nh\ny\ns\ni\nc\na\n \n9\nd\n \n(\n1\n9\n8\n3\n)\n \n1\n8\n9\n-\n2\n0\n8\n \nn\no\nr\nt\nh\n-\nh\no\nl\nl\na\nn\nd\n \np\nu\nb\nl\ni\ns\nh\ni\nn\ng\n \nc\no\nm\np\na\nn\ny\n \nm\ne\na\ns\nu\nr\ni\nn\ng\n \nt\nh\ne\n \ns\nt\nr\na\nn\ng\ne\nn\ne\ns\ns\n \no\nf\n \ns\nt\nr\na\nn\ng\ne\n \na\nt\nt\nr\na\nc\nt\no\nr\ns\n \np\ne\nt\ne\nr\n \ng\nr\na\ns\ns\nb\ne\nr\ng\ne\nr\nt\n \na\nn\nd\n \ni\nt\na\nm\na\nr\n \np\nr\no\nc\na\nc\nc\ni\na\n \nd\ne\np\na\nr\nt\nm\ne\nn\nt\n \no\nf\n \nc\nh\ne\nm\ni\nc\na\nl\n \np\nh\ny\ns\ni\nc\ns\n,\n \nw\ne\ni\nz\nm\na\nn\nn\n \ni\nn\ns\nt\ni\nt\nu\nt\ne\n \no\nf\n \ns\nc\ni\ne\nn\nc\ne\n,\n \nr\ne\nh\no\nv\no\nt\n \n7\n6\n1\n0\n0\n,\n \ni\ns\nr\na\ne\nl\n \nr\ne\nc\ne\ni\nv\ne\nd\n \n1\n6\n \nn\no\nv\ne\nm\nb\ne\nr\n \n1\n9\n8\n2\n \nr\ne\nv\ni\ns\ne\nd\n \n2\n6\n \nm\na\ny\n \n1\n9\n8\n3\n \nw\ne\n \ns\nt\nu\nd\ny\n \nt\nh\ne\n \nc\no\nr\nr\ne\nl\na\nt\ni\no\nn\n \ne\nx\np\no\nn\ne\nn\nt\n \nv\n \ni\nn\nt\nr\no\nd\nu\nc\ne\nd\n \nr\ne\nc\ne\nn\nt\nl\ny\n \na\ns\n \na\n \nc\nh\na\nr\na\nc\nt\ne\nr\ni\ns\nt\ni\nc\n \nm\ne\na\ns\nu\nr\ne\n \no\nf\n \ns\nt\nr\na\nn\ng\ne\n \na\nt\nt\nr\na\nc\nt\no\nr\ns\n \nw\nh\ni\nc\nh\n \na\nl\nl\no\nw\ns\n \no\nn\ne\n \nt\no\n \nd\ni\ns\nt\ni\nn\ng\nu\ni\ns\nh\n \nb\ne\nt\nw\ne\ne\nn\n \nd\ne\nt\ne\nr\nm\ni\nn\ni\ns\nt\ni\nc\n \nc\nh\na\no\ns\n \na\nn\nd\n \nr\na\nn\nd\no\nm\n \nn\no\ni\ns\ne\n.\n \nt\nh\ne\n \ne\nx\np\no\nn\ne\nn\nt\n \nv\n \ni\ns\n \nc\nl\no\ns\ne\nl\ny\n \nr\ne\nl\na\nt\ne\nd\n \nt\no\n \nt\n", "..............................................................\neffective leadership and decision-\nmaking in animal groups on the move\n\niain d. couzin1,2, jens krause3, nigel r. franks4 & simon a. levin1\n\n1department of ecology and evolutionary biology, princeton university,\nprinceton, new jersey 08544, usa\n2department of zoology, south parks road, university of oxford, oxford\nox1 3ps, uk\n3centre for biodiversity and conservation, school of biology, university of leeds,\nleeds ls2 9jt, uk\n4centre for behavioural biology, school of biological sciences, university of\nbristol, woodland road, bristol bs8 1ug, uk\n.............................................................................................................................................................................\nfor animals that forage or travel in groups, making movement\ndecisions often depends on social interactions among group\nmembers1,2. however, in many cases, few individuals have perti-\nnent information, such as kn", "p1: fxz\njanuary 12, 2001\n\n14:38\n\nannual reviews\n\nar121-07\n\nannu. rev. neurosci. 2001. 24:167\u2013202\ncopyright c(cid:13) 2001 by annual reviews. all rights reserved\n\nan integrative theory of prefrontal\ncortex function\n\nearl k. miller\ncenter for learning and memory, riken-mit neuroscience research center and\ndepartment of brain and cognitive sciences, massachusetts institute of technology,\ncambridge, massachusetts 02139; e-mail: ekm@ai.mit.edu\n\njonathan d. cohen\ncenter for the study of brain, mind, and behavior and department of psychology,\nprinceton university, princeton, new jersey 08544; e-mail: jdc@princeton.edu\n\nkey words\nfrontal lobes, cognition, executive control, working memory, attention\nn abstract the prefrontal cortex has long been suspected to play an important role\nin cognitive control, in the ability to orchestrate thought and action in accordance with\ninternal goals. its neural basis, however, has remained a mystery. here, we propose\nthat cognitive control stems from the acti", "7\n1\n0\n2\n\n \nr\na\n\nm\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n4\n0\n1\n8\n0\n\n.\n\n5\n0\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\ndeep predictive coding networks for video\nprediction and unsupervised learning\n\nwilliam lotter, gabriel kreiman & david cox\nharvard university\ncambridge, ma 02215, usa\n{lotter,davidcox}@fas.harvard.edu\ngabriel.kreiman@tch.harvard.edu\n\nabstract\n\nwhile great strides have been made in using deep learning algorithms to solve\nsupervised learning tasks, the problem of unsupervised learning \u2014 leveraging un-\nlabeled examples to learn about the structure of a domain \u2014 remains a dif\ufb01cult\nunsolved challenge. here, we explore prediction of future frames in a video se-\nquence as an unsupervised learning rule for learning about the structure of the\nvisual world. we describe a predictive neural network (\u201cprednet\u201d) architecture\nthat is inspired by the concept of \u201cpredictive coding\u201d from the neuroscience lit-\nerature. these networks learn to predict future frames in a vid", "brain-inspired learning on neuromorphic\n\n1\n\nsubstrates\n\nfriedemann zenke\u2020, emre o. neftci\u2020\n\n\u2020 all authors contributed equally\n\n0\n2\n0\n2\n\n \nt\nc\no\n2\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n1\n3\n9\n1\n1\n\n.\n\n0\n1\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\u2014neuromorphic hardware strives to emulate\nbrain-like neural networks and thus holds the promise for\nscalable, low-power information processing on temporal\ndata streams. yet, to solve real-world problems, these\nnetworks need to be trained. however, training on neuro-\nmorphic substrates creates signi\ufb01cant challenges due to the\nof\ufb02ine character and the required non-local computations\nof gradient-based learning algorithms. this article pro-\nvides a mathematical framework for the design of practical\nonline learning algorithms for neuromorphic substrates.\nspeci\ufb01cally, we show a direct connection between real-\ntime recurrent learning (rtrl), an online algorithm\nfor computing gradients in conventional recurrent neural\nnetworks (rnns), and biologically plausible learning rules", "vol 447 | 28 june 2007 | doi:10.1038/nature05860\n\nletters\n\nlateral habenula as a source of negative reward\nsignals in dopamine neurons\nmasayuki matsumoto1 & okihide hikosaka1\n\nmidbrain dopamine neurons are key components of the brain\u2019s\nreward system1, which is thought\nto guide reward-seeking\nbehaviours2\u20134. although recent studies have shown how dopamine\nneurons respond to rewards and sensory stimuli predicting\nreward1,5,6, it is unclear which parts of the brain provide dopamine\nneurons with signals necessary for these actions. here we show that\nthe primate lateral habenula, part of the structure called the epitha-\nlamus, is a major candidate for a source of negative reward-related\nsignals in dopamine neurons. we recorded the activity of habenula\nneurons and dopamine neurons while rhesus monkeys were per-\nforming a visually guided saccade task with positionally biased\nreward outcomes7. many habenula neurons were excited by a no-\nreward-predicting target and inhibited by a reward-predict", "3\n2\n0\n2\n\n \nt\nc\no\n0\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n0\n5\n3\n1\n\n.\n\n3\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nthe quantization model of neural scaling\n\neric j. michaud\u2217, ziming liu, uzay girit, and max tegmark\n\nmit & iaifi\n\nabstract\n\nwe propose the quantization model of neural scaling laws, explaining both the\nobserved power law dropoff of loss with model and data size, and also the sudden\nemergence of new capabilities with scale. we derive this model from what we call\nthe quantization hypothesis, where network knowledge and skills are \u201cquantized\u201d\ninto discrete chunks (quanta). we show that when quanta are learned in order of\ndecreasing use frequency, then a power law in use frequencies explains observed\npower law scaling of loss. we validate this prediction on toy datasets, then study\nhow scaling curves decompose for large language models. using language model\ngradients, we automatically decompose model behavior into a diverse set of skills\n(quanta). we tentatively find that the frequency at which these qu", "7\n1\n0\n2\n\n \nr\na\n\nm\n3\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n3\nv\n4\n2\n7\n1\n0\n\n.\n\n1\n0\n7\n1\n:\nv\ni\nx\nr\na\n\ndeepstack: expert-level arti\ufb01cial intelligence in\n\nheads-up no-limit poker\n\nmatej morav\u02c7c\u00b4\u0131k\u2660,\u2665,\u2020, martin schmid\u2660,\u2665,\u2020, neil burch\u2660, viliam lis\u00b4y\u2660,\u2663,\n\ndustin morrill\u2660, nolan bard\u2660, trevor davis\u2660,\n\nkevin waugh\u2660, michael johanson\u2660, michael bowling\u2660,\u2217\n\n\u2660department of computing science, university of alberta,\n\nedmonton, alberta, t6g2e8, canada\n\n\u2665department of applied mathematics, charles university,\n\n\u2663department of computer science, fee, czech technical university,\n\nprague, czech republic\n\nprague, czech republic\n\n\u2020these authors contributed equally to this work and are listed in alphabetical order.\n\u2217to whom correspondence should be addressed; e-mail: bowling@cs.ualberta.ca\n\narti\ufb01cial intelligence has seen several breakthroughs in recent years, with\ngames often serving as milestones. a common feature of these games is that\nplayers have perfect information. poker is the quintessential game of imperfect\ninfor", "0\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n1\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n2\n5\n1\n0\n\n.\n\n0\n1\n9\n1\n:\nv\ni\nx\nr\na\n\ngated linear networks\n\njoel veness * 1 tor lattimore * 1 david budden * 1 avishkar bhoopchand * 1 christopher mattern 1\n\nagnieszka grabska-barwinska 1 eren sezener 1 jianan wang 1 peter toth 1 simon schmitt 1 marcus hutter 1\n\nabstract\n\npaper\n\npresents\n\na\nneural\n\nnew family\n\nof\nthis\nbackpropagation-free\narchitectures,\ngated linear networks (glns). what dis-\ntinguishes glns from contemporary neural\nnetworks is the distributed and local nature of\ntheir credit assignment mechanism; each neuron\ndirectly predicts the target, forgoing the ability\nto learn feature representations in favor of rapid\nonline learning. individual neurons can model\nnonlinear functions via the use of data-dependent\ngating in conjunction with online convex opti-\nmization. we show that this architecture gives\nrise to universal learning capabilities in the limit,\nwith effective model capacity increasing as a\nfunction of network size", "feudal networks for hierarchical reinforcement learning\n\n7\n1\n0\n2\n\n \nr\na\n\nm\n6\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n2\nv\n1\n6\n1\n1\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nalexander sasha vezhnevets\nsimon osindero\ntom schaul\nnicolas heess\nmax jaderberg\ndavid silver\nkoray kavukcuoglu\ndeepmind\n\nabstract\n\nwe introduce feudal networks (funs): a novel\narchitecture for hierarchical reinforcement learn-\ning. our approach is inspired by the feudal rein-\nforcement learning proposal of dayan and hin-\nton, and gains power and ef\ufb01cacy by decou-\npling end-to-end learning across multiple levels\n\u2013 allowing it to utilise different resolutions of\ntime. our framework employs a manager mod-\nule and a worker module. the manager operates\nat a lower temporal resolution and sets abstract\ngoals which are conveyed to and enacted by the\nworker. the worker generates primitive actions\nat every tick of the environment. the decoupled\nstructure of fun conveys several bene\ufb01ts \u2013 in ad-\ndition to facilitating very long timescale credit\nassignment", "published as a conference paper at iclr 2019\n\nbayesian deep convolutional networks with\nmany channels are gaussian processes\n\njaehoon lee \u2021 \u2217 yasaman bahri \u2021 greg yang\u25e6,\n\nroman novak \u2020, lechao xiao \u2020 \u2217 ,\njiri hron(cid:5), daniel a. abola\ufb01a,\ngoogle brain, \u25e6microsoft research ai, (cid:5) department of engineering, university of cambridge\n{romann, xlc, jaehlee, yasamanb, \u25e6gregyang@microsoft.com,\n(cid:5)jh2084@cam.ac.uk, danabo, jpennin, jaschasd}@google.com\n\njascha sohl-dickstein\n\njeffrey pennington,\n\nabstract\n\nthere is a previously identi\ufb01ed equivalence between wide fully connected neural\nnetworks (fcns) and gaussian processes (gps). this equivalence enables, for\ninstance, test set predictions that would have resulted from a fully bayesian, in-\n\ufb01nitely wide trained fcn to be computed without ever instantiating the fcn, but\nby instead evaluating the corresponding gp. in this work, we derive an analogous\nequivalence for multi-layer convolutional neural networks (cnns) both with and\nwithout", "trade-off between curvature tuning and position\ninvariance in visual area v4\n\ntatyana o. sharpeea,b,1, minjoon kouha,b,c,2, and john h. reynoldsc\n\nacomputational and csystems neurobiology laboratories, salk institute for biological studies, la jolla, ca 92037; and bcenter for theoretical biological\nphysics, university of california, san diego, la jolla, ca 92093\n\nedited by charles gross, princeton university, princeton, nj, and approved may 30, 2013 (received for review october 9, 2012)\n\nhumans can rapidly recognize a multitude of objects despite dif-\nferences in their appearance. the neural mechanisms that endow\nhigh-level sensory neurons with both selectivity to complex stimu-\nlus features and \u201ctolerance\u201d or invariance to identity-preserving\ntransformations, such as spatial translation, remain poorly under-\nstood. previous studies have demonstrated that both tolerance and\nselectivity to conjunctions of features are increased at successive\nstages of the ventral visual stream that medi", "simpli\ufb01ed ppo-clip objective\n\njoshua achiam\n\njuly 30, 2018\n\nthe objective for ppo-clip is given by schulman et al. as\n\n(cid:20)\n\n(cid:18) \u03c0\u03b8(a|s)\n\n\u03c0\u03b8k(a|s)\n\nlclip\n\n\u03b8k\n\n(\u03b8)\n\n.\n= e\n\ns,a\u223c\u03b8k\n\nmin\n\na\u03b8k(s, a), clip\n\n(cid:18) \u03c0\u03b8(a|s)\n\n\u03c0\u03b8k(a|s)\n\n(cid:19)\n\n, 1 \u2212 \u0001, 1 + \u0001\n\n(cid:19)(cid:21)\n\na\u03b8k(s, a)\n\n,\n\nwhere \u03b8k are the parameters of the policy at iteration k and \u0001 is a small hyperparameter.\n\nproposition 1. the ppo-clip objective can be simpli\ufb01ed to\n\nlclip\n\n\u03b8k\n\n(\u03b8) = e\n\ns,a\u223c\u03b8k\n\nwhere\n\n(cid:20)\n\nmin\n\na\u03b8k(s, a), g(cid:0)\u0001, a\u03b8k(s, a)(cid:1)(cid:19)(cid:21)\n\n,\n\n(cid:18) \u03c0\u03b8(a|s)\n(cid:26) (1 + \u0001)a a \u2265 0\n\n\u03c0\u03b8k(a|s)\n\ng(\u0001, a) =\n\n(1 \u2212 \u0001)a otherwise\n\nproof. suppose \u0001 \u2208 (0, 1). de\ufb01ne\n\nf (r, a, \u0001)\n\n= min (ra, clip(r, 1 \u2212 \u0001, 1 + \u0001)a) .\n.\n\n1\n\n\f", "rapid communication\n\nhippocampus 00:00\u201300 (2015)\n\nstatistical learning of temporal community\n\nstructure in the hippocampus\n\nanna c. schapiro,* nicholas b. turk-browne,\nkenneth a. norman, and matthew m. botvinick\n\nabstract:\nthe hippocampus is involved in the learning and repre-\nsentation of temporal statistics, but little is understood about the kinds\nof statistics it can uncover. prior studies have tested various forms of\nstructure that can be learned by tracking the strength of transition prob-\nabilities between adjacent items in a sequence. we test whether the\nhippocampus can learn higher-order structure using sequences that\nhave no variance in transition probability and instead exhibit temporal\ncommunity structure. we \ufb01nd that the hippocampus is indeed sensitive\nto this form of structure, as revealed by its representations, activity\ndynamics, and connectivity with other regions. these \ufb01ndings suggest\nthat the hippocampus is a sophisticated learner of environmental regu-\nlarities, ab", "predictive information in a sensory population\n\nstephanie e. palmera,b, olivier marrec,d, michael j. berry iic,d, and william bialeka,b,1\n\najoseph henry laboratories of physics and blewis\u2013sigler institute for integrative genomics, and cdepartment of molecular biology and dprinceton\nneuroscience institute, princeton university, princeton, nj 08544\n\ncontributed by william bialek, april 13, 2015 (sent for review january 19, 2014)\n\nguiding behavior requires the brain to make predictions about the\nfuture values of sensory inputs. here, we show that efficient pre-\ndictive computation starts at the earliest stages of the visual system.\nwe compute how much information groups of retinal ganglion cells\ncarry about the future state of their visual inputs and show that\nnearly every cell in the retina participates in a group of cells for\nwhich this predictive information is close to the physical limit set by\nthe statistical structure of the inputs themselves. groups of cells in\nthe retina carry inf", "ieee transactions on evolutionary computation, vol. 11, no. 2, april 2007\n\n265\n\nintrinsic motivation systems for autonomous\n\nmental development\n\npierre-yves oudeyer, fr\u00e9d\u00e9ric kaplan, and verena v. hafner\n\nabstract\u2014exploratory activities seem to be intrinsically re-\nwarding for children and crucial for their cognitive development.\ncan a machine be endowed with such an intrinsic motivation\nsystem? this is the question we study in this paper, presenting a\nnumber of computational systems that try to capture this drive\ntowards novel or curious situations. after discussing related\nresearch coming from developmental psychology, neuroscience,\ndevelopmental robotics, and active learning, this paper presents\nthe mechanism of intelligent adaptive curiosity, an intrinsic\nmotivation system which pushes a robot towards situations in\nwhich it maximizes its learning progress. this drive makes the\nrobot focus on situations which are neither too predictable nor too\nunpredictable, thus permitting autonom", "predictive coding of dynamical variables in balanced\nspiking networks\n\nmartin boerlin1, christian k. machens2, sophie dene` ve1*\n1 group for neural theory, de\u00b4partement d\u2019e\u00b4tudes cognitives, e\u00b4cole normale supe\u00b4rieure, paris, france, 2 champalimaud neuroscience programme, champalimaud centre\nfor the unknown, lisbon, portugal\n\nabstract\n\ntwo observations about the cortex have puzzled neuroscientists for a long time. first, neural responses are highly variable.\nsecond, the level of excitation and inhibition received by each neuron is tightly balanced at all times. here, we demonstrate\nthat both properties are necessary consequences of neural networks that represent information efficiently in their spikes.\nwe illustrate this insight with spiking networks that represent dynamical variables. our approach is based on two\nassumptions: we assume that information about dynamical variables can be read out linearly from neural spike trains, and\nwe assume that neurons only fire a spike if that impr", "a task-optimized neural network replicates human\nauditory behavior, predicts brain responses, and\nreveals a cortical processing hierarchy\n\narticle\n\nhighlights\nd a deep neural network optimized for speech and music tasks\n\nperformed as well as human listeners\n\nd the optimization produced separate music and speech\n\npathways after a shared front end\n\nd the network made human-like error patterns and predicted\n\nauditory cortical responses\n\nd network predictions suggest hierarchical organization in\n\nhuman auditory cortex\n\nauthors\n\nalexander j.e. kell, daniel l.k. yamins,\nerica n. shook,\nsam v. norman-haignere,\njosh h. mcdermott\n\ncorrespondence\nalexkell@mit.edu (a.j.e.k.),\njhm@mit.edu (j.h.m.)\n\nin brief\nkell et al. show that a deep neural network\noptimized to recognize speech and music\nreplicated human auditory behavior and\npredicted cortical fmri responses.\ndifferent network layers best predict\nprimary and non-primary voxels,\nrevealing hierarchical organization in\nhuman auditory cortex.\n\nkell", "5\n1\n0\n2\n\n \nr\np\na\n3\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n6\n0\n8\n6\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\naccepted as a workshop contribution at iclr 2015\n\nstriving for simplicity:\nthe all convolutional net\n\njost tobias springenberg\u2217, alexey dosovitskiy\u2217, thomas brox, martin riedmiller\ndepartment of computer science\nuniversity of freiburg\nfreiburg, 79110, germany\n{springj, dosovits, brox, riedmiller}@cs.uni-freiburg.de\n\nabstract\n\nmost modern convolutional neural networks (cnns) used for object recognition\nare built using the same principles: alternating convolution and max-pooling lay-\ners followed by a small number of fully connected layers. we re-evaluate the state\nof the art for object recognition from small images with convolutional networks,\nquestioning the necessity of different components in the pipeline. we \ufb01nd that\nmax-pooling can simply be replaced by a convolutional layer with increased stride\nwithout loss in accuracy on several image recognition benchmarks. following\nthis \ufb01nding \u2013 and building", "research\n\nneuroscience\n\nring attractor dynamics in the\ndrosophila central brain\n\nsung soo kim,* herv\u00e9 rouault,* shaul druckmann,\u2020 vivek jayaraman\u2020\n\nring attractors are a class of recurrent networks hypothesized to underlie the representation\nof heading direction. such network structures, schematized as a ring of neurons whose\nconnectivity depends on their heading preferences, can sustain a bump-like activity pattern\nwhose location can be updated by continuous shifts along either turn direction. we recently\nreported that a population of fly neurons represents the animal\u2019s heading via bump-like\nactivity dynamics. we combined two-photon calcium imaging in head-fixed flying flies with\noptogenetics to overwrite the existing population representation with an artificial one, which\nwas then maintained by the circuit with naturalistic dynamics. a network with local\nexcitation and global inhibition enforces this unique and persistent heading representation.\nring attractor networks have long been", "variational learning for recurrent spiking networks\n\ndanilo jimenez rezende\n\nbrain mind institute\n\n\u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\n1015 lausanne epfl, switzerland\ndanilo.rezende@epfl.ch\n\nschool of computer and communication sciences, brain mind institute\n\ndaan wierstra\n\n\u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\n1015 lausanne epfl, switzerland\n\ndaan.wierstra@epfl.ch\n\nschool of computer and communication sciences, brain mind institute\n\nwulfram gerstner\n\n\u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\n1015 lausanne epfl, switzerland\nwulfram.gerstner@epfl.ch\n\nabstract\n\nwe derive a plausible learning rule for feedforward, feedback and lateral connec-\ntions in a recurrent network of spiking neurons. operating in the context of a\ngenerative model for distributions of spike sequences, the learning mechanism is\nderived from variational inference principles. the synaptic plasticity rules found\nare interesting in that they are strongly reminiscent of experimental spike time\ndependent", "0\n2\n0\n2\n\n \n\ng\nu\na\n1\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n8\n1\n8\n0\n\n.\n\n8\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nprevalence of neural collapse during the terminal\nphase of deep learning training\n\nvardan papyana,1, x.y. hanb,1, and david l. donohoa,2\nadepartment of statistics, stanford university; bschool of operations research and information engineering, cornell university\n\nthis manuscript was compiled on august 24, 2020\n\nmodern practice for training classi\ufb01cation deepnets involves a termi-\nnal phase of training (tpt), which begins at the epoch where train-\ning error \ufb01rst vanishes; during tpt, the training error stays effec-\ntively zero while training loss is pushed towards zero. direct mea-\nsurements of tpt, for three prototypical deepnet architectures and\nacross seven canonical classi\ufb01cation datasets, expose a pervasive\ninductive bias we call neural collapse, involving four deeply intercon-\nnected phenomena: (nc1) cross-example within-class variability of\nlast-layer training activations collapses to zero, ", "research article\n\none-shot learning and behavioral\neligibility traces in sequential decision\nmaking\nmarco p lehmann1,2*, he a xu3, vasiliki liakoni1,2, michael h herzog3\u2020,\nwulfram gerstner1,2\u2020, kerstin preuschoff4\u2020\n\n1brain-mind-institute, school of life sciences, e\u00b4 cole polytechnique fe\u00b4 de\u00b4 rale de\nlausanne, lausanne, switzerland; 2school of computer and communication\nsciences, e\u00b4 cole polytechnique fe\u00b4 de\u00b4 rale de lausanne, lausanne, switzerland;\n3laboratory of psychophysics, school of life sciences, e\u00b4 cole polytechnique fe\u00b4 de\u00b4 rale\nde lausanne, lausanne, switzerland; 4swiss center for affective sciences, university\nof geneva, geneva, switzerland\n\nabstract in many daily tasks, we make multiple decisions before reaching a goal. in order to\nlearn such sequences of decisions, a mechanism to link earlier actions to later reward is necessary.\nreinforcement learning (rl) theory suggests two classes of algorithms solving this credit\nassignment problem: in classic temporal-difference lear", "ensembles of spiking neurons with noise support\noptimal probabilistic inference in a dynamically\nchanging environment\n\nrobert legenstein*, wolfgang maass\n\ninstitute for theoretical computer science, graz university of technology, graz, austria\n\nabstract\n\nit has recently been shown that networks of spiking neurons with noise can emulate simple forms of probabilistic inference\nthrough \u2018\u2018neural sampling\u2019\u2019, i.e., by treating spikes as samples from a probability distribution of network states that is\nencoded in the network. deficiencies of the existing model are its reliance on single neurons for sampling from each\nrandom variable, and the resulting limitation in representing quickly varying probabilistic information. we show that both\ndeficiencies can be overcome by moving to a biologically more realistic encoding of each salient random variable through\nthe stochastic firing activity of an ensemble of neurons. the resulting model demonstrates that networks of spiking neurons\nwith noise can", "neuroresource\n\nwhole-neuron synaptic mapping reveals spatially\nprecise excitatory/inhibitory balance limiting\ndendritic and somatic spiking\n\nhighlights\nd open source software allows mapping of synapses across\n\nwhole neurons\n\nd annotated database of >90,000 synapses reveals several\n\nscales of synaptic organization\n\nd analysis reveals local balance between e and i synapses in\n\nspeci\ufb01c dendritic domains\n\nauthors\n\ndaniel maxim iascone, yujie li,\nuygar s\u20acumb\u20acul, ..., idan segev,\nhanchuan peng, franck polleux\n\ncorrespondence\nidan.segev@mail.huji.ac.il (i.s.),\nhanchuan.peng@gmail.com (h.p.),\nfp2304@columbia.edu (f.p.)\n\nd computational modeling shows that local e/i balance\n\nrestricts dendritic and somatic \ufb01ring\n\nin brief\niascone et al. developed an open source\nsoftware within vaa3d to map the\ndistribution and morphology of synapses\nacross the dendritic arbor of mouse layer\n2/3 cortical pns. reconstruction of\n>90,000 synapses and computational\nmodeling reveal several scales of\norganization, inc", "7\n1\n0\n2\n\n \nc\ne\nd\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n0\n1\n3\n0\n0\n\n.\n\n2\n1\n7\n1\n:\nv\ni\nx\nr\na\n\ndeep learning with permutation-invariant operator\n\nfor multi-instance histopathology classi\ufb01cation\n\njakub m. tomczak\n\nuniversity of amsterdam\n\nmaximilian ilse\n\nuniversity of amsterdam\n\nmax welling\n\nuniversity of amsterdam\n\nabstract\n\nthe computer-aided analysis of medical scans is a longstanding goal in the medical\nimaging \ufb01eld. currently, deep learning has became a dominant methodology\nfor supporting pathologists and radiologist. deep learning algorithms have been\nsuccessfully applied to digital pathology and radiology, nevertheless, there are\nstill practical issues that prevent these tools to be widely used in practice. the\nmain obstacles are low number of available cases and large size of images (a.k.a.\nthe small n, large p problem in machine learning), and a very limited access to\nannotation at a pixel level that can lead to severe over\ufb01tting and large computational\nrequirements. we propose to hand", "published as a conference paper at iclr 2017\n\ndeep variational information bottleneck\n\nalexander a. alemi, ian fischer, joshua v. dillon, kevin murphy\ngoogle research\n{alemi,iansf,jvdillon,kpmurphy}@google.com\n\nabstract\n\nwe present a variational approximation to the information bottleneck of tishby\net al. (1999). this variational approach allows us to parameterize the informa-\ntion bottleneck model using a neural network and leverage the reparameterization\ntrick for ef\ufb01cient training. we call this method \u201cdeep variational information\nbottleneck\u201d, or deep vib. we show that models trained with the vib objective\noutperform those that are trained with other forms of regularization, in terms of\ngeneralization performance and robustness to adversarial attack.\n\n1\n\nintroduction\n\n(cid:90)\n\nwe adopt an information theoretic view of deep networks. we regard the internal representation of\nsome intermediate layer as a stochastic encoding z of the input source x, de\ufb01ned by a parametric\nencoder p(z|x", "biorxiv preprint \nthe copyright holder for this preprint (which was\nnot certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available \n\nthis version posted november 12, 2017. \n\nhttps://doi.org/10.1101/214262\n; \n\ndoi: \n\nunder a\n\ncc-by-nc-nd 4.0 international license\n.\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\na theory of multineuronal dimensionality, dynamics and\n\nmeasurement\n\npeiran gao1, eric trautmann2, byron yu3, gopal santhanam4, stephen ryu5,4, krishna\n\nshenoy4,1,6,7,8,9 and surya ganguli\u221710,2,4,8,9\n\n1department of bioengineering, stanford university, stanford, ca 94305, usa\n\n2neurosciences program, school of medicine, stanford university, stanford, ca 94305,\n\nusa\n\n3department of electrical computer engineering and biomedical engineering, carnegie\n\nmellon university, pittsburgh, pa 15213, usa\n\n4department of electrical engineering, s", "3\n2\n0\n2\n\n \nt\nc\no\n4\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n6\n4\n0\n6\n1\n\n.\n\n0\n1\n3\n2\n:\nv\ni\nx\nr\na\n\na unified, scalable framework for\n\nneural population decoding\n\nmehdi azabou1,\u2217, vinam arora1, venkataramana ganesh1, ximeng mao2,3 ,\n\nsantosh nachimuthu1, michael j. mendelson1, blake richards2,4,\n\nmatthew g. perich2,3, guillaume lajoie2,3, eva l. dyer1,\u2217\n\n1 georgia tech, 2 mila, 3 universit\u00e9 de montr\u00e9al, 4 mcgill university\n\nabstract\n\nour ability to use deep learning approaches to decipher neural activity would likely\nbenefit from greater scale, in terms of both model size and datasets. however, the\nintegration of many neural recordings into one unified model is challenging, as\neach recording contains the activity of different neurons from different individual\nanimals. in this paper, we introduce a training framework and architecture designed\nto model the population dynamics of neural activity across diverse, large-scale\nneural recordings. our method first tokenizes individual spikes within the dat", "letter\n\ncommunicated by mark van rossum\n\nidenti\ufb01cation of stable spike-timing-dependent\nplasticity from spiking activity with generalized\nmultilinear modeling\n\nbrian s. robinson\nbsrobins@usc.edu\ntheodore w. berger\nberger@usc.edu\ndong song\ndsong@usc.edu\ndepartment of biomedical engineering, university of southern california,\nlos angeles, ca 90089, u.s.a.\n\ncharacterization of long-term activity-dependent plasticity from behav-\niorally driven spiking activity is important for understanding the under-\nlying mechanisms of learning and memory. in this letter, we present a\ncomputational framework for quantifying spike-timing-dependent plas-\nticity (stdp) during behavior by identifying a functional plasticity rule\nsolely from spiking activity. first, we formulate a \ufb02exible point-process\nspiking neuron model structure with stdp, which includes functions\nthat characterize the stationary and plastic properties of the neuron. the\nstdp model includes a novel function for prolonged plasticity induct", "neuron, vol. 22, 435\u2013450, march, 1999, copyright \u00aa 1999 by cell press\n\nthe neural code of the retina\n\nreview\n\nmarkus meister* and michael j. berry ii\ndepartment of molecular and cellular biology\nharvard university\ncambridge, massachusetts 02138\n\nintroduction\nhow do action potentials represent information?\naction potentials are the standard signal conveyed be-\ntween neurons in the central nervous system. it is a long-\nstanding question how these spikes represent sensory\ninput, internal states of the brain, or motor commands\n(perkel and bullock, 1968; rieke et al., 1997). to fully\nunderstand communication among neurons, one would\nlike to obtain a dictionary for this language, in which\neach spike or pattern of spikes is assigned meaning\nwithin the processing task under study. this review will\nfocus on the neural code employed by the ganglion cells\nof the vertebrate retina in conveying visual information\nfrom the eye to the brain: what are the rules by which\nthe spike trains of optic nerve", "longshort-termmemoryandlearning-to-learninnetworksofspikingneuronsguillaumebellec*,darjansalaj*,anandsubramoney*,robertlegenstein&wolfgangmaassinstitutefortheoreticalcomputersciencegrazuniversityoftechnology,austria{bellec,salaj,subramoney,legenstein,maass}@igi.tugraz.at*equalcontributionsabstractrecurrentnetworksofspikingneurons(rsnns)underlietheastoundingcomput-ingandlearningcapabilitiesofthebrain.butcomputingandlearningcapabilitiesofrsnnmodelshaveremainedpoor,atleastincomparisonwitharti\ufb01cialneuralnetworks(anns).weaddresstwopossiblereasonsforthat.oneisthatrsnnsinthebrainarenotrandomlyconnectedordesignedaccordingtosimplerules,andtheydonotstartlearningasatabularasanetwork.rather,rsnnsinthebrainwereoptimizedfortheirtasksthroughevolution,development,andpriorexperience.detailsoftheseoptimizationprocessesarelargelyunknown.buttheirfunctionalcontributioncanbeapproximatedthroughpowerfuloptimizationmethods,suchasbackpropagationthroughtime(bptt).asecondmajormismatchbetweenrsnnsinthebrainandmode", "journal of the experimental analysis of behavior\n\n2005, 84, 555\u2013579\n\nnumber 3 (november)\n\ndynamic response-by-response models of matching behavior in rhesus monkeys\n\nbrian lau and paul w. glimcher\n\nnew york university\n\nwe studied the choice behavior of 2 monkeys in a discrete-trial task with reinforcement contingencies\nsimilar to those herrnstein (1961) used when he described the matching law. in each session, the\nmonkeys experienced blocks of discrete trials at different relative-reinforcer frequencies or magnitudes\nwith unsignalled transitions between the blocks. steady-state data following adjustment to each\ntransition were well characterized by the generalized matching law; response ratios undermatched\nreinforcer frequency ratios but matched reinforcer magnitude ratios. we modelled response-by-\nresponse behavior with linear models that used past reinforcers as well as past choices to predict the\nmonkeys\u2019 choices on each trial. we found that more recently obtained reinforcers more s", "i an update to this article is included at the end\n\nll\n\nprimer\narti\ufb01cial neural networks\nfor neuroscientists: a primer\n\nguangyu robert yang1,* and xiao-jing wang2,*\n1center for theoretical neuroscience, columbia university, new york, ny, usa\n2center for neural science, new york university, new york, ny, usa\n*correspondence: robert.yang@columbia.edu (g.r.y.), xjwang@nyu.edu (x.-j.w.)\nhttps://doi.org/10.1016/j.neuron.2020.09.005\n\nsummary\n\narti\ufb01cial neural networks (anns) are essential tools in machine learning that have drawn increasing attention\nin neuroscience. besides offering powerful techniques for data analysis, anns provide a new approach for\nneuroscientists to build models for complex behaviors, heterogeneous neural activity, and circuit connectiv-\nity, as well as to explore optimization in neural systems, in ways that traditional models are not designed for.\nin this pedagogical primer, we introduce anns and demonstrate how they have been fruitfully deployed to\nstudy neuroscienti", "8\n1\n0\n2\n\n \nt\nc\no\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n6\n9\n2\n8\n0\n\n.\n\n5\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ndata-ef\ufb01cient hierarchical reinforcement learning\n\no\ufb01r nachum\ngoogle brain\n\nofirnachum@google.com\n\nhonglak lee\ngoogle brain\n\nhonglak@google.com\n\nshixiang gu\u2217\ngoogle brain\n\nshanegu@google.com\n\nsergey levine\u2020\ngoogle brain\n\nslevine@google.com\n\nabstract\n\nhierarchical reinforcement learning (hrl) is a promising approach to extend\ntraditional reinforcement learning (rl) methods to solve more complex tasks.\nyet, the majority of current hrl methods require careful task-speci\ufb01c design and\non-policy training, making them dif\ufb01cult to apply in real-world scenarios. in this\npaper, we study how we can develop hrl algorithms that are general, in that they\ndo not make onerous additional assumptions beyond standard rl algorithms, and\nef\ufb01cient, in the sense that they can be used with modest numbers of interaction\nsamples, making them suitable for real-world problems such as robotic control. for\ngenerality, we develop a", "backpropagation through time: what it \ndoes and how to do it \n\npaul  j. werbos \n\nbackpropagation  is  now the most  widely  used tool in the field \nof artificial neural  networks.  at  the core of backpropagation is  a \nmethod  for  calculating derivatives  exactly  and  efficiently  in any \nlarge  system made  up of elementary  subsystems or  calculations \nwhich are represented  by  known,  differentiable  functions;  thus, \nbackpropagation  has  many  applications  which  do  not  involve \nneural networks  as  such. \n\nthis paper first reviews basic backpropagation,  a simple method \nwhich is  now being widely used in areas like pattern recognition \nand fault diagnosis. next, it presents the basic equations  for back- \npropagation through time, and discusses applications to areas like \npattern  recognition  involving dynamic  systems,  systems  identifi- \ncation, and control. finally, it describes further extensions  of this \nmethod, to deal with systems other than neural networks, sy", "an anatomically constrained model for path\nintegration in the bee brain\n\narticle\n\nhighlights\nd central complex neuroarchitecture suf\ufb01ces as a path\n\nintegration circuit\n\nd compass heading and forward speed information converge\n\nin the bee central complex\n\nd columnar noduli neurons plausibly encode a distributed\n\nmemory of the home vector\n\nd the circuit additionally compares current and desired\n\nheadings to initiate steering\n\nauthors\n\nthomas stone, barbara webb,\nandrea adden, ..., luca scimeca,\neric warrant, stanley heinze\n\ncorrespondence\nstanley.heinze@biol.lu.se\n\nin brief\nstone et al. present the \ufb01rst fully\nbiologically constrained model of path\nintegration in the insect brain. this\ncombines newly identi\ufb01ed speed and\ncompass neuron convergence with\nknown anatomical features of the central\ncomplex and provides a novel functional\ninterpretation of this neuropil\u2019s role\nacross all orientation behaviors.\n\nstone et al., 2017, current biology 27, 3069\u20133085\noctober 23, 2017 \u00aa 2017 elsevier ltd", "deep learning via hessian-free optimization\n\njames martens\nuniversity of toronto, ontario, m5s 1a1, canada\n\njmartens@cs.toronto.edu\n\nabstract\n\nwe develop a 2nd-order optimization method\nbased on the \u201chessian-free\u201d approach, and apply\nit to training deep auto-encoders. without using\npre-training, we obtain results superior to those\nreported by hinton & salakhutdinov (2006) on\nthe same tasks they considered. our method is\npractical, easy to use, scales nicely to very large\ndatasets, and isn\u2019t limited in applicability to auto-\nencoders, or any speci\ufb01c model class. we also\ndiscuss the issue of \u201cpathological curvature\u201d as\na possible explanation for the dif\ufb01culty of deep-\nlearning and how 2nd-order optimization, and our\nmethod in particular, effectively deals with it.\n\n1. introduction\nlearning the parameters of neural networks is perhaps one\nof the most well studied problems within the \ufb01eld of ma-\nchine learning. early work on backpropagation algorithms\nshowed that the gradient of the neural", "improved variance-aware con\ufb01dence sets for\n\nlinear bandits and linear mixture mdp\n\nzihan zhang\u02da\ntsinghua university\n\nzihan-zh17@mails.tsinghua.edu.cn\n\njiaqi yang\u02da\n\ntsinghua university\n\nyangjq17@gmail.com\n\nxiangyang ji\n\ntsinghua university\n\nxyji@tsinghua.edu.cn\n\nsimon s. du\n\nuniversity of washington\n\nssdu@cs.washington.edu\n\nabstract\n\n\u0159\n\nk\nk\u201c1 \u03c32\n\nb\n1 `\n\u02d8\n\nthis paper presents new variance-aware con\ufb01dence sets for linear bandits and\nlinear mixture markov decision processes (mdps). with the new con\ufb01dence sets,\nwe obtain the follow regret bounds:\n\nkq data-dependent\nregret bound, where d is the feature dimension, k is the number of rounds,\nk is the unknown variance of the reward at the k-th round. this is the\nand \u03c32\n\ufb01rst regret bound that only scales with the variance and the dimension but no\nexplicit polynomial dependency on k. when variances are small, this bound\n\n\u2022 for linear bandits, we obtain an roppolypdq\n`\ncan be signi\ufb01cantly smaller than ther\u03b8\n\u2022 for linear mixture mdps, we obtain an ", "6\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n5\n1\n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\nv\n5\n9\n6\n4\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nstrategic attentive writer for learning\n\nmacro-actions\n\nalexander (sasha) vezhnevets, volodymyr mnih, john agapiou,\nsimon osindero, alex graves, oriol vinyals, koray kavukcuoglu\n\ngoogle deepmind\n\n{vezhnick,vmnih,jagapiou,osindero,gravesa,vinyals,korayk}@google.com\n\nabstract\n\nwe present a novel deep recurrent neural network architecture that learns to build\nimplicit plans in an end-to-end manner by purely interacting with an environment\nin reinforcement learning setting. the network builds an internal plan, which is\ncontinuously updated upon observation of the next input from the environment.\nit can also partition this internal representation into contiguous sub- sequences\nby learning for how long the plan can be committed to \u2013 i.e. followed without\nre-planing. combining these properties, the proposed model, dubbed strategic at-\ntentive writer (straw) can learn high-level, temporally abstracted macr", "neural system identi\ufb01cation for large populations\n\nseparating \u201cwhat\u201d and \u201cwhere\u201d\n\ndavid a. klindt * 1-3, alexander s. ecker * 1,2,4,6, thomas euler 1-3, matthias bethge 1,2,4-6\n\n* authors contributed equally\n\n1 centre for integrative neuroscience, university of t\u00fcbingen, germany\n\n2 bernstein center for computational neuroscience, university of t\u00fcbingen, germany\n\n3 institute for ophthalmic research, university of t\u00fcbingen, germany\n4 institute for theoretical physics, university of t\u00fcbingen, germany\n5 max planck institute for biological cybernetics, t\u00fcbingen, germany\n\n6 center for neuroscience and arti\ufb01cial intelligence, baylor college of medicine, houston, usa\n\nklindt.david@gmail.com, alexander.ecker@uni-tuebingen.de,\n\nthomas.euler@cin.uni-tuebingen.de, matthias.bethge@bethgelab.org\n\nabstract\n\nneuroscientists classify neurons into different types that perform similar compu-\ntations at different locations in the visual \ufb01eld. traditional methods for neural\nsystem identi\ufb01cation do not capi", "addressing function approximation error in actor-critic methods\n\n8\n1\n0\n2\n\n \nt\nc\no\n2\n2\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n3\nv\n7\n7\n4\n9\n0\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nscott fujimoto 1 herke van hoof 2 david meger 1\n\nabstract\n\nin value-based reinforcement learning methods\nsuch as deep q-learning, function approximation\nerrors are known to lead to overestimated value\nestimates and suboptimal policies. we show that\nthis problem persists in an actor-critic setting and\npropose novel mechanisms to minimize its effects\non both the actor and the critic. our algorithm\nbuilds on double q-learning, by taking the mini-\nmum value between a pair of critics to limit over-\nestimation. we draw the connection between tar-\nget networks and overestimation bias, and suggest\ndelaying policy updates to reduce per-update error\nand further improve performance. we evaluate\nour method on the suite of openai gym tasks,\noutperforming the state of the art in every envi-\nronment tested.\n\n1. introduction\nin reinforcement learning ", "clique topology reveals intrinsic geometric structure in\nneural correlations\n\nchad giustia,b, eva pastalkovac, carina curtob,d,1, and vladimir itskovb,d,1,2\n\nawarren center for network and data science, departments of bioengineering and mathematics, university of pennsylvania, philadelphia, pa 19104;\nbdepartment of mathematics, university of nebraska, lincoln, ne 68588; cjanelia research campus, howard hughes medical institute, ashburn, va\n20147; and ddepartment of mathematics, the pennsylvania state university, university park, pa 16802\n\nedited by william bialek, princeton university, princeton, nj, and approved september 23, 2015 (received for review april 28, 2015)\n\ndetecting meaningful structure in neural activity and connectiv-\nity data is challenging in the presence of hidden nonlinearities,\nwhere traditional eigenvalue-based methods may be misleading.\nwe introduce a novel approach to matrix analysis, called clique\ntopology, that extracts features of the data invariant under non-", "", "macroscopic gradients of synaptic \nexcitation and inhibition in the \nneocortex\n\nxiao- jing\u00a0wang \n\n \n\nabstract | with advances in connectomics, transcriptome and neurophysiological \ntechnologies, the neuroscience of brain- wide neural circuits is poised to take off. \na\u00a0major challenge is to understand how a vast diversity of functions is subserved by \nparcellated areas of mammalian neocortex composed of repetitions of a canonical \nlocal circuit. areas of the cerebral cortex differ from each other not only in their \ninput\u2013output patterns but also in their biological properties. recent experimental \nand theoretical work has revealed that such variations are not random \nheterogeneities; rather, synaptic excitation and inhibition display systematic \nmacroscopic gradients across the entire cortex, and they are abnormal in mental \nillness. quantitative differences along these gradients can lead to qualitatively \nnovel behaviours in non- linear neural dynamical systems, by virtue of a phenomen", "perspective\n\nhttps://doi.org/10.1038/s41467-019-11786-6\n\nopen\n\na critique of pure learning and what arti\ufb01cial neural\nnetworks can learn from animal brains\nanthony m. zador1\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\narti\ufb01cial neural networks (anns) have undergone a revolution, catalyzed by better super-\nvised learning algorithms. however, in stark contrast to young animals (including humans),\ntraining such networks requires enormous numbers of labeled examples, leading to the belief\nthat animals must rely instead mainly on unsupervised learning. here we argue that most\nanimal behavior is not the result of clever learning algorithms\u2014supervised or unsupervised\u2014\nbut is encoded in the genome. speci\ufb01cally, animals are born with highly structured brain\nconnectivity, which enables them to learn very rapidly. because the wiring diagram is far too\ncomplex to be speci\ufb01ed explicitly in the genome, it must be compressed through a \u201cgenomic\nbottleneck\u201d. the genomic bottleneck suggests a path toward anns capabl", "model-agnostic meta-learning for fast adaptation of deep networks\n\n7\n1\n0\n2\n\n \nl\nu\nj\n \n\n8\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n0\n4\n3\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nchelsea finn 1 pieter abbeel 1 2 sergey levine 1\n\nabstract\n\nwe propose an algorithm for meta-learning that\nis model-agnostic, in the sense that it is com-\npatible with any model trained with gradient de-\nscent and applicable to a variety of different\nlearning problems, including classi\ufb01cation, re-\ngression, and reinforcement learning. the goal\nof meta-learning is to train a model on a vari-\nety of learning tasks, such that it can solve new\nlearning tasks using only a small number of train-\ning samples. in our approach, the parameters of\nthe model are explicitly trained such that a small\nnumber of gradient steps with a small amount\nof training data from a new task will produce\ngood generalization performance on that task. in\neffect, our method trains the model to be easy\nto \ufb01ne-tune. we demonstrate that this approach\nleads to state-of", "small stimuli, barely covering the neurons\u2019 receptive\nfields, are used.\n\n29. a. renart et al., science 327, 587 (2010).\n30. v. braitenberg, a. sch\u00fcz, anatomy of the cortex:\nstatistics and geometry (springer, berlin, 1991).\n31. b. hellwig, a. sch\u00fcz, a. aertsen, biol. cybern. 71,\n\n1 (1994).\na. reyes, nature 448, 802 (2007).\n\n32. j. de la rocha, b. doiron, e. shea-brown, k. josi\u0107,\n33. y. c. yu, r. s. bultje, x. wang, s. h. shi, nature 458,\n34. j. hertz, neural comput., published online 20 october\n\n501 (2009).\n\n2009; 10.1162/neco.2009.06-08-806.\n\nreports\n\n35. we thank m. subramanian, a. hoenselaar, t. j. williford,\nand d. murray for help with experiments; r. j. cotton for\nhis contribution in setting up recording equipment; and\nthe siapas laboratory at caltech for providing the\nrecording software. we thank r. j. cotton, p. dayan,\ns. deneve, k. d. harris, e. v. lubenov, w. j. ma,\nj. h. macke, a. renart, j. de la rocha, a. g. siapas, and\ns. m. smirnakis for comments on the manuscript and\ndisc", "online decision transformer\n\nqinqing zheng 1 amy zhang 1 2 aditya grover 1 3\n\nabstract\n\nrecent work has shown that offline reinforcement\nlearning (rl) can be formulated as a sequence\nmodeling problem (chen et al., 2021; janner et al.,\n2021) and solved via approaches similar to large-\nscale language modeling. however, any practical\ninstantiation of rl also involves an online compo-\nnent, where policies pretrained on passive offline\ndatasets are finetuned via task-specific interac-\ntions with the environment. we propose online\ndecision transformers (odt), an rl algorithm\nbased on sequence modeling that blends offline\npretraining with online finetuning in a unified\nframework. our framework uses sequence-level\nentropy regularizers in conjunction with autore-\ngressive modeling objectives for sample-efficient\nexploration and finetuning. empirically, we show\nthat odt is competitive with the state-of-the-art\nin absolute performance on the d4rl benchmark\nbut shows much more significant gains du", "extracting latent structure from multiple\n\ninteracting neural populations\n\njo\u02dcao d. semedo1,2,3, amin zandvakili4, adam kohn4,\n\n\u2217christian k. machens3, \u2217byron m. yu1,5\n\n1department of electrical and computer engineering, carnegie mellon university\n2department of electrical and computer engineering, instituto superior t\u00b4ecnico\n\n3champalimaud neuroscience programme, champalimaud center for the unknown\n4dominick purpura department of neuroscience, albert einstein college of medicine\n\n5department of biomedical engineering, carnegie mellon university\n\njsemedo@cmu.edu\nchristian.machens@neuro.fchampalimaud.org\n\n{amin.zandvakili,adam.kohn}@einstein.yu.edu\nbyronyu@cmu.edu\n\n\u2217 denotes equal contribution.\n\nabstract\n\ndevelopments in neural recording technology are rapidly enabling the record-\ning of populations of neurons in multiple brain areas simultaneously, as well as\nthe identi\ufb01cation of the types of neurons being recorded (e.g., excitatory vs. in-\nhibitory). there is a growing need for statis", "rspb.royalsocietypublishing.org\n\nresearch\n\ncite this article: clune j, mouret j-b, lipson\nh. 2013 the evolutionary origins of modularity.\nproc r soc b 280: 20122863.\nhttp://dx.doi.org/10.1098/rspb.2012.2863\n\nreceived: 30 november 2012\naccepted: 9 january 2013\n\nsubject areas:\ncomputational biology, evolution,\nsystems biology\n\nkeywords:\nmodularity, evolution, networks, evolvability,\nsystems biology\n\nauthor for correspondence:\njeff clune\ne-mail: jclune@uwyo.edu\n\n\u2020these authors contributed equally to this\nwork.\n\nelectronic supplementary material is available\nat http://dx.doi.org/10.1098/rspb.2012.2863 or\nvia http://rspb.royalsocietypublishing.org.\n\nthe evolutionary origins of modularity\n\njeff clune1,2,\u2020, jean-baptiste mouret3,\u2020 and hod lipson1\n\n1cornell university, ithaca, ny, usa\n2university of wyoming, laramie, wy, usa\n3isir, universite\u00b4 pierre et marie curie-paris 6, cnrs umr 7222, paris, france\n\na central biological question is how natural organisms are so evolvable (capable\nof quickly", "appl. comput. harmon. anal. 14 (2003) 257\u2013275\n\nwww.elsevier.com/locate/acha\n\ngrassmannian frames with applications\n\nto coding and communication\n\nthomas strohmer a,\u2217,1 and robert w. heath jr. b\n\na department of mathematics, university of california, davis, ca 95616-8633, usa\n\nb department of electrical and computer engineering, the university of texas at austin, austin, tx 78727, usa\n\nreceived 19 july 2002; revised 28 february 2003; accepted 26 march 2003\n\ncommunicated by henrique malvar\n\nabstract\nfor a given class f of unit norm frames of \ufb01xed redundancy we de\ufb01ne a grassmannian frame as one that\nminimizes the maximal correlation |(cid:3)fk , fl(cid:4)| among all frames {fk}k\u2208i \u2208 f. we \ufb01rst analyze \ufb01nite-dimensional\ngrassmannian frames. using links to packings in grassmannian spaces and antipodal spherical codes we derive\nbounds on the minimal achievable correlation for grassmannian frames. these bounds yield a simple condition\nunder which grassmannian frames coincide with unit norm tig", "downloaded from \n\nhttp://cshperspectives.cshlp.org/\n\n at nyu med ctr library on september 5, 2023 - published by cold spring\n\nharbor laboratory press \n\nnmda receptor-dependent long-term\npotentiation and long-term depression\n(ltp/ltd)\n\nchristian lu\u00a8 scher1 and robert c. malenka2\n\n1department of basic neurosciences and clinic of neurology, university of geneva and geneva university\nhospital, 1211 geneva, switzerland\n2nancy pritzker laboratory, department of psychiatry and behavioral sciences, stanford university school of\nmedicine, palo alto, california 94305-5453\n\ncorrespondence: christian.luscher@unige.ch and malenka@stanford.edu\n\nlong-term potentiation and long-term depression (ltp/ltd) can be elicited by activating\nn-methyl-d-aspartate (nmda)-type glutamate receptors, typically by the coincident activity\nof pre- and postsynaptic neurons. the early phases of expression are mediated by a redistri-\nbution of ampa-type glutamate receptors: more receptors are added to potentiate the\nsynap", "6\n1\n0\n2\n\n \n\nn\na\nj\n \n\n6\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n6\nv\n4\n3\n0\n3\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nunder review as a conference paper at iclr 2016\n\nlearning with a strong adversary\n\nruitong huang, bing xu, dale schuurmans and csaba szepesv\u00b4ari\ndepartment of computer science\nuniversity of alberta\nedmonton, ab t6g 2e8, canada\n{ruitong,bx3,daes,szepesva}@ualberta.ca\n\nabstract\n\nthe robustness of neural networks to intended perturbations has recently attracted\nsigni\ufb01cant attention. in this paper, we propose a new method, learning with a\nstrong adversary, that learns robust classi\ufb01ers from supervised data by generating\nadversarial examples as an intermediate step. a new and simple way of \ufb01nd-\ning adversarial examples is presented that is empirically stronger than existing\napproaches in terms of the accuracy reduction as a function of perturbation mag-\nnitude. experimental results demonstrate that resulting learning method greatly\nimproves the robustness of the classi\ufb01cation models produced.\n\n1\n\nintrod", "r e v i e w\n\nthe distinct modes of vision offered by\nfeedforward and recurrent processing\n\nvictor a.f. lamme and pieter r. roelfsema\n\nan analysis of response latencies shows that when an image is presented to the visual system,\nneuronal activity is rapidly routed to a large number of visual areas.however,the activity of cortical\nneurons is not determined by this feedforward sweep alone. horizontal connections within areas,\nand higher areas providing feedback, result in dynamic changes in tuning.the differences between\nfeedforward  and  recurrent  processing  could  prove  pivotal  in  understanding  the  distinctions\nbetween attentive and pre-attentive vision as well as between conscious and unconscious vision.\nthe feedforward sweep rapidly groups feature constellations that are hardwired in the visual brain,\nyet  is  probably  incapable  of  yielding  visual  awareness; in  many  cases, recurrent  processing  is\nnecessary before the features of an object are attentively grouped and th", "deep physical neural networks trained with \nbackpropagation\n\nhttps://doi.org/10.1038/s41586-021-04223-6\nreceived: 19 may 2021\naccepted: 9 november 2021\npublished online: 26 january 2022\nopen access\n\n check for updates\n\nlogan g. wright1,2,4\u2009\u2709, tatsuhiro onodera1,2,4\u2009\u2709, martin m. stein1, tianyu wang1, \ndarren t. schachter3, zoey hu1 & peter l. mcmahon1\u2009\u2709\n\ndeep-learning models have become pervasive tools in science and engineering. \nhowever, their energy requirements now increasingly limit their scalability1. \ndeep-learning accelerators2\u20139 aim to perform deep learning energy-efficiently, usually \ntargeting the inference phase and often by exploiting physical substrates beyond \nconventional electronics. approaches so far10\u201322 have been unable to apply the \nbackpropagation algorithm to train unconventional novel hardware in\u00a0situ.  \nthe advantages of backpropagation have made it the\u00a0de facto\u00a0training method for \nlarge-scale neural networks, so this deficiency constitutes a major impediment. ", "biorxiv preprint \nthe copyright holder for this preprint (which was\nnot certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available \n\nthis version posted december 2, 2018. \n\nhttps://doi.org/10.1101/338947\n; \n\ndoi: \n\nunder a\n\ncc-by 4.0 international license\n.\n\nmanifold-tiling localized receptive fields are\n\noptimal in similarity-preserving neural networks\n\nanirvan m. sengupta\u2020\u2021\n\nmariano tepper\u2021\u21e4\n\ncengiz pehlevan\u2021\u21e4\n\nalexander genkin\u00a7\n\ndmitri b. chklovskii\u2021\u00a7\n\n\u2020rutgers university\n\n\u2021flatiron institute\n\n\u00a7nyu langone medical center\n\nanirvans@physics.rutgers.edu, alexander.genkin@gmail.com\n{mtepper,cpehlevan,dchklovskii}@flatironinstitute.org\n\nabstract\n\nmany neurons in the brain, such as place cells in the rodent hippocampus, have lo-\ncalized receptive \ufb01elds, i.e., they respond to a small neighborhood of stimulus space.\nwhat is the functional signi\ufb01cance of such representations and how can they arise?\nhere, we pr", "co11ch22_ganguli\n\narjats.cls\n\nfebruary 13, 2020\n\n10:27\n\nannual review of condensed matter physics\nstatistical mechanics of\ndeep learning\n\nyasaman bahri,1 jonathan kadmon,2\njeffrey pennington,1 sam s. schoenholz,1\njascha sohl-dickstein,1 and surya ganguli1,2\n1google brain, google inc., mountain view, california 94043, usa\n2department of applied physics, stanford university, stanford, california 94035, usa;\nemail: sganguli@stanford.edu\n\nkeywords\nneural networks, machine learning, dynamical phase transitions, chaos, spin\nglasses, jamming, random matrix theory, interacting particle systems,\nnonequilibrium statistical mechanics\n\nabstract\nthe recent striking success of deep neural networks in machine learning\nraises profound questions about the theoretical principles underlying their\nsuccess. for example, what can such deep networks compute? how can we\ntrain them? how does information propagate through them? why can they\ngeneralize? and how can we teach them to imagine? we review recent work", "research article\n\nthe effects of chloride dynamics on\nsubstantia nigra pars reticulata responses\nto pallidal and striatal inputs\nryan s phillips1,2, ian rosner2,3, aryn h gittis2,3, jonathan e rubin1,2*\n\n1department of mathematics, university of pittsburgh, pittsburgh, united states;\n2center for the neural basis of cognition, pittsburgh, united states; 3department\nof biological sciences, carnegie mellon university, pittsburgh, united states\n\nabstract as a rodent basal ganglia (bg) output nucleus, the substantia nigra pars reticulata\n(snr) is well positioned to impact behavior. snr neurons receive gabaergic inputs from the\nstriatum (direct pathway) and globus pallidus (gpe, indirect pathway). dominant theories of action\nselection rely on these pathways\u2019 inhibitory actions. yet, experimental results on snr responses to\nthese inputs are limited and include excitatory effects. our study combines experimental and\ncomputational work to characterize, explain, and make predictions about these ", "trends in\ncognitive sciences\n\nopen access\n\nfeature review\nrationalizing constraints on the capacity for\ncognitive control\n\nsebastian musslick\n\n1,* and jonathan d. cohen1,2\n\nhumans are remarkably limited in: (i) how many control-dependent tasks they\ncan execute simultaneously, and (ii) how intensely they can focus on a single\ntask. these limitations are universal assumptions of most theories of cognition.\nyet, a rationale for why humans are subject to these constraints remains elusive.\nthis feature review draws on recent insights from psychology, neuroscience, and\nmachine learning, to suggest that constraints on cognitive control may result\nfrom a rational adaptation to fundamental, computational dilemmas in neural\narchitectures. the reviewed literature implies that limitations in multitasking\nmay result from a trade-off between learning ef\ufb01cacy and processing ef\ufb01ciency\nand that limitations in the intensity of commitment to a single task may re\ufb02ect a\ntrade-off between cognitive stabilit", "on the relation between universality, characteristic kernels and\n\nrkhs embedding of measures\n\nbharath k. sriperumbudur\ndept. of ece, uc san diego\n\nla jolla, usa.\n\nbharathsv@ucsd.edu\n\nkenji fukumizu\n\nthe institute of statistical\nmathematics, tokyo, japan.\n\nfukumizu@ism.ac.jp\n\ngert r. g. lanckriet\n\ndept. of ece, uc san diego\n\nla jolla, usa.\n\ngert@ece.ucsd.edu\n\nabstract\n\ntion in a rkhs that has the representation,\n\nuniversal kernels have been shown to play\nan important role in the achievability of the\nbayes risk by many kernel-based algorithms\nthat include binary classi\ufb01cation, regression,\netc. in this paper, we propose a notion of uni-\nversality that generalizes the notions intro-\nduced by steinwart and micchelli et al. and\nstudy the necessary and su\ufb03cient conditions\nfor a kernel to be universal. we show that\nall these notions of universality are closely\nlinked to the injective embedding of a cer-\ntain class of borel measures into a reproduc-\ning kernel hilbert space (rkhs). by exploit-\n", "5 \n\n10 \n\n15 \n\n20 \n\nmanuscript (not proofed); final version at http://dx.doi.org/10.1126/science.aax6239 \n\ndendritic action potentials and computation in human layer 2/3 cortical \n\nneurons \n\n \n \n\nshort title: dendritic spikes in humans cortical neurons \n\nalbert gidon1, timothy adam zolnik1, pawel fidzinski2, felix bolduan3, athanasia \n\npapoutsi4, panayiota poirazi4, martin holtkamp2, imre vida3,5, matthew evan larkum1* \n\n \n\n \n\n \n\n1 institute for biology, humboldt-universit\u00e4t zu berlin, berlin, germany. \n\n2 epilepsy-center berlin-brandenburg, department of neurology, charit\u00e9 - universit\u00e4tsmedizin \nberlin, germany. \n\n3 institute of integrative neuroanatomy, charit\u00e9-universit\u00e4tsmedizin berlin, berlin, germany. \n\n4  institute  of  molecular  biology  and  biotechnology-foundation  for  research  and  technology \nhellas (imbb-forth), crete, greece. \n\n5 neurocure cluster, charit\u00e9 - universit\u00e4tsmedizin berlin, germany. \n\n*corresponding author e-mail: matthew.larkum@hu-berlin.de \n\n \n \n\n \n\n \n\n1 ", "1\n2\n0\n2\n\n \nt\nc\no\n5\n2\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n2\nv\n0\n7\n4\n0\n1\n\n.\n\n0\n1\n1\n2\n:\nv\ni\nx\nr\na\n\ninterpreting deep learning models in natural language processing:\n\na review\n\nxiaofei sun1, diyi yang2, xiaoya li1, tianwei zhang3,\n\nyuxian meng1, han qiu4, guoyin wang5, eduard hovy6, jiwei li1,7\n\n1shannon.ai, 2georgia institute of technology\n\n3nanyang technological university, 4tsinghua university\n\n5amazon alexa ai, 6carnegie mellon university, 7zhejiang university\n\nabstract\n\nneural network models have achieved state-of-the-art performances in a wide range of natural language processing (nlp)\ntasks. however, a long-standing criticism against neural network models is the lack of interpretability, which not only reduces\nthe reliability of neural nlp systems but also limits the scope of their applications in areas where interpretability is essential\n(e.g., health care applications). in response, the increasing interest in interpreting neural nlp models has spurred a diverse array\nof interpretation meth", "memory-ef\ufb01cient backpropagation through time\n\naudr\u00afunas gruslys\ngoogle deepmind\n\naudrunas@google.com\n\nr\u00e9mi munos\n\ngoogle deepmind\n\nmunos@google.com\n\nivo danihelka\n\ngoogle deepmind\n\ndanihelka@google.com\n\nmarc lanctot\n\ngoogle deepmind\n\nlanctot@google.com\n\nalex graves\n\ngoogle deepmind\n\ngravesa@google.com\n\nabstract\n\nwe propose a novel approach to reduce memory consumption of the backpropa-\ngation through time (bptt) algorithm when training recurrent neural networks\n(rnns). our approach uses dynamic programming to balance a trade-off between\ncaching of intermediate results and recomputation. the algorithm is capable of\ntightly \ufb01tting within almost any user-set memory budget while \ufb01nding an optimal\nexecution policy minimizing the computational cost. computational devices have\nlimited memory capacity and maximizing a computational performance given a\n\ufb01xed memory budget is a practical use-case. we provide asymptotic computational\nupper bounds for various regimes. the algorithm is particularly ", "12366 \u2022 the journal of neuroscience, september 15, 2010 \u2022 30(37):12366 \u201312378\n\nbehavioral/systems/cognitive\n\nan approximately bayesian delta-rule model explains the\ndynamics of belief updating in a changing environment\n\nmatthew r. nassar,1 robert c. wilson,2 benjamin heasly,1 and joshua i. gold1\n1department of neuroscience, university of pennsylvania, philadelphia, pennsylvania 19104, and 2department of psychology, princeton university,\nprinceton, new jersey 08540\n\nmaintaining appropriate beliefs about variables needed for effective decision making can be difficult in a dynamic environment. one key\nissue is the amount of influence that unexpected outcomes should have on existing beliefs. in general, outcomes that are unexpected\nbecause of a fundamental change in the environment should carry more influence than outcomes that are unexpected because of\npersistent environmental stochasticity. here we use a novel task to characterize how well human subjects follow these principles under a\nr", "9\n1\n0\n2\n\n \nr\na\n\n \n\nm\n2\n1\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n5\n6\n9\n7\n0\n\n.\n\n2\n1\n8\n1\n:\nv\ni\nx\nr\na\n\ndeep learning with asymmetric connections and hebbian\n\nupdates\n\nyali amit\n\nuniversity of chicago\n\nmarch 14, 2019\n\nabstract\n\nwe show that deep networks can be trained using hebbian updates yielding similar\nperformance to ordinary back-propagation on challenging image datasets. to overcome\nthe unrealistic symmetry in connections between layers, implicit in back-propagation,\nthe feedback weights are separate from the feedforward weights. the feedback weights\nare also updated with a local rule, the same as the feedforward weights - a weight is\nupdated solely based on the product of activity of the units it connects. with \ufb01xed\nfeedback weights as proposed in [17] performance degrades quickly as the depth of\nthe network increases.\nif the feedforward and feedback weights are initialized with\nthe same values, as proposed in [30], they remain the same throughout training thus\nprecisely implementing back-prop", "m\na\nc\nh\ni\nn\ne\n \nl\ne\na\nr\nn\ni\nn\ng\n,\n \n2\n2\n,\n \n2\n8\n3\n-\n2\n9\n0\n \n(\n1\n9\n9\n6\n)\n \no\n \n1\n9\n9\n6\n \nk\nl\nu\nw\ne\nr\n \na\nc\na\nd\ne\nm\ni\nc\n \np\nu\nb\nl\ni\ns\nh\ne\nr\ns\n,\n \nb\no\ns\nt\no\nn\n.\n \nm\na\nn\nu\nf\na\nc\nt\nu\nr\ne\nd\n \ni\nn\n \nt\nh\ne\n \nn\ne\nt\nh\ne\nr\nl\na\nn\nd\ns\n.\n \nt\ne\nc\nh\nn\ni\nc\na\nl\n \nn\no\nt\ne\n \ni\nn\nc\nr\ne\nm\ne\nn\nt\na\nl\n \nm\nu\nl\nt\ni\n-\ns\nt\ne\np\n \nq\n-\nl\ne\na\nr\nn\ni\nn\ng\n \nj\ni\nn\ng\n \np\ne\nn\ng\n \nc\no\nl\nl\ne\ng\ne\n \no\nf\n \ne\nn\ng\ni\nn\ne\ne\nr\ni\nn\ng\n,\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n \no\nf\n \nc\na\nl\ni\nf\no\nr\nn\ni\na\n,\n \nr\ni\nv\ne\nr\ns\ni\nd\ne\n,\n \nc\na\n \n9\n2\n5\n2\n1\n \nr\no\nn\na\nl\nd\n \nj\n.\n \nw\ni\nl\nl\ni\na\nm\ns\n \nc\no\nl\nl\ne\ng\ne\n \no\nf\n \nc\no\nm\np\nu\nt\ne\nr\n \ns\nc\ni\ne\nn\nc\ne\n,\n \nn\no\nr\nt\nh\ne\na\ns\nt\ne\nr\nn\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n,\n \nb\no\ns\nt\no\nn\n,\n \nm\na\n \n0\n2\n1\n1\n5\n \nj\np\n \n@\nv\ni\ns\nl\na\nb\n.\nu\ne\nr\n.\ne\nd\nu\n \nr\nj\nw\n@\nc\nc\ns\n.\nn\ne\nu\n.\ne\nd\nu\n \ne\nd\ni\nt\no\nr\n:\n \nl\ne\ns\nl\ni\ne\n \np\na\nc\nk\n \nk\na\ne\nl\nb\nl\ni\nn\ng\n \na\nb\ns\nt\nr\na\nc\nt\n.\n \nt\nh\ni\ns\n \np\na\np\ne\nr\n \np\nr\ne\ns\ne\nn\nt\ns\n \na\n \nn\no\nv\ne\nl\n \ni\nn\nc\nr\ne\nm\ne\nn\nt\na\nl\n \na\nl\ng\no\nr\ni\nt\nh\nm\n \nt\nh\na\nt\n \nc\no\nm\nb\ni\nn\ne\ns\n \nq\n-\nl\ne\na\nr\nn\ni\nn\ng\n,\n \na\n \nw\ne\nl\nl\n-\nk\nn\no\nw\nn\n \nd\ny\nn\na\n", "neuron, vol. 44, 23\u201330, september 30, 2004, copyright \uf8e92004 by cell press\n\nspike timing-dependent\nplasticity of neural circuits\n\nreview\n\nyang dan* and mu-ming poo*\ndivision of neurobiology\ndepartment of molecular and cell biology and\nhelen wills neuroscience institute\nuniversity of california, berkeley\nberkeley, california 94720\n\nrecent findings of spike timing-dependent plasticity\n(stdp) have stimulated much interest among experi-\nmentalists and theorists. beyond the traditional corre-\nlation-based hebbian plasticity, stdp opens up new\navenues for understanding information coding and cir-\ncuit plasticity that depend on the precise timing of\nneuronal spikes. here we summarize experimental\ncharacterization of stdp at various synapses, the un-\nderlying cellular mechanisms, and the associated\nchanges in neuronal excitability and dendritic integra-\ntion. we also describe stdp in the context of complex\nspike patterns and its dependence on the dendritic\nlocation of the synapse. finally, we d", "9\n1\n0\n2\n\n \nc\ne\nd\n4\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n0\n2\n7\n0\n1\n\n.\n\n6\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nreverse engineering recurrent networks for sentiment\n\nclassi\ufb01cation reveals line attractor dynamics\n\nniru maheswaranathan\u2217\ngoogle brain, google inc.\n\nmountain view, ca\nnirum@google.com\n\nalex h. williams\u2217\nstanford university\n\nstanford, ca\n\nahwillia@stanford.edu\n\nmatthew d. golub\nstanford university\n\nstanford, ca\n\nmgolub@stanford.edu\n\nsurya ganguli\n\nstanford and google brain, google inc.\n\nstanford, ca\n\nsganguli@stanford.edu\n\ndavid sussillo\n\ngoogle brain, google inc.\n\nmountain view, ca\n\nsussillo@google.com\n\nabstract\n\nrecurrent neural networks (rnns) are a widely used tool for modeling sequential\ndata, yet they are often treated as inscrutable black boxes. given a trained recurrent\nnetwork, we would like to reverse engineer it\u2013to obtain a quantitative, interpretable\ndescription of how it solves a particular task. even for simple tasks, a detailed\nunderstanding of how recurrent networks work, or a prescrip", "article\n\nattractor dynamics in networks with learning rules\ninferred from in vivo data\n\nhighlights\nd a network model with a learning rule inferred from data\n\nauthors\n\nulises pereira, nicolas brunel\n\nexhibits attractor dynamics\n\nd the storage capacity of the model is close to optimal\n\nd attractors can be \ufb01xed points or chaotic, depending on\n\nparameters\n\nd chaotic dynamics lead to highly irregular activity but stable\n\nmemory\n\ncorrespondence\nnicolas.brunel@duke.edu\n\nin brief\na network model with a learning rule\ninferred from data recorded in the primate\ncortex exhibits attractor dynamics with\nnearly optimal storage capacity. a\ntransition to chaotic dynamics is found at\nstrong coupling, leading to highly\nirregular activity but stable memory.\n\npereira & brunel, 2018, neuron 99, 227\u2013238\njuly 11, 2018 \u00aa 2018 elsevier inc.\nhttps://doi.org/10.1016/j.neuron.2018.05.038\n\n\f", "a  natural  policy  gradient \n\nsham kakade \n\ngatsby  computational neuroscience unit \n17  queen square, london, uk  wc1n 3ar \n\nhttp: //www.gatsby.ucl.ac.uk \n\nsham@gatsby.ucl.ac.uk \n\nabstract \n\nwe provide a  natural gradient method that represents the steepest \ndescent  direction based on the underlying structure of the param \neter space.  although gradient methods cannot make large changes \nin  the  values  of the  parameters,  we  show  that  the  natural  gradi \nent is moving toward choosing a greedy optimal action rather than \njust a  better action.  these greedy optimal  actions  are those  that \nwould  be  chosen  under  one  improvement  step  of  policy  iteration \nwith  approximate,  compatible  value  functions,  as  defined  by  sut \nton  et al.  [9].  we  then show drastic performance improvements in \nsimple mdps and in the more challenging mdp of tetris. \n\n1 \n\nintroduction \n\nthere has been a growing interest in direct policy-gradient methods for approximate \nplanning  in  l", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/248544743\n\nattention as an organ system\n\narticle \u00b7 february 2008\n\ndoi: 10.1017/cbo9780511541681.005\n\ncitations\n299\n\n2 authors:\n\nmichael posner\nuniversity of oregon\n\n462 publications\u00a0\u00a0\u00a0111,722 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n2,362\n\njin fan\ncity university of new york - queens college\n\n224 publications\u00a0\u00a0\u00a020,326 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nsome of the authors of this publication are also working on these related projects:\n\ndevelopment of attentional networks and individual differences in temperament. view project\n\nan information theory account of cognitive control view project\n\nall content following this page was uploaded by jin fan on 27 may 2014.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "3\n2\n0\n2\n\n \n\nn\na\nj\n \n4\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n4\nv\n8\n3\n3\n1\n0\n\n.\n\n6\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nbiologically-plausible backpropagation through\narbitrary timespans via local neuromodulators\n\nyuhan helena liu1,2,3,*, stephen smith2,4, stefan mihalas1,2,3, eric shea-brown1,2,3, and uygar\n\ns\u00fcmb\u00fcl2,*\n\n1department of applied mathematics, university of washington, seattle, wa, usa\n\n2allen institute for brain science, 615 westlake ave n, seattle wa, usa\n\n3computational neuroscience center, university of washington, seattle, wa, usa\n\n4department of molecular and cellular physiology, stanford university, stanford ca, usa\n\n*correspondence: hyliu24@uw.edu, uygars@alleninstitute.org\n\nabstract\n\nthe spectacular successes of recurrent neural network models where key parame-\nters are adjusted via backpropagation-based gradient descent have inspired much\nthought as to how biological neuronal networks might solve the corresponding\nsynaptic credit assignment problem [1\u20133]. there is so far little agreement,", "expectation backpropagation: parameter-free\ntraining of multilayer neural networks with\n\ncontinuous or discrete weights\n\ndaniel soudry1, itay hubara2, ron meir2\n\n(1) department of statistics, columbia university\n\n(2) department of electrical engineering, technion, israel institute of technology\n\ndaniel.soudry@gmail.com,itayhubara@gmail.com,rmeir@ee.technion.ac.il\n\nabstract\n\nmultilayer neural networks (mnns) are commonly trained using gradient\ndescent-based methods, such as backpropagation (bp). inference in probabilistic\ngraphical models is often done using variational bayes methods, such as expec-\ntation propagation (ep). we show how an ep based approach can also be used\nto train deterministic mnns. speci\ufb01cally, we approximate the posterior of the\nweights given the data using a \u201cmean-\ufb01eld\u201d factorized distribution, in an online\nsetting. using online ep and the central limit theorem we \ufb01nd an analytical ap-\nproximation to the bayes update of this posterior, as well as the resulting baye", "ica with reconstruction cost for ef\ufb01cient\n\novercomplete feature learning\n\nquoc v. le, alexandre karpenko, jiquan ngiam and andrew y. ng\n\n{quocle,akarpenko,jngiam,ang}@cs.stanford.edu\ncomputer science department, stanford university\n\nabstract\n\nindependent components analysis (ica) and its variants have been successfully\nused for unsupervised feature learning. however, standard ica requires an or-\nthonoramlity constraint to be enforced, which makes it dif\ufb01cult to learn overcom-\nplete features. in addition, ica is sensitive to whitening. these properties make\nit challenging to scale ica to high dimensional data. in this paper, we propose a\nrobust soft reconstruction cost for ica that allows us to learn highly overcomplete\nsparse features even on unwhitened data. our formulation reveals formal connec-\ntions between ica and sparse autoencoders, which have previously been observed\nonly empirically. our algorithm can be used in conjunction with off-the-shelf fast\nunconstrained optimizers. we ", "neuron, vol. 25, 707\u2013715, march, 2000, copyright \u00aa 2000 by cell press\n\nexperience-dependent asymmetric shape\nof hippocampal receptive fields\n\nmayank r. mehta,* michael c. quirk,\u2020\nand matthew a. wilson\ncenter for learning and memory\nriken-mit neuroscience research center\ndepartment of brain and cognitive sciences\ndepartment of biology\nmassachusetts institute of technology\ncambridge, massachusetts 02139\n\nand abbott, 1996; tsodyks et al., 1996) works. further,\nwhile these previous studies provided a novel connec-\ntion between nmda-dependent long-term potentiation\n(ltp) and changes in the average receptive field proper-\nties, such as size and specificity, the present work ex-\nplores the relationship between ltp and the structure\nof a receptive field, which can be detected within a single\ntrial. these results may provide insights into the role of\nplasticity in the structure of cortical receptive fields.\n\nsummary\n\nresults\n\nwe propose a novel parameter, namely, the skewness,\nor asymmetry, of ", "5\n1\n0\n2\n\n \n\nv\no\nn\n9\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n3\n5\n6\n5\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nunder review as a conference paper at iclr 2016\n\nwhy are deep nets reversible: a simple theory,\nwith implications for training\n\nsanjeev arora, yingyu liang & tengyu ma\ndepartment of computer science\nprinceton university\nprinceton, nj 08540, usa\n{arora,yingyul,tengyu}@cs.princeton.edu\n\nabstract\n\ngenerative models for deep learning are promising both to improve understanding of the\nmodel, and yield training methods requiring fewer labeled samples.\nrecent works use generative model approaches to produce the deep net\u2019s input given the\nvalue of a hidden layer several levels above. however, there is no accompanying \u201cproof\nof correctness\u201d for the generative model, showing that the feedforward deep net is the\ncorrect inference method for recovering the hidden layer given the input. furthermore,\nthese models are complicated.\nthe current paper takes a more theoretical tack.\nit presents a very simple generati", "hypothesis and theory article\npublished: 09 january 2014\ndoi: 10.3389/fncom.2013.00194\n\ntime representation in reinforcement learning models of\nthe basal ganglia\nsamuel j. gershman 1*, ahmed a. moustafa 2 and elliot a. ludvig 3,4\n\n1 department of brain and cognitive sciences, massachusetts institute of technology, cambridge, ma, usa\n2 school of social sciences and psychology, marcs institute for brain and behaviour, university of western sydney, sydney, nsw, australia\n3 princeton neuroscience institute and department of mechanical and aerospace engineering, princeton university, princeton, nj, usa\n4 department of psychology, university of warwick, coventry, uk\n\nedited by:\nhagai bergman, the hebrew\nuniversity- hadassah medical\nschool, israel\nreviewed by:\nyoram burak, hebrew university,\nisrael\ndaoyun ji, baylor college of\nmedicine, usa\n*correspondence:\nsamuel j. gershman, department\nof brain and cognitive sciences,\nmassachusetts institute of\ntechnology, room 46-4053, 77\nmassachusetts ave", "tuning-free step-size adaptation\n\nashique rupam mahmood\n\nrichard s. sutton\n\nthomas degris\n\npatrick m. pilarski\n\ndepartment of computing science, university of alberta, edmonton, ab, canada\n\nabstract\n\nincremental learning algorithms based on gradient descent are\neffective and popular in online supervised learning, reinforce-\nment learning, signal processing, and many other application\nareas. an oft-noted drawback of these algorithms is that they\ninclude a step-size parameter that needs to be tuned for best\nperformance, which may require manual intervention and sig-\nni\ufb01cant domain knowledge or additional data. in many cases,\nan entire vector of step-size parameters (e.g., one for each\ninput feature) needs to be tuned in order to attain the best per-\nformance of the algorithm. to address this, several methods\nhave been proposed for adapting step sizes online. for exam-\nple, sutton\u2019s idbd method can \ufb01nd the best vector step size\nfor the lms algorithm, and schraudolph\u2019s elk1 method,\nan exte", "letters to nature\n\n17. crevier, d. w. & meister, m. synchronous period-doubling in \ufb02icker vision of salamander and man.\n\nj. neurophysiol. 79, 1869\u20131878 (1998).\n\n18. merigan, w. h. & maunsell, j. h. how parallel are the primate visual pathways? annu. rev. neurosci.\n\n16, 369\u2013402 (1993).\n\n19. sparks, d. l. translation of sensory signals into commands for control of saccadic eye movements:\n\nrole of primate superior colliculus. physiol. rev. 66, 118\u2013171 (1986).\n\n20. knudsen, e. i. auditory and visual maps of space in the optic tectum of the owl. j. neurosci. 2, 1177\u2013\n\n1194 (1982).\n\n21. corey, d. p. & hudspeth, a. j. response latency of vertebrate hair cells. biophys. j. 26, 499\u2013506 (1979).\n22. carandini, m., heeger, d. j. & movshon, j. a. linearity and normalization in simple cells of the\n\nmacaque primary visual cortex. j. neurosci. 17, 8621\u20138644 (1997).\n\n23. abbott, l. f., varela, j. a., sen, k. & nelson, s. b. synaptic depression and cortical gain control. science\n\n275, 220\u2013224 (1997).\n\n2", "8\n1\n0\n2\n\n \n\nv\no\nn\n3\n\n \n\n \n \n]\n\no\nc\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n6\n6\n9\n6\n0\n\n.\n\n0\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nbiologically plausible online principal component\n\nanalysis without recurrent neural dynamics\n\nvictor minden1, cengiz pehlevan1, and dmitri b. chklovskii*1,2\n\n1center for computational biology, flatiron institute, new york, ny 10010\n\nemail: {vminden, cpehlevan, mitya}@flatironinstitute.org\n\n2neuroscience institute, nyu langone medical center, new york, ny 10016\n\nabstract\u2014arti\ufb01cial neural networks that learn to perform\nprincipal component analysis (pca) and related tasks using\nstrictly local learning rules have been previously derived based\non the principle of similarity matching: similar pairs of inputs\nshould map to similar pairs of outputs. however, the operation\nof these networks (and of similar networks) requires a \ufb01xed-\npoint iteration to determine the output corresponding to a\ngiven input, which means that dynamics must operate on a\nfaster time scale than the variation of\nthe input. furthe", "7\n1\n0\n2\n\n \nc\ne\nd\n4\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n9\n6\n9\n8\n0\n\n.\n\n2\n1\n7\n1\n:\nv\ni\nx\nr\na\n\nmean field residual networks: on the edge of chaos\n\ngreg yang\u2217\n\nmicrosoft research ai\n\ngregyang@microsoft.com\n\nsamuel s. schoenholz\n\ngoogle brain\n\nschsam@google.com\n\nabstract\n\nwe study randomly initialized residual networks using mean \ufb01eld theory and the\ntheory of difference equations. classical feedforward neural networks, such as\nthose with tanh activations, exhibit exponential behavior on the average when\npropagating inputs forward or gradients backward. the exponential forward dy-\nnamics causes rapid collapsing of the input space geometry, while the exponential\nbackward dynamics causes drastic vanishing or exploding gradients. we show,\nin contrast, that by adding skip connections, the network will, depending on the\nnonlinearity, adopt subexponential forward and backward dynamics, and in many\ncases in fact polynomial. the exponents of these polynomials are obtained through\nanalytic methods and pro", "on spectral  clustering: \n\nanalysis  and an algorithm \n\nandrew y.  ng \n\ncs  division \nu.c.  berkeley \n\nmichael i. jordan \n\ncs  div.  &  dept.  of stat. \n\nu.c.  berkeley \n\nang@cs.berkeley.edu \n\njordan@cs.berkeley.edu \n\nyair  weiss \n\nschool of cs  &  engr. \n\nthe hebrew univ. \nyweiss@cs.huji.ac.il \n\nabstract \n\ndespite many empirical successes of spectral  clustering  methods \nalgorithms  that  cluster  points  using  eigenvectors  of  matrices  de \nrived  from  the  data-\nthere  are  several  unresolved  issues.  first, \nthere  are  a  wide  variety  of  algorithms  that  use  the  eigenvectors \nin  slightly  different  ways.  second,  many of these  algorithms  have \nno  proof that  they  will  actually  compute  a  reasonable  clustering. \nin  this  paper,  we  present  a  simple  spectral  clustering  algorithm \nthat can be implemented using a  few  lines  of matlab.  using  tools \nfrom  matrix  perturbation  theory,  we  analyze  the  algorithm,  and \ngive  conditions  under  which  i", "article \n\ncommunicated by john platt and simon haykin \n\nan information-maximization approach to \nblind separation and blind deconvolution \n\nanthony j.  bell \nterrence i. sejnowski \nhoward  hughes medical institute, \ncomputational neurobiology  laboratory, the salk institute, \n10010 n. torrey pines road, la jolla, ca 92037 usa and \ndepartment  of biology, university of california, san diego, la jolla, c a  92093 u s a  \n\nwe  derive  a  new  self-organizing  learning  algorithm  that  maximizes \nthe  information transferred  in a  network  of  nonlinear  units.  the al- \ngorithm  does not  assume  any  knowledge  of  the  input  distributions, \nand is  defined here  for the zero-noise limit.  under  these conditions, \ninformation maximization has extra properties not found in the linear \ncase (linsker 1989). the nonlinearities in the transfer function are able \nto pick  up higher-order  moments of the  input  distributions and  per- \nform  something akin to  true redundancy  reduction  b", "8\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n9\n1\n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n2\nv\n1\n5\n6\n4\n0\n\n.\n\n4\n0\n7\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2018\n\nthe reactor:\na fast and sample-efficient actor-critic\nagent for reinforcement learning\n\naudr\u00afunas gruslys,\ndeepmind\naudrunas@google.com\n\nwill dabney,\ndeepmind\nwdabney@google.com\n\nmohammad gheshlaghi azar,\ndeepmind\nmazar@google.com\n\nbilal piot,\ndeepmind\npiot@google.com\n\nmarc g. bellemare,\ngoogle brain\nbellemare@google.com\n\nr\u00e9mi munos,\ndeepmind\nmunos@google.com\n\nabstract\n\nin this work, we present a new agent architecture, called reactor, which combines\nmultiple algorithmic and architectural contributions to produce an agent with higher\nsample-ef\ufb01ciency than prioritized dueling dqn (wang et al., 2017) and categori-\ncal dqn (bellemare et al., 2017), while giving better run-time performance than\na3c (mnih et al., 2016). our \ufb01rst contribution is a new policy evaluation algorithm\ncalled distributional retrace, which brings multi-step off-policy updates to the\n", "cerebral cortex october 2007;17:2443--2452\ndoi:10.1093/cercor/bhl152\nadvance access publication january 13, 2007\n\nsolving the distal reward problem through\nlinkage of stdp and dopamine signaling\n\neugene m. izhikevich\n\nthe neurosciences institute, 10640 john jay hopkins drive,\nsan diego, ca 92121, usa\n\nin pavlovian and instrumental conditioning, reward typically comes\nseconds after reward-triggering actions, creating an explanatory\nconundrum known as \u2018\u2018distal reward problem\u2019\u2019: how does the brain\nknow what \ufb01ring patterns of what neurons are responsible for the\nreward if 1) the patterns are no longer there when the reward\narrives and 2) all neurons and synapses are active during the\nwaiting period to the reward? here, we show how the conundrum\nis resolved by a model network of cortical spiking neurons with\nspike-timing--dependent plasticity (stdp) modulated by dopamine\n(da). although stdp is triggered by nearly coincident \ufb01ring\npatterns on a millisecond timescale, slow kinetics of subsequ", "ll\n\nperspective\nhow learning unfolds in the brain:\ntoward an optimization view\n\njay a. hennig,1,2,3,* emily r. oby,2,4 darby m. losey,1,2,3 aaron p. batista,2,4,7 byron m. yu,1,2,5,6,7\nand steven m. chase1,2,6,7\n1neuroscience institute, carnegie mellon university, pittsburgh, pa, usa\n2center for the neural basis of cognition, pittsburgh, pa, usa\n3machine learning department, carnegie mellon university, pittsburgh, pa, usa\n4department of bioengineering, university of pittsburgh, pittsburgh, pa, usa\n5department of electrical and computer engineering, carnegie mellon university, pittsburgh, pa, usa\n6department of biomedical engineering, carnegie mellon university, pittsburgh, pa, usa\n7these authors contributed equally\n*correspondence: jhennig@fas.harvard.edu\nhttps://doi.org/10.1016/j.neuron.2021.09.005\n\nsummary\n\nhow do changes in the brain lead to learning? to answer this question, consider an arti\ufb01cial neural network\n(ann), where learning proceeds by optimizing a given objective or cost ", "1\n2\n0\n2\n\n \nt\nc\no\n1\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n4\nv\n8\n5\n2\n9\n0\n\n.\n\n1\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nmaximum likelihood training of\n\nscore-based diffusion models\n\nyang song\u02da\n\ncomputer science department\n\nstanford university\n\nyangsong@cs.stanford.edu\n\nconor durkan\u02da\n\nschool of informatics\nuniversity of edinburgh\n\nconor.durkan@ed.ac.uk\n\niain murray\n\nschool of informatics\nuniversity of edinburgh\ni.murray@ed.ac.uk\n\nstefano ermon\n\ncomputer science department\n\nstanford university\n\nermon@cs.stanford.edu\n\nabstract\n\nscore-based diffusion models synthesize samples by reversing a stochastic process\nthat diffuses data to noise, and are trained by minimizing a weighted combination\nof score matching losses. the log-likelihood of score-based diffusion models can\nbe tractably computed through a connection to continuous normalizing \ufb02ows, but\nlog-likelihood is not directly optimized by the weighted combination of score\nmatching losses. we show that for a speci\ufb01c weighting scheme, the objective upper\nbounds the ne", "spectral methods for neural characterization using\n\ngeneralized quadratic models\n\nil memming park\u2217123, evan archer\u221713, nicholas priebe14, & jonathan w. pillow123\n\n1. center for perceptual systems, 2. dept. of psychology,\n\n3. division of statistics & scienti\ufb01c computation, 4. section of neurobiology,\n\n{memming@austin., earcher@, nicholas@, pillow@mail.} utexas.edu\n\nthe university of texas at austin\n\nabstract\n\nwe describe a set of fast, tractable methods for characterizing neural responses\nto high-dimensional sensory stimuli using a model we refer to as the generalized\nquadratic model (gqm). the gqm consists of a low-rank quadratic function fol-\nlowed by a point nonlinearity and exponential-family noise. the quadratic func-\ntion characterizes the neuron\u2019s stimulus selectivity in terms of a set linear receptive\n\ufb01elds followed by a quadratic combination rule, and the invertible nonlinearity\nmaps this output to the desired response range. special cases of the gqm include\nthe 2nd-order volte", "neuropsychopharmacology reviews (2008) 33, 18\u201341\n& 2008 nature publishing group all rights reserved 0893-133x/08 $30.00\n...............................................................................................................................................................\n18\nwww.neuropsychopharmacology.org\n\nreview\n\nsynaptic plasticity: multiple forms, functions, and\nmechanisms\n\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\nami citri1 and robert c malenka*,1\n1department of psychiatry and behavioral sciences, nancy pritzker laboratory, stanford university school of medicine,\npalo alto, ca, usa\n\nexperiences, whether they be learning in a classroom, a stressful event, or ingestion of a psychoactive substance, impact the\nbrain by modifying the activity and organization of specific neural circuitry. a major mechanism by which the neural activity\ngenerated by an experience modifies brain function is via modifications of synaptic transmission; that is, synaptic plas", "behavioral and brain sciences (2001) 24, 629\u2013640\nprinted in the united states of america\n\ngeneralization, similarity, \nand bayesian inference\n\njoshua b. tenenbaum and thomas l. griffiths\ndepartment of psychology, stanford university, stanford, ca94305-2130\njbt@psych.stanford.edu\nhttp://www-psych.stanford.edu/~jbt\nhttp://www-psych.stanford.edu/~gruffydd/\n\ngruffydd@psych.stanford.edu\n\nabstract: shepard has argued that a universal law should govern generalization across different domains of perception and cognition,\nas well as across organisms from different species or even different planets. starting with some basic assumptions about natural kinds,\nhe derived an exponential decay function as the form of the universal generalization gradient, which accords strikingly well with a wide\nrange of empirical data. however, his original formulation applied only to the ideal case of generalization from a single encountered stim-\nulus to a single novel stimulus, and for stimuli that can be represe", "randaugment: practical automated data augmentation\n\nwith a reduced search space\n\nekin d. cubuk \u2217, barret zoph\u2217, jonathon shlens, quoc v. le\n\n{cubuk, barretzoph, shlens, qvl}@google.com\n\ngoogle research, brain team\n\n9\n1\n0\n2\n\n \n\nv\no\nn\n4\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n9\n1\n7\n3\n1\n\n.\n\n9\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nrecent work has shown that data augmentation has the\npotential to signi\ufb01cantly improve the generalization of deep\nlearning models. recently, automated augmentation strate-\ngies have led to state-of-the-art results in image classi\ufb01ca-\ntion and object detection. while these strategies were op-\ntimized for improving validation accuracy, they also led to\nstate-of-the-art results in semi-supervised learning and im-\nproved robustness to common corruptions of images. an\nobstacle to a large-scale adoption of these methods is a sep-\narate search phase which increases the training complex-\nity and may substantially increase the computational cost.\nadditionally, due to the separate search", "reports\n\nuninformed individuals promote\ndemocratic consensus in animal groups\n\niain d. couzin,1* christos c. ioannou,1\u2020 g\u00fcven demirel,2 thilo gross,2\u2021 colin j. torney,1\nandrew hartnett,1 larissa conradt,3\u00a7 simon a. levin,1 naomi e. leonard4\n\nconflicting interests among group members are common when making collective decisions,\nyet failure to achieve consensus can be costly. under these circumstances individuals may be\nsusceptible to manipulation by a strongly opinionated, or extremist, minority. it has previously\nbeen argued, for humans and animals, that social groups containing individuals who are\nuninformed, or exhibit weak preferences, are particularly vulnerable to such manipulative agents.\nhere, we use theory and experiment to demonstrate that, for a wide range of conditions, a strongly\nopinionated minority can dictate group choice, but the presence of uninformed individuals\nspontaneously inhibits this process, returning control to the numerical majority. our results\nemphasize the", "neuron\n\narticle\n\nspontaneous events outline the realm\nof possible sensory responses\nin neocortical populations\n\nartur luczak,1,2 peter bartho\u00b4 ,1 and kenneth d. harris1,*\n1center for molecular and behavioural neuroscience, rutgers university, 197 university avenue, newark, nj 07102, usa\n2present address: ccbn, university of lethbridge, lethbridge, ab t1k 3m4, canada\n*correspondence: kdharris@rutgers.edu\ndoi 10.1016/j.neuron.2009.03.014\n\nsummary\n\nneocortical assemblies produce complex activity\npatterns both in response to sensory stimuli and\nspontaneously without sensory input. to investigate\nthe structure of these patterns, we recorded from\npopulations of 40\u2013100 neurons in auditory and\nsomatosensory cortices of anesthetized and awake\nrats using silicon microelectrodes. population spike\ntime patterns were broadly conserved across multiple\nsensory stimuli and spontaneous events. although\nindividual neurons showed timing variations between\nstimuli, these were not suf\ufb01cient to disturb a ge", "colorful image colorization\n\nrichard zhang, phillip isola, alexei a. efros\n\n{rich.zhang,isola,efros}@eecs.berkeley.edu\n\nuniversity of california, berkeley\n\nabstract. given a grayscale photograph as input, this paper attacks\nthe problem of hallucinating a plausible color version of the photograph.\nthis problem is clearly underconstrained, so previous approaches have\neither relied on signi\ufb01cant user interaction or resulted in desaturated col-\norizations. we propose a fully automatic approach that produces vibrant\nand realistic colorizations. we embrace the underlying uncertainty of the\nproblem by posing it as a classi\ufb01cation task and use class-rebalancing at\ntraining time to increase the diversity of colors in the result. the sys-\ntem is implemented as a feed-forward pass in a cnn at test time and is\ntrained on over a million color images. we evaluate our algorithm using a\n\u201ccolorization turing test,\u201d asking human participants to choose between\na generated and ground truth color image. ou", "original research\npublished: 12 june 2017\ndoi: 10.3389/fncom.2017.00048\n\ncliques of neurons bound into\ncavities provide a missing link\nbetween structure and function\n\nmichael w. reimann 1 \u2020, max nolte 1 \u2020, martina scolamiero 2, katharine turner 2,\nrodrigo perin 3, giuseppe chindemi 1, pawe\u0142 d\u0142otko 4\u2021, ran levi 5\u2021, kathryn hess 2*\u2021 and\nhenry markram 1, 3*\u2021\n\n1 blue brain project, \u00e9cole polytechnique f\u00e9d\u00e9rale de lausanne, geneva, switzerland, 2 laboratory for topology and\nneuroscience, brain mind institute, \u00e9cole polytechnique f\u00e9d\u00e9rale de lausanne, lausanne, switzerland, 3 laboratory of\nneural microcircuitry, brain mind institute, \u00e9cole polytechnique f\u00e9d\u00e9rale de lausanne, lausanne, switzerland, 4 datashape,\ninria saclay, palaiseau, france, 5 institute of mathematics, university of aberdeen, aberdeen, united kingdom\n\nlink between neural network structure and its emergent function\nthe lack of a formal\nhas hampered our understanding of how the brain processes information. we have now\ncome cl", "forms of prediction in the \nnervous\u00a0system\n\nchristoph\u00a0teufel \n\n  and paul\u00a0c.\u00a0fletcher \n\n \n\nabstract | the idea that predictions shape how we perceive and comprehend the \nworld has become increasingly influential in the field of systems neuroscience.  \nit also forms an important framework for understanding neuropsychiatric disorders, \nwhich are proposed to be the result of disturbances in the mechanisms through \nwhich prior information influences perception and belief, leading to the production \nof suboptimal models of the world. there is a widespread tendency to conceptualize \nthe influence of predictions exclusively in terms of \u2018top-down\u2019 processes, whereby \npredictions generated in higher-level areas exert their influence on lower-level \nareas within an information processing hierarchy. however, this excludes from \nconsideration the predictive information embedded in the \u2018bottom-up\u2019 stream of \ninformation processing. we describe evidence for the importance of this distinction \nand ar", "lifted proximal operator machines\n\njia li\n\ncong fang\n\nzhouchen lin(cid:66)\n\nkey lab. of machine perception, school of eecs, peking university\n\njiali.gm@gmail.com fangcong@pku.edu.cn zlin@pku.edu.cn\n\n8\n1\n0\n2\n\n \n\nv\no\nn\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n1\n0\n5\n1\n0\n\n.\n\n1\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe propose a new optimization method for training\nfeed-forward neural networks. by rewriting the activa-\ntion function as an equivalent proximal operator, we\napproximate a feed-forward neural network by adding\nthe proximal operators to the objective function as penal-\nties, hence we call the lifted proximal operator machine\n(lpom). lpom is block multi-convex in all layer-wise\nweights and activations. this allows us to use block co-\nordinate descent to update the layer-wise weights and\nactivations in parallel. most notably, we only use the\nmapping of the activation function itself, rather than its\nderivatives, thus avoiding the gradient vanishing or blow-\nup issues in gradient based training meth", "hessian eigenmaps: locally linear embedding\ntechniques for high-dimensional data\n\ndavid l. donoho* and carrie grimes\n\ndepartment of statistics, stanford university, stanford, ca 94305-4065\n\ncontributed by david l. donoho, march 19, 2003\n\nwe describe a method for recovering the underlying parametriza-\ntion of scattered data (mi) lying on a manifold m embedded in\nhigh-dimensional euclidean space. the method, hessian-based\nlocally linear embedding, derives from a conceptual framework of\nlocal isometry in which the manifold m, viewed as a riemannian\nsubmanifold of the ambient euclidean space \u2ea2n, is locally isometric\nto an open, connected subset \u2330 of euclidean space \u2ea2d. because \u2330\ndoes not have to be convex, this framework is able to handle a\nsigni\ufb01cantly wider class of situations than the original isomap\nalgorithm. the theoretical framework revolves around a quadratic\nform h (f ) \u2d1d \u5170m \u50a8hf (m)\u50a8f\n2dm de\ufb01ned on functions f : m \u54eb \u2ea2. here\nhf denotes the hessian of f, and h ( f ) averages the fro", "a r t i c l e s\n\nsynapses with short-term plasticity are optimal \nestimators of presynaptic membrane potentials\njean-pascal pfister1, peter dayan2 & m\u00e1t\u00e9 lengyel1\nthe trajectory of the somatic membrane potential of a cortical neuron exactly reflects the computations performed on its afferent \ninputs. however, the spikes of such a neuron are a very low-dimensional and discrete projection of this continually evolving signal. \nwe explored the possibility that the neuron\u2032s efferent synapses perform the critical computational step of estimating the membrane \npotential trajectory from the spikes. we found that short-term changes in synaptic efficacy can be interpreted as implementing an \noptimal estimator of this trajectory. short-term depression arose when presynaptic spiking was sufficiently intense as to reduce the \nuncertainty associated with the estimate; short-term facilitation reflected structural features of the statistics of the presynaptic \nneuron such as up and down states. our an", "0\n2\n0\n2\n\n \nr\np\na\n \n2\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n4\nv\n9\n8\n8\n0\n0\n\n.\n\n6\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nlearning to solve the credit assignment\nproblem\n\nbenjamin james lansdell\ndepartment of bioengineering\nuniversity of pennsylvania\npennsylvania, pa 19104\nlansdell@seas.upenn.edu\n\nkonrad paul kording\ndepartment of bioengineering\nuniversity of pennsylvania\npennsylvania, pa 19104\n\nprashanth ravi prakash\ndepartment of bioengineering\nuniversity of pennsylvania\npennsylvania, pa 19104\n\nabstract\n\nbackpropagation is driving today\u2019s arti\ufb01cial neural networks (anns). however,\ndespite extensive research, it remains unclear if the brain implements this algo-\nrithm. among neuroscientists, reinforcement learning (rl) algorithms are often\nseen as a realistic alternative: neurons can randomly introduce change, and use un-\nspeci\ufb01c feedback signals to observe their effect on the cost and thus approximate\ntheir gradient. however, the convergence rate of such learning scales poorly with\nthe number of involved neuron", "r e v i e w s\n\n n e u r o i m ag i n g\n\ndecoding mental states from brain \nactivity in humans\n\njohn-dylan haynes*\u2021\u00a7 and geraint rees\u2021\u00a7\nabstract | recent advances in human neuroimaging have shown that it is possible to \naccurately decode a person\u2019s conscious experience based only on non-invasive measurements \nof their brain activity. such \u2018brain reading\u2019 has mostly been studied in the domain of visual \nperception, where it helps reveal the way in which individual experiences are encoded in the \nhuman brain. the same approach can also be extended to other types of mental state, such as \ncovert attitudes and lie detection. such applications raise important ethical issues concerning \nthe privacy of personal thought.\n\nmultivariate analysis\nan analytical technique that \nconsiders (or solves) multiple \ndecision variables. in the \npresent context, multivariate \nanalysis takes into account \npatterns of information that \nmight be present across \nmultiple voxels measured by \nneuroimaging techniqu", "eesen: end-to-end speech recognition using deep rnn models and\n\nwfst-based decoding\n\nyajie miao, mohammad gowayyed, florian metze\n\nlanguage technologies institute, school of computer science, carnegie mellon university\n\n5\n1\n0\n2\n\n \nt\nc\no\n8\n1\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n3\nv\n0\n4\n2\n8\n0\n\n.\n\n7\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nthe performance of automatic speech recognition (asr) has\nimproved tremendously due to the application of deep neu-\nral networks (dnns). despite this progress, building a new\nasr system remains a challenging task, requiring various\nresources, multiple training stages and signi\ufb01cant expertise.\nthis paper presents our eesen framework which drastically\nsimpli\ufb01es the existing pipeline to build state-of-the-art asr\nsystems. acoustic modeling in eesen involves learning a\nsingle recurrent neural network (rnn) predicting context-\nindependent targets (phonemes or characters). to remove the\nneed for pre-generated frame labels, we adopt the connection-\nist temporal classi\ufb01cation (ctc) ", "visualizing and understanding\n\nconvolutional networks\n\nmatthew d. zeiler and rob fergus\n\ndept. of computer science,\nnew york university, usa\n{zeiler,fergus}@cs.nyu.edu\n\nabstract. large convolutional network models have recently demon-\nstrated impressive classi\ufb01cation performance on the imagenet bench-\nmark krizhevsky et al. [18]. however there is no clear understanding of\nwhy they perform so well, or how they might be improved. in this paper\nwe explore both issues. we introduce a novel visualization technique that\ngives insight into the function of intermediate feature layers and the oper-\nation of the classi\ufb01er. used in a diagnostic role, these visualizations allow\nus to \ufb01nd model architectures that outperform krizhevsky et al. on the\nimagenet classi\ufb01cation benchmark. we also perform an ablation study\nto discover the performance contribution from di\ufb00erent model layers. we\nshow our imagenet model generalizes well to other datasets: when the\nsoftmax classi\ufb01er is retrained, it convincing", "2\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n8\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n1\n8\n8\n3\n\n.\n\n6\n0\n2\n1\n:\nv\ni\nx\nr\na\n\ndanco: dimensionality from angle and norm\n\nconcentration\n\nc. ceruti, s. bassis, a. rozza, g. lombardi, e. casiraghi, and p. campadelli\n\ndipartimento di scienze dell\u2019informazione, universit`a degli studi di milano,\n\nvia comelico 39-41, 20135 milano, italy\n\nclaudio.ceruti@unimi.it\n\nabstract. in the last decades the estimation of the intrinsic dimen-\nsionality of a dataset has gained considerable importance. despite the\ngreat deal of research work devoted to this task, most of the proposed\nsolutions prove to be unreliable when the intrinsic dimensionality of the\ninput dataset is high and the manifold where the points lie is nonlin-\nearly embedded in a higher dimensional space. in this paper we pro-\npose a novel robust intrinsic dimensionality estimator that exploits the\ntwofold complementary information conveyed both by the normalized\nnearest neighbor distances and by the angles computed on couples of\nn", "neuron\n\nreport\n\nrepresentation of geometric borders\nin the developing rat\n\ntale l. bjerknes,1 edvard i. moser,1 and may-britt moser1,*\n1kavli institute for systems neuroscience, norwegian university of science and technology, no-7489 trondheim, norway\n*correspondence: maybm@ntnu.no\nhttp://dx.doi.org/10.1016/j.neuron.2014.02.014\n\nsummary\n\nlocal space is represented by a number of function-\nally speci\ufb01c cell types, including place cells in the\nhippocampus and grid cells, head direction cells,\nand border cells in the medial entorhinal cortex\n(mec). these cells form a functional map of external\nspace already at the time when rat pups leave the\nnest for the \ufb01rst time in their life, at the age of\n2.5 weeks. however, while place cells have adult-\nlike \ufb01ring \ufb01elds from the outset, grid cells have irreg-\nular and variable \ufb01elds until the fourth week, raising\ndoubts about their contribution to place cell \ufb01ring\nat young age. recording in mec of juvenile rats, we\nshow that, unlike grid cells, bord", "a large-scale circuit mechanism for hierarchical\ndynamical processing in the primate cortex\n\narticle\n\nhighlights\nd large-scale model of the macaque cortex with a gradient of\n\nsynaptic excitation\n\nd sensory areas show fast responses while cognitive areas\n\nshow slow integrative activity\n\nd multiple temporal hierarchies in the same anatomical network\n\nd functional connectivity analysis needs to incorporate inter-\n\nareal heterogeneity\n\nauthors\n\nrishidev chaudhuri, kenneth\nknoblauch, marie-alice gariel, henry\nkennedy, xiao-jing wang\n\ncorrespondence\nxjwang@nyu.edu\n\nin brief\nchaudhuri et al. report a large-scale\nmodel of the macaque cortex\nincorporating quantitative anatomical\ndata and inter-areal heterogeneity. this\nmodel gives rise to a hierarchy of\ntimescales and suggests a revision of\nfunctional connectivity analysis of global\nbrain dynamics.\n\nchaudhuri et al., 2015, neuron 88, 419\u2013431\noctober 21, 2015 \u00aa2015 elsevier inc.\nhttp://dx.doi.org/10.1016/j.neuron.2015.09.008\n\n\f", "https://doi.org/10.1038/s41583-023-00740-7\n\n check for updates\n\nreconstructing computational \nsystem dynamics from neural data \nwith recurrent neural networks\n\nsections\n\nintroduction\n\ndynamical systems theory \nprimer\n\ndynamical systems theory \nand recurrent neural networks \nin neuroscience\n\nreconstructing trajectories \nfrom time series data\n\ndynamical systems \nreconstruction\n\nevaluating dynamical system \nreconstructions\n\noutlook and future challenges\n\ndaniel durstewitz\u2009\nabstract\n\n \u20091,2,3 \n\n, georgia koppe\u2009\n\n \u20091,4,5 & max ingo thurm1\n\ncomputational models in neuroscience usually take the form of systems \nof differential equations. the behaviour of such systems is the subject of \ndynamical systems theory. dynamical systems\u00a0theory provides a powerful \nmathematical toolbox for analysing neurobiological processes and has \nbeen a mainstay of computational neuroscience for decades. recently, \nrecurrent neural networks (rnns) have become a popular machine \nlearning tool for studying the non-li", "understanding black-box decisions with su\ufb03cient input subsets\n\nwhat made you do this?\n\nbrandon carter*\n\njonas mueller*\n\nsiddhartha jain\n\nmit computer science and arti\ufb01cial intelligence laboratory\n\ndavid gi\ufb00ord\n\nabstract\n\nlocal explanation frameworks aim to rational-\nize particular decisions made by a black-box\nprediction model. existing techniques are of-\nten restricted to a speci\ufb01c type of predictor\nor based on input saliency, which may be un-\ndesirably sensitive to factors unrelated to the\nmodel\u2019s decision making process. we instead\npropose su\ufb03cient input subsets that identify\nminimal subsets of features whose observed\nvalues alone su\ufb03ce for the same decision to\nbe reached, even if all other input feature\nvalues are missing. general principles that\nglobally govern a model\u2019s decision-making can\nalso be revealed by searching for clusters of\nsuch input patterns across many data points.\nour approach is conceptually straightforward,\nentirely model-agnostic, simply implemented\nusing instan", "1024 \u2022 the journal of neuroscience, january 21, 2015 \u2022 35(3):1024 \u20131037\n\nsystems/circuits\n\ndistribution and function of hcn channels in the apical\ndendritic tuft of neocortical pyramidal neurons\n\nmark t. harnett,1 jeffrey c. magee,1 and stephen r. williams2\n1howard hughes medical institute, janelia farm research campus, ashburn, virginia 20147, and 2queensland brain institute, the university of\nqueensland, brisbane qld 4072, australia\n\nthe apical tuft is the most remote area of the dendritic tree of neocortical pyramidal neurons. despite its distal location, the apical\ndendritic tuft of layer 5 pyramidal neurons receives substantial excitatory synaptic drive and actively processes corticocortical input\nduring behavior. the properties of the voltage-activated ion channels that regulate synaptic integration in tuft dendrites have, however,\nnot been thoroughly investigated. here, we use electrophysiological and optical approaches to examine the subcellular distribution and\nfunction of hyp", "linear dynamical neural population models through\n\nnonlinear embeddings\n\nyuanjun gao\u21e4 1 , evan archer\u21e412, liam paninski12, john p. cunningham12\n\ndepartment of statistics1 and grossman center2\n\ncolumbia university\n\nnew york, ny, united states\n\nyg2312@columbia.edu, evan@stat.columbia.edu,\nliam@stat.columbia.edu, jpc2181@columbia.edu\n\nabstract\n\na body of recent work in modeling neural activity focuses on recovering low-\ndimensional latent features that capture the statistical structure of large-scale neural\npopulations. most such approaches have focused on linear generative models,\nwhere inference is computationally tractable. here, we propose flds, a general\nclass of nonlinear generative models that permits the \ufb01ring rate of each neuron\nto vary as an arbitrary smooth function of a latent, linear dynamical state. this\nextra \ufb02exibility allows the model to capture a richer set of neural variability than\na purely linear model, but retains an easily visualizable low-dimensional latent\nspace. ", "learning algorithms and signal processing \nfor brain-inspired computing\n\ncengiz pehlevan and dmitri b. chklovskii\n\nneuroscience-inspired online unsupervised  \nlearning algorithms\nartificial neural networks\n\ninventors of the original artificial neural networks (anns) de-\n\nrived their inspiration from biology [1]. however, today, most \nanns, such as backpropagation-based convolutional deep-\nlearning networks, resemble natural nns only superficially. \ngiven that, on some tasks, such anns achieve human or even \nsuperhuman performance, why should one care about such dis-\nsimilarity with natural nns? the algorithms of natural nns are \nrelevant if one\u2019s goal is not just to outperform humans on certain \ntasks but to develop general-purpose artificial intelligence rivaling \nthat of a human. as contemporary anns are far from achieving \nthis goal and natural nns, by definition, achieve it, natural nns \nmust contain some \u201csecret sauce\u201d that anns lack. this is why we \nneed to understand the algorit", "0\n2\n0\n2\n\n \n\nn\na\nj\n \n\n9\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n1\n9\n3\n5\n0\n\n.\n\n4\n0\n9\n1\n:\nv\ni\nx\nr\na\n\ndeep learning without weight transport\n\nmohamed akrout\n\nuniversity of toronto, triage\n\ncollin wilson\n\nuniversity of toronto\n\npeter c. humphreys\n\ndeepmind\n\ntimothy lillicrap\n\ndeepmind, university college london\n\ndouglas tweed\n\nuniversity of toronto, york university\n\nabstract\n\ncurrent algorithms for deep learning probably cannot run in the brain because\nthey rely on weight transport, where forward-path neurons transmit their synaptic\nweights to a feedback path, in a way that is likely impossible biologically. an algo-\nrithm called feedback alignment achieves deep learning without weight transport by\nusing random feedback weights, but it performs poorly on hard visual-recognition\ntasks. here we describe two mechanisms \u2014 a neural circuit called a weight mirror\nand a modi\ufb01cation of an algorithm proposed by kolen and pollack in 1994 \u2014 both\nof which let the feedback path learn appropriate synaptic weights q", "7\n1\n0\n2\n\n \nr\np\na\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n0\n6\n1\n0\n0\n\n.\n\n1\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nnips 2016 tutorial:\n\ngenerative adversarial networks\n\nian goodfellow\n\nopenai, ian@openai.com\n\nabstract\n\nthis report summarizes the tutorial presented by the author at nips\n2016 on generative adversarial networks (gans). the tutorial describes:\n(1) why generative modeling is a topic worth studying, (2) how generative\nmodels work, and how gans compare to other generative models, (3)\nthe details of how gans work, (4) research frontiers in gans, and (5)\nstate-of-the-art image models that combine gans with other methods.\nfinally, the tutorial contains three exercises for readers to complete, and\nthe solutions to these exercises.\n\nintroduction\n\nthis report1 summarizes the content of the nips 2016 tutorial on generative\nadversarial networks (gans) (goodfellow et al., 2014b). the tutorial was de-\nsigned primarily to ensure that it answered most of the questions asked by\naudience members ahead of time, in orde", "the synaptic plasticity and memory\nhypothesis: encoding, storage\nand persistence\n\nrstb.royalsocietypublishing.org\n\ntomonori takeuchi, adrian j. duszkiewicz and richard g. m. morris\n\ncentre for cognitive and neural systems, university of edinburgh, 1 george square, edinburgh eh8 9jz, uk\n\nreview\n\ncite this article: takeuchi t, duszkiewicz aj,\nmorris rgm. 2014 the synaptic plasticity and\nmemory hypothesis: encoding, storage\nand persistence. phil. trans. r. soc. b 369:\n20130288.\nhttp://dx.doi.org/10.1098/rstb.2013.0288\n\none contribution of 35 to a discussion meeting\nissue \u2018synaptic plasticity in health and disease\u2019.\n\nsubject areas:\nneuroscience, behaviour, cognition, physiology\n\nkeywords:\nsynaptic plasticity, memory, long-term\npotentiation, engram, initial consolidation,\ndopamine\n\nauthor for correspondence:\nrichard g. m. morris\ne-mail: r.g.m.morris@ed.ac.uk\n\nthe synaptic plasticity and memory hypothesis asserts that activity-dependent\nsynaptic plasticity is induced at appropriate synapses ", "decoupled greedy learning of cnns\n\neugene belilovsky 1 michael eickenberg 2 edouard oyallon 3\n\nabstract\n\na commonly cited inef\ufb01ciency of neural network\ntraining by back-propagation is the update lock-\ning problem: each layer must wait for the signal\nto propagate through the full network before up-\ndating. several alternatives that can alleviate this\nissue have been proposed. in this context, we con-\nsider a simpler, but more effective, substitute that\nuses minimal feedback, which we call decoupled\ngreedy learning (dgl). it is based on a greedy\nrelaxation of the joint training objective, recently\nshown to be effective in the context of convolu-\ntional neural networks (cnns) on large-scale\nimage classi\ufb01cation. we consider an optimization\nof this objective that permits us to decouple the\nlayer training, allowing for layers or modules in\nnetworks to be trained with a potentially linear\nparallelization in layers. with the use of a replay\nbuffer we show this approach can be extended\nto async", "adapting deep network features to capture psychological representations\n\njoshua c. peterson (jpeterson@berkeley.edu)\njoshua t. abbott (joshua.abbott@berkeley.edu)\n\nthomas l. grif\ufb01ths (thomas grif\ufb01ths@berkeley.edu)\n\ndepartment of psychology, university of california, berkeley, ca 94720 usa\n\n6\n1\n0\n2\n\n \n\ng\nu\na\n6\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n4\n6\n1\n2\n0\n\n.\n\n8\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\ndeep neural networks have become increasingly successful at\nsolving classic perception problems such as object recognition,\nsemantic segmentation, and scene understanding, often reach-\ning or surpassing human-level accuracy. this success is due\nin part to the ability of dnns to learn useful representations\nof high-dimensional inputs, a problem that humans must also\nsolve. we examine the relationship between the representa-\ntions learned by these networks and human psychological rep-\nresentations recovered from similarity judgments. we \ufb01nd that\ndeep features learned in service of object classi\ufb01cation accou", "cerebral cortex, january 2022;32: 15\u201328\n\nhttps://doi.org/10.1093/cercor/bhab191\nadvance access publication date: 19 july 2021\noriginal article\n\no r i g i n a l a r t i c l e\nstructural attributes and principles of the neocortical\nconnectome in the marmoset monkey\npanagiota theodoni1,2,3, piotr majka4,5,6, david h. reser5,7,\ndaniel k. w\u00f3jcik4, marcello g.p. rosa5,6,\u2020 and xiao-jing wang1,\u2020\n1center for neural science, new york university, new york, ny 10003, usa, 2new york university shanghai,\nshanghai 200122, china, 3nyu-ecnu institute of brain and cognitive science at new york university\nshanghai, shanghai 200062, china, 4laboratory of neuroinformatics, nencki institute of experimental biology\nof polish academy of sciences, warsaw 02-093, poland, 5australian research council, centre of excellence for\nintegrative brain function, monash university node, clayton, vic 3800, australia, 6neuroscience program,\nbiomedicine discovery institute and department of physiology, monash university, cla", "ne39ch09-ulanovsky\n\nari\n\n11 june 2016\n\n9:8\n\n3-d maps and compasses\nin the brain\narseny finkelstein, liora las, and nachum ulanovsky\ndepartment of neurobiology, weizmann institute of science, rehovot 76100, israel;\nemail: nachum.ulanovsky@weizmann.ac.il\n\nannu. rev. neurosci. 2016. 39:171\u201396\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev-neuro-070815-013831\ncopyright c(cid:2) 2016 by annual reviews.\nall rights reserved\n\nkeywords\nspatial cognition, 3-d topology, place cells, grid cells, head-direction cells,\nrodents, primates, bats\n\nabstract\nthe world has a complex, three-dimensional (3-d) spatial structure, but\nuntil recently the neural representation of space was studied primarily in\nplanar horizontal environments. here we review the emerging literature on\nallocentric spatial representations in 3-d and discuss the relations between\n3-d spatial perception and the underlying neural codes. we suggest that\nthe statistics of movem", "a low-power band of neuronal spiking activity \ndominated by local single units improves the \nperformance of brain\u2013machine interfaces\n\n\u200a1, alex k. vaskov\u200a\n\n\u200a4, \n\u200a1, autumn j. bullard1, chrono s. nu1, jonathan c. kao5,6, krishna v. shenoy7,8,9,10,11,12, \n\n\u200a2, matthew s. willsey1,3, elissa j. welle\u200a\n\nsamuel r. nason\u200a\nphilip p. vu\u200a\ntaekwang jang4,13, hun-seok kim4, david blaauw4, parag g. patil\u200a\ncynthia a. chestek\u200a\n\n\u200a1, hyochan an\u200a\n\n\u200a1,3,14,15 and \n\n\u200a1,2,4,15\u2009\u2709\n\nthe large power requirement of current brain\u2013machine interfaces is a major hindrance to their clinical translation. in basic \nbehavioural tasks, the downsampled magnitude of the 300\u20131,000\u2009hz band of spiking activity can predict movement similarly \nto the threshold crossing rate (tcr) at 30 kilo-samples per second. however, the relationship between such a spiking-band \npower (sbp) and neural activity remains unclear, as does the capability of using the sbp to decode complicated behaviour. \nby using simulations of recordings of neura", "neuron\n\narticle\n\naction selection and action value\nin frontal-striatal circuits\n\nmoonsang seo,1 eunjeong lee,1 and bruno b. averbeck1,*\n1laboratory of neuropsychology, national institute of mental health, national institutes of health, bethesda, md 20892-4415, usa\n*correspondence: bruno.averbeck@nih.gov\ndoi 10.1016/j.neuron.2012.03.037\n\nsummary\n\nreinforcement\n\nthe role that frontal-striatal circuits play in normal\nbehavior remains unclear. two of the leading hypoth-\neses suggest that these circuits are important for\naction selection or\nlearning. to\nexamine these hypotheses, we carried out an exper-\niment in which monkeys had to select actions in two\ndifferent task conditions. in the \ufb01rst (random) condi-\ntion, actions were selected on the basis of perceptual\ninference. in the second (\ufb01xed) condition, the animals\nused reinforcement from previous trials to select\nactions. examination of neural activity showed that\nthe representation of\nthe selected action was\nstronger in lateral prefronta", "j neurophysiol 93: 2600 \u20132613, 2005.\nfirst published december 29, 2004; doi:10.1152/jn.00803.2004.\n\ncalcium time course as a signal for spike-timing\u2013dependent plasticity\n\njonathan e. rubin,1,2,3 richard c. gerkin,2,3 guo-qiang bi,2,3,4 and carson c. chow1,2,3,4,5\n1department of mathematics, 2center for the neural basis of cognition, 3center for neuroscience at university of pittsburgh, and\n4department of neurobiology, university of pittsburgh, pittsburgh, pennsylvania; and 5laboratory of biological modeling, national\ninstitute of diabetes and digestive and kidney diseases, national institutes of health, bethesda, maryland\n\nsubmitted 5 august 2004; accepted in \ufb01nal form 6 december 2004\n\nrubin, jonathan e., richard c. gerkin, guo-qiang bi, and\ncarson c. chow. calcium time course as a signal for spike-timing\u2013\ndependent plasticity. j neurophysiol 93: 2600 \u20132613, 2005. first\npublished december 29, 2004; doi:10.1152/jn.00803.2004. calcium\nhas been proposed as a postsynaptic signal underlying", "6\n1\n0\n2\n\n \n\ng\nu\na\n8\n1\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\nv\n1\n5\n1\n5\n0\n\n.\n\n8\n0\n6\n1\n:\nv\ni\nx\nr\na\n\neffective multi-step temporal-difference learning\n\nfor non-linear function approximation\n\nharm van seijen\n\nrlai lab\n\nuniversity of alberta\n\nmaluuba research\n\n2000 peel street, montreal, qc\n\ncanada, h3a 2w5\n\nharm.vanseijen@maluuba.com\n\nabstract\n\nmulti-step temporal-difference (td) learning, where the update targets contain\ninformation from multiple time steps ahead, is one of the most popular forms of td\nlearning for linear function approximation. the reason is that multi-step methods\noften yield substantially better performance than their single-step counter-parts,\ndue to a lower bias of the update targets. for non-linear function approximation,\nhowever, single-step methods appear to be the norm. part of the reason could be\nthat on many domains the popular multi-step methods td(\u03bb) and sarsa(\u03bb) do not\nperform well when combined with non-linear function approximation. in particular,\nthey are very sus", "the geometry of decision-making in individuals and\ncollectives\n\nvivek h. sridhara,b,c,1\nnir s. govi\n\n, liang lia,b,c,1\n, and iain d. couzina,b,c,1\n\n, dan gorbonosa,b,c, m\u00e1t\u00e9 nagya,b,c,d,e,f, bianca r. schellg\n\n, timothy sorochkinh,i\n\n,\n\nadepartment of collective behaviour, max planck institute of animal behavior, 78464 konstanz, germany; bcentre for the advanced study of collective\nbehaviour, university of konstanz, 78464 konstanz, germany; cdepartment of biology, university of konstanz, 78464 konstanz, germany; dmta-elte\n\u201clend\u00fclet\u201d collective behaviour research group, e\u00f6tv\u00f6s lor\u00e1nd research network, 1117 budapest, hungary; edepartment of biological physics, e\u00f6tv\u00f6s\nlor\u00e1nd university, 1117 budapest, hungary; fmta-elte statistical and biological physics research group, e\u00f6tv\u00f6s lor\u00e1nd research network, 1117 budapest,\nhungary; gdepartment of chemistry, university of konstanz, 78464 konstanz, germany; hdepartment of physics and astronomy, university of waterloo,\nwaterloo, on n2l 3g1, canada;", "1295\n\nnonlinearlow-dimensionalregressionusingauxiliarycoordinatesweiranwangmiguel\u00b4a.carreira-perpi\u02dcn\u00b4aneecs,schoolofengineering,universityofcalifornia,mercedabstractwhendoingregressionwithinputsandout-putsthatarehigh-dimensional,itoftenmakessensetoreducethedimensionalityofthein-putsbeforemappingtotheoutputs.muchworkinstatisticsandmachinelearning,suchasreduced-rankregression,slicedinversere-gressionandtheirvariants,hasfocusedonlineardimensionalityreduction,oronesti-matingthedimensionalityreduction\ufb01rstandthenthemapping.weproposeamethodwhereboththedimensionalityreductionandthemappingcanbenonlinearandareesti-matedjointly.ourkeyideaistode\ufb01neanobjectivefunctionwherethelow-dimensionalcoordinatesarefreeparameters,inadditiontothedimensionalityreductionandthemap-ping.thishasthee\ufb00ectofdecouplingmanygroupsofparametersfromeachother,a\ufb00ord-ingafarmoree\ufb00ectiveoptimizationthanifusingadeepnetworkwithnestedmappings,andtouseagoodinitializationfromslicedin-verseregressionorspectralmethods.ourex-perimentswi", "annales de la facult\u00e9 des sciences de toulouse\n\npascal massart\nsome applications of concentration\ninequalities to statistics\nannales de la facult\u00e9 des sciences de toulouse 6e s\u00e9rie, tome 9, no 2\n(2000), p. 245-303\n<http://www.numdam.org/item?id=afst_2000_6_9_2_245_0>\n\n\u00a9 universit\u00e9 paul sabatier, 2000, tous droits r\u00e9serv\u00e9s.\nl\u2019acc\u00e8s aux archives de la revue \u00ab annales de la facult\u00e9 des sciences de\ntoulouse \u00bb (http://picard.ups-tlse.fr/~annales/) implique l\u2019accord avec les\nconditions g\u00e9n\u00e9rales d\u2019utilisation (http://www.numdam.org/conditions).\ntoute utilisation commerciale ou impression syst\u00e9matique est constitu-\ntive d\u2019une infraction p\u00e9nale. toute copie ou impression de ce \ufb01chier\ndoit contenir la pr\u00e9sente mention de copyright.\n\narticle num\u00e9ris\u00e9 dans le cadre du programme\n\nnum\u00e9risation de documents anciens math\u00e9matiques\n\nhttp://www.numdam.org/\n\n\f", "research\n\nresearch article summary \u25e5\n\nneuroscience\n\nlocal connectivity and synaptic dynamics\nin mouse and human neocortex\n\nluke campagnola\u2020, stephanie c. seeman\u2020, thomas chartrand, lisa kim, alex hoggarth,\nclare gamlin, shinya ito, jessica trinh, pasha davoudian, cristina radaelli, mean-hwan kim,\ntravis hage, thomas braun, lauren alfiler, julia andrade, phillip bohn, rachel dalley, alex henry,\nsara kebede, alice mukora, david sandman, grace williams, rachael larsen, corinne teeter,\ntanya l. daigle, kyla berry, nadia dotson, rachel enstrom, melissa gorham, madie hupp,\nsamuel dingman lee, kiet ngo, philip r. nicovich, lydia potekhina, shea ransford, amanda gary,\njeff goldy, delissa mcmillen, trangthanh pham, michael tieu, la\u2019akea siverts, miranda walker,\ncolin farrell, martin schroedter, cliff slaughterbeck, charles cobb, richard ellenbogen,\nryder p. gwinn, c. dirk keene, andrew l. ko, jeffrey g. ojemann, daniel l. silbergeld, daniel carey,\ntamara casper, kirsten crichton, michael clark,", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/335548152\n\nbiologically motivated algorithms for propagating local target representations\n\narticle\u00a0\u00a0in\u00a0\u00a0proceedings of the aaai conference on artificial intelligence \u00b7 july 2019\n\ndoi: 10.1609/aaai.v33i01.33014651\n\ncitations\n31\n\n2 authors:\n\nalexander ororbia\nrochester institute of technology\n\n110 publications\u00a0\u00a0\u00a01,440 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n48\n\nankur a. mali\nuniversity of south florida\n\n46 publications\u00a0\u00a0\u00a0398 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nall content following this page was uploaded by ankur a. mali on 12 january 2021.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "fairwashing: the risk of rationalization\n\nulrich a\u00a8\u0131vodji 1 hiromi arai 2 3 olivier fortineau 4 s\u00b4ebastien gambs 1 satoshi hara 5 alain tapp 6 7\n\nabstract\n\nblack-box explanation is the problem of explain-\ning how a machine learning model \u2013 whose in-\nternal logic is hidden to the auditor and gen-\nerally complex \u2013 produces its outcomes. cur-\nrent approaches for solving this problem include\nmodel explanation, outcome explanation as well\nas model inspection. while these techniques can\nbe bene\ufb01cial by providing interpretability, they\ncan be used in a negative manner to perform fair-\nwashing, which we de\ufb01ne as promoting the false\nperception that a machine learning model respects\nsome ethical values. in particular, we demonstrate\nthat it is possible to systematically rationalize de-\ncisions taken by an unfair black-box model using\nthe model explanation as well as the outcome ex-\nplanation approaches with a given fairness metric.\nour solution, laundryml, is based on a regular-\nized rule list e", "4\n1\n0\n2\n\n \n\ng\nu\na\n9\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n6\n4\n2\n4\n\n.\n\n1\n1\n2\n1\n:\nv\ni\nx\nr\na\n\nwhat regularized auto-encoders learn from the data\n\ngenerating distribution\n\nguillaume alain and yoshua bengio\nguillaume.alain@umontreal.ca, yoshua.bengio@umontreal.ca\n\ndepartment of computer science and operations research\nuniversity of montreal\nmontreal, h3c 3j7, quebec, canada\n\nabstract\n\nwhat do auto-encoders learn about the underlying data generating distribution? recent work\nsuggests that some auto-encoder variants do a good job of capturing the local manifold structure\nof data. this paper clari\ufb01es some of these previous observations by showing that minimizing\na particular form of regularized reconstruction error yields a reconstruction function that lo-\ncally characterizes the shape of the data generating density. we show that the auto-encoder\ncaptures the score (derivative of the log-density with respect to the input). it contradicts pre-\nvious interpretations of reconstruction error as an ene", "cortical preparatory activity indexes \nlearned motor memories\n\nhttps://doi.org/10.1038/s41586-021-04329-x\nreceived: 25 january 2020\naccepted: 9 december 2021\npublished online: 26 january 2022\n\n check for updates\n\nxulu sun1,2,9\u2009\u2709, daniel j. o\u2019shea2,3,9, matthew d. golub2,3, eric m. trautmann2,3, \nsaurabh vyas2,4, stephen i. ryu3,5,6 & krishna v. shenoy2,3,4,6,7,8\u2009\u2709\n\nthe brain\u2019s remarkable ability to learn and execute various motor behaviours \nharnesses the capacity of neural populations to generate a variety of activity patterns. \nhere we explore systematic changes in preparatory activity in motor cortex that \naccompany motor learning. we trained rhesus monkeys to learn an arm-reaching task1 \nin a curl force field that elicited new muscle forces for some, but not all, movement \ndirections2,3. we found that in a neural subspace predictive of hand forces, changes in \npreparatory activity tracked the learned behavioural modifications and reassociated4 \nexisting activity patterns with updat", "kickback cuts backprop\u2019s red-tape:\n\nbiologically plausible credit assignment in neural networks\n\ndavid balduzzi\n\ndavid.balduzzi@vuw.ac.nz\n\nvictoria university of wellington\n\nhastagiri vanchinathan\n\nhastagiri@inf.ethz.ch\n\neth zurich\n\njoachim buhmann\njbuhmann@inf.ethz.ch\n\neth zurich\n\nabstract\n\nerror backpropagation is an extremely effective algorithm\nfor assigning credit in arti\ufb01cial neural networks. however,\nweight updates under backprop depend on lengthy recursive\ncomputations and require separate output and error messages\n\u2013 features not shared by biological neurons, that are perhaps\nunnecessary. in this paper, we revisit backprop and the credit\nassignment problem.\nwe \ufb01rst decompose backprop into a collection of interact-\ning learning algorithms; provide regret bounds on the perfor-\nmance of these sub-algorithms; and factorize backprop\u2019s er-\nror signals. using these results, we derive a new credit assign-\nment algorithm for nonparametric regression, kickback, that\nis signi\ufb01cantly simpl", "article\n\ndoi:10.1038/nature12112\n\nhippocampal place-cell sequences depict\nfuture paths to remembered goals\n\nbrad e. pfeiffer1 & david j. foster1\n\neffective navigation requires planning extended routes to remembered goal locations. hippocampal place cells have\nbeen proposed to have a role in navigational planning, but direct evidence has been lacking. here we show that before\ngoal-directed navigation in an open arena, the rat hippocampus generates brief sequences encoding spatial trajectories\nstrongly biased to progress from the subject\u2019s current location to a known goal location. these sequences predict\nimmediate future behaviour, even in cases in which the specific combination of start and goal locations is novel. these\nresults indicate that hippocampal sequence events characterized previously in linearly constrained environments as\n\u2018replay\u2019 are also capable of supporting a goal-directed, trajectory-finding mechanism, which identifies important\nplaces and relevant behavioural paths, a", "letter\n\ncommunicated by peter dayan\n\nmaking working memory work: a computational model of\nlearning in the prefrontal cortex and basal ganglia\n\nrandall c. o\u2019reilly\noreilly@psych.colorado.edu\nmichael j. frank\nfrankmj@psych.colorado.edu\ndepartment of psychology, university of colorado boulder, boulder, co 80309,\nu.s.a.\n\nthe prefrontal cortex has long been thought to subserve both working\nmemory (the holding of information online for processing) and executive\nfunctions (deciding how to manipulate working memory and perform\nprocessing). although many computational models of working memory\nhave been developed, the mechanistic basis of executive function\nremains elusive, often amounting to a homunculus. this article presents\nan attempt to deconstruct this homunculus through powerful learning\nmechanisms that allow a computational model of the prefrontal cortex to\ncontrol both itself and other brain areas in a strategic, task-appropriate\nmanner. these learning mechanisms are based on subcortica", "neuron, vol. 48, 661\u2013673, november 23, 2005, copyright \u00aa2005 by elsevier inc. doi 10.1016/j.neuron.2005.09.032\n\ntransient dynamics versus fixed points in\nodor representations by locust antennal\nlobe projection neurons\n\nofer mazor1 and gilles laurent*\ncomputation and neural systems program\ndivision of biology\ncalifornia institute of technology\npasadena, california 91125\n\nsummary\n\nprojection neurons (pns) in the locust antennal lobe\nexhibit odor-speci\ufb01c dynamic responses. we studied a\npn population, stimulated with \ufb01ve odorants and pulse\ndurations between 0.3 and 10 s. odor representations\nwere characterized as time series of vectors of pn ac-\ntivity, constructed from the \ufb01ring rates of all pns in suc-\ncessive 50 ms time bins. odor representations by the\npn population can be described as trajectories in pn\nstate space with three main phases: an on transient,\nlasting 1\u20132 s; a \ufb01xed point, stable for at least 8 s; and\nan off transient, lasting a few seconds as activity re-\nturns to baseline", "thephysicsofoptimaldecisionmaking:aformalanalysisofmodelsofperformanceintwo-alternativeforced-choicetasksrafalbogacz,ericbrown,jeffmoehlis,philipholmes,andjonathand.cohenprincetonuniversityinthisarticle,theauthorsconsideroptimaldecisionmakingintwo-alternativeforced-choice(tafc)tasks.theybeginbyanalyzing6modelsoftafcdecisionmakingandshowthatallbutonecanbereducedtothedriftdiffusionmodel,implementingthestatisticallyoptimalalgorithm(mostaccurateforagivenspeedorfastestforagivenaccuracy).theyprovefurtherthatthereisalwaysanoptimaltrade-offbetweenspeedandaccuracythatmaximizesvariousrewardfunctions,includingrewardrate(percentageofcorrectresponsesperunittime),aswellasseveralotherobjectivefunctions,includingonesweightedforaccuracy.theyusethesefindingstoaddressempiricaldataandmakenovelpredictionsaboutperformanceunderoptimality.keywords:driftdiffusionmodel,rewardrate,optimalperformance,speed\u2013accuracytrade-off,perceptualchoicethisarticleconcernsoptimalstrategiesfordecisionmakinginthetwo-alternativef", "neural residual flow fields\n\nfor efficient video representations\n\ndaniel rho1[0000\u22120002\u22128568\u22129489], junwoo cho1, jong hwan ko1,2\u22c6, and\n\neunbyung park1,2\u22c6\n\n1 department of artificial intelligence, sungkyunkwan university\n\n2 department of electrical and computer engineering, sungkyunkwan university\n\n{daniel231,jwcho000,jhko,epark}@skku.edu\n\nabstract. neural fields have emerged as a powerful paradigm for repre-\nsenting various signals, including videos. however, research on improving\nthe parameter efficiency of neural fields is still in its early stages. even\nthough neural fields that map coordinates to colors can be used to en-\ncode video signals, this scheme does not exploit the spatial and temporal\nredundancy of video signals. inspired by standard video compression al-\ngorithms, we propose a neural field architecture for representing and\ncompressing videos that deliberately removes data redundancy through\nthe use of motion information across video frames. maintaining motion\ninformation", "7\n1\n0\n2\n\n \n\nn\na\nj\n \n\n3\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n8\n3\n5\n6\n0\n\n.\n\n1\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nunder review as a conference paper at iclr 2017\n\noutrageously large neural networks:\nthe sparsely-gated mixture-of-experts layer\n\nnoam shazeer1, azalia mirhoseini\u2217\u20201, krzysztof maziarz\u22172, andy davis1, quoc le1, geoffrey\n\nhinton1 and jeff dean1\n\n1google brain, {noam,azalia,andydavis,qvl,geoffhinton,jeff}@google.com\n\n2jagiellonian university, cracow, krzysztof.maziarz@student.uj.edu.pl\n\nabstract\n\nthe capacity of a neural network to absorb information is limited by its number of\nparameters. conditional computation, where parts of the network are active on a\nper-example basis, has been proposed in theory as a way of dramatically increas-\ning model capacity without a proportional increase in computation. in practice,\nhowever, there are signi\ufb01cant algorithmic and performance challenges.\nin this\nwork, we address these challenges and \ufb01nally realize the promise of conditional\ncomputation, achieving greater ", "ne39ch02-moser\n\nari\n\n26 may 2016\n\n13:5\n\nten years of grid cells\ndavid c. rowland, yasser roudi, may-britt moser,\nand edvard i. moser\nkavli institute for systems neuroscience and centre for neural computation, norwegian\nuniversity of science and technology, 7491 trondheim, norway;\nemail: david.c.rowland@ntnu.no, yasser.roudi@ntnu.no, may-britt.moser@ntnu.no,\nedvard.moser@ntnu.no\n\nannu. rev. neurosci. 2016. 39:19\u201340\n\nfirst published online as a review in advance on\nmarch 9, 2016\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev-neuro-070815-013824\ncopyright c(cid:2) 2016 by annual reviews.\nall rights reserved\n\nkeywords\nentorhinal cortex, spatial navigation, memory, attractor networks,\nhippocampus, association cortex\n\nabstract\nthe medial entorhinal cortex (mec) creates a neural representation of space\nthrough a set of functionally dedicated cell types: grid cells, border cells, head\ndirection cells, and speed cells. grid cells, th", "unleashing the power of pre-trained\nlanguage models for offline reinforcement\nlearning\n\n\u2217\n\nruizhe shi1* yuyao liu1\n1tsinghua university, iiis 2shanghai qi zhi institute\n4university of washington 5shanghai ai lab\n\nyanjie ze23 simon s. du4 huazhe xu125\n\n3shanghai jiao tong university\n\n3\n2\n0\n2\n\n \n\nv\no\nn\n7\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n7\n8\n5\n0\n2\n\n.\n\n0\n1\n3\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\noffline reinforcement learning (rl) aims to find a near-optimal policy using\npre-collected datasets.\nin real-world scenarios, data collection could be costly\nand risky; therefore, offline rl becomes particularly challenging when the in-\ndomain data is limited. given recent advances in large language models (llms)\nand their few-shot learning prowess, this paper introduces language models for\nmotion control (lamo), a general framework based on decision transformers\nto effectively use pre-trained language models (lms) for offline rl. our frame-\nwork highlights four crucial components: (1) initializing decision tran", "dynamical mean-\ufb01eld theory for stochastic gradient\n\ndescent in gaussian mixture classi\ufb01cation\n\nfrancesca mignacco1\n\nflorent krzakala2,3\n\npierfrancesco urbani1\n\nlenka zdeborov\u00e11,4\n\n1 institut de physique th\u00e9orique, universit\u00e9 paris-saclay, cnrs, cea, gif-sur-yvette, france\n2 laboratoire de physique, cnrs, \u00e9cole normale sup\u00e9rieure, psl university, paris, france\n\n3 idephics laboratory, epfl, switzerland\n\n4 spoc laboratory, epfl, switzerland\n\ncorrespondence to: francesca.mignacco@ipht.fr\n\nabstract\n\nwe analyze in a closed form the learning dynamics of stochastic gradient descent\n(sgd) for a single layer neural network classifying a high-dimensional gaussian\nmixture where each cluster is assigned one of two labels. this problem provides a\nprototype of a non-convex loss landscape with interpolating regimes and a large\ngeneralization gap. we de\ufb01ne a particular stochastic process for which sgd can\nbe extended to a continuous-time limit that we call stochastic gradient \ufb02ow. in\nthe full-batch lim", "j neurophysiol 98: 1733\u20131750, 2007.\nfirst published june 27, 2007; doi:10.1152/jn.01265.2006.\n\na model of v4 shape selectivity and invariance\n\ncharles cadieu,1 minjoon kouh,1 anitha pasupathy,2 charles e. connor,3 maximilian riesenhuber,4 and\ntomaso poggio1\n1center for biological and computational learning, mcgovern institute, massachusetts institute of technology, cambridge,\nmassachusetts; 2department of biological structure, university of washington, seattle, washington; 3department of neuroscience, johns\nhopkins university, baltimore, maryland; and 4department of neuroscience, georgetown university medical center, washington, dc\n\nsubmitted 2 december 2006; accepted in \ufb01nal form 24 june 2007\n\ncadieu c, kouh m, pasupathy a, connor ce, riesenhuber m,\npoggio t. a model of v4 shape selectivity and invariance. j\nneurophysiol 98: 1733\u20131750, 2007. first published june 27, 2007;\ndoi:10.1152/jn.01265.2006. object recognition in primates is medi-\nated by the ventral visual pathway and is class", "journal of machine learning research 10 (2009) 207-244\n\nsubmitted 12/07; revised 9/08; published 2/09\n\ndistance metric learning for large margin\n\nnearest neighbor classi\ufb01cation\n\nkilian q. weinberger\nyahoo! research\n2821 mission college blvd\nsanta clara, ca 9505\n\nlawrence k. saul\ndepartment of computer science and engineering\nuniversity of california, san diego\n9500 gilman drive, mail code 0404\nla jolla, ca 92093-0404\n\neditor: sam roweis\n\nkilian@yahoo-inc.com\n\nsaul@cs.ucsd.edu\n\nabstract\n\nthe accuracy of k-nearest neighbor (knn) classi\ufb01cation depends signi\ufb01cantly on the metric used\nto compute distances between different examples. in this paper, we show how to learn a maha-\nlanobis distance metric for knn classi\ufb01cation from labeled examples. the mahalanobis metric\ncan equivalently be viewed as a global linear transformation of the input space that precedes knn\nclassi\ufb01cation using euclidean distances. in our approach, the metric is trained with the goal that\nthe k-nearest neighbors always ", "generalized intersection over union: a metric and a loss for bounding box\n\nregression\n\nhamid rezato\ufb01ghi1,2 nathan tsoi1\n\njunyoung gwak1 amir sadeghian1,3\n\nian reid2 silvio savarese1\n\n9\n1\n0\n2\n\n \nr\np\na\n5\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n0\n3\n6\n9\n0\n\n.\n\n2\n0\n9\n1\n:\nv\ni\nx\nr\na\n\n1computer science department, stanford university, united states\n2school of computer science, the university of adelaide, australia\n\n3aibee inc, usa\n\nhamidrt@stanford.edu\n\nabstract\n\nintersection over union (iou) is the most popular evalu-\nation metric used in the object detection benchmarks. how-\never, there is a gap between optimizing the commonly used\ndistance losses for regressing the parameters of a bounding\nbox and maximizing this metric value. the optimal objec-\ntive for a metric is the metric itself.\nin the case of axis-\naligned 2d bounding boxes, it can be shown that iou can\nbe directly used as a regression loss. however, iou has a\nplateau making it infeasible to optimize in the case of non-\noverlapping bounding", "letter\n\ncommunicated by joshua gold\n\nthe basal ganglia and cortex implement optimal decision\nmaking between alternative actions\n\nrafal bogacz\nr.bogacz@bristol.ac.uk\ndepartment of computer science, university of bristol, bristol bs8 1ub, u.k.\n\nkevin gurney\nk.gurney@shef.ac.uk\ndepartment of psychology, university of shef\ufb01eld, shef\ufb01eld s10 2tp, u.k.\n\nneurophysiological studies have identi\ufb01ed a number of brain regions\ncritically involved in solving the problem of action selection or deci-\nsion making. in the case of highly practiced tasks, these regions include\ncortical areas hypothesized to integrate evidence supporting alternative\nactions and the basal ganglia, hypothesized to act as a central switch\nin gating behavioral requests. however, despite our relatively detailed\nknowledge of basal ganglia biology and its connectivity with the cor-\ntex and numerical simulation studies demonstrating selective function,\nno formal theoretical framework exists that supplies an algorithmic de-\nscripti", "resurrecting the sigmoid in deep learning through\n\ndynamical isometry: theory and practice\n\njeffrey pennington\n\ngoogle brain\n\nsamuel s. schoenholz\n\ngoogle brain\n\napplied physics, stanford university and google brain\n\nsurya ganguli\n\nabstract\n\nit is well known that weight initialization in deep networks can have a dramatic\nimpact on learning speed. for example, ensuring the mean squared singular value\nof a network\u2019s input-output jacobian is o(1) is essential for avoiding exponentially\nvanishing or exploding gradients. moreover, in deep linear networks, ensuring that\nall singular values of the jacobian are concentrated near 1 can yield a dramatic\nadditional speed-up in learning; this is a property known as dynamical isometry.\nhowever, it is unclear how to achieve dynamical isometry in nonlinear deep net-\nworks. we address this question by employing powerful tools from free probability\ntheory to analytically compute the entire singular value distribution of a deep\nnetwork\u2019s input-output ja", "the  journal  of  neuroscience,  march  1995,  15(3):  1669-1682 \n\nthe  morphoelectrotonic \ndendritic  function \n\ntransform: \n\na  graphical  approach \n\nto \n\nanthony  m.  zador,\u2019 \n\u2018salk \ninstitute  of  life  sciences,  hebrew  university, \n\ninstitute,  san  diego,  california \n\nhagai  agmon-snir,2 \n\nand \n\nldan  segev2 \n92037,  and  2department \n\njerusalem, \n\nisrael  91904 \n\nof  neurobiology \n\nand  center \n\nfor  neural  computation, \n\nis \n\njust \nsimilarly \n\nelectrotonic \nneuronal \nvelop \nscribe \nometry. \nthe \nthe  propagation \ntween  any \nin  the  sense \nl, \nand \nbasis \nical  mapping \nin  a  met,  either \nfundamental \nsures \nbitrary \near \naction \nmeasure \neach  emphasizing \ndendritic \ntical \nthat \nhavior \ncell \nto \n(cortical \nramidal, \nto  electrical \nanalysis \ndemonstrate \ntaining \nerties \n\na  rapid \n\nof \n\nin \nrole \nwe  de- \nthat  de- \nge- \nand \nbe- \n\nstructure \n\ncomputation \n\nof  dendrites \nand  plasticity. \n\nplays \nin \n\ntwo  novel  measures \nintraneuronal \n\nsignaling \n\nof  electrotoni", "letters\n\nvol 457 | 26 february 2009 | doi:10.1038/nature07709\n\nthe subcellular organization of neocortical excitatory\nconnections\nleopoldo petreanu1, tianyi mao1, scott m. sternson1 & karel svoboda1\n\nunderstanding cortical circuits will require mapping the connec-\ntions between specific populations of neurons1, as well as deter-\nmining the dendritic locations where the synapses occur2. the\ndendrites of individual cortical neurons overlap with numerous\ntypes of local and long-range excitatory axons, but axodendritic\noverlap is not always a good predictor of actual connection\nstrength3\u20135. here we developed an efficient channelrhodopsin-2\n(chr2)-assisted method6\u20138 to map the spatial distribution of syna-\nptic inputs, defined by presynaptic chr2 expression, within the\ndendritic arborizations of recorded neurons. we expressed chr2\nin two thalamic nuclei, the whisker motor cortex and local excit-\natory neurons and mapped their synapses with pyramidal neurons\nin layers 3, 5a and 5b (l3, l5a a", "a r t i c l e s\n\ndifferential triggering of spontaneous glutamate \nrelease by p/q-, n- and r-type ca2+ channels\nyaroslav s ermolyuk1,5, felicity g alder1,5, rainer surges1,4,5, ivan y pavlov1, yulia timofeeva2,3,  \ndimitri m kullmann1 & kirill e volynski1\n\nthe role of voltage-gated ca2+ channels (vgccs) in spontaneous miniature neurotransmitter release is incompletely understood. \nwe found that stochastic opening of p/q-, n- and r-type vgccs accounts for ~50% of all spontaneous glutamate release at \nrat cultured hippocampal synapses, and that r-type channels have a far greater role in spontaneous than in action potential\u2013\nevoked exocytosis. vgcc-dependent miniature neurotransmitter release (minis) showed similar sensitivity to presynaptic ca2+ \nchelation as evoked release, arguing for direct triggering of spontaneous release by transient spatially localized ca2+ domains. \nexperimentally constrained three-dimensional diffusion modeling of ca2+ influx\u2013exocytosis coupling was consistent w", "em algorithms for pca and spca\n\nsam roweis\u0000\nabstract\n\ni present an expectation-maximization (em) algorithm for principal\ncomponent analysis (pca). the algorithm allows a few eigenvectors and\neigenvalues to be extracted from large collections of high dimensional\ndata. it is computationally very ef\ufb01cient in space and time. it also natu-\nrally accommodates missing information. i also introduce a new variant\nof pca called sensible principal component analysis (spca) which de-\n\ufb01nes a proper density model in the data space. learning for spca is also\ndone with an em algorithm. i report results on synthetic and real data\nshowing that these em algorithms correctly and ef\ufb01ciently \ufb01nd the lead-\ning eigenvectors of the covariance of datasets in a few iterations using up\nto hundreds of thousands of datapoints in thousands of dimensions.\n\n1 why em for pca?\nprincipal component analysis (pca) is a widely used dimensionality reduction technique in\ndata analysis. its popularity comes from three importan", "the journal of neuroscience, december 10, 2008 \u2022 28(50):13457\u201313466 \u2022 13457\n\ncellular/molecular\n\nspine neck plasticity controls postsynaptic calcium signals\nthrough electrical compartmentalization\n\n\u00e5sa grunditz,1* niklaus holbro,1* lei tian,1 yi zuo,2 and thomas g. oertner1\n1friedrich miescher institute, maulbeerstrasse 66, ch-4058 basel, switzerland, and 2department for molecular cell and developmental biology, university\nof california, santa cruz, santa cruz, california 95064\n\ndendritic spines have been proposed to function as electrical compartments for the active processing of local synaptic signals. however,\nestimates of the resistance between the spine head and the parent dendrite suggest that compartmentalization is not tight enough to\nelectrically decouple the synapse. here we show in acute hippocampal slices that spine compartmentalization is initially very weak, but\nincreases dramatically upon postsynaptic depolarization. using nmda receptors as voltage sensors, we provide ev", "nature communications\n\narticle\n\nhttps://doi.org/10.1038/s41467-022-32646-w\n\nsmall, correlated changes in synaptic\nconnectivity may facilitate rapid\nmotor learning\n\nreceived: 28 october 2021\n\naccepted: 8 august 2022\n\nbarbara feulner\nlee e. miller4,5,6, juan a. gallego 1,7 & claudia clopath 1,7\n\n1, matthew g. perich 2, raeed h. chowdhury 3,\n\ncheck for updates\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\nanimals rapidly adapt their movements to external perturbations, a process\nparalleled by changes in neural activity in the motor cortex. experimental\nstudies suggest that these changes originate from altered inputs (hinput) rather\nthan from changes in local connectivity (hlocal), as neural covariance is largely\npreserved during adaptation. since measuring synaptic changes in vivo\nremains very challenging, we used a modular recurrent neural network to\nqualitatively test this interpretation. as expected, hinput resulted in small\nactivity changes and largely preserved cov", "a basal ganglia-forebrain circuit in the songbird\nbiases motor output to avoid vocal errors\n\naaron s. andalman and michale s. fee1\n\nmcgovern institute for brain research, department of brain and cognitive sciences, massachusetts institute of technology, cambridge, ma 02139\n\nedited by fernando nottebohm, the rockefeller university, millbrook, ny, and approved may 29, 2009 (received for review march 24, 2009)\n\nin songbirds, as in mammals, basal ganglia-forebrain circuits are\nnecessary for the learning and production of complex motor\nbehaviors; however, the precise role of these circuits remains\nunknown. it has recently been shown that a basal ganglia-fore-\nbrain circuit in the songbird, which projects directly to vocal\u2013motor\ncircuitry, has a premotor function driving exploration necessary for\nvocal learning. it has also been hypothesized that this circuit,\nknown as the anterior forebrain pathway (afp), may generate an\ninstructive signal that improves performance in the motor path-\nway. h", "elifesciences.org\n\nshort report\n\nhippocampal place cells construct reward\nrelated sequences through unexplored\nspace\nh freyja \u00b4olafsd \u00b4ottir1,2*\u2020, caswell barry2\u2020, aman b saleem3, demis hassabis4\u2021,\nhugo j spiers1*\u2021\n\n1institute of behavioural neuroscience, department of experimental psychology,\ndivision of psychology and language sciences, university college london, london,\nunited kingdom; 2department of cell and developmental biology, university college\nlondon, london, united kingdom; 3ucl institute of ophthalmology, university\ncollege london, london, united kingdom; 4gatsby computational neuroscience\nunit, university college london, london, united kingdom\n\nabstract dominant theories of hippocampal function propose that place cell representations are\nformed during an animal\u2019s first encounter with a novel environment and are subsequently replayed\nduring off-line states to support consolidation and future behaviour. here we report that viewing the\ndelivery of food to an unvisited portion", "ne43ch05_magee\n\narjats.cls\n\njune 22, 2020\n\n9:51\n\nannual review of neuroscience\nsynaptic plasticity forms\nand functions\njeffrey c. magee and christine grienberger\ndepartment of neuroscience and howard hughes medical institute, baylor college of\nmedicine, houston, texas 77030, usa; email: jcmagee@bcm.edu\n\nannu. rev. neurosci. 2020. 43:95\u2013117\n\nfirst published as a review in advance on\nfebruary 19, 2020\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nhttps://doi.org/10.1146/annurev-neuro-090919-\n022842\n\ncopyright \u00a9 2020 by annual reviews.\nall rights reserved\n\nkeywords\nlearning, memory, synapses, dendrites, neuromodulation, eligibility traces\n\nabstract\nsynaptic plasticity, the activity-dependent change in neuronal connection\nstrength, has long been considered an important component of learning\nand memory. computational and engineering work corroborate the power\nof learning through the directed adjustment of connection weights. here\nwe review the fundamental elements", "reviews\n\nextracting information from neuronal \npopulations: information theory  \nand decoding approaches\n\nrodrigo quian quiroga* and stefano panzeri\u2021\u00a7\n\nabstract | to a large extent, progress in neuroscience has been driven by the study of \nsingle-cell responses averaged over several repetitions of stimuli or behaviours. however, \nthe brain typically makes decisions based on single events by evaluating the activity of large \nneuronal populations. therefore, to further understand how the brain processes information, \nit is important to shift from a single-neuron, multiple-trial framework to multiple-neuron, \nsingle-trial methodologies. two related approaches \u2014 decoding and information theory \u2014 \ncan be used to extract single-trial information from the activity of neuronal populations. \nsuch population analysis can give us more information about how neurons encode stimulus \nfeatures than traditional single-cell studies.\n\nspike sorting\nthe grouping of spikes into \nclusters based on the simi", "the journal of neuroscience, december 15, 1998, 18(24):10464\u201310472\n\nsynaptic modi\ufb01cations in cultured hippocampal neurons:\ndependence on spike timing, synaptic strength, and\npostsynaptic cell type\n\nguo-qiang bi and mu-ming poo\ndepartment of biology, university of california at san diego, la jolla, california 92093\n\nin cultures of dissociated rat hippocampal neurons, persistent\npotentiation and depression of glutamatergic synapses were\ninduced by correlated spiking of presynaptic and postsynaptic\nneurons. the relative timing between the presynaptic and\npostsynaptic spiking determined the direction and the extent of\nsynaptic changes. repetitive postsynaptic spiking within a time\nwindow of 20 msec after presynaptic activation resulted in\nlong-term potentiation (ltp), whereas postsynaptic spiking\nwithin a window of 20 msec before the repetitive presynaptic\nactivation led to long-term depression (ltd). signi\ufb01cant ltp\noccurred only at synapses with relatively low initial strength,\nwhereas th", "lettercommunicatedbygarrettstanleybayesianpopulationdecodingofmotorcorticalactivityusingakalmanfilterweiwuweiwu@dam.brown.eduyungaogao@dam.brown.edudivisionofappliedmathematics,brownuniversity,providence,ri02912,u.s.a.eliebienenstockelie@dam.brown.edudivisionofappliedmathematicsanddepartmentofneuroscience,brownuniversity,providence,ri02912,u.s.a.johnp.donoghuejohndonoghue@brown.edudepartmentofneuroscience,brownuniversity,providence,ri02912,u.s.a.michaelj.blackblack@cs.brown.edudepartmentofcomputerscience,brownuniversity,providence,ri02912,u.s.a.effectiveneuralmotorprosthesesrequireamethodfordecodingneuralactivityrepresentingdesiredmovement.inparticular,theaccuratere-constructionofacontinuousmotionsignalisnecessaryforthecontrolofdevicessuchascomputercursors,robots,orapatient\u2019sownparalyzedlimbs.forsuchapplications,wedevelopedareal-timesystemthatusesbayesianinferencetechniquestoestimatehandmotionfromthe\ufb01ringratesofmultipleneurons.inthisstudy,weusedrecordingsthatwerepreviouslymadeinthearma", "siam j. sci. comput.\nvol. 26, no. 1, pp. 313\u2013338\n\nc(cid:2) 2004 society for industrial and applied mathematics\n\nprincipal manifolds and nonlinear dimensionality\n\n\u2217\nreduction via tangent space alignment\n\n\u2020\nzhenyue zhang\n\n\u2021\nand hongyuan zha\n\nabstract. we present a new algorithm for manifold learning and nonlinear dimensionality\nreduction. based on a set of unorganized data points sampled with noise from a parameterized man-\nifold, the local geometry of the manifold is learned by constructing an approximation for the tangent\nspace at each data point, and those tangent spaces are then aligned to give the global coordinates\nof the data points with respect to the underlying manifold. we also present an error analysis of\nour algorithm showing that reconstruction errors can be quite small in some cases. we illustrate\nour algorithm using curves and surfaces both in two-dimensional/three-dimensional (2d/3d) eu-\nclidean spaces and in higher-dimensional euclidean spaces. we also address several th", "institute of mathematical statistics is collaborating with jstor to digitize, preserve, and extend access to\nstatistical science.\n\nwww.jstor.org\n\n\u00ae\n\n\f", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nanalyzing  biological  and  arti\ufb01cial  neural  networks:\nchallenges  with  opportunities  for  synergy?\ndavid  gt  barrett1,3,  ari  s  morcos1,3,4 and  jakob  h  macke2\n\ndeep  neural  networks  (dnns)  transform  stimuli  across  multiple\nprocessing  stages  to  produce  representations  that  can  be  used\nto  solve  complex  tasks,  such  as  object  recognition  in  images.\nhowever,  a  full  understanding  of  how  they  achieve  this  remains\nelusive.  the  complexity  of  biological  neural  networks\nsubstantially  exceeds  the  complexity  of  dnns,  making  it  even\nmore  challenging  to  understand  the  representations  they  learn.\nthus,  both  machine  learning  and  computational  neuroscience\nare  faced  with  a  shared  challenge:  how  can  we  analyze  their\nrepresentations  in  order  to  understand  how  they  solve  complex\ntasks?  we  review  how  data-analysis  concepts  and  techniques\ndevelope", "made: masked autoencoder for distribution estimation\n\n5\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n5\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n9\n0\n5\n3\n0\n\n.\n\n2\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nmathieu germain\nuniversit\u00b4e de sherbrooke, canada\nkarol gregor\ngoogle deepmind\niain murray\nuniversity of edinburgh, united kingdom\nhugo larochelle\nuniversit\u00b4e de sherbrooke, canada\n\nmathieu.germain2@usherbrooke.ca\n\nkarol.gregor@gmail.com\n\ni.murray@ed.ac.uk\n\nhugo.larochelle@usherbrooke.ca\n\nabstract\n\nthere has been a lot of recent interest in designing\nneural network models to estimate a distribution\nfrom a set of examples. we introduce a simple\nmodi\ufb01cation for autoencoder neural networks that\nyields powerful generative models. our method\nmasks the autoencoder\u2019s parameters to respect\nautoregressive constraints: each input is recon-\nstructed only from previous inputs in a given or-\ndering. constrained this way, the autoencoder\noutputs can be interpreted as a set of conditional\nprobabilities, and their product, the full joint prob-\nability. we can als", "end-to-end learned random walker\n\nfor seeded image segmentation\n\nlorenzo cerrone, alexander zeilmann, fred a. hamprecht\n\nheidelberg collaboratory for image processing\n\niwr, heidelberg university, germany\n\n{lorenzo.cerrone, alexander.zeilmann, fred.hamprecht}@iwr.uni-heidelberg.de\n\nabstract\n\nwe present an end-to-end learned algorithm for seeded\nsegmentation. our method is based on the random walker\nalgorithm, where we predict the edge weights of the under-\nlying graph using a convolutional neural network. this can\nbe interpreted as learning context-dependent diffusivities\nfor a linear diffusion process. besides calculating the exact\ngradient for optimizing these diffusivities, we also propose\nsimpli\ufb01cations that sparsely sample the gradient and still\nyield competitive results. the proposed method achieves\nthe currently best results on a seeded version of the cremi\nneuron segmentation challenge.\n\n1. introduction\n\nimage segmentation is the task of partitioning an image\ninto regions that a", "\u0006ajjahi\u0014j\u0006\u0014\u0006=jkha\n\nsynaptic plasticity in a \ncerebellum-like structure \ndepends on temporal order \ncurtis c. bell*, victor z. han*, yoshiko sugawarat \n& kirsty grant:!: \n* r.  s.  dow neurological sciences institute,  good samaritan hospital and \nmedical center,  1120 n. w  20th avenue, portland,  oregon  97209,  usa \nt institut alfred fessard,  cnrs 91190 gif-sur-yvette,  france \n:j:  department of physiology,  teikyo  university school of medicine,  kaga 2-11-1, \nitabashi-ku,  tokyo  173, japan \n\ncerebellum-like  structures  in  fish  appear  to  act  as  adaptive \nsensory processors,  in which learned  predictions about sensory \ninput  are  generated  and  subtracted  from  actual  sensory input, \nallowing unpredicted inputs to stand out'-3\u2022 pairing sensory input \nwith  centrally  originating  predictive  signals,  such  as  corollary \ndischarge  signals  linked  to  motor commands,  results  in  neural \nresponses to the predictive signals alone that are 'negative images' \nof the pr", "8\n1\n0\n2\n\n \n\nb\ne\nf\n3\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n5\n9\n4\n1\n0\n\n.\n\n7\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nhindsightexperiencereplaymarcinandrychowicz\u2217,filipwolski,alexray,jonasschneider,rachelfong,peterwelinder,bobmcgrew,joshtobin,pieterabbeel\u2020,wojciechzaremba\u2020openaiabstractdealingwithsparserewardsisoneofthebiggestchallengesinreinforcementlearning(rl).wepresentanoveltechniquecalledhindsightexperiencereplaywhichallowssample-ef\ufb01cientlearningfromrewardswhicharesparseandbinaryandthereforeavoidtheneedforcomplicatedrewardengineering.itcanbecom-binedwithanarbitraryoff-policyrlalgorithmandmaybeseenasaformofimplicitcurriculum.wedemonstrateourapproachonthetaskofmanipulatingobjectswitharoboticarm.inparticular,werunexperimentsonthreedifferenttasks:pushing,sliding,andpick-and-place,ineachcaseusingonlybinaryrewardsindicatingwhetherornotthetaskiscompleted.ourablationstudiesshowthathindsightexperiencereplayisacrucialingredientwhichmakestrainingpossibleinthesechallengingenvironments.weshowthatourpoliciestrainedonaphysi", "learning to simulate complex physics with graph networks\n\nalvaro sanchez-gonzalez * 1 jonathan godwin * 1 tobias pfaff * 1 rex ying * 1 2 jure leskovec 2\n\npeter w. battaglia 1\n\n0\n2\n0\n2\n\n \n\np\ne\ns\n4\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n0\n4\n9\n0\n\n.\n\n2\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nhere we present a machine learning framework\nand model implementation that can learn to\nsimulate a wide variety of challenging physi-\ncal domains, involving \ufb02uids, rigid solids, and\ndeformable materials interacting with one an-\nother. our framework\u2014which we term \u201cgraph\nnetwork-based simulators\u201d (gns)\u2014represents\nthe state of a physical system with particles, ex-\npressed as nodes in a graph, and computes dy-\nnamics via learned message-passing. our re-\nsults show that our model can generalize from\nsingle-timestep predictions with thousands of par-\nticles during training, to different initial condi-\ntions, thousands of timesteps, and at least an\norder of magnitude more particles at test time.\nour model was robust to hy", "bmc neuroscience\n\nbiomed central\n\noral presentation\nan online hebbian learning rule that performs independent \ncomponent analysis\nclaudia clopath*1, andr\u00e9 longtin2 and wulfram gerstner1\n\nopen access\n\naddress: 1lcn, epfl, lausanne, 1015, switzerland and 2cnd, university of ottawa, ottawa, ontario, canada\n\nemail: claudia clopath* - claudia.clopath@epfl.ch\n* corresponding author    \n\nfrom seventeenth annual computational neuroscience meeting: cns*2008\nportland, or, usa. 19\u201324 july 2008\n\npublished: 11 july 2008\n\nbmc neuroscience 2008, 9(suppl 1):o13\nthis abstract is available from: http://www.biomedcentral.com/1471-2202/9/s1/o13\n\ndoi:10.1186/1471-2202-9-s1-o13\n\n\u00a9 2008 clopath et al; licensee biomed central ltd. \n\nthe so-called cocktail party problem refers to a situation\nwhere  several  sound  sources  are  simultaneously  active,\ne.g. persons talking at the same time. the goal is to recover\nthe  initial  sound  sources  from  the  measurement  of  the\nmixed signals. a standard method of s", "research article\ntarget spike patterns enable efficient and\nbiologically plausible learning for complex\ntemporal tasks\n\npaolo muratoreid1\u262f*, cristiano capone2\u262f, pier stanislao paolucciid2\n\n1 sissa\u2014international school for advanced studies, trieste, italy, 2 infn, sezione di roma, rome, italy\n\n\u262f these authors contributed equally to this work.\n* pmurator@sissa.it\n\nabstract\n\nrecurrent spiking neural networks (rsnn) in the brain learn to perform a wide range of per-\nceptual, cognitive and motor tasks very efficiently in terms of energy consumption and their\ntraining requires very few examples. this motivates the search for biologically inspired\nlearning rules for rsnns, aiming to improve our understanding of brain computation and\nthe efficiency of artificial intelligence. several spiking models and learning rules have been\nproposed, but it remains a challenge to design rsnns whose learning relies on biologically\nplausible mechanisms and are capable of solving complex temporal tasks. in thi", "neuroscience  research  74  (2012)  177\u2013183\n\ncontents  lists  available  at  sciverse  sciencedirect\n\nneuroscience\n\n \n\nresearch\n\nj o  u  r  n  a l  h o m  e p  a g e :  w w w . e l s e v i e r . c o m / l o c a t e / n e u r e s\n\nupdate   article\nlearning   to   represent   reward   structure:   a   key   to   adapting   to   complex\nenvironments\nhiroyuki   nakahara a,\u2217,   okihide   hikosaka b\n\na laboratory  for  integrated  theoretical  neuroscience,  riken  brain  science  institute,  wako,  saitama  351-0198,  japan\nb laboratory  of  sensorimotor  research,  national  eye  institute,  bethesda,  md  20892,  usa\n\na\n\n \n\nr\n\n \n\nt\n\n \n\ni\n\n \n\nc\n\n \n\nl\n\n \n\ne\n\n \n\ni\n\n \n\nn\n\n \n\nf\n\n \n\no\n\na\n\nb\n\ns\n\nt\n\nr\n\na\n\nc\n\nt\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\narticle  history:\nreceived  25  april  2012\nreceived  in  revised  form  31  august  2012\naccepted  6  september  2012\navailable  online  13  october  2012\n\nkeywords:\nreward\ndopamine\nreinforcement  learning\ndecision\nvalue\nsalience\nstructure\n\n1.  introduction\n\npredictin", "deep generative stochastic networks trainable by backprop\n\nyoshua bengio\u2217\n\u00b4eric thibodeau-laufer\nguillaume alain\nd\u00b4epartement d\u2019informatique et recherche op\u00b4erationnelle, universit\u00b4e de montr\u00b4eal,\u2217& canadian inst. for advanced research\njason yosinski\ndepartment of computer science, cornell university\n\nfind.us@on.the.web\n\nabstract\n\ntraining principle for\nwe introduce a novel\nis an alternative to\nprobabilistic models that\nmaximum likelihood. the proposed generative\nstochastic networks (gsn) framework is based\non learning the transition operator of a markov\nchain whose stationary distribution estimates the\ndata distribution. the transition distribution of\nthe markov chain is conditional on the previous\nstate, generally involving a small move, so this\nconditional distribution has fewer dominant\nmodes, being unimodal in the limit of small\nmoves. thus, it is easier to learn because it\nis easier to approximate its partition function,\nmore like learning to perform supervised func-\ntion approxi", "article\nlocus coeruleus and dopaminergic \nconsolidation of everyday memory\n\ntomonori takeuchi1*, adrian j. duszkiewicz1*, alex sonneborn2*, patrick a. spooner1, miwako yamasaki3, \nmasahiko watanabe3, caroline c. smith2, guill\u00e9n fern\u00e1ndez4, karl deisseroth5, robert w. greene2,6 & richard g. m. morris1,7\n\ndoi:10.1038/nature19325\n\nthe retention of episodic-like memory is enhanced, in humans and animals, when something novel happens shortly \nbefore or after encoding. using an everyday memory task in mice, we sought the neurons mediating this dopamine-\ndependent novelty effect, previously thought to originate exclusively from the tyrosine-hydroxylase-expressing (th+) \nneurons in the ventral tegmental area. here we report that neuronal firing in the locus coeruleus is especially sensitive \nto environmental novelty, locus coeruleus th+ neurons project more profusely than ventral tegmental area th+ neurons \nto the hippocampus, optogenetic activation of locus coeruleus th+ neurons mimics the no", "making the invisible visible: verbal but not visual cues\nenhance visual detection\n\ngary lupyan1*, michael j. spivey2\n\n1 university of pennsylvania, philadelphia, pennsylvania, united states of america, 2 university of california merced, merced, california, united states of america\n\nabstract\n\nbackground:can hearing a word change what one sees? although visual sensitivity is known to be enhanced by attending\nto the location of the target, perceptual enhancements of following cues to the identity of an object have been difficult to\nfind. here, we show that perceptual sensitivity is enhanced by verbal, but not visual cues.\n\nmethodology/principalfindings:participants completed an object detection task in which they made an object-presence\nor -absence decision to briefly-presented letters. hearing the letter name prior to the detection task increased perceptual\nsensitivity (d9). a visual cue in the form of a preview of the to-be-detected letter did not. follow-up experiments found that\nthe a", "23; right 36, 13, and 27); superior frontal gyrus (left\n29, 31, and 45; right 17, 35, and 37).\n\n17. although the improvement in wm performance with\ncholinergic enhancement was a nonsigni(cid:222)cant trend\nin the current study (p 5 0.07), in a previous study\n(9) with a larger sample (n 5 13) the effect was\nhighly signi(cid:222)cant (p , 0.001). in the current study,\nwe analyzed rt data for six of our seven subjects\nbecause the behavioral data for one subject were\nunavailable due to a computer failure. the difference\nin the signi(cid:222)cance of the two (cid:222)ndings is simply a\nresult of the difference in sample sizes. a power\nanalysis shows that the size of the rt difference and\nvariability in the current sample would yield a signif-\nicant result (p 5 0.01) with a sample size of 13.\nduring the memory trials, mean rt was 1180 ms\nduring placebo and 1119 ms during physostigmine.\nduring the control trials, mean rt was 735 ms during\nplacebo and 709 ms during physostigmine, a differ-\nenc", "learning phrase representations using rnn encoder\u2013decoder\n\nfor statistical machine translation\n\nkyunghyun cho\n\nbart van merri\u00a8enboer caglar gulcehre\n\nuniversit\u00b4e de montr\u00b4eal\n\ndzmitry bahdanau\n\njacobs university, germany\n\n4\n1\n0\n2\n\n \n\np\ne\ns\n3\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n3\nv\n8\n7\n0\n1\n\n.\n\n6\n0\n4\n1\n:\nv\ni\nx\nr\na\n\nfirstname.lastname@umontreal.ca\n\nd.bahdanau@jacobs-university.de\n\nfethi bougares holger schwenk\n\nuniversit\u00b4e du maine, france\n\nyoshua bengio\n\nuniversit\u00b4e de montr\u00b4eal, cifar senior fellow\n\nfirstname.lastname@lium.univ-lemans.fr\n\nfind.me@on.the.web\n\nabstract\n\nin this paper, we propose a novel neu-\nral network model called rnn encoder\u2013\ndecoder that consists of two recurrent\nneural networks (rnn). one rnn en-\ncodes a sequence of symbols into a \ufb01xed-\nlength vector representation, and the other\ndecodes the representation into another se-\nquence of symbols. the encoder and de-\ncoder of the proposed model are jointly\ntrained to maximize the conditional prob-\nability of a target sequence given", "neuron, vol. 45, 599\u2013611, february 17, copyright \u00a92005 by elsevier inc. doi 10.1016/j.neuron.2005.02.001\n\ncascade models of synaptically stored memories\n\nstefano fusi,1 patrick j. drew,2 and l.f. abbott2,*\n1institute of physiology\nuniversity of bern\nb\u00fchlplatz 5\nch-3012, bern\nswitzerland\n2 volen center and\ndepartment of biology\nbrandeis university\nwaltham, massachusetts 02454\n\nsummary\n\nstoring memories of ongoing, everyday experiences\nrequires a high degree of plasticity, but retaining\nthese memories demands protection against changes\ninduced by further activity and experience. models in\nwhich memories are stored through switch-like tran-\nsitions in synaptic efficacy are good at storing but\nbad at retaining memories if these transitions are\nlikely, and they are poor at storage but good at reten-\ntion if they are unlikely. we construct and study a\nmodel in which each synapse has a cascade of states\nwith different levels of plasticity, connected by meta-\nplastic transitions. this cascade ", "massively parallel methods for deep reinforcement learning\n\n5\n1\n0\n2\n\n \nl\nu\nj\n \n\n6\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n9\n2\n4\n0\n\n.\n\n7\n0\n5\n1\n:\nv\ni\nx\nr\na\n\narun nair, praveen srinivasan, sam blackwell, cagdas alcicek, rory fearon, alessandro de maria, vedavyas\npanneershelvam, mustafa suleyman, charles beattie, stig petersen, shane legg, volodymyr mnih, koray\nkavukcuoglu, david silver\n{arunsnair, prav, blackwells, cagdasalcicek, roryf, ademaria, darthveda, mustafasul, cbeattie,\nsvp, legg, vmnih, korayk, davidsilver @google.com }\ngoogle deepmind, london\n\nabstract\n\nwe present the \ufb01rst massively distributed archi-\ntecture for deep reinforcement learning. this\narchitecture uses four main components: paral-\nlel actors that generate new behaviour; paral-\nlel learners that are trained from stored experi-\nence; a distributed neural network to represent\nthe value function or behaviour policy; and a dis-\ntributed store of experience. we used our archi-\ntecture to implement the deep q-network algo-\nrithm ", "lettercommunicatedbypeterdayanmakingworkingmemorywork:acomputationalmodeloflearningintheprefrontalcortexandbasalgangliarandallc.o\u2019reillyoreilly@psych.colorado.edumichaelj.frankfrankmj@psych.colorado.edudepartmentofpsychology,universityofcoloradoboulder,boulder,co80309,u.s.a.theprefrontalcortexhaslongbeenthoughttosubservebothworkingmemory(theholdingofinformationonlineforprocessing)andexecutivefunctions(decidinghowtomanipulateworkingmemoryandperformprocessing).althoughmanycomputationalmodelsofworkingmemoryhavebeendeveloped,themechanisticbasisofexecutivefunctionremainselusive,oftenamountingtoahomunculus.thisarticlepresentsanattempttodeconstructthishomunculusthroughpowerfullearningmechanismsthatallowacomputationalmodeloftheprefrontalcortextocontrolbothitselfandotherbrainareasinastrategic,task-appropriatemanner.theselearningmechanismsarebasedonsubcorticalstructuresinthemidbrain,basalganglia,andamygdala,whichtogetherformanactor-criticarchitecture.thecriticsystemlearnswhichprefrontalrepresent", "2\n1\n0\n2\n\n \nl\nu\nj\n \n\n3\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n0\n8\n5\n0\n\n.\n\n7\n0\n2\n1\n:\nv\ni\nx\nr\na\n\nimproving neural networks by preventing\n\nco-adaptation of feature detectors\n\ng. e. hinton\u2217, n. srivastava, a. krizhevsky, i. sutskever and r. r. salakhutdinov\n\ndepartment of computer science, university of toronto,\n6 king\u2019s college rd, toronto, ontario m5s 3g4, canada\n\n\u2217to whom correspondence should be addressed; e-mail: hinton@cs.toronto.edu\n\nwhen a large feedforward neural network is trained on a small training set,\nit typically performs poorly on held-out test data. this \u201cover\ufb01tting\u201d is greatly\nreduced by randomly omitting half of the feature detectors on each training\ncase. this prevents complex co-adaptations in which a feature detector is only\nhelpful in the context of several other speci\ufb01c feature detectors. instead, each\nneuron learns to detect a feature that is generally helpful for producing the\ncorrect answer given the combinatorially large variety of internal contexts in\nwhich it must operate", "12176 \u2022 the journal of neuroscience, november 7, 2007 \u2022 27(45):12176 \u201312189\n\nbehavioral/systems/cognitive\n\nneural ensembles in ca3 transiently encode paths forward\nof the animal at a decision point\n\nadam johnson1 and a. david redish2\n1center for cognitive sciences and 2department of neuroscience, univeristy of minnesota, minneapolis, minnesota 55455\n\nneural ensembles were recorded from the ca3 region of rats running on t-based decision tasks. examination of neural representations\nof space at fast time scales revealed a transient but repeatable phenomenon as rats made a decision: the location reconstructed from the\nneural ensemble swept forward, first down one path and then the other. estimated representations were coherent and preferentially\nswept ahead of the animal rather than behind the animal, implying it represented future possibilities rather than recently traveled paths.\nsimilar phenomena occurred at other important decisions (such as in recovery from an error). local field pote", "(2006).\n\n7, 365 (2005).\n\n16. c. j. marshall, cell 80, 179 (1995).\n17. s. sasagawa, y. ozaki, k. fujita, s. kuroda, nat. cell biol.\n18. l. o. murphy, j. blenis, trends biochem. sci. 31, 268\n19. m. villedieu et al., gynecol. oncol. 101, 507 (2006).\n20. b. k. choi, c. h. choi, h. l. oh, y. k. kim, neurotoxicity\n21. s. d. santos, p. j. verveer, p. i. bastiaens, nat. cell biol.\n22. a. acharya, s. b. ruvinov, j. gal, c. vinson, biochemistry\n23. m. ramezani-rad, curr. genet. 43, 161 (2003).\n24. c. wu, e. leberer, d. y. thomas, m. whiteway, mol. biol.\n25. x. l. zhan, r. j. deschenes, k. l. guan, genes dev. 11,\n\n25, 915 (2004).\n9, 247 (2007).\n41, 14122 (2002).\n\ncell 10, 2425 (1999).\n\n26. j. andersson, d. m. simpson, m. qi, y. wang, e. a. elion,\n\n1690 (1997).\nembo j. 23, 2564 (2004).\n\n785 (2002).\n\nlondon, ed. 2, 2003)\n\n27. r. p. bhattacharyya et al., science 311, 822 (2006).\n28. n. t. ingolia, a. w. murray, curr. biol. 17, 668 (2007).\n29. n. barkai, s. leibler, nature 387, 913 (1997).\n30. b. alb", "m\na\nc\nh\ni\nn\ne\n \nl\ne\na\nr\nn\ni\nn\ng\n,\n \n8\n,\n \n2\n7\n9\n-\n2\n9\n2\n \n(\n1\n9\n9\n2\n)\n \n\u00a9\n \n1\n9\n9\n2\n \nk\nl\nu\nw\ne\nr\n \na\nc\na\nd\ne\nm\ni\nc\n \np\nu\nb\nl\ni\ns\nh\ne\nr\ns\n,\n \nb\no\ns\nt\no\nn\n.\n \nm\na\nn\nu\nf\na\nc\nt\nu\nr\ne\nd\n \ni\nn\n \nt\nh\ne\n \nn\ne\nt\nh\ne\nr\nl\na\nn\nd\ns\n.\n \nt\ne\nc\nh\nn\ni\nc\na\nl\n \nn\no\nt\ne\n \nq\n-\nl\ne\na\nr\nn\ni\nn\ng\n \nc\nh\nr\ni\ns\nt\no\np\nh\ne\nr\n \nj\n.\nc\n.\nh\n.\n \nw\na\nt\nk\ni\nn\ns\n \n2\n5\nb\n \nf\nr\na\nm\nf\ni\ne\nl\nd\n \nr\no\na\nd\n,\n \nh\ni\ng\nh\nb\nu\nr\ny\n,\n \nl\no\nn\nd\no\nn\n \nn\n5\n \n1\nu\nu\n,\n \ne\nn\ng\nl\na\nn\nd\n \np\ne\nt\ne\nr\n \nd\na\ny\na\nn\n \nc\ne\nn\nt\nr\ne\n \nf\no\nr\n \nc\no\ng\nn\ni\nt\ni\nv\ne\n \ns\nc\ni\ne\nn\nc\ne\n,\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n \no\nf\n \ne\nd\ni\nn\nb\nu\nr\ng\nh\n,\n \n2\n \nb\nu\nc\nc\nl\ne\nu\nc\nh\n \np\nl\na\nc\ne\n,\n \ne\nd\ni\nn\nb\nu\nr\ng\nh\n \ne\nh\n8\n \n9\ne\nh\n,\n \ns\nc\no\nt\nl\na\nn\nd\n \na\nb\ns\nt\nr\na\nc\nt\n.\n \n~\n-\nl\ne\na\nr\nn\ni\nn\ng\n \n(\nw\na\nt\nk\ni\nn\ns\n,\n \n1\n9\n8\n9\n)\n \ni\ns\n \na\n \ns\ni\nm\np\nl\ne\n \nw\na\ny\n \nf\no\nr\n \na\ng\ne\nn\nt\ns\n \nt\no\n \nl\ne\na\nr\nn\n \nh\no\nw\n \nt\no\n \na\nc\nt\n \no\np\nt\ni\nm\na\nl\nl\ny\n \ni\nn\n \nc\no\nn\nt\nr\no\nl\nl\ne\nd\n \nm\na\nr\nk\no\nv\ni\na\nn\n \nd\no\nm\na\ni\nn\ns\n.\n \ni\nt\n \na\nm\no\nu\nn\nt\ns\n \nt\no\n \na\nn\n \ni\nn\nc\nr\ne\nm\ne\nn\nt\na\nl\n \nm\ne\nt\nh\no\nd\n \nf\no\nr\n \nd\n", "align, then memorise:\n\nthe dynamics of learning with feedback alignment\n\nmaria re\ufb01netti * 1 2 st\u00b4ephane d\u2019ascoli * 1 3 ruben ohana 1 4 sebastian goldt 5\n\nabstract\n\ndirect feedback alignment (dfa) is emerging\nas an ef\ufb01cient and biologically plausible alterna-\ntive to backpropagation for training deep neural\nnetworks. despite relying on random feedback\nweights for the backward pass, dfa successfully\ntrains state-of-the-art models such as transform-\ners. on the other hand, it notoriously fails to train\nconvolutional networks. an understanding of the\ninner workings of dfa to explain these diverg-\ning results remains elusive. here, we propose\na theory of feedback alignment algorithms. we\n\ufb01rst show that learning in shallow networks pro-\nceeds in two steps: an alignment phase, where the\nmodel adapts its weights to align the approximate\ngradient with the true gradient of the loss function,\nis followed by a memorisation phase, where the\nmodel focuses on \ufb01tting the data. this two-step\nprocess ha", "cell-type\u2013speci\ufb01c neuromodulation guides synaptic\ncredit assignment in a spiking neural network\n\nyuhan helena liua,b,c,1\n\n, stephen smithb,d\n\n, stefan mihalasa,b,c, eric shea-browna,b,c, and uygar s\u00fcmb\u00fclb,1\n\nadepartment of applied mathematics, university of washington, seattle, wa 98195; ballen institute for brain science, seattle, wa 98109; ccomputational\nneuroscience center, university of washington, seattle, wa 98195; and ddepartment of molecular and cellular physiology, stanford university, stanford,\nca 94305\n\nedited by terrence sejnowski, computational neurobiology laboratory, salk institute for biological studies, la jolla, ca; received june 28, 2021; accepted\noctober 28, 2021\n\nbrains learn tasks via experience-driven differential adjustment of\ntheir myriad individual synaptic connections, but the mechanisms\nthat target appropriate adjustment to particular connections\nremain deeply enigmatic. while hebbian synaptic plasticity,\nsynaptic eligibility traces, and top-down feedback si", "neuron\n\nperspective\n\nwhat is a cognitive map?\norganizing knowledge for flexible behavior\n\ntimothy e.j. behrens,1,2,* timothy h. muller,1 james c.r. whittington,1 shirley mark,2 alon b. baram,1\nkimberly l. stachenfeld,3 and zeb kurth-nelson3,4\n1wellcome centre for integrative neuroimaging, centre for functional magnetic resonance imaging of the brain, university of oxford,\njohn radcliffe hospital, oxford ox3 9du, uk\n2wellcome centre for human neuroimaging, institute of neurology, university college london, 12 queen square, london wc1n 3bg\n3deepmind, london, uk\n4max planck-ucl centre for computational psychiatry and ageing research, university college london, london, uk\n*correspondence: behrens@fmrib.ox.ac.uk\nhttps://doi.org/10.1016/j.neuron.2018.10.002\n\nit is proposed that a cognitive map encoding the relationships between entities in the world supports \ufb02exible\nbehavior, but the majority of the neural evidence for such a system comes from studies of spatial navigation.\nrecent work descr", "operant matching is a generic outcome of synaptic\nplasticity based on the covariance between reward\nand neural activity\n\nyonatan loewenstein* and h. sebastian seung\n\nhoward hughes medical institute and the department of brain and cognitive sciences, massachusetts institute of technology, cambridge, ma 02139\n\nedited by william t. newsome, stanford university school of medicine, stanford, ca, and approved august 12, 2006 (received for review june 23, 2005)\n\nthe probability of choosing an alternative in a long sequence of\nrepeated choices is proportional to the total reward derived from\nthat alternative, a phenomenon known as herrnstein\u2019s matching\nlaw. this behavior is remarkably conserved across species and\nexperimental conditions, but its underlying neural mechanisms still\nare unknown. here, we propose a neural explanation of this\nempirical law of behavior. we hypothesize that there are forms of\nsynaptic plasticity driven by the covariance between reward and\nneural activity and prove ma", "circuit models of low-dimensional shared\nvariability in cortical networks\n\narticle\n\nhighlights\nd low-dimensional shared variability can be generated in\n\nspatial network models\n\nd synaptic spatial and temporal scales determine the\n\ndimensions of shared variability\n\nd depolarizing inhibitory neurons suppresses the population-\n\nwide \ufb02uctuations\n\nd modeling the attentional modulation of variability within and\n\nbetween brain areas\n\nauthors\n\nchengcheng huang, douglas a. ruff,\nryan pyle, robert rosenbaum,\nmarlene r. cohen, brent doiron\n\ncorrespondence\nbdoiron@pitt.edu\n\nin brief\npopulation-wide \ufb02uctuations of neural\npopulation activity are widely observed in\ncortical recordings. huang et al. show\nthat turbulent dynamics in spatially\nordered recurrent networks give rise to\nlow-dimensional shared variability, which\ncan be suppressed by depolarizing\ninhibitory neurons.\n\nhuang et al., 2019, neuron 101, 337\u2013348\njanuary 16, 2019 \u00aa 2018 elsevier inc.\nhttps://doi.org/10.1016/j.neuron.2018.11.034\n\n\f", "siam review\nvol. 53, no. 2, pp. 217\u2013288\n\nc(cid:1) 2011 society for industrial and applied mathematics\n\nfinding structure with randomness:\nprobabilistic algorithms for\nconstructing approximate\n\u2217\nmatrix decompositions\n\nn. halko\np. g. martinsson\nj. a. tropp\n\n\u2020\n\u2020\n\u2021\n\nabstract. low-rank matrix approximations, such as the truncated singular value decomposition and\nthe rank-revealing qr decomposition, play a central role in data analysis and scienti\ufb01c\ncomputing. this work surveys and extends recent research which demonstrates that ran-\ndomization o\ufb00ers a powerful tool for performing low-rank matrix approximation. these\ntechniques exploit modern computational architectures more fully than classical methods\nand open the possibility of dealing with truly massive data sets. this paper presents a\nmodular framework for constructing randomized algorithms that compute partial matrix\ndecompositions. these methods use random sampling to identify a subspace that captures\nmost of the action of a matrix. t", "https://doi.org/10.1038/s41583-023-00693-x\n\n check for updates\n check for updates\n\na unifying perspective on neural \nmanifolds and circuits for cognition\n\nsections\n\nintroduction\n\ncircuit\u2013manifold convergence: \nhead direction system\n\ntowards convergence in  \ngrid cells\n\ncircuits with mixed selectivity\n\nconclusions and perspectives\n\nchristopher langdon\u2009\nabstract\n\n \u20091,2,3, mikhail genkin\u2009\n\n \u20092,3 & tatiana a. engel\u2009\n\n \u20091,2 \n\ntwo different perspectives have informed efforts to explain the link \nbetween the brain and behaviour. one approach seeks to identify \nneural circuit elements that carry out specific functions, emphasizing \nconnectivity between neurons as a substrate for neural computations. \nanother approach centres on neural manifolds \u2014 low-dimensional \nrepresentations of behavioural signals in neural population activity \u2014  \nand suggests that neural computations are realized by emergent \ndynamics. although manifolds reveal an interpretable structure in \nheterogeneous neuronal activit", "learning important features through propagating activation differences\n\navanti shrikumar 1 peyton greenside 1 anshul kundaje 1\n\nabstract\n\nthe purported \u201cblack box\u201d nature of neural\nnetworks is a barrier to adoption in applica-\ntions where interpretability is essential. here\nwe present deeplift (deep learning impor-\ntant features), a method for decomposing the\noutput prediction of a neural network on a spe-\nci\ufb01c input by backpropagating the contributions\nof all neurons in the network to every feature\nof the input. deeplift compares the activa-\ntion of each neuron to its \u2018reference activation\u2019\nand assigns contribution scores according to the\ndifference. by optionally giving separate con-\nsideration to positive and negative contributions,\ndeeplift can also reveal dependencies which\nare missed by other approaches. scores can\nbe computed ef\ufb01ciently in a single backward\npass. we apply deeplift to models trained\non mnist and simulated genomic data, and\nshow signi\ufb01cant advantages over gradient", "article\n\ndoi: 10.1038/s41467-018-06560-z\n\nopen\n\ncortical population activity within a preserved\nneural manifold underlies multiple motor behaviors\n\njuan a. gallego\nsara a. solla1,5 & lee e. miller\n\n1,3,6\n\n1,2, matthew g. perich\n\n3, stephanie n. naufel3, christian ethier\n\n4,\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\npopulations of cortical neurons \ufb02exibly perform different functions; for the primary motor\ncortex (m1) this means a rich repertoire of motor behaviors. we investigate the \ufb02exibility of\nm1 movement control by analyzing neural population activity during a variety of skilled wrist\nand reach-to-grasp tasks. we compare across tasks the neural modes that capture dominant\nneural covariance patterns during each task. while each task requires different patterns of\nmuscle and single unit activity, we \ufb01nd unexpected similarities at the neural population level:\nthe structure and activity of the neural modes is largely preserved across tasks. furthermore,\nwe \ufb01nd two sets of neural modes with task-", "b\nr\na\ni\nn\n \nr\ne\ns\ne\na\nr\nc\nh\n \nr\ne\nv\ni\ne\nw\ns\n \ne\nl\ns\ne\nv\ni\ne\nr\n \nb\nr\na\ni\nn\n \nr\ne\ns\ne\na\nr\nc\nh\n \nr\ne\nv\ni\ne\nw\ns\n \n2\n1\n \n(\n1\n9\n9\n6\n)\n \n2\n1\n9\n-\n2\n4\n5\n \nf\nu\nl\nl\n-\nl\ne\nn\ng\nt\nh\n \nr\ne\nv\ni\ne\nw\n \nt\nh\ne\n \na\ns\nc\ne\nn\nd\ni\nn\ng\n \nn\ne\nu\nr\no\nm\no\nd\nu\nl\na\nt\no\nr\ny\n \ns\ny\ns\nt\ne\nm\ns\n \ni\nn\n \nl\ne\na\nr\nn\ni\nn\ng\n \nb\ny\n \nr\ne\ni\nn\nf\no\nr\nc\ne\nm\ne\nn\nt\n:\n \nc\no\nm\np\na\nr\ni\nn\ng\n \nc\no\nm\np\nu\nt\na\nt\ni\no\nn\na\nl\n \nc\no\nn\nj\ne\nc\nt\nu\nr\ne\ns\n \nw\ni\nt\nh\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nf\ni\nn\nd\ni\nn\ng\ns\n \nc\ny\nr\ni\ne\nl\n \nm\n.\na\n.\n \np\ne\nn\nn\na\nr\nt\nz\n \n*\n \np\nh\ny\ns\ni\nc\ns\n \no\nf\n \nc\no\nm\np\nu\nt\na\nt\ni\no\nn\n,\n \nc\na\nl\ni\nf\no\nr\nn\ni\na\n \ni\nn\ns\nt\ni\nt\nu\nt\ne\n \no\nf\n \nt\ne\nc\nh\nn\no\nl\no\ng\ny\n,\n \nm\na\ni\nl\n \nc\no\nd\ne\n \n1\n3\n9\n-\n7\n4\n,\n \np\na\ns\na\nd\ne\nn\na\n,\n \nc\na\n \n9\n1\n1\n2\n5\n,\n \nu\ns\na\n \na\nc\nc\ne\np\nt\ne\nd\n \n2\n7\n \nn\no\nv\ne\nm\nb\ne\nr\n \n1\n9\n9\n5\n \na\nb\ns\nt\nr\na\nc\nt\n \na\n \nc\ne\nn\nt\nr\na\nl\n \np\nr\no\nb\nl\ne\nm\n \ni\nn\n \nc\no\ng\nn\ni\nt\ni\nv\ne\n \nn\ne\nu\nr\no\ns\nc\ni\ne\nn\nc\ne\n \ni\ns\n \nh\no\nw\n \na\nn\ni\nm\na\nl\ns\n \nc\na\nn\n \nm\na\nn\na\ng\ne\n \nt\no\n \nr\na\np\ni\nd\nl\ny\n \nm\na\ns\nt\ne\nr\n \nc\no\nm\np\nl\ne\nx\n \ns\ne\nn\ns\no\nr\ni\nm\no\nt\no\nr\n \nt\na\ns\nk\ns\n \nw\nh\n", "7\n1\n0\n2\n\n \n\ny\na\nm\n5\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n3\nv\n0\n4\n9\n6\n0\n\n.\n\n0\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nsafety veri\ufb01cation of deep neural networks(cid:63)\n\nxiaowei huang, marta kwiatkowska, sen wang and min wu\n\ndepartment of computer science, university of oxford\n\nabstract. deep neural networks have achieved impressive experimental results\nin image classi\ufb01cation, but can surprisingly be unstable with respect to adversar-\nial perturbations, that is, minimal changes to the input image that cause the net-\nwork to misclassify it. with potential applications including perception modules\nand end-to-end controllers for self-driving cars, this raises concerns about their\nsafety. we develop a novel automated veri\ufb01cation framework for feed-forward\nmulti-layer neural networks based on satis\ufb01ability modulo theory (smt). we\nfocus on safety of image classi\ufb01cation decisions with respect to image manipu-\nlations, such as scratches or changes to camera angle or lighting conditions that\nwould result in the same class be", "gradient descent for spiking neural networks\n\ndongsung huh\nsalk institute\n\nla jolla, ca 92037\n\nhuh@salk.edu\n\nterrence j. sejnowski\n\nsalk institute\n\nla jolla, ca 92037\nterry@salk.edu\n\nabstract\n\nmost large-scale network models use neurons with static nonlinearities that pro-\nduce analog output, despite the fact that information processing in the brain is\npredominantly carried out by dynamic neurons that produce discrete pulses called\nspikes. research in spike-based computation has been impeded by the lack of\nef\ufb01cient supervised learning algorithm for spiking neural networks. here, we\npresent a gradient descent method for optimizing spiking network models by in-\ntroducing a differentiable formulation of spiking dynamics and deriving the exact\ngradient calculation. for demonstration, we trained recurrent spiking networks on\ntwo dynamic tasks: one that requires optimizing fast (\u2248 millisecond) spike-based\ninteractions for ef\ufb01cient encoding of information, and a delayed-memory task over\nexten", "3\n2\n0\n2\n\n \n\np\ne\ns\n \n2\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n2\n0\n4\n6\n0\n\n.\n\n9\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nexpressive dynamics models with nonlinear injective\nreadouts enable reliable recovery of latent features\n\nfrom neural activity\n\nchristopher versteeg1 andrew r. sedler1,2\n\nchethan pandarinath1,2\n\njonathan d. mccart1,2\n\n1 wallace h. coulter department of biomedical engineering\n\nemory university and georgia institute of technology\n\natlanta, ga, usa\n\n2 center for machine learning\ngeorgia institute of technology\n\natlanta, ga, usa\n\nabstract\n\nthe advent of large-scale neural recordings has enabled new approaches that\naim to discover the computational mechanisms of neural circuits by understand-\ning the rules that govern how their state evolves over time. while these neural\ndynamics cannot be directly measured, they can typically be approximated by\nlow-dimensional models in a latent space. how these models represent the map-\nping from latent space to neural space can affect the interpretability of th", "8\n1\n0\n2\n\n \nr\na\n\nm\n \n1\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n0\n7\n7\n7\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2018\n\nemergence of grid-like representations by\ntraining recurrent neural networks to\nperform spatial localization\n\nchristopher j. cueva\u2217, xue-xin wei\u2217\ncolumbia university\nnew york, ny 10027, usa\n{ccueva,weixxpku}@gmail.com\n\nabstract\n\ndecades of research on the neural code underlying spatial navigation have re-\nvealed a diverse set of neural response properties. the entorhinal cortex (ec) of\nthe mammalian brain contains a rich set of spatial correlates, including grid cells\nwhich encode space using tessellating patterns. however, the mechanisms and\nfunctional signi\ufb01cance of these spatial representations remain largely mysterious.\nas a new way to understand these neural representations, we trained recurrent\nneural networks (rnns) to perform navigation tasks in 2d arenas based on veloc-\nity inputs. surprisingly, we \ufb01nd that grid-like spatial response patt", "letter\n\ncommunicated by michael hasselmo\n\nhow inhibitory oscillations can train neural networks and\npunish competitors\n\n\u2217\n\n\u2217\n\nkenneth a. norman\nknorman@princeton.edu\nehren newman\nenewman@princeton.edu\ngreg detre\ngdetre@princeton.edu\nsean polyn\npolyn@psych.upenn.edu\ndepartment of psychology, princeton university, princeton, nj 08544, u.s.a.\n\nwe present a new learning algorithm that leverages oscillations in the\nstrength of neural inhibition to train neural networks. raising inhibition\ncan be used to identify weak parts of target memories, which are then\nstrengthened. conversely, lowering inhibition can be used to identify\ncompetitors, which are then weakened. to update weights, we apply the\ncontrastive hebbian learning equation to successive time steps of the\nnetwork. the sign of the weight change equation varies as a function of\nthe phase of the inhibitory oscillation. we show that the learning algo-\nrithm can memorize large numbers of correlated input patterns without\ncollapsing and t", "the journal of neuroscience, february 4, 2004 \u2022 24(5):1089 \u20131100 \u2022 1089\n\nbehavioral/systems/cognitive\n\nlinearity of cortical receptive fields measured with\nnatural sounds\n\nchristian k. machens, michael s. wehr, and anthony m. zador\ncold spring harbor laboratory, cold spring harbor, new york 11724\n\nhow do cortical neurons represent the acoustic environment? this question is often addressed by probing with simple stimuli such as\nclicks or tone pips. such stimuli have the advantage of yielding easily interpreted answers, but have the disadvantage that they may fail to\nuncover complex or higher-order neuronal response properties. here, we adopt an alternative approach, probing neuronal responses\nwith complex acoustic stimuli, including animal vocalizations. we used in vivo whole-cell methods in the rat auditory cortex to record\nsubthreshold membrane potential fluctuations elicited by these stimuli. most neurons responded robustly and reliably to the complex\nstimuli in our ensemble. using r", "6\n1\n0\n2\n\n \n\nv\no\nn\n2\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n3\nv\n9\n8\n2\n0\n0\n\n.\n\n4\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nin press at behavioral and brain sciences.\n\nbuilding machines that learn and think like people\n\nbrenden m. lake,1 tomer d. ullman,2,4 joshua b. tenenbaum,2,4 and samuel j. gershman3,4\n\n1center for data science, new york university\n\n2department of brain and cognitive sciences, mit\n\n3department of psychology and center for brain science, harvard university\n\n4center for brains minds and machines\n\nabstract\n\nrecent progress in arti\ufb01cial intelligence (ai) has renewed interest in building systems that\nlearn and think like people. many advances have come from using deep neural networks trained\nend-to-end in tasks such as object recognition, video games, and board games, achieving perfor-\nmance that equals or even beats humans in some respects. despite their biological inspiration\nand performance achievements, these systems di\ufb00er from human intelligence in crucial ways.\nwe review progress in cognitive science", "a r t i c l e s\n\nbrief optogenetic inhibition of dopamine neurons \nmimics endogenous negative reward prediction errors\nchun yun chang1, guillem r esber2, yasmin marrero-garcia1, hau-jie yau1, antonello bonci1,3,4 &  \ngeoffrey schoenbaum1,3,5\n\ncorrelative studies have strongly linked phasic changes in dopamine activity with reward prediction error signaling. but causal \nevidence that these brief changes in firing actually serve as error signals to drive associative learning is more tenuous. although \nthere is direct evidence that brief increases can substitute for positive prediction errors, there is no comparable evidence that \nsimilarly brief pauses can substitute for negative prediction errors. in the absence of such evidence, the effect of increases in \nfiring could reflect novelty or salience, variables also correlated with dopamine activity. here we provide evidence in support of \nthe proposed linkage, showing in a modified pavlovian over-expectation task that brief pauses in the ", "a r t i c l e s\n\nneural antecedents of self-initiated actions  \nin secondary motor cortex\nmasayoshi murakami, m in\u00eas vicente, gil m costa & zachary f mainen\nthe neural origins of spontaneous or self-initiated actions are not well understood and their interpretation is controversial.  \nto address these issues, we used a task in which rats decide when to abort waiting for a delayed tone. we recorded neurons  \nin the secondary motor cortex (m2) and interpreted our findings in light of an integration-to-bound decision model. a first \npopulation of m2 neurons ramped to a constant threshold at rates proportional to waiting time, strongly resembling integrator \noutput. a second population, which we propose provide input to the integrator, fired in sequences and showed trial-to-trial rate \nfluctuations correlated with waiting times. an integration model fit to these data also quantitatively predicted the observed  \ninter-neuronal correlations. together, these results reinforce the generality o", "cognitive psychology 96 (2017) 1\u201325\n\ncontents lists available at sciencedirect\n\ncognitive psychology\n\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / c o g p s y c h\n\nwhere do hypotheses come from?\nishita dasgupta a,\u21d1\n\n, eric schulz b, samuel j. gershman c\n\na department of physics and center for brain science, harvard university, united states\nb department of experimental psychology, university college london, united kingdom\nc department of psychology and center for brain science, harvard university, united states\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\naccepted 16 may 2017\n\nkeywords:\nhypothesis generation\nbayesian inference\nmonte carlo methods\n\nwhy are human inferences sometimes remarkably close to the bayesian ideal and other\ntimes systematically biased? in particular, why do humans make near-rational inferences\nin some natural domains where the candidate hypotheses are explicitly available, whereas\ntasks in similar domains requiring", "original research\npublished: 17 december 2015\ndoi: 10.3389/fncom.2015.00149\n\nneural network model of memory\nretrieval\n\nstefano recanatesi 1, mikhail katkov 1, sandro romani 2 and misha tsodyks 1, 3*\n\n1 department of neurobiology, weizmann institute of science, rehovot, israel, 2 janelia farm research campus, howard\nhughes medical institute, ashburn, va, usa, 3 department of neurotechnologies, lobachevsky state university of nizhny\n\nnovgorod, nizhny novgorod, russia\n\nhuman memory can store large amount of information. nevertheless, recalling is often\na challenging task. in a classical free recall paradigm, where participants are asked to\nrepeat a brie\ufb02y presented list of words, people make mistakes for lists as short as 5\nwords. we present a model for memory retrieval based on a hop\ufb01eld neural network\nwhere transition between items are determined by similarities in their long-term memory\nrepresentations. mean\ufb01eld analysis of the model reveals stable states of the network\ncorresponding (", "scaling learning algorithms towards ai\n\nyoshua bengio (1) and yann lecun (2)\n\n(1) yoshua.bengio@umontreal.ca\n\nd\u00b4epartement d\u2019informatique et recherche op\u00b4erationnelle\n\nuniversit\u00b4e de montr\u00b4eal,\n\n(2) yann@cs.nyu.edu\n\nthe courant institute of mathematical sciences,\n\nnew york university, new york, ny\n\nto appear in \u201clarge-scale kernel machines\u201d,\n\nl. bottou, o. chapelle, d. decoste, j. weston (eds)\n\nmit press, 2007\n\nabstract\n\none long-term goal of machine learning research is to produce methods that\nare applicable to highly complex tasks, such as perception (vision, audition), rea-\nsoning, intelligent control, and other arti\ufb01cially intelligent behaviors. we argue\nthat in order to progress toward this goal, the machine learning community must\nendeavor to discover algorithms that can learn highly complex functions, with min-\nimal need for prior knowledge, and with minimal human intervention. we present\nmathematical and empirical evidence suggesting that many popular approaches\nto non-parametr", "chapter\n\n22\n\nmedial prefrontal cortex\nand the adaptive regulation\nof reinforcement learning\nparameters\nmehdi khamassi*,{,{,},1, pierre enel*,{, peter ford dominey*,{, emmanuel procyk*,{\n\u204e\ninserm u846, stem cell and brain research institute, bron, france\n{\nuniversite\u00b4 de lyon, lyon 1, umr-s 846, lyon, france\n{institut des syste`mes intelligents et de robotique, universite\u00b4 pierre et marie\ncurie-paris 6, paris cedex 05, france\n}\ncnrs umr 7222, paris, france\n1corresponding author. tel.: \u00fe33-144272885, fax.: \u00fe33-144275145, e-mail address: mehdi.\nkhamassi@isir.upmc.fr\n\nabstract\n\nconverging evidence suggest that the medial prefrontal cortex (mpfc) is involved in feedback\ncategorization, performance monitoring, and task monitoring, and may contribute to the online\nregulation of reinforcement learning (rl) parameters that would affect decision-making pro-\ncesses in the lateral prefrontal cortex (lpfc). previous neurophysiological experiments have\nshown mpfc activities encoding error likelihood", "article\n\nreceived 17 feb 2017 | accepted 26 may 2017 | published 13 jul 2017\n\ndoi: 10.1038/ncomms16091\n\nopen\n\nprecise inhibitory microcircuit assembly of\ndevelopmentally related neocortical interneurons\nin clusters\nxin-jun zhang1, zhizhong li1, zhi han2,3, khadeejah t. sultan1,4, kun huang2 & song-hai shi1,4\n\ngaba-ergic interneurons provide diverse inhibitions that are essential for the operation of\nneuronal circuits in the neocortex. however, the mechanisms that control the functional\norganization of neocortical\ninterneurons remain largely unknown. here we show that\ndevelopmental origins in\ufb02uence \ufb01ne-scale synapse formation and microcircuit assembly of\nneocortical interneurons. spatially clustered neocortical interneurons originating from low-\ntitre retrovirus-infected radial glial progenitors in the embryonic medial ganglionic eminence\nand preoptic area preferentially develop electrical, but not chemical, synapses with each\nother. this lineage-related electrical coupling forms predom", "pre-train, prompt, and predict: a systematic survey of\nprompting methods in natural language processing\n\npengfei liu\n\nweizhe yuan\n\njinlan fu\n\ncarnegie mellon university\n\npliu3@cs.cmu.edu\n\ncarnegie mellon university\nweizhey@cs.cmu.edu\n\nnational university of singapore\njinlanjonna@gmail.com\n\nzhengbao jiang\n\ncarnegie mellon university\nzhengbaj@cs.cmu.edu\n\nhiroaki hayashi\n\ncarnegie mellon university\nhiroakih@cs.cmu.edu\n\ngraham neubig\n\ncarnegie mellon university\ngneubig@cs.cmu.edu\n\nabstract\n\nthis paper surveys and organizes research works in a new paradigm in natural language processing, which\nwe dub \u201cprompt-based learning\u201d. unlike traditional supervised learning, which trains a model to take in an\ninput x and predict an output y as p (y|x), prompt-based learning is based on language models that model\nthe probability of text directly. to use these models to perform prediction tasks, the original input x is\nmodi\ufb01ed using a template into a textual string prompt x(cid:48) that has some un\ufb01lled", "a\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\nt\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nhhs public access\nauthor manuscript\nneural comput. author manuscript; available in pmc 2021 november 17.\n\npublished in final edited form as:\nneural comput. 2021 april 13; 33(5): 1300\u20131328. doi:10.1162/neco_a_01374.\n\ncontrastive similarity matching for supervised learning\n\nshanshan qin,\njohn a. paulson school of engineering and applied sciences, harvard university, cambridge,\nma 02138, u.s.a.\n\nnayantara mudur,\ndepartment of physics, harvard university, cambridge, ma 02138, u.s.a.\n\ncengiz pehlevan\njohn a. paulson school of engineering and applied sciences, harvard university, cambridge,\nma 02138, u.s.a.\n\nabstract\n\nwe propose a novel biologically plausible solution to the credit assignment problem motivated by\nobservations in the ventral visual pathway and trained deep neural networks. in both,\nrepresentations of objects in the same cate", "assembling old tricks for new tasks: a neural model\n\nof instructional learning and control\n\ntsung-ren huang, thomas e. hazy, seth a. herd,\n\nand randall c. o\u02bcreilly\n\nabstract\n\u25a0 we can learn from the wisdom of others to maximize success.\nhowever, it is unclear how humans take advice to flexibly\nadapt behavior. on the basis of data from neuroanatomy, neuro-\nphysiology, and neuroimaging, a biologically plausible model is\ndeveloped to illustrate the neural mechanisms of learning from\ninstructions. the model consists of two complementary learn-\ning pathways. the slow-learning parietal pathway carries out\nsimple or habitual stimulus\u2013response (s-r) mappings, whereas\n\nthe fast-learning hippocampal pathway implements novel s-r\nrules. specifically, the hippocampus can rapidly encode arbitrary\ns-r associations, and stimulus-cued responses are later recalled\ninto the basal ganglia-gated pfc to bias response selection in\nthe premotor and motor cortices. the interactions between\nthe two model learnin", "neural networks 22 (2009) 1484\u20131497\n\ncontents lists available at sciencedirect\n\nneural networks\n\njournal homepage: www.elsevier.com/locate/neunet\n\nreal-time reinforcement learning by sequential actor\u2013critics and experience\nreplay\npawe\u0142 wawrzy\u0144ski\nwarsaw university of technology, institute of control and computation engineering, nowowiejska 15/19, 00-665 warsaw, poland\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 4 december 2008\nrevised and accepted 25 may 2009\n\nkeywords:\ndirect adaptive control\nmachine learning\nreinforcement learning\nexperience replay\n\nactor\u2013critics constitute an important class of reinforcement learning algorithms that can deal with\ncontinuous actions and states in an easy and natural way. this paper shows how these algorithms can\nbe augmented by the technique of experience replay without degrading their convergence properties, by\nappropriately estimating the policy change direction. this is achieved by truncated importance sampling\napplied to t", "biorxiv preprint \nthe copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is \n\nhttps://doi.org/10.1101/2023.07.18.549575\n; \n\nthis version posted july 21, 2023. \n\ndoi: \n\nmade available under a\n\ncc-by-nc 4.0 international license\n.\n\nnonlinear manifolds underlie neural population\n\nactivity during behaviour\n\nc\u00b4atia fortunato1, jorge bennasar-v\u00b4azquez1, junchol park2, joanna c. chang1,\nlee e. miller3,4,5, joshua t. dudman2, matthew g. perich6,7, juan a. gallego 1,\u2020\n\n1department of bioengineering, imperial college london, london uk.\n\n2janelia research campus, howard hughes medical institute, ashburn va, usa.\n\n3department of neurosciences, northwestern university, chicago il, usa.\n\n4department of biomedical engineering, northwestern university, chicago il, usa.\n\n5department of physical medicine and rehabilitation, northwestern university, chicago il, usa, and shirley ryan ", "trap of feature diversity in the learning of mlps\n\ndongrui liua\u2217\n\nshaobo wangb\u2217 jie rena kangrui wanga\n\nhuiqi denga quanshi zhanga \u2020\n\nsheng yina\n\nashanghai jiao tong university\n\nbharbin institute of technology\n\n2\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n0\n8\n9\n0\n0\n\n.\n\n2\n1\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nin this paper, we focus on a typical two-phase phenomenon in the learning of\nmulti-layer perceptrons (mlps), and we aim to explain the reason for the decrease\nof feature diversity in the \ufb01rst phase. speci\ufb01cally, people \ufb01nd that, in the training\nof mlps, the training loss does not decrease signi\ufb01cantly until the second phase.\nto this end, we further explore the reason why the diversity of features over\ndifferent samples keeps decreasing in the \ufb01rst phase, which hurts the optimization\nof mlps. we explain such a phenomenon in terms of the learning dynamics of\nmlps. furthermore, we theoretically explain why four typical operations can\nalleviate the decrease of the feature diversity. the ", "architectural universality of neural tangent kernel training dynamics\n\ntensor programs iib:\n\ngreg yang 1 * etai littwin 2 *\n\nabstract\n\nyang (2020a) recently showed that the neural\ntangent kernel (ntk) at initialization has an\nin\ufb01nite-width limit for a large class of architec-\ntures including modern staples such as resnet\nand transformers. however, their analysis does\nnot apply to training. here, we show the same neu-\nral networks (in the so-called ntk parametriza-\ntion) during training follow a kernel gradient de-\nscent dynamics in function space, where the ker-\nnel is the in\ufb01nite-width ntk. this completes the\nproof of the architectural universality of ntk\nbehavior. to achieve this result, we apply the ten-\nsor programs technique: write the entire sgd\ndynamics inside a tensor program and analyze\nit via the master theorem. to facilitate this\nproof, we develop a graphical notation for tensor\nprograms. see the full version of our paper at\narxiv.org/abs/2105.03703.\n\n1. introduction\n(jacot ", "gaussian gated linear networks\n\ndavid budden\u2217 adam h. marblestone\u2217 eren sezener\u2217\n\ntor lattimore greg wayne\u2020\n\njoel veness\u2020\n\ndeepmind\n\naixi@google.com\n\nabstract\n\nwe propose the gaussian gated linear network (g-gln), an extension to the\nrecently proposed gln family of deep neural networks. instead of using back-\npropagation to learn features, glns have a distributed and local credit assignment\nmechanism based on optimizing a convex objective. this gives rise to many\ndesirable properties including universality, data-ef\ufb01cient online learning, trivial\ninterpretability and robustness to catastrophic forgetting. we extend the gln\nframework from classi\ufb01cation to multiple regression and density modelling by\ngeneralizing geometric mixing to a product of gaussian densities. the g-gln\nachieves competitive or state-of-the-art performance on several univariate and mul-\ntivariate regression benchmarks, and we demonstrate its applicability to practical\ntasks including online contextual bandits and dens", "7\n1\n0\n2\n\n \nr\np\na\n9\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n6\n9\n7\n5\n0\n\n.\n\n4\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nquantifying interpretability of deep visual representations\n\nnetwork dissection:\n\ndavid bau\u2217, bolei zhou\u2217, aditya khosla, aude oliva, and antonio torralba\n\n{davidbau, bzhou, khosla, oliva, torralba}@csail.mit.edu\n\ncsail, mit\n\nabstract\n\nlamps in places net\n\nwheels in object net\n\npeople in video net\n\nwe propose a general framework called network dissec-\ntion for quantifying the interpretability of latent representa-\ntions of cnns by evaluating the alignment between individ-\nual hidden units and a set of semantic concepts. given any\ncnn model, the proposed method draws on a broad data\nset of visual concepts to score the semantics of hidden units\nat each intermediate convolutional layer. the units with\nsemantics are given labels across a range of objects, parts,\nscenes, textures, materials, and colors. we use the proposed\nmethod to test the hypothesis that interpretability of units\nis equivalent to rand", "letter\n\ncommunicated by lea duncker\n\nprobing the relationship between latent linear dynamical\nsystems and low-rank recurrent neural network models\n\nadrian valente\nadrian.valente@ens.fr\nsrdjan ostojic\nsrdjan.ostojic@ens.fr\nlaboratoire de neurosciences cognitives et computationnelles, inserm u960,\necole normale superieure\u2013psl research university, 75005 paris, france\n\njonathan w. pillow\npillow@princeton.edu\nprinceton neuroscience institute, princeton university, princeton,\nnj 08544, u.s.a.\n\na large body of work has suggested that neural populations exhibit low-\ndimensional dynamics during behavior. however, there are a variety of\ndifferent approaches for modeling low-dimensional neural population\nactivity. one approach involves latent linear dynamical system (lds)\nmodels, in which population activity is described by a projection of low-\ndimensional latent variables with linear dynamics. a second approach\ninvolves low-rank recurrent neural networks (rnns), in which popula-\ntion activity ar", "letter\n\ncommunicated by laurence abbott\n\nspike-timing-dependent hebbian plasticity as temporal\ndifference learning\n\nrajesh p. n. rao\ndepartment of computer science and engineering, university of washington,\nseattle, wa 98195-2350, u.s.a.\n\nterrence j. sejnowski\nhoward hughes medical institute, the salk institute for biological studies, la jolla,\nca 92037, u.s.a., and department of biology, university of california at san diego,\nla jolla, ca 92037, u.s.a.\n\na spike-timing-dependent hebbian mechanism governs the plasticity\nof recurrent excitatory synapses in the neocortex: synapses that are ac-\ntivated a few milliseconds before a postsynaptic spike are potentiated,\nwhile those that are activated a few milliseconds after are depressed. we\nshow that such a mechanism can implement a form of temporal difference\nlearning for prediction of input sequences. using a biophysical model of\na cortical neuron, we show that a temporal difference rule used in con-\njunction with dendritic backpropagating ", "empirical evaluation of\n\ngated recurrent neural networks\n\non sequence modeling\n\n4\n1\n0\n2\n\n \nc\ne\nd\n1\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n5\n5\n5\n3\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\njunyoung chung\n\ncaglar gulcehre\n\nuniversit\u00b4e de montr\u00b4eal\n\nkyunghyun cho\n\nyoshua bengio\n\nuniversit\u00b4e de montr\u00b4eal\ncifar senior fellow\n\nabstract\n\nin this paper we compare different types of recurrent units in recurrent neural net-\nworks (rnns). especially, we focus on more sophisticated units that implement\na gating mechanism, such as a long short-term memory (lstm) unit and a re-\ncently proposed gated recurrent unit (gru). we evaluate these recurrent units on\nthe tasks of polyphonic music modeling and speech signal modeling. our exper-\niments revealed that these advanced recurrent units are indeed better than more\ntraditional recurrent units such as tanh units. also, we found gru to be compa-\nrable to lstm.\n\n1\n\nintroduction\n\nrecurrent neural networks have recently shown promising results in many machine learning tasks,\nespe", "published as a conference paper at iclr 2021\n\ntowards nonlinear disentanglement in\nnatural data with temporal sparse coding\n\ndavid klindt\u2217\nuniversity of t\u00fcbingen\nklindt.david@gmail.com\n\nlukas schott\u2217\nuniversity of t\u00fcbingen\nlukas.schott@bethgelab.org\n\nyash sharma\u2217\nuniversity of t\u00fcbingen\nyash.sharma@bethgelab.org\n\nivan ustyuzhaninov\nuniversity of t\u00fcbingen\nivan.ustyuzhaninov@bethgelab.org\n\nmatthias bethge\u2021\nuniversity of t\u00fcbingen\nmatthias.bethge@bethgelab.org\n\nwieland brendel\nuniversity of t\u00fcbingen\nwieland.brendel@bethgelab.org\n\ndylan m paiton\u2021\nuniversity of t\u00fcbingen\ndylan.paiton@bethgelab.org\n\nabstract\n\ndisentangling the underlying generative factors from data has so far been limited\nto carefully constructed scenarios. we propose a path towards natural data by\n\ufb01rst showing that the statistics of natural data provide enough structure to enable\ndisentanglement, both theoretically and empirically. speci\ufb01cally, we provide\nevidence that objects in natural movies undergo transitions that are ty", "the journal of neuroscience, december 8, 2010 \u2022 30(49):16601\u201316608 \u2022 16601\n\nbehavioral/systems/cognitive\n\nexpectation and surprise determine neural population\nresponses in the ventral visual stream\n\ntobias egner,1,2 jim m. monti,3 and christopher summerfield4\n1department of psychology and neuroscience, and 2center for cognitive neuroscience, duke university, durham, north carolina 27708, 3department of\npsychology, university of illinois, beckman institute, urbana-champaign, illinois 61801, and 4department of experimental psychology, university of\noxford, oxford ox1 3ud, united kingdom\n\nvisual cortex is traditionally viewed as a hierarchy of neural feature detectors, with neural population responses being driven by\nbottom-up stimulus features. conversely, \u201cpredictive coding\u201d models propose that each stage of the visual hierarchy harbors two\ncomputationally distinct classes of processing unit: representational units that encode the conditional probability of a stimulus and\nprovide predic", "under review as a conference paper at iclr 2018\n\nlearning independent features with adver-\nsarial nets for non-linear ica\n\nanonymous authors\npaper under double-blind review\n\nabstract\n\nreliable measures of statistical dependence could be useful tools for learning in-\ndependent features and performing tasks like source separation using indepen-\ndent component analysis (ica). unfortunately, many of such measures, like the\nmutual information, are hard to estimate and optimize directly. we propose to\nlearn independent features with adversarial objectives (goodfellow et al., 2014;\narjovsky et al., 2017; huszar, 2016) which optimize such measures implicitly.\nthese objectives compare samples from the joint distribution and the product of\nthe marginals without the need to compute any probability densities. we also pro-\npose two methods for obtaining samples from the product of the marginals using\neither a simple resampling trick or a separate parametric distribution. our exper-\niments show that", "1\n2\n0\n2\n\n \nt\nc\no\n7\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n0\n2\n6\n9\n0\n\n.\n\n6\n0\n1\n2\n:\nv\ni\nx\nr\na\n\ndisentangling identi\ufb01able features from noisy data\n\nwith structured nonlinear ica\n\nhermanni h\u00e4lv\u00e41 \u2217\n\nsylvain le corff2\n\nluc leh\u00e9ricy3\n\njonathan so4\n\nyongjie zhu1\n\nelisabeth gassiat5 \u2020\n\naapo hyv\u00e4rinen1 \u2020\n\n1department of computer science, university of helsinki, finland\n\n2 samovar, t\u00e9l\u00e9com sudparis, d\u00e9partement citi, institut polytechnique de paris, palaiseau, france\n\n3laboratoire j. a. dieudonn\u00e9, universit\u00e9 c\u00f4te d\u2019azur, cnrs, 06100, nice, france\n\n5universit\u00e9 paris-saclay, cnrs, laboratoire de math\u00e9matiques d\u2019orsay, 91405, orsay, france\n\n4department of engineering, university of cambridge, uk\n\nabstract\n\nwe introduce a new general identi\ufb01able framework for principled disentanglement\nreferred to as structured nonlinear independent component analysis (snica).\nour contribution is to extend the identi\ufb01ability theory of deep generative models\nfor a very broad class of structured models. while previous ", "learning and generalization in overparameterized neural\n\nnetworks, going beyond two layers\n\nzeyuan allen-zhu\n\nzeyuan@csail.mit.edu\nmicrosoft research ai\n\nyuanzhi li\n\nyingyu liang\n\nyuanzhil@stanford.edu\n\nyliang@cs.wisc.edu\n\nstanford university\n\nuniversity of wisconsin-madison\n\nnovember 12, 2018\n\n(version 6)\u2217\n\nabstract\n\nthe fundamental learning theory behind neural networks remains largely open. what classes\nof functions can neural networks actually learn? why doesn\u2019t the trained network over\ufb01t when\nit is overparameterized?\n\nin this work, we prove that overparameterized neural networks can learn some notable con-\ncept classes, including two and three-layer networks with fewer parameters and smooth activa-\ntions. moreover, the learning can be simply done by sgd (stochastic gradient descent) or its\nvariants in polynomial time using polynomially many samples. the sample complexity can also\nbe almost independent of the number of parameters in the network.\n\non the technique side, our analysis", "biorxiv preprint \n\nhttps://doi.org/10.1101/2022.05.17.492325\n; \n\ndoi: \nwas not certified by peer review) is the author/funder. all rights reserved. no reuse allowed without permission. \n\nthis version posted may 18, 2022. \n\nthe copyright holder for this preprint (which\n\ninferring neural activity before plasticity: a foundation\nfor learning beyond backpropagation\nyuhang song1,2,*, beren millidge2, tommaso salvatori1, thomas lukasiewicz1,*, zhenghua xu1,3, and\nrafal bogacz2,*\n\n1department of computer science, university of oxford, oxford, united kingdom\n2medical research council brain networks dynamics unit, university of oxford, oxford, united kingdom\n3state key laboratory of reliability and intelligence of electrical equipment, hebei university of technology, tianjin, china\n*corresponding authors: yuhang.song@bndu.ox.ac.uk; thomas.lukasiewicz@cs.ox.ac.uk; rafal.bogacz@ndcn.ox.ac.uk\n\nabstract\n\nfor both humans and machines, the essence of learning is to pinpoint which components in its in", "spike-timing dependent plasticity and mutual\ninformation maximization for a spiking neuron\n\nmodel\n\ntaro toyoizumiyz,\n\njean-pascal p\ufb01sterz\n\nkazuyuki aiharax \u2044,\n\nwulfram gerstnerz\n\ny department of complexity science and engineering,\n\nthe university of tokyo, 153-8505 tokyo, japan\n\nz ecole polytechnique f\u00b4ed\u00b4erale de lausanne (epfl),\nschool of computer and communication sciences and\n\nbrain-mind institute, 1015 lausanne, switzerland\n\nx graduate school of information science and technology,\n\nthe university of tokyo, 153-8505 tokyo, japan\n\ntaro@sat.t.u-tokyo.ac.jp,\naihara@sat.t.u-tokyo.ac.jp,\n\njean-pascal.pfister@epfl.ch\nwulfram.gerstner@epfl.ch\n\nabstract\n\nwe derive an optimal learning rule in the sense of mutual information\nmaximization for a spiking neuron model. under the assumption of\nsmall \ufb02uctuations of the input, we \ufb01nd a spike-timing dependent plas-\nticity (stdp) function which depends on the time course of excitatory\npostsynaptic potentials (epsps) and the autocorrelation function o", "reviewed preprint\n\npublished from the original\npreprint after peer review\nand assessment by elife.\n\nabout elife's process\n\nreviewed preprint posted\noctober 23, 2023 (this version)\n\nsent for peer review\nseptember 6, 2023\n\nposted to biorxiv\njune 27, 2023\n\nneuroscience\n\nexpansion-assisted selective plane\nillumination microscopy for nanoscale\nimaging of centimeter-scale tissues\n\n, jayaram chandrashekar\n\n, joshua vasquez, cameron arshadi, naveen ouellette,\n\nadam glaser\nxiaoyun jiang, judith baka, gabor kovacs, micah woodard, sharmishtaa seshamani, kevin cao,\nnathan clack, andrew recknagel, anna grim, pooja balaram, emily turschak, alan liddell, john rohde,\nayana hellevik, kevin takasaki, lindsey erion barner, molly logsdon, chris chronopoulos, saskia de vries,\njonathan ting, steve perlmutter, brian kalmbach, nikolai dembrow, r. clay reid, david feng,\nkarel svoboda\n\nallen institute for neural dynamics, seattle, wa \u2022 chan zuckerberg initiative, redwood city, ca \u2022 allen institute for\nbrain sci", "685\n\nneural mechanisms of reward-related motor learning\njeffery r wickens\u0003, john n j reynolds and brian i hyland\n\nthe analysis of the neural mechanisms responsible for reward-\nrelated learning has bene\ufb01ted from recent studies of the effects\nof dopamine on synaptic plasticity. dopamine-dependent\nsynaptic plasticity may lead to strengthening of selected inputs\non the basis of an activity-dependent conjunction of sensory\nafferent activity, motor output activity, and temporally related\n\ufb01ring of dopamine cells. such plasticity may provide a link\nbetween the reward-related \ufb01ring of dopamine cells and the\nacquisition of changes in striatal cell activity during learning. this\nlearning mechanism may play a special role in the translation of\nreward signals into context-dependent response probability or\ndirectional bias in movement responses.\n\naddresses\ndepartment of anatomy and structural biology, school of medical\nsciences, university of otago, p.o. box 913, dunedin, new zealand\n\u0003e-mail: jeff.w", "current biology 23, 2011\u20132015, october 21, 2013 \u00aa2013 elsevier ltd all rights reserved http://dx.doi.org/10.1016/j.cub.2013.08.015\n\nrapid innate defensive responses\nof mice to looming visual stimuli\n\nreport\n\nmelis yilmaz1,2 and markus meister1,*\n1division of biology, california institute of technology,\npasadena, ca 91125, usa\n2department of molecular and cellular biology,\nharvard university, cambridge, ma 02138, usa\n\nsummary\n\nmuch of brain science is concerned with understanding the\nneural circuits that underlie speci\ufb01c behaviors. while the\nmouse has become a favorite experimental subject, the be-\nhaviors of this species are still poorly explored. for example,\nthe mouse retina, like that of other mammals, contains w20\ndifferent circuits that compute distinct features of the visual\nscene [1, 2]. by comparison, only a handful of innate visual\nbehaviors are known in this species\u2014the pupil re\ufb02ex [3],\nphototaxis [4], the optomotor response [5], and the cliff\nresponse [6]\u2014two of which are si", "the journal of neuroscience, september 20, 2006 \u2022 26(38):9673\u20139682 \u2022 9673\n\nbehavioral/systems/cognitive\n\ntriplets of spikes in a model of\nspike timing-dependent plasticity\n\njean-pascal pfister and wulfram gerstner\nlaboratory of computational neuroscience, school of computer and communication sciences and brain-mind institute, ecole polytechnique fe\u00b4de\u00b4rale de\nlausanne, ch-1015 lausanne, switzerland\n\nclassical experiments on spike timing-dependent plasticity (stdp) use a protocol based on pairs of presynaptic and postsynaptic spikes\nrepeated at a given frequency to induce synaptic potentiation or depression. therefore, standard stdp models have expressed the weight\nchange as a function of pairs of presynaptic and postsynaptic spike. unfortunately, those paired-based stdp models cannot account for\nthe dependence on the repetition frequency of the pairs of spike. moreover, those stdp models cannot reproduce recent triplet and\nquadruplet experiments. here, we examine a triplet rule (i.e., ", "noise-contrastive estimation: a new estimation principle for\n\nunnormalized statistical models\n\nmichael gutmann\n\naapo hyv\u00a8arinen\n\ndept of computer science\n\ndept of mathematics & statistics, dept of computer\n\nand hiit, university of helsinki\n\nmichael.gutmann@helsinki.\ufb01\n\nscience and hiit, university of helsinki\n\naapo.hyvarinen@helsinki.\ufb01\n\nabstract\n\nwe present a new estimation principle for\nparameterized statistical models. the idea\nis to perform nonlinear logistic regression to\ndiscriminate between the observed data and\nsome arti\ufb01cially generated noise, using the\nmodel log-density function in the regression\nnonlinearity. we show that this leads to a\nconsistent (convergent) estimator of the pa-\nrameters, and analyze the asymptotic vari-\nance. in particular, the method is shown to\ndirectly work for unnormalized models, i.e.\nmodels where the density function does not\nintegrate to one. the normalization constant\ncan be estimated just like any other parame-\nter. for a tractable ica model, we c", "motor cortical representation of speed and direction\nduring reaching\n\ndaniel w. moran and andrew b. schwartz\nthe neurosciences institute, san diego, california 92121\n\nmoran, daniel w. and andrew b. schwartz. motor cortical repre-\nsentation of speed and direction during reaching. j. neurophysiol. 82:\n2676 \u20132692, 1999. the motor cortical substrate associated with reach-\ning was studied as monkeys moved their hands from a central position\nto one of eight targets spaced around a circle. single-cell activity\npatterns were recorded in the proximal arm area of motor cortex\nduring the task. in addition to the well-studied average directional\nselectivity (\u201cpreferred direction\u201d) of single-cell activity, we also found\nthe time-varying speed of movement to be represented in the cortical\nactivity. a single equation relating motor cortical discharge rate to\nthese two parameters was developed. this equation, which has both\nindependent (speed only) and interactive (speed and direction) com-\nponents, d", "the statistical structure of the hippocampal code\nfor space as a function of time, context, and value\n\narticle\n\ngraphical abstract\n\nauthors\njae sung lee, john j. briguglio,\njeremy d. cohen, sandro romani,\nalbert k. lee\n\ncorrespondence\nleej13@janelia.hhmi.org (j.s.l.),\nbriguglioj@janelia.hhmi.org (j.j.b.),\nsandro.romani@gmail.com (s.r.),\nleea@janelia.hhmi.org (a.k.l.)\n\nin brief\na uni\ufb01ed response pattern of\nhippocampal place cells, driven by\nindividual cell-intrinsic mechanisms,\nquantitatively predicts the structure of\nrepresentation of space at the population\nlevel across varying conditions by the\nhippocampus in mice.\n\nhighlights\nd each ca1 pyramidal neuron has a propensity to have place\n\n\ufb01elds that differs across cells\n\nd a cell\u2019s propensity is \ufb01xed across contexts and time, scaled\n\nin rewarded and novel areas\n\nd the subpopulation of low propensity cells yields a sparse\n\ncode for location\n\nd intracellular data provide evidence that excitability underlies\n\npropensity differences\n\nlee et", "variational inference: a review for statisticians\n\ndavid m. blei\n\ndepartment of computer science and statistics\n\ncolumbia university\n\nalp kucukelbir\n\ndepartment of computer science\n\ncolumbia university\n\njon d. mcauliffe\n\ndepartment of statistics\n\nuniversity of california, berkeley\n\nmay 11, 2018\n\nabstract\n\none of the core problems of modern statistics is to approximate dif\ufb01cult-to-compute\nprobability densities. this problem is especially important in bayesian statistics, which\nframes all inference about unknown quantities as a calculation involving the posterior\ndensity. in this paper, we review variational inference (vi), a method from machine\nlearning that approximates probability densities through optimization. vi has been used\nin many applications and tends to be faster than classical methods, such as markov chain\nmonte carlo sampling. the idea behind vi is to \ufb01rst posit a family of densities and then\nto \ufb01nd the member of that family which is close to the target. closeness is measur", "6\n1\n0\n2\n\n \n\nb\ne\nf\n2\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n2\n4\n3\n6\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2016\n\nactor-mimic\ndeep multitask and transfer reinforcement\nlearning\n\nemilio parisotto, jimmy ba, ruslan salakhutdinov\ndepartment of computer science\nuniversity of toronto\ntoronto, ontario, canada\n{eparisotto,jimmy,rsalakhu}@cs.toronto.edu\n\nabstract\n\nthe ability to act in multiple environments and transfer previous knowledge to\nnew situations can be considered a critical aspect of any intelligent agent. to-\nwards this goal, we de\ufb01ne a novel method of multitask and transfer learning that\nenables an autonomous agent to learn how to behave in multiple tasks simultane-\nously, and then generalize its knowledge to new domains. this method, termed\n\u201cactor-mimic\u201d, exploits the use of deep reinforcement learning and model com-\npression techniques to train a single policy network that learns how to act in a set\nof distinct tasks by using the guidance of several expert t", "5\n1\n0\n2\n\n \n\nv\no\nn\n5\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n5\n2\n5\n7\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\ndifference target propagation\n\ndong-hyun lee1, saizheng zhang1, asja fischer1, and\n\nyoshua bengio1,2\n\n1 universit\u00b4e de montr\u00b4eal, quebec, canada\n\n2 cifar senior fellow\n\nabstract. back-propagation has been the workhorse of recent successes of deep\nlearning but it relies on in\ufb01nitesimal effects (partial derivatives) in order to per-\nform credit assignment. this could become a serious issue as one considers\ndeeper and more non-linear functions, e.g., consider the extreme case of non-\nlinearity where the relation between parameters and cost is actually discrete. in-\nspired by the biological implausibility of back-propagation, a few approaches\nhave been proposed in the past that could play a similar credit assignment role. in\nthis spirit, we explore a novel approach to credit assignment in deep networks that\nwe call target propagation. the main idea is to compute targets rather than gradi-\nents, at each l", "published as a conference paper at iclr 2018\n\non the information bottleneck\ntheory of deep learning\n\nandrew m. saxe, yamini bansal, joel dapello, madhu advani\nharvard university\n{asaxe,madvani}@fas.harvard.edu,{ybansal,dapello}@g.harvard.edu\n\nartemy kolchinsky, brendan d. tracey\nsanta fe institute\n{artemyk,tracey.brendan}@gmail.com\n\ndavid d. cox\nharvard university\nmit-ibm watson ai lab\ndavidcox@fas.harvard.edu\ndavid.d.cox@ibm.com\n\nabstract\n\nthe practical successes of deep neural networks have not been matched by theoret-\nical progress that satisfyingly explains their behavior. in this work, we study the\ninformation bottleneck (ib) theory of deep learning, which makes three speci\ufb01c\nclaims: \ufb01rst, that deep networks undergo two distinct phases consisting of an\ninitial \ufb01tting phase and a subsequent compression phase; second, that the compres-\nsion phase is causally related to the excellent generalization performance of deep\nnetworks; and third, that the compression phase occurs due to the ", "exponential expressivity in deep neural networks\n\nthrough transient chaos\n\nben poole1, subhaneil lahiri1, maithra raghu2, jascha sohl-dickstein2, surya ganguli1\n\n{benpoole,sulahiri,sganguli}@stanford.edu, {maithra,jaschasd}@google.com\n\n1stanford university, 2google brain\n\nabstract\n\nwe combine riemannian geometry with the mean \ufb01eld theory of high dimensional\nchaos to study the nature of signal propagation in generic, deep neural networks\nwith random weights. our results reveal an order-to-chaos expressivity phase\ntransition, with networks in the chaotic phase computing nonlinear functions whose\nglobal curvature grows exponentially with depth but not width. we prove this\ngeneric class of deep random functions cannot be ef\ufb01ciently computed by any shal-\nlow network, going beyond prior work restricted to the analysis of single functions.\nmoreover, we formalize and quantitatively demonstrate the long conjectured idea\nthat deep networks can disentangle highly curved manifolds in input space i", "biorxiv preprint \n\nthe copyright holder for this preprint (which was\nnot certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available \n\nthis version posted september 16, 2018. \n\nhttps://doi.org/10.1101/418939\n; \n\ndoi: \n\nunder a\n\ncc-by-nc-nd 4.0 international license\n.\n\nprobing variability in a cognitive map using manifold inference from neural dynamics\nryan j. low(cid:89),1, sam lewallen(cid:89),1,3, dmitriy aronov1,4, rhino nevers1, david w. tank1,2\n\n(cid:89) these authors contributed equally to this work. 1princeton neuroscience institute, princeton university, 2bezos center for neural\ncircuit dynamics, princeton university, 3present address: center for brain science, harvard university, 4present address: department of\nneuroscience, columbia university\n\nhippocampal neurons \ufb01re selectively in local behavioral contexts such as the position in an environment\nor phase of a task,1\u20133 and are thought to form a ", "cognitive \n\nscience \n\n11, \n\n23-63 \n\n(1987) \n\ncompetitive learning: \n\nfrom interactive  activation  to \n\nadaptive resonance \n\nstephen \n\ngrossberg \nboston university \n\nmechanistic \n\ncomparisons \n\nare  mode \n\nbetween \n\nseveral \n\nnetwork \n\nond \ncognitive \n\nof \nresonance, \n\nand \n\nprocessing: \nback \nond \n\narticle \nlearning. \nshown \n\nof  rumelhart \n\nall \n\nthe  models \n\nin  grossberg \n\n(1976b) \n\ncompetitive \ninput \n\nenvironment \n\nlearning \n\nexpectancies, \n\ntop-down \n\nexpectancies; \n\nsituation, \n\nrepresentation. \n\nthereby \n\nupdating \nnetwork \n\ncompetitive \n\npropagation. \nzipser \nwhich \n\n(1985) \nrumelhart \n\nthe \non \n\nlearning, \nstarting \n\nfeature \nand \n\nto  exhibit \nmechanisms \n\na  type \ncon \n\nof \nbe \n\ninteractive \nof \n\npoint \ndiscovery \n\n(1985) \n\nzipser \nlearning \nstabilized \nwith  mechanisms \n\nwhich \nin \n\nthis \nthrough \nhave \n\nactivation, \ncomparison \n\ncompe- \n\ndescribed \nis  temporally \n\nresponse \n\nby  being \nor \n\ntemplates; \n\nsupplemented \n\nand \nfor \nshort-term \narchitectures \n\nfor  matching", " \n\nparallel distributed processing copyrighted material \f", "audio adversarial examples: targeted attacks on speech-to-text\n\nnicholas carlini\n\ndavid wagner\nuniversity of california, berkeley\n\n8\n1\n0\n2\n\n \nr\na\n\n \n\nm\n0\n3\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n4\n4\n9\n1\n0\n\n.\n\n1\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014we construct targeted audio adversarial examples\non automatic speech recognition. given any audio waveform,\nwe can produce another that\nis over 99.9% similar, but\ntranscribes as any phrase we choose (recognizing up to 50\ncharacters per second of audio). we apply our white-box\niterative optimization-based attack to mozilla\u2019s implementation\ndeepspeech end-to-end, and show it has a 100% success rate.\nthe feasibility of this attack introduce a new domain to study\nadversarial examples.\n\ni. introduction\n\nas the use of neural networks continues to grow, it is\ncritical to examine their behavior in adversarial settings.\nprior work [8] has shown that neural networks are vulnerable\ninstances x(cid:48) similar to a\nto adversarial examples [40],\nnatural instance x, but clas", "report\n\ndendritic spines prevent synaptic voltage clamp\n\nhighlights\nd high spine neck resistance prevents voltage-clamp control of\n\nauthors\n\nlou beaulieu-laroche, mark t. harnett\n\nexcitatory synapses.\n\nd voltage-clamp measurements are severely distorted for\n\nspiny neurons.\n\nd large single-spine ampa conductance saturates synaptic\n\ncurrent.\n\ncorrespondence\nharnett@mit.edu\n\nin brief\nbeaulieu-laroche and harnett\ndemonstrate that voltage clamp, the main\napproach to investigate synaptic\nphysiology, is ineffective for most\nsynapses because they reside on\nelectrically isolated spines.\n\nbeaulieu-laroche & harnett, 2018, neuron 97, 75\u201382\njanuary 3, 2018 \u00aa 2017 elsevier inc.\nhttps://doi.org/10.1016/j.neuron.2017.11.016\n\n\f", "e\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\n\nl\n\ny\ng\no\no\ni\ns\ny\nh\np\n\nf\no\n\nl\n\na\nn\nr\nu\no\nj\n\ne\nh\nt\n\nj physiol 591.22 (2013) pp 5425\u20135431\n\ns y m p o s i u m r e v i e w\n\n5425\n\nthe evolutionary origin of the vertebrate basal ganglia\nand its role in action selection\n\nsten grillner, brita robertson and marcus stephenson-jones\n\ndepartment of neuroscience, karolinska institutet, se-17177 stockholm, sweden\n\nabstract the group of nuclei within the basal ganglia of the forebrain is central to the control of\nmovement. we present data showing that the structure and function of the basal ganglia have been\nconserved throughout vertebrate evolution over some 560 million years. the interaction between\nthe different nuclei within the basal ganglia is conserved as well as the cellular and synaptic\nproperties and transmitters. we consider the role of the conserved basal ganglia circuitry for basic\npatterns of motor behaviour controlled via brainstem circuits. the output of the basal ganglia\nconsists of tonically active gabaergi", "toroidal topology of population activity in \ngrid cells\n\nhttps://doi.org/10.1038/s41586-021-04268-7\nreceived: 24 february 2021\naccepted: 19 november 2021\npublished online: 12 january 2022\nopen access\n\n check for updates\n\nrichard j. gardner1,6\u2009\u2709, erik hermansen2,6, marius pachitariu3, yoram burak4,5, nils a. baas2\u2009\u2709, \nbenjamin a. dunn1,2\u2009\u2709, may-britt moser1 & edvard i. moser1\u2009\u2709\n\nthe medial entorhinal cortex is part of a neural system for mapping the position of an \nindividual within a physical environment1. grid cells, a key component of this system, \nfire in a characteristic hexagonal pattern of locations2, and are organized in modules3 \nthat collectively form a population code for the animal\u2019s allocentric position1.  \nthe invariance of the correlation structure of this population code across \nenvironments4,5 and behavioural states6,7, independent of specific sensory inputs,  \nhas pointed to intrinsic, recurrently connected continuous attractor networks (cans) \nas a possible substrate ", "reliability of spike timing in neocortical neurons \nauthor(s): zachary f. mainen and terrence j. sejnowski \nsource: science, jun. 9, 1995, new series, vol. 268, no. 5216 (jun. 9, 1995), pp. 1503-\n1506\npublished by: american association for the advancement of science \n\n \n\nstable url: https://www.jstor.org/stable/2887763\n\njstor is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide \nrange of content in a trusted digital archive. we use information technology and tools to increase productivity and \nfacilitate new forms of scholarship. for more information about jstor, please contact support@jstor.org. \n \nyour use of the jstor archive indicates your acceptance of the terms & conditions of use, available at \nhttps://about.jstor.org/terms\n\namerican association for the advancement of science is collaborating with jstor to digitize, \npreserve and extend access to science\n\nthis content downloaded from \n\n(cid:0)205.175.106.113 on thu, 28 s", "neural episodic control\n\nalexander pritzel 1\ndemis hassabis 1 daan wierstra 1 charles blundell 1\n\nbenigno uria 1\n\nsriram srinivasan 1\n\nadri`a puigdom`enech badia 1\n\noriol vinyals 1\n\nabstract\n\ndeep reinforcement\nlearning methods attain\nsuper-human performance in a wide range of en-\nvironments. such methods are grossly inef\ufb01cient,\noften taking orders of magnitudes more data than\nhumans to achieve reasonable performance. we\npropose neural episodic control: a deep rein-\nforcement learning agent that is able to rapidly\nassimilate new experiences and act upon them.\nour agent uses a semi-tabular representation of\nthe value function: a buffer of past experience con-\ntaining slowly changing state representations and\nrapidly updated estimates of the value function.\nwe show across a wide range of environments\nthat our agent learns signi\ufb01cantly faster than other\nstate-of-the-art, general purpose deep reinforce-\nment learning agents.\n\n1. introduction\ndeep reinforcement learning agents have achieved", "siam review\nvol. 64, no. 1, pp. 153\u2013178\n\nc\\bigcirc  2022 society for industrial and applied mathematics\n\ndimensionality reduction via\ndynamical systems: the case of t-sne\\ast \n\ngeorge c. linderman\\dagger \nstefan steinerberger\\ddagger \n\nabstract. t-distributed stochastic neighborhood embedding (t-sne), a clustering and visualization\nmethod proposed by van der maaten and hinton in 2008, has rapidly become a standard\ntool in the natural sciences. despite its overwhelming success, it has a distinct lack of\nmathematical foundations and the inner workings of the algorithm are not well understood.\nthe purpose of this paper is to prove that t-sne is able to recover well-separated clusters.\nas a by-product, the proof suggests that t-sne is merely one of many possible algorithms\nof a large family of methods generated by dynamical systems---this perspective suggests\nnew questions and problems, some of which we discuss.\n\nkey words. dimensionality reduction, visualization, attraction-repulsion dyna", "ne34ch19-gerfen\n\nari\n\n13 may 2011\n\n10:57\n\nannu. rev. neurosci. 2011. 34:441\u201366\n\nfirst published online as a review in advance on\napril 4, 2011\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev-neuro-061010-113641\ncopyright c(cid:2) 2011 by annual reviews.\nall rights reserved\n\n0147-006x/11/0721-0441$20.00\n\nmodulation of striatal\nprojection systems by\ndopamine\n\ncharles r. gerfen1 and d. james surmeier2\n1laboratory of systems neuroscience, national institute of mental health, bethesda,\nmaryland 20892; email: gerfenc@mail.nih.gov\n2department of physiology, northwestern university feinberg school of medicine,\nchicago, illinois 60611; email: j-surmeier@northwestern.edu\n\nkeywords\nbasal ganglia, synaptic plasticity, movement disorders, parkinson\u2019s\ndisease, motor learning\n\nabstract\nthe basal ganglia are a chain of subcortical nuclei that facilitate action\nselection. two striatal projection systems\u2014so-called direct and indirect\npathways\u2014", "0270~6474/82/0201-0032$02.00/o \ncopyright \nprinted \n\nin  u.s.a. \n\n0  society \n\nfor  neuroscience \n\ntheory  for  the  development \norientation \nvisual  cortex1 \n\nspecificity \n\nof  neuron \n\nand  binocular \n\nthe \n\njournal \n\nof  neuroscience \n\nvol.  2,  no. \n\n1,  pp.  32-48 \n1982 \n\njanuary \n\nselectivity: \n\ninteraction \n\nin \n\nelie  l.  bienenstock,2 \n\nleon  n  cooper,3 \n\nand  paul  w.  munro \n\ncenter \n\nfor  neural \n\nscience,  department \n\nof  physics,  and  division \nisland \n\nrhode \n\nof applied  mathematics, \n02912 \n\nbrown \n\nuniversity,  providence, \n\nreceived \n\njune \n\n5,  1981;  revised \n\naugust \n\n27,  1981;  accepted \n\nseptember \n\n1,  1981 \n\nabstract \nin  the  primary  sensory  cortex  of  higher  vertebrates \n\nthe  development  of  stimulus  selectivity \n\nform  for  this  dependence,  development  of  selectivity \n\nis \nconsidered  in  a  general  mathematical \nframework.  a  synaptic  evolution  scheme of  a new  kind  is \nproposed  in which  incoming  patterns  rather  than  converging ", "learning representations that support extrapolation\n\ntaylor w. webb 1 zachary dulberg 2 steven m. frankland 2 alexander a. petrov 3 randall c. o\u2019reilly 4\n\njonathan d. cohen 2\n\nabstract\n\nextrapolation \u2013 the ability to make inferences that\ngo beyond the scope of one\u2019s experiences \u2013 is a\nhallmark of human intelligence. by contrast, the\ngeneralization exhibited by contemporary neural\nnetwork algorithms is largely limited to interpola-\ntion between data points in their training corpora.\nin this paper, we consider the challenge of learn-\ning representations that support extrapolation. we\nintroduce a novel visual analogy benchmark that\nallows the graded evaluation of extrapolation as\na function of distance from the convex domain\nde\ufb01ned by the training data. we also introduce\na simple technique, temporal context normaliza-\ntion, that encourages representations that empha-\nsize the relations between objects. we \ufb01nd that\nthis technique enables a signi\ufb01cant improvement\nin the ability to extrapola", "7\n1\n0\n2\n\n \n\nv\no\nn\n5\n2\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n2\nv\n4\n7\n8\n7\n0\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\na uni\ufb01ed approach to interpreting model\n\npredictions\n\nscott m. lundberg\n\nsu-in lee\n\npaul g. allen school of computer science\n\npaul g. allen school of computer science\n\nuniversity of washington\n\nseattle, wa 98105\n\nslund1@cs.washington.edu\n\ndepartment of genome sciences\n\nuniversity of washington\n\nseattle, wa 98105\n\nsuinlee@cs.washington.edu\n\nabstract\n\nunderstanding why a model makes a certain prediction can be as crucial as the\nprediction\u2019s accuracy in many applications. however, the highest accuracy for large\nmodern datasets is often achieved by complex models that even experts struggle to\ninterpret, such as ensemble or deep learning models, creating a tension between\naccuracy and interpretability. in response, various methods have recently been\nproposed to help users interpret the predictions of complex models, but it is often\nunclear how these methods are related and when one method is preferable ", "3\n2\n0\n2\n\n \n\ny\na\nm\n5\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n7\n5\n1\n2\n0\n\n.\n\n0\n1\n2\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2023\n\nthe influence of learning rule on represen-\ntation dynamics in wide neural networks\n\nblake bordelon & cengiz pehlevan\nschool of engineering and applied science\nharvard university\ncambridge, ma 02138, usa\n{blake bordelon,cpehlevan}@g.harvard.edu\n\nabstract\n\nit is unclear how changing the learning rule of a deep neural network alters its\nlearning dynamics and representations. to gain insight into the relationship be-\ntween learned features, function approximation, and the learning rule, we analyze\ninfinite-width deep networks trained with gradient descent (gd) and biologically-\nplausible alternatives including feedback alignment (fa), direct feedback align-\nment (dfa), and error modulated hebbian learning (hebb), as well as gated linear\nnetworks (gln). we show that, for each of these learning rules, the evolution of\nthe output function at infinite widt", "bernoulli 19(4), 2013, 1378\u20131390\ndoi: 10.3150/12-bejsp17\n\non statistics, computation and scalability\n\nmichael i. jordan\n\ndepartment of statistics and department of eecs, university of california, berkeley, ca, usa.\ne-mail: jordan@stat.berkeley.edu; url:www.cs.berkeley.edu/~jordan\n\nhow should statistical procedures be designed so as to be scalable computationally to the massive datasets\nthat are increasingly the norm? when coupled with the requirement that an answer to an inferential question\nbe delivered within a certain time budget, this question has signi\ufb01cant repercussions for the \ufb01eld of statistics.\nwith the goal of identifying \u201ctime-data tradeoffs,\u201d we investigate some of the statistical consequences of\ncomputational perspectives on scability, in particular divide-and-conquer methodology and hierarchies of\nconvex relaxations.\n\nthe \ufb01elds of computer science and statistics have undergone mostly separate evolutions during\ntheir respective histories. this is changing, due in part to t", "the journal of neuroscience, november 15, 1996, 16(22):7376 \u20137389\n\ncontextual modulation in primary visual cortex\n\nkarl zipser,1 victor a. f. lamme,2 and peter h. schiller1\n1the department of brain and cognitive sciences, massachusetts institute of technology, cambridge, massachusetts\n02139, and 2graduate school of neurosciences, department of medical physics, amc, university of amsterdam,\namsterdam, the netherlands, and the netherlands ophthalmic research institute, 1100 ac amsterdam, the\nnetherlands\n\nwe studied extra-receptive \ufb01eld contextual modulation in area\nv1 of awake, behaving macaque monkeys. contextual modu-\nlation was studied using texture displays in which texture cov-\nering the receptive \ufb01eld (rf) was the same in all trials, but the\nperceptual context of this texture could vary depending on the\ncon\ufb01guration of extra-rf texture elements. we found robust\ncontextual modulation when disparity, color, luminance, and\norientation cues variously de\ufb01ned a textured \ufb01gure centered on", "received may 2, 2020, accepted may 31, 2020, date of publication june 9, 2020, date of current version june 18, 2020.\n\ndigital object identifier 10.1109/access.2020.3001184\n\ngaussian mixture variational autoencoder for\nsemi-supervised topic modeling\n\ncangqi zhou 1, hao ban 2, jing zhang 1, (senior member, ieee), qianmu li 3,4,\nand yinghua zhang 5\n1school of computer science and engineering, nanjing university of science and technology, nanjing 210094, china\n2school of information science and engineering, southeast university, nanjing 210096, china\n3school of cyber science and engineering, nanjing university of science and technology, nanjing 210094, china\n4information department, nanjing university of science and technology, nanjing 210094, china\n5sensedeal intelligent technology company ltd., beijing 100084, china\ncorresponding authors: hao ban (banhao@seu.edu.cn) and yinghua zhang (yhzh@sensedeal.ai)\n\nthis work was supported in part by the national natural science foundation of china", "hippocampal place cell assemblies are\nspeed-controlled oscillators\n\ncaroline geisler, david robbe, michae\u00a8 l zugaro*, anton sirota, and gyo\u00a8 rgy buzsa\u00b4 ki\u2020\n\ncenter for molecular and behavioral neuroscience, rutgers, the state university of new jersey, 197 university avenue, newark, nj 07102\n\nedited by nancy j. kopell, boston university, boston, ma, and approved march 28, 2007 (received for review november 14, 2006)\n\nthe phase of spikes of hippocampal pyramidal cells relative to the\nlocal \ufb01eld \u242a oscillation shifts forward (\u2018\u2018phase precession\u2019\u2019) over a\nfull \u242a cycle as the animal crosses the cell\u2019s receptive \ufb01eld (\u2018\u2018place\n\ufb01eld\u2019\u2019). the linear relationship between the phase of the spikes and\nthe travel distance within the place \ufb01eld is independent of the\nanimal\u2019s running speed. this invariance of the phase\u2013 distance\nrelationship is likely to be important for coordinated activity of\nhippocampal cells and space coding, yet the mechanism responsi-\nble for it is not known. here we show that at ", "research article\n\ntraining excitatory-inhibitory recurrent\nneural networks for cognitive tasks: a\nsimple and flexible framework\nh. francis song1\u262f, guangyu r. yang1\u262f, xiao-jing wang1,2*\n\n1 center for neural science, new york university, new york, new york, united states of america, 2 nyu-\necnu institute of brain and cognitive science, nyu shanghai, shanghai, china\n\n\u262f these authors contributed equally to this work.\n* xjwang@nyu.edu\n\nabstract\n\nthe ability to simultaneously record from large numbers of neurons in behaving animals\nhas ushered in a new era for the study of the neural circuit mechanisms underlying cognitive\nfunctions. one promising approach to uncovering the dynamical and computational princi-\nples governing population responses is to analyze model recurrent neural networks (rnns)\nthat have been optimized to perform the same tasks as behaving animals. because the opti-\nmization of network parameters specifies the desired output but not the manner in which to\nachieve this outp", "6\n1\n0\n2\n\n \n\nv\no\nn\n0\n1\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n2\nv\n9\n7\n7\n2\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nunderreviewasaconferencepaperaticlr2017rl2:fastreinforcementlearningviaslowreinforcementlearningyanduan\u2020\u2021,johnschulman\u2020\u2021,xichen\u2020\u2021,peterl.bartlett\u2020,ilyasutskever\u2021,pieterabbeel\u2020\u2021\u2020ucberkeley,departmentofelectricalengineeringandcomputerscience\u2021openai{rocky,joschu,peter}@openai.com,peter@berkeley.edu,{ilyasu,pieter}@openai.comabstractdeepreinforcementlearning(deeprl)hasbeensuccessfulinlearningsophis-ticatedbehaviorsautomatically;however,thelearningprocessrequiresahugenumberoftrials.incontrast,animalscanlearnnewtasksinjustafewtrials,bene-\ufb01tingfromtheirpriorknowledgeabouttheworld.thispaperseekstobridgethisgap.ratherthandesigninga\u201cfast\u201dreinforcementlearningalgorithm,weproposetorepresentitasarecurrentneuralnetwork(rnn)andlearnitfromdata.inourproposedmethod,rl2,thealgorithmisencodedintheweightsofthernn,whicharelearnedslowlythroughageneral-purpose(\u201cslow\u201d)rlalgorithm.thernnreceivesallinformationatypicalrlalgor", "simulating a primary visual cortex at the front of\ncnns improves robustness to image perturbations\n\nmartin schrimpf1,2,4, franziska geiger2,5,6,7, david d. cox8,3, james j. dicarlo1,2,4\n\njoel dapello\u2217,1,2,3, tiago marques\u2217,1,2,4\n\n\u2217joint \ufb01rst authors (equal contribution)\n\n1department of brain and cognitive sciences, mit, cambridge, ma02139\n\n2mcgovern institute for brain research, mit, cambridge, ma02139\n\n3school of engineering and applied sciences, harvard university, cambridge, ma02139\n\n4center for brains, minds and machines, mit, cambridge, ma02139\n\n5university of augsburg\n\n6ludwig maximilian university\n7technical university of munich\n\n8mit-ibm watson ai lab\n\ndapello@mit.edu\n\ntmarques@mit.edu\n\nabstract\n\ncurrent state-of-the-art object recognition models are largely based on convo-\nlutional neural network (cnn) architectures, which are loosely inspired by the\nprimate visual system. however, these cnns can be fooled by imperceptibly small,\nexplicitly crafted perturbations, and struggle ", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/083824\n; \n\nthis version posted july 4, 2017. \n\nthe copyright holder for this preprint (which was not\n\ncertified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available under \n\na\n\ncc-by-nd 4.0 international license\n.\n\n \n\nthe successor representation in human reinforcement learning \n\nmomennejad i1*, russek em2*, cheong jh3, botvinick mm4, daw nd1, gershman sj5 \n\n \n1        princeton neuroscience institute and the psychology department, princeton university \n2        center for neural science, nyu \n3        department of psychological and brain sciences, dartmouth college \n4        deepmind and gatsby computational neuroscience unit, ucl \n5        department of psychology and center for brain science, harvard university \n \n*        equal contribution \n \n \n\nabstract \n\nsuccessor representation, retrospective revaluation, planning, decision making, human behavior, \n\nt", "neuron\n\narticle\n\ntranslaminar inhibitory cells recruited\nby layer 6 corticothalamic neurons\nsuppress visual cortex\n\ndante s. bortone,1 shawn r. olsen,1 and massimo scanziani1,*\n1howard hughes medical institute, center for neural circuits and behavior, neurobiology section and department of neuroscience,\nuniversity of california san diego, la jolla, ca 92093-0634, usa\n*correspondence: massimo@ucsd.edu\nhttp://dx.doi.org/10.1016/j.neuron.2014.02.021\n\nsummary\n\nin layer 6 (l6), a principal output layer of the mamma-\nlian cerebral cortex, a population of excitatory neu-\nrons de\ufb01ned by the ntsr1-cre mouse line inhibit\ncortical responses to visual stimuli. here we show\nthat of the two major types of excitatory neurons ex-\nisting in l6, the ntsr1-cre line selectively targets\nthose whose axons innervate both cortex and thal-\namus and not those whose axons remain within\nthe cortex. these corticothalamic neurons mediate\nwidespread inhibition across all cortical layers by re-\ncruiting fast-spiking ", "journal of neural engineeringpaper \u2022 open accessdecoding hand kinematics from populationresponses in sensorimotor cortex during graspingto cite this article: elizaveta v okorokova et al 2020 j. neural eng. 17 046035 view the article online for updates and enhancements.you may also likemechanical design of a low-cost abs handprosthesis using the finite element methoda bastarrechea, q estrada, j zubrzycki etal.-enhancing gesture decoding performanceusing signals from posterior parietalcortex: a stereo-electroencephalograhy(seeg) studymeng wang, guangye li, shize jiang etal.-decoding trajectories of imagined handmovement using electrocorticograms forbrain\u2013machine interfacesang jin jang, yu jin yang, seokyunryun et al.-this content was downloaded from ip address 98.109.152.168 on 14/05/2023 at 21:48\f", "9\n1\n0\n2\n\n \nl\nu\nj\n \n\n5\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n6\nv\n1\n7\n9\n2\n0\n\n.\n\n9\n0\n5\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2016\n\ncontinuous control with deep reinforcement\nlearning\n\ntimothy p. lillicrap\u2217, jonathan j. hunt\u2217, alexander pritzel, nicolas heess,\ntom erez, yuval tassa, david silver & daan wierstra\ngoogle deepmind\nlondon, uk\n{countzero, jjhunt, apritzel, heess,\netom, tassa, davidsilver, wierstra} @ google.com\n\nabstract\n\nwe adapt the ideas underlying the success of deep q-learning to the continuous\naction domain. we present an actor-critic, model-free algorithm based on the de-\nterministic policy gradient that can operate over continuous action spaces. using\nthe same learning algorithm, network architecture and hyper-parameters, our al-\ngorithm robustly solves more than 20 simulated physics tasks, including classic\nproblems such as cartpole swing-up, dexterous manipulation, legged locomotion\nand car driving. our algorithm is able to \ufb01nd policies whose performance is com-\npe", "3\n1\n0\n2\n\n \n\ng\nu\na\n5\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n3\n4\n3\n\n.\n\n8\n0\n3\n1\n:\nv\ni\nx\nr\na\n\nestimating or propagating gradients through\nstochastic neurons for conditional computation\n\nyoshua bengio, nicholas l\u00b4eonard and aaron courville\nd\u00b4epartement d\u2019informatique et recherche op\u00b4erationnelle\n\nuniversit\u00b4e de montr\u00b4eal\n\nabstract\n\nstochastic neurons and hard non-linearities can be useful for a number of rea-\nsons in deep learning models, but in many cases they pose a challenging problem:\nhow to estimate the gradient of a loss function with respect to the input of such\nstochastic or non-smooth neurons? i.e., can we \u201cback-propagate\u201d through these\nstochastic neurons? we examine this question, existing approaches, and compare\nfour families of solutions, applicable in different settings. one of them is the min-\nimum variance unbiased gradient estimator for stochatic binary neurons (a special\ncase of the reinforce algorithm). a second approach, introduced here, de-\ncomposes the operation of a binar", "neuron\n\narticle\n\nstructural and molecular remodeling\nof dendritic spine substructures\nduring long-term potentiation\n\nmiquel bosch,1,2,* jorge castro,2,5 takeo saneyoshi,3,5 hitomi matsuno,3 mriganka sur,2 and yasunori hayashi1,2,3,4,*\n1riken-mit neuroscience research center\n2the picower institute for learning and memory, department of brain and cognitive sciences\nmassachusetts institute of technology, cambridge, ma 02139, usa\n3brain science institute, riken, wako, saitama 351-0198, japan\n4saitama university brain science institute, saitama university, saitama 338-8570, japan\n5these authors contributed equally to this work\n*correspondence: mbosch@mit.edu (m.b.), yhayashi@brain.riken.jp (y.h.)\nhttp://dx.doi.org/10.1016/j.neuron.2014.03.021\n\nsummary\n\nsynapses store information by long-lasting modi\ufb01ca-\ntions of their structure and molecular composition,\nbut the precise chronology of these changes has\nnot been studied at single-synapse resolution in\nreal\ntime. here we describe the spatiotem", "a r t i c l e s\n\nthe descending corticocollicular pathway mediates \nlearning-induced auditory plasticity\n\nvictoria m bajo1, fernando r nodal1, david r moore1,2 & andrew j king1\n\ndescending projections from sensory areas of the cerebral cortex are among the largest pathways in the brain, suggesting that \nthey are important for subcortical processing. although corticofugal inputs have been shown to modulate neuronal responses in \nthe thalamus and midbrain, the behavioral importance of these changes remains unknown. in the auditory system, one of the \nmajor descending pathways is from cortical layer v pyramidal cells to the inferior colliculus in the midbrain. we examined the \nrole of these neurons in experience-dependent recalibration of sound localization in adult ferrets by selectively killing the neurons \nusing chromophore-targeted laser photolysis. when provided with appropriate training, animals normally relearn to localize sound \naccurately after altering the spatial cues available", "research article\n\nthe computational nature of memory\nmodification\nsamuel j gershman1*, marie-h monfils2, kenneth a norman3, yael niv3\n\n1department of psychology and center for brain science, harvard university,\ncambridge, united states; 2department of psychology, university of texas, austin,\nunited states; 3princeton neuroscience institute and department of psychology,\nprinceton university, princeton, united states\n\nabstract retrieving a memory can modify its influence on subsequent behavior. we develop a\ncomputational theory of memory modification, according to which modification of a memory trace\noccurs through classical associative learning, but which memory trace is eligible for modification\ndepends on a structure learning mechanism that discovers the units of association by segmenting\nthe stream of experience into statistically distinct clusters (latent causes). new memories are\nformed when the structure learning mechanism infers that a new latent cause underlies current\nsensory o", "innovative methodology\n\nj neurophysiol 102: 614 \u2013 635, 2009.\nfirst published april 8, 2009; doi:10.1152/jn.90941.2008.\n\ngaussian-process factor analysis for low-dimensional single-trial analysis\nof neural population activity\n\nbyron m. yu,1,2,4 john p. cunningham,1 gopal santhanam,1 stephen i. ryu,1,3 krishna v. shenoy,1,2,*\nand maneesh sahani4,*\n1department of electrical engineering, 2neurosciences program, and 3department of neurosurgery, stanford university, stanford,\ncalifornia; and 4gatsby computational neuroscience unit, university college london, london, united kingdom\n\nsubmitted 19 august 2008; accepted in \ufb01nal form 24 march 2009\n\nyu bm, cunningham jp, santhanam g, ryu si, shenoy kv, sahani\nm. gaussian-process factor analysis for low-dimensional single-trial\nanalysis of neural population activity. j neurophysiol 102: 614\u2013635,\n2009. first published april 8, 2009; doi:10.1152/jn.90941.2008. we\nconsider the problem of extracting smooth, low-dimensional neural\ntrajectories that summ", "language models are few-shot learners\n\ntom b. brown\u2217\n\nbenjamin mann\u2217\n\nnick ryder\u2217\n\nmelanie subbiah\u2217\n\njared kaplan\u2020\n\nprafulla dhariwal\n\narvind neelakantan\n\npranav shyam\n\ngirish sastry\n\namanda askell\n\nsandhini agarwal\n\nariel herbert-voss\n\ngretchen krueger\n\ntom henighan\n\nrewon child\n\naditya ramesh\n\ndaniel m. ziegler\n\njeffrey wu\n\nclemens winter\n\nchristopher hesse\n\nmark chen\n\neric sigler\n\nmateusz litwin\n\nscott gray\n\nbenjamin chess\n\njack clark\n\nchristopher berner\n\nsam mccandlish\n\nalec radford\n\nilya sutskever\n\ndario amodei\n\nopenai\n\nabstract\n\nrecent work has demonstrated substantial gains on many nlp tasks and benchmarks by pre-training\non a large corpus of text followed by \ufb01ne-tuning on a speci\ufb01c task. while typically task-agnostic\nin architecture, this method still requires task-speci\ufb01c \ufb01ne-tuning datasets of thousands or tens of\nthousands of examples. by contrast, humans can generally perform a new language task from only\na few examples or from simple instructions \u2013 something which current ", "6\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n7\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n0\n4\n3\n5\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nexponential expressivity in deep neural networks\n\nthrough transient chaos\n\nben poole\n\nstanford university\n\npoole@cs.stanford.edu\n\nsubhaneil lahiri\nstanford university\nsulahiri@stanford.edu\n\nmaithra raghu\n\ngoogle brain and cornell university\n\nmaithrar@gmail.com\n\njascha sohl-dickstein\n\ngoogle brain\n\njaschasd@google.com\n\nsurya ganguli\n\nstanford university\n\nsganguli@stanford.edu\n\nabstract\n\nwe combine riemannian geometry with the mean \ufb01eld theory of high dimensional\nchaos to study the nature of signal propagation in generic, deep neural networks\nwith random weights. our results reveal an order-to-chaos expressivity phase\ntransition, with networks in the chaotic phase computing nonlinear functions whose\nglobal curvature grows exponentially with depth but not width. we prove this\ngeneric class of deep random functions cannot be ef\ufb01ciently computed by any shal-\nlow network, going beyond prior work rest", "ieee transactions on visualization and computer graphics  vol. 23,  no. 1,  january 2017 \n\n461\n\nembedded data representations\n\nwesley willett, yvonne jansen, pierre dragicevic\n\nfig. 1. examples of embedded data representations1: (a) dye-based \ufb02ow visualization on a 1/48 scale airplane model, (b) yelp\u2019s\nmonocle application uses a mobile phone to display business ratings in front of the establishments, (c) concept image of an augmented\nreality visualization of urban wind \ufb02ow [42], (d) a visualization of wi\ufb01 signal strength made with a moving led rod and long exposure\nphotography, (e) mri overlay for joint arthrography [13], and (f) a concept image of a drone swarm visualizing crop health [51].\n\nabstract\u2014we introduce embedded data representations, the use of visual and physical representations of data that are deeply\nintegrated with the physical spaces, objects, and entities to which the data refers. technologies like lightweight wireless displays,\nmixed reality hardware, and autonomous v", "nature|vol 445|15 february 2007\n\ncollective minds \n\nessay\n\nputting the pieces together\n\niain couzin\n\nby tapping into social cues, individuals in a group may gain access to higher-order computational \ncapacities that mirror the group\u2019s responses to its environment.\ndetected by only a relatively small propor-\ntion of group members due to limitations \nin individual sensory capabilities, often \nfurther restricted by crowding. close \nbehavioural coupling among near neigh-\nbours, however, allows a localized change \nin direction to be amplified, creating a \nrapidly growing and propagating wave \nof turning across the group. this positive \nfeedback results from the ability of indi-\n\nin 1905 the field naturalist edmund selous, \na confirmed darwinian and meticulous \nobserver of bird behaviour, wrote of his \nwonderment when observing tens of thou-\nsands of starlings coming together to roost: \n\u201cthey circle; now dense like a polished roof, \nnow disseminated like the meshes of some \nvast all-heaven-s", "discovering symbolic models from deep learning\n\nwith inductive biases\n\nmiles cranmer1\n\nalvaro sanchez-gonzalez2\n\npeter battaglia2\n\nrui xu1\n\nkyle cranmer3\n\ndavid spergel4,1\n\nshirley ho4,3,1,5\n\n1 princeton university, princeton, usa\n\n2 deepmind, london, uk\n\n3 new york university, new york city, usa\n\n4 flatiron institute, new york city, usa\n\n5 carnegie mellon university, pittsburgh, usa\n\nabstract\n\nwe develop a general approach to distill symbolic representations of a learned\ndeep model by introducing strong inductive biases. we focus on graph neural\nnetworks (gnns). the technique works as follows: we \ufb01rst encourage sparse\nlatent representations when we train a gnn in a supervised setting, then we\napply symbolic regression to components of the learned model to extract explicit\nphysical relations. we \ufb01nd the correct known equations, including force laws and\nhamiltonians, can be extracted from the neural network. we then apply our method\nto a non-trivial cosmology example\u2014a detailed dark mat", "shape perception reduces activity in human primary\nvisual cortex\n\nscott o. murray*\u2020, daniel kersten\u2021, bruno a. olshausen*\u00a7, paul schrater\u2021\u00b6, and david l. woods\u50a8**\n*center for neuroscience, \u00a7department of psychology, and \u50a8department of neurology, university of california, davis, ca 95616; departments of\n\u2021psychology and \u00b6computer science and engineering, university of minnesota, minneapolis, mn 55455; and **neurology service (127e),\ndepartment of veterans affairs northern california health care system, 150 muir road, martinez, ca 94553\n\ncommunicated by david mumford, brown university, providence, ri, september 24, 2002 (received for review april 25, 2002)\n\nvisual perception involves the grouping of individual elements into\ncoherent patterns that reduce the descriptive complexity of a\nvisual scene. the physiological basis of this perceptual simpli\ufb01ca-\ntion remains poorly understood. we used functional mri to mea-\nsure activity in a higher object processing area, the lateral occipital\ncomp", "artif intell rev (2014) 42:275\u2013293\ndoi 10.1007/s10462-012-9338-y\n\nmixture of experts: a literature survey\nsaeed masoudnia \u00b7 reza ebrahimpour\n\npublished online: 12 may 2012\n\u00a9 springer science+business media b.v. 2012\n\nabstract mixture of experts (me) is one of the most popular and interesting combin-\ning methods, which has great potential to improve performance in machine learning. me is\nestablished based on the divide-and-conquer principle in which the problem space is divided\nbetween a few neural network experts, supervised by a gating network. in earlier works on\nme, different strategies were developed to divide the problem space between the experts. to\nsurvey and analyse these methods more clearly, we present a categorisation of the me litera-\nture based on this difference. various me implementations were classi\ufb01ed into two groups,\naccording to the partitioning strategies used and both how and when the gating network is\ninvolved in the partitioning and combining procedures. in the \ufb01", "neuron\n\nperspective\n\nnavigating the neural space\nin search of the neural code\n\nmehrdad jazayeri1,* and arash afraz1\n1department of brain & cognitive sciences, mcgovern institute for brain research, massachusetts institute of technology, cambridge, ma\n02139, usa\n*correspondence: mjaz@mit.edu\nhttp://dx.doi.org/10.1016/j.neuron.2017.02.019\n\nthe advent of powerful perturbation tools, such as optogenetics, has created new frontiers for probing causal\ndependencies in neural and behavioral states. these approaches have signi\ufb01cantly enhanced the ability to\ncharacterize the contribution of different cells and circuits to neural function in health and disease. they\nhave shifted the emphasis of research toward causal interrogations and increased the demand for more\nprecise and powerful tools to control and manipulate neural activity. here, we clarify the conditions under\nwhich measurements and perturbations support causal inferences. we note that the brain functions at\nmultiple scales and that ca", "the journal of neuroscience, june 29, 2005 \u2022 25(26):6235\u2013 6242 \u2022 6235\n\nbehavioral/systems/cognitive\n\ndopamine cells respond to predicted events during\nclassical conditioning: evidence for eligibility traces in the\nreward-learning network\n\nwei-xing pan,1 robert schmidt,2 jeffery r. wickens,2 and brian i. hyland1\ndepartments of 1physiology and 2anatomy and structural biology, school of medical sciences, university of otago, dunedin 9001, new zealand\n\nbehavioral conditioning of cue\u2013reward pairing results in a shift of midbrain dopamine (da) cell activity from responding to the reward\nto responding to the predictive cue. however, the precise time course and mechanism underlying this shift remain unclear. here, we\nreport a combined single-unit recording and temporal difference (td) modeling approach to this question. the data from recordings in\nconscious rats showed that da cells retain responses to predicted reward after responses to conditioned cues have developed, at least\nearly in train", "a genetic programming approach to designing convolutional\n\nneural network architectures\u2217\n\nmasanori suganuma\n\nyokohama national university\n79-7 tokiwadai hodogaya-ku\nyokohama, japan 240-8501\n\nsuganuma-masanori-hf@ynu.jp\n\nshinichi shirakawa\n\nyokohama national university\n79-7 tokiwadai hodogaya-ku\nyokohama, japan 240-8501\n\nshirakawa-shinichi-bg@ynu.ac.jp\n\ntomoharu nagao\n\nyokohama national university\n79-7 tokiwadai hodogaya-ku\nyokohama, japan 240-8501\n\nnagao@ynu.ac.jp\n\n7\n1\n0\n2\n\n \n\ng\nu\na\n1\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n4\n6\n7\n0\n0\n\n.\n\n4\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\nthe convolutional neural network (cnn), which is one of the deep\nlearning models, has seen much success in a variety of computer\nvision tasks. however, designing cnn architectures still requires\nexpert knowledge and a lot of trial and error. in this paper, we\nattempt to automatically construct cnn architectures for an image\nclassification task based on cartesian genetic programming (cgp).\nin our method, we adopt highly functional", "research article\ndetection and analysis of spatiotemporal\npatterns in brain activity\n\nrory g. townsendid1,2, pulin gong1,2*\n\n1 school of physics, the university of sydney, nsw, australia, 2 arc centre of excellence for integrative\nbrain function, the university of sydney, nsw, australia\n\n* pulin.gong@sydney.edu.au\n\nabstract\n\nthere is growing evidence that population-level brain activity is often organized into propa-\ngating waves that are structured in both space and time. such spatiotemporal patterns have\nbeen linked to brain function and observed across multiple recording methodologies and\nscales. the ability to detect and analyze these patterns is thus essential for understanding\nthe working mechanisms of neural circuits. here we present a mathematical and computa-\ntional framework for the identification and analysis of multiple classes of wave patterns in\nneural population-level recordings. by drawing a conceptual link between spatiotemporal\npatterns found in the brain and coherent", "5\n1\n0\n2\n\n \n\nb\ne\nf\n7\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n8\n5\n5\n6\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\nunder review as a conference paper at iclr 2015\n\nrandom walk initialization for training\nvery deep feedforward networks\n\ndavid sussillo\ngoogle inc.\nmountain view, ca, 94303, usa\nsussillo@google.com\n\nl.f. abbott\ndepartments of neuroscience and physiology and cellular biophysics\ncolumbia university\nnew york, ny, 10032, usa\nlfabbott@columbia.edu\n\nabstract\n\ntraining very deep networks is an important open problem in machine learning.\none of many dif\ufb01culties is that the norm of the back-propagated error gradient can\ngrow or decay exponentially. here we show that training very deep feed-forward\nnetworks (ffns) is not as dif\ufb01cult as previously thought. unlike when back-\npropagation is applied to a recurrent network, application to an ffn amounts to\nmultiplying the error gradient by a different random matrix at each layer. we show\nthat the successive application of correctly scaled random matrices to an initi", "predictive coding approximates backprop along\n\narbitrary computation graphs\n\n0\n2\n0\n2\n\n \nt\nc\no\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n2\n8\n1\n4\n0\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nberen millidge\n\nschool of informatics\nuniversity of edinburgh\nberen@millidge.name\n\nalexander tschantz\n\nsackler centre for consciousness science\n\nschool of engineering and informatics\n\nuniversity of sussex\n\ntschantz.alec@gmail.com\n\nchristopher l buckley\n\nevolutionary and adaptive systems research group\n\nschool of engineering and informatics\n\nuniversity of sussex\n\nc.l.buckley@sussex.ac.uk\n\noctober 7, 2020\n\nabstract\n\nbackpropagation of error (backprop) is a powerful algorithm for training machine learning architec-\ntures through end-to-end differentiation. recently it has been shown that backprop in multilayer-\nperceptrons (mlps) can be approximated using predictive coding, a biologically-plausible process\ntheory of cortical computation which relies solely on local and hebbian updates. the power of\nbackprop, however, lies not in", "r e v i e w s\n\n s l e e p\n\nthe memory function of sleep\n\nsusanne diekelmann and jan born\n\nabstract | sleep has been identified as a state that optimizes the consolidation of newly \nacquired information in memory, depending on the specific conditions of learning and the \ntiming of sleep. consolidation during sleep promotes both quantitative and qualitative \nchanges of memory representations. through specific patterns of neuromodulatory activity \nand electric field potential oscillations, slow-wave sleep (sws) and rapid eye movement \n(rem) sleep support system consolidation and synaptic consolidation, respectively. during \nsws, slow oscillations, spindles and ripples \u2014 at minimum cholinergic activity \u2014 coordinate \nthe re-activation and redistribution of hippocampus-dependent memories to neocortical \nsites, whereas during rem sleep, local increases in plasticity-related immediate-early gene \nactivity \u2014 at high cholinergic and theta activity \u2014 might favour the subsequent synaptic \nconsolid", "published as a conference paper at iclr 2019\n\nlearning deep representations by mutual in-\nformation estimation and maximization\n\nr devon hjelm\nmsr montreal, mila, udem, ivado\ndevon.hjelm@microsoft.com\n\nalex fedorov\nmrn, unm\n\nsamuel lavoie-marchildon\nmila, udem\n\nkaran grewal\nu toronto\n\nphil bachman\nmsr montreal\n\nadam trischler\nmsr montreal\n\nyoshua bengio\nmila, udem, ivado, cifar\n\nabstract\n\nthis work investigates unsupervised learning of representations by maximizing\nmutual information between an input and the output of a deep neural network en-\ncoder. importantly, we show that structure matters: incorporating knowledge about\nlocality in the input into the objective can signi\ufb01cantly improve a representation\u2019s\nsuitability for downstream tasks. we further control characteristics of the repre-\nsentation by matching to a prior distribution adversarially. our method, which we\ncall deep infomax (dim), outperforms a number of popular unsupervised learning\nmethods and compares favorably with ful", "two-moment decision models and expected utility maximization \nauthor(s): jack meyer \nsource: the american economic review, jun., 1987, vol. 77, no. 3 (jun., 1987), pp. 421-\n430\npublished by: american economic association \n\n \n\nstable url: https://www.jstor.org/stable/1804104\n \nreferences \nlinked references are available on jstor for this article: \nhttps://www.jstor.org/stable/1804104?seq=1&cid=pdf-\nreference#references_tab_contents \nyou may need to log in to jstor to access the linked references.\n\njstor is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide \nrange of content in a trusted digital archive. we use information technology and tools to increase productivity and \nfacilitate new forms of scholarship. for more information about jstor, please contact support@jstor.org. \n \nyour use of the jstor archive indicates your acceptance of the terms & conditions of use, available at \nhttps://about.jstor.org/terms\n\nis collaborating wi", "n\ne\nu\nr\na\nl\n \nn\ne\nt\nw\no\nr\nk\ns\n,\n \nv\no\nl\n.\n \n5\n,\n \np\np\n.\n \n3\n-\n1\n7\n,\n \n1\n9\n9\n2\n \n0\n8\n9\n3\n-\n6\n0\n8\n0\n/\n9\n2\n \n$\n5\n.\n0\n0\n \n+\n \n.\n0\n0\n \np\nr\ni\nn\nt\ne\nd\n \ni\nn\n \nt\nh\ne\n \nu\ns\na\n.\n \na\nl\nl\n \nr\ni\ng\nh\nt\ns\n \nr\ne\ns\ne\nr\nv\ne\nd\n.\n \nc\no\np\ny\nr\ni\ng\nh\nt\n \n\u00a9\n \n1\n9\n9\n2\n \np\ne\nr\ng\na\nm\no\nn\n \np\nr\ne\ns\ns\n \np\nl\nc\n \ni\nn\nv\ni\nt\ne\nd\n \na\nr\nt\ni\nc\nl\ne\n \no\nb\nj\ne\nc\nt\ni\nv\ne\n \nf\nu\nn\nc\nt\ni\no\nn\n \nf\no\nr\nm\nu\nl\na\nt\ni\no\nn\n \no\nf\n \nt\nh\ne\n \nb\nc\nm\n \nt\nh\ne\no\nr\ny\n \no\nf\n \nv\ni\ns\nu\na\nl\n \nc\no\nr\nt\ni\nc\na\nl\n \np\nl\na\ns\nt\ni\nc\ni\nt\ny\n:\n \ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n \nc\no\nn\nn\ne\nc\nt\ni\no\nn\ns\n,\n \ns\nt\na\nb\ni\nl\ni\nt\ny\n \nc\no\nn\nd\ni\nt\ni\no\nn\ns\n*\n \nn\na\nt\nh\na\nn\n \ni\nn\nt\nr\na\nt\no\nr\n \na\nn\nd\n \nl\ne\no\nn\n \nn\n \nc\no\no\np\ne\nr\n \nb\nr\no\nw\nn\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n \n(\nr\ne\nc\ne\ni\nv\ne\nd\n \na\nn\nd\n \na\nc\nc\ne\np\nt\ne\nd\n \n2\n6\n \nj\nu\nl\ny\n \ni\nn\n \np\nr\ne\ns\ns\n)\n \na\nb\ns\nt\nr\na\nc\nt\n-\n-\ni\nn\n \nt\nh\ni\ns\n \np\na\np\ne\nr\n,\n \nw\ne\n \np\nr\ne\ns\ne\nn\nt\n \na\nn\n \no\nb\nj\ne\nc\nt\ni\nv\ne\n \nf\nu\nn\nc\nt\ni\no\nn\n \nf\no\nr\nm\nu\nl\na\nt\ni\no\nn\n \no\nf\n \nt\nh\ne\n \nb\ni\ne\nn\ne\nn\ns\nt\no\nc\nk\n,\n \nc\no\no\np\ne\nr\n,\n \na\nn\nd\n \nm\nu\nn\nr\no\n \n(\n \nb\nc\nm\n)\n \nt\nh\ne\no\nr\ny\n \no\n", "deep learning models of the retinal response to\n\nnatural scenes\n\nlane t. mcintosh\u22171, niru maheswaranathan\u22171, aran nayebi1,\n\nsurya ganguli2,3, stephen a. baccus3\n\n1neurosciences phd program, 2department of applied physics, 3neurobiology department\n\n{lmcintosh, nirum, anayebi, sganguli, baccus}@stanford.edu\n\nstanford university\n\nabstract\n\na central challenge in sensory neuroscience is to understand neural computations\nand circuit mechanisms that underlie the encoding of ethologically relevant, natu-\nral stimuli. in multilayered neural circuits, nonlinear processes such as synaptic\ntransmission and spiking dynamics present a signi\ufb01cant obstacle to the creation of\naccurate computational models of responses to natural stimuli. here we demon-\nstrate that deep convolutional neural networks (cnns) capture retinal responses to\nnatural scenes nearly to within the variability of a cell\u2019s response, and are markedly\nmore accurate than linear-nonlinear (ln) models and generalized linear mod-\nels (gl", "distributed deep q-learning\n\nkevin chavez1, hao yi ong1, and augustus hong1\n\nabstract\u2014 we propose a distributed deep learning model\nto successfully learn control policies directly from high-\ndimensional sensory input using reinforcement learning. the\nmodel is based on the deep q-network, a convolutional neural\nnetwork trained with a variant of q-learning. its input is\nraw pixels and its output\nis a value function estimating\nfuture rewards from taking an action given a system state.\nto distribute the deep q-network training, we adapt\nthe\ndistbelief software framework to the context of ef\ufb01ciently\ntraining reinforcement learning agents. as a result, the method\nis completely asynchronous and scales well with the number\nof machines. we demonstrate that the deep q-network agent,\nreceiving only the pixels and the game score as inputs, was able\nto achieve reasonable success on a simple game with minimal\nparameter tuning.\n\ni. introduction\n\nreinforcement learning (rl) agents face a tremendous\nch", "reviewarticleintrinsicdimensionestimation:relevanttechniquesandabenchmarkframeworkp.campadelli,1e.casiraghi,1c.ceruti,1anda.rozza21dipartimentodiinformatica,universit`adeglistudidimilano,viacomelico39,20135milano,italy2researchgroup,hyerasoftware,viamattei2,coccaglio,25030brescia,italycorrespondenceshouldbeaddressedtoe.casiraghi;casiraghi@di.unimi.itreceived25february2015;accepted17may2015academiceditor:sangminleecopyright\u00a92015p.campadellietal.thisisanopenaccessarticledistributedunderthecreativecommonsattributionlicense,whichpermitsunrestricteduse,distribution,andreproductioninanymedium,providedtheoriginalworkisproperlycited.whendealingwithdatasetscomprisinghigh-dimensionalpoints,itisusuallyadvantageoustodiscoversomedatastructure.afundamentalinformationneededtothisaimistheminimumnumberofparametersrequiredtodescribethedatawhileminimizingtheinformationloss.thisnumber,usuallycalledintrinsicdimension,canbeinterpretedasthedimensionofthemanifoldfromwhichtheinputdataaresupposedtobedrawn.dueto", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/2826691\n\nthe role of constraints in hebbian learning\n\narticle\u00a0\u00a0in\u00a0\u00a0neural computation \u00b7 july 1997\n\ndoi: 10.1162/neco.1994.6.1.100\u00a0\u00b7\u00a0source: citeseer\n\ncitations\n360\n\n2 authors, including:\n\nkenneth d miller\ncolumbia university\n\n123 publications\u00a0\u00a0\u00a010,715 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n734\n\nall content following this page was uploaded by kenneth d miller on 19 december 2013.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "foundations and trends r(cid:1) in\nmachine learning\nvol. 1, nos. 1\u20132 (2008) 1\u2013305\nc(cid:1) 2008 m. j. wainwright and m. i. jordan\ndoi: 10.1561/2200000001\n\ngraphical models, exponential families, and\n\nvariational inference\n\nmartin j. wainwright1 and michael i. jordan2\n\n1 department of statistics, and department of electrical engineering and\n\ncomputer science, university of california, berkeley 94720, usa,\nwainwrig@stat.berkeley.edu\n\n2 department of statistics, and department of electrical engineering and\n\ncomputer science, university of california, berkeley 94720, usa,\njordan@stat.berkeley.edu\n\nabstract\n\nthe formalism of probabilistic graphical models provides a unifying\nframework for capturing complex dependencies among random\nvariables, and building large-scale multivariate statistical models.\ngraphical models have become a focus of research in many statisti-\ncal, computational and mathematical \ufb01elds, including bioinformatics,\ncommunication theory, statistical physics, combinatorial o", "letter\nnonlinear dendritic integration of sensory and motor\ninput during an active sensing task\n\ndoi:10.1038/nature11601\n\nning-long xu1, mark t. harnett1, stephen r. williams2, daniel huber1{, daniel h. o\u2019connor1{, karel svoboda1 & jeffrey c. magee1\n\nactive dendrites provide neurons with powerful processing capabi-\nlities. however, little is known about the role of neuronal dendrites\nin behaviourally related circuit computations. here we report that a\nnovel global dendritic nonlinearity is involved in the integration of\nsensory and motor information within layer 5 pyramidal neurons\nduring an active sensing behaviour. layer 5 pyramidal neurons possess\nelaborate dendritic arborizations that receive functionally distinct\ninputs, each targeted to spatially separate regions1,2. at the cellular\nlevel, coincident input from these segregated pathways initiates\nregenerative dendritic electrical events that produce bursts of action\npotential output3,4 and circuits featuring this powerful dendrit", "connectivity\t\r \u00a0 reflects\t\r \u00a0 coding:\t\r \u00a0 a\t\r \u00a0 model\t\r \u00a0 of\t\r \u00a0 voltage-\u00ad\u2010based\t\r \u00a0 spike-\u00ad\u2010\ntiming-\u00ad\u2010dependent-\u00ad\u2010plasticity\t\r \u00a0with\t\r \u00a0homeostasis\t\r \u00a0\n\nclaudia\t\r \u00a0clopath,\t\r \u00a0lars\t\r \u00a0b\u00fcsing*,\t\r \u00a0eleni\t\r \u00a0vasilaki,\t\r \u00a0wulfram\t\r \u00a0gerstner\t\r \u00a0\n\t\r \u00a0\nlaboratory\t\r \u00a0of\t\r \u00a0computational\t\r \u00a0neuroscience\t\r \u00a0\nbrain-\u00ad\u2010mind\t\r \u00a0institute\t\r \u00a0and\t\r \u00a0school\t\r \u00a0of\t\r \u00a0computer\t\r \u00a0and\t\r \u00a0communication\t\r \u00a0sciences\t\r \u00a0\necole\t\r \u00a0polytechnique\t\r \u00a0f\u00e9d\u00e9rale\t\r \u00a0de\t\r \u00a0lausanne\t\r \u00a0\n1015\t\r \u00a0lausanne\t\r \u00a0epfl,\t\r \u00a0switzerland\t\r \u00a0\t\r \u00a0\n*\t\r \u00a0permanent\t\r \u00a0address:\t\r \u00a0institut\t\r \u00a0f\u00fcr\t\r \u00a0grundlagen\t\r \u00a0der\t\r \u00a0informationsverarbeitung,\t\r \u00a0tu\t\r \u00a0graz,\t\r \u00a0austria\t\r \u00a0\n\t\r \u00a0\n\nabstract\t\r \u00a0\n\nelectrophysiological\t\r \u00a0 connectivity\t\r \u00a0 patterns\t\r \u00a0 in\t\r \u00a0 cortex\t\r \u00a0 often\t\r \u00a0 show\t\r \u00a0 a\t\r \u00a0 few\t\r \u00a0 strong\t\r \u00a0 connections,\t\r \u00a0 sometimes\t\r \u00a0\nbidirectional,\t\r \u00a0 in\t\r \u00a0 a\t\r \u00a0 sea\t\r \u00a0 of\t\r \u00a0 weak\t\r \u00a0 connections.\t\r \u00a0 in\t\r \u00a0 order\t\r \u00a0 to\t\r \u00a0 explain\t\r \u00a0 these\t\r \u00a0 connectivity\t\r \u00a0 patterns,\t\r \u00a0 we\t\r \u00a0 use\t\r \u00a0 a\t\r \u00a0\nmodel\t\r \u00a0of\t\r \u00a0spike-\u00ad\u2010tim", "improved preconditioner for\nhessian free optimization\n\nolivier chapelle\n\nyahoo! labs\n\nsanta clara, ca\n\ndumitru erhan\n\nyahoo! labs\nsunnyvale, ca\n\nchap@yahoo-inc.com\n\ndumitru@yahoo-inc.com\n\nabstract\n\nwe investigate the use of hessian free optimization for learning deep au-\ntoencoders. one of the critical components in that algorithm is the choice\nof the preconditioner. we argue in this paper that the jacobi precondi-\ntioner leads to faster optimization and we show how it can be accurately\nand e\ufb03ciently estimated using a randomized algorithm.\n\n1 introduction\n\ndeep architectures have gained a lot of attention recently, as they have been shown to\nextract meaningful non-linear features from the data, which tend to generalize well in a\nvariety of supervised learning settings [hinton and salakhutdinov, 2006, bengio, 2009]. a\ntypical semi-supervised setting is that of pre-training a deep architecture using unsupervised\nbuilding blocks such as restricted boltzmann machines [hinton and salakhutdi", "8\n1\n0\n2\n\n \nt\nc\no\n \n6\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n3\n9\n3\n1\n1\n\n.\n\n0\n1\n8\n1\n:\nv\ni\nx\nr\na\n\ndendritic cortical microcircuits\n\napproximate the backpropagation algorithm\n\njo\u00e3o sacramento\u2217\n\ndepartment of physiology\n\nuniversity of bern, switzerland\nsacramento@pyl.unibe.ch\n\nyoshua bengio\u2021\n\nmila and universit\u00e9 de montr\u00e9al, canada\n\nyoshua.bengio@mila.quebec\n\nrui ponte costa\u2020\n\ndepartment of physiology\n\nuniversity of bern, switzerland\n\ncosta@pyl.unibe.ch\n\nwalter senn\n\ndepartment of physiology\n\nuniversity of bern, switzerland\n\nsenn@pyl.unibe.ch\n\nabstract\n\ndeep learning has seen remarkable developments over the last years, many of\nthem inspired by neuroscience. however, the main learning mechanism behind\nthese advances \u2013 error backpropagation \u2013 appears to be at odds with neurobiology.\nhere, we introduce a multilayer neuronal network model with simpli\ufb01ed dendritic\ncompartments in which error-driven synaptic plasticity adapts the network towards\na global desired output. in contrast to previous wo", "european journal of neuroscience, vol. 19, pp. 1997\u00b12002, 2004\n\n\u00df federation of european neuroscience societies\n\nshort communication\ncocaine-experienced rats exhibit learning de\u00aecits in a\ntask sensitive to orbitofrontal cortex lesions\n\ngeoffrey schoenbaum1, michael p. saddoris,2 seth j. ramus,3 yavin shaham4 and barry setlow5\n1university of maryland school of medicine, department of anatomy and neurobiology, 685 west baltimore street, hsf-1 rm 280k,\nbaltimore, md 21201, usa\n2department of psychological and brain sciences, johns hopkins university, 3400 north charles street, baltimore, md 21218, usa\n3departments of psychology and neuroscience, bowdoin college, 6900 college station, brunswick, me 04011, usa\n4behavioural neuroscience branch, intramural research program nida/nih/dhhs, 5500 nathan shock dr., baltimore, md, usa\n5department of psychology, texas a&m university, college station, tx 77843-4235, usa\n\nkeywords: addiction, discrimination, learning, motivation, olfactory, reversal\n\n", "physical review x 11, 021064 (2021)\n\ntransient chaotic dimensionality expansion by recurrent networks\nchristian keup ,1,2,*,\u2020\n\ntobias k\u00fchn ,1,2,3,* david dahmen ,1 and moritz helias 1,4\n\n1institute of neuroscience and medicine (inm-6) and institute for advanced simulation (ias-6) and jara\n\ninstitut brain structure-function relationships (inm-10), j\u00fclich research centre, j\u00fclich, germany\n\n3laboratoire de physique de l\u2019ens, laboratoire msc de l\u2019universit\u00b4e de paris, cnrs, paris, france\n\n4department of physics, faculty 1, rwth aachen university, aachen, germany\n\n2rwth aachen university, aachen, germany\n\n \n\n(received 7 may 2020; revised 18 march 2021; accepted 23 april 2021; published 25 june 2021)\n\nneurons in the brain communicate with spikes, which are discrete events in time and value. functional\nnetwork models often employ rate units that are continuously coupled by analog signals. is there a\nqualitative difference implied by these two forms of signaling? we develop a unified mean-field", "published as a conference paper at iclr 2020\n\nfantastic generalization measures\nand where to find them\n\nyiding jiang\u2217\u2020, behnam neyshabur\u2217, hossein mobahi, dilip krishnan, samy bengio\ngoogle research\n{ydjiang,neyshabur,hmobahi,dilipkay,bengio}@google.com\n\nabstract\n\ngeneralization of deep networks has lately been of great interest, resulting in a\nnumber of theoretically and empirically motivated complexity measures. how-\never, most papers proposing such measures study only a small set of models, leav-\ning open the question of whether the conclusion drawn from those experiments\nwould remain valid in other settings. we present the \ufb01rst large scale study of gen-\neralization in deep networks. we investigate more then 40 complexity measures\ntaken from both theoretical bounds and empirical studies. we train over 10,000\nconvolutional networks by systematically varying commonly used hyperparame-\nters. hoping to uncover potentially causal relationships between each measure and\ngeneralization, we ", "a comparative analysis of expected and distributional reinforcement learning\n\nclare lyle,1 pablo samuel castro, 2 marc g. bellemare 2\n\n1university of oxford (work done while at google brain)\n\nclare.lyle@cs.ox.ac.uk, psc@google.com, bellemare@google.com\n\n2google brain\n\nabstract\n\nsince their introduction a year ago, distributional approaches\nto reinforcement learning (distributional rl) have produced\nstrong results relative to the standard approach which models\nexpected values (expected rl). however, aside from con-\nvergence guarantees, there have been few theoretical results\ninvestigating the reasons behind the improvements distribu-\ntional rl provides. in this paper we begin the investigation\ninto this fundamental question by analyzing the differences\nin the tabular, linear approximation, and non-linear approx-\nimation settings. we prove that in many realizations of the\ntabular and linear approximation settings, distributional rl\nbehaves exactly the same as expected rl. in cases where ", "c h a p t e r\n\n15\n\nvalue learning through reinforcement:\n\nthe basics of dopamine and reinforcement\n\nlearning\n\nnathaniel d. daw and philippe n. tobler\n\no u t l i n e\n\nintroduction\n\nlearning: prediction and prediction errors\n\nfunctional anatomy of dopamine and striatum\n\nresponses of dopamine neurons to outcomes\nsequential predictions: from rescorla\u0000wagner to\ntemporal difference learning\n\n283\n\n283\n\n285\n\n287\n\n289\n\ntemporal difference learning and the dopamine\nresponse\n\nfrom error-driven learning to choice\n\nconclusions\n\nreferences\n\n293\n\n294\n\n296\n\n296\n\nintroduction\n\nlearning: prediction and\n\nprediction errors\n\nthis chapter provides an overview of reinforcement\nlearning and temporal difference learning and relates\nthese topics to the firing properties of midbrain dopa-\nmine neurons. first, we review the rescorla\u0000wagner\nlearning rule and basic learning phenomena, such as\nblocking, which the rule explains. then we introduce\nthe basic functional anatomy of the dopamine system\nand review studies ", "1 of 27\n\ntext  \nonly \n\nmythosthe\n\nof model \ninterpretability \n\nin machine learning, the \nconcept of interpretability is \nboth important and slippery.\n\nzachary c. lipton \n\nsupervised machine-learning models boast \n\nremarkable predictive capabilities. but can you \ntrust your model? will it work in deployment? \nwhat else can it tell you about the world? \nmodels should be not only good, but also \ninterpretable, yet the task of interpretation appears \nunderspecified. the academic literature has provided \ndiverse and sometimes non-overlapping motivations for \ninterpretability and has offered myriad techniques for \nrendering interpretable models. despite this ambiguity, \nmany authors proclaim their models to be interpretable \naxiomatically, absent further argument. problematically, \nit is not clear what common properties unite these \ntechniques.\n\nthis article seeks to refine the discourse on \n\ninterpretability. first it examines the objectives of previous \n\nacmqueue | may-june 2018   1\n\nmachi", "article\n\ninhibitory and excitatory spike-timing-dependent\nplasticity in the auditory cortex\n\nhighlights\nd spike pairing induces stdp of excitatory and inhibitory layer\n\nauthors\n\njames a. d\u2019amour, robert c. froemke\n\n5 cortical synapses\n\nd inhibitory potentiation occurs when either pre- or\n\npostsynaptic spikes come \ufb01rst\n\nd inhibitory potentiation depends on the initial excitatory-\n\ninhibitory ratio\n\nd excitatory and inhibitory inputs become bound together by\n\npostsynaptic spiking\n\ncorrespondence\nrobert.froemke@med.nyu.edu\n\nin brief\nd\u2019amour and froemke show how\ninhibitory and excitatory synapses are co-\nmodi\ufb01ed by pre- and postsynaptic spike\npairing. inhibitory plasticity depends on\nthe initial excitatory-inhibitory ratio to\nhelp balance inhibition with excitation.\n\nd\u2019amour & froemke, 2015, neuron 86, 514\u2013528\napril 22, 2015 \u00aa2015 elsevier inc.\nhttp://dx.doi.org/10.1016/j.neuron.2015.03.014\n\n\f", "5\n1\n0\n2\n\n \n\ny\na\nm\n8\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n7\n9\n5\n4\n0\n\n.\n\n5\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nu-net: convolutional networks for biomedical\n\nimage segmentation\n\nolaf ronneberger, philipp fischer, and thomas brox\n\ncomputer science department and bioss centre for biological signalling studies,\n\nuniversity of freiburg, germany\n\nronneber@informatik.uni-freiburg.de,\n\nwww home page: http://lmb.informatik.uni-freiburg.de/\n\nabstract. there is large consent that successful training of deep net-\nworks requires many thousand annotated training samples. in this pa-\nper, we present a network and training strategy that relies on the strong\nuse of data augmentation to use the available annotated samples more\ne\ufb03ciently. the architecture consists of a contracting path to capture\ncontext and a symmetric expanding path that enables precise localiza-\ntion. we show that such a network can be trained end-to-end from very\nfew images and outperforms the prior best method (a sliding-window\nconvolutional network) on ", "active predicting coding: brain-inspired\n\nreinforcement learning for sparse reward robotic\n\ncontrol problems\n\n2\n2\n0\n2\n\n \n\np\ne\ns\n9\n1\n\n \n\n \n \n]\n\no\nr\n.\ns\nc\n[\n \n \n\n1\nv\n4\n7\n1\n9\n0\n\n.\n\n9\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nalexander ororbia\n\nrochester institute of technology\n\nago@cs.rit.edu\n\nankur mali\n\nthe university of south florida\n\nankurarjunmali@usf.edu\n\nabstract\n\nin this article, we propose a backpropagation-free approach to robotic control through the neuro-\ncognitive computational framework of neural generative coding (ngc), designing an agent built\ncompletely from powerful predictive coding/processing circuits that facilitate dynamic, online learning\nfrom sparse rewards, embodying the principles of planning-as-inference. concretely, we craft an\nadaptive agent system, which we call active predictive coding (actpc), that balances an internally-\ngenerated epistemic signal (meant to encourage intelligent exploration) with an internally-generated\ninstrumental signal (meant to encourage goal-seeking behavi", "barcodes: the persistent topology of data\n\nrobert ghrist\n\nabstract. this article surveys recent work of carlsson and collaborators on\napplications of computational algebraic topology to problems of feature de-\ntection and shape recognition in high-dimensional data. the primary math-\nematical tool considered is a homology theory for point-cloud data sets \u2014\npersistent homology \u2014 and a novel representation of this algebraic charac-\nterization \u2014 barcodes. we sketch an application of these techniques to the\nclassi\ufb01cation of natural images.\n\n1. the shape of data\n\nwhen a topologist is asked, \u201chow do you visualize a four-dimensional object?\u201d\nthe appropriate response is a socratic rejoinder: \u201chow do you visualize a three-\ndimensional object?\u201d we do not see in three spatial dimensions directly, but rather\nvia sequences of planar projections integrated in a manner that is sensed if not com-\nprehended. we spend a signi\ufb01cant portion of our \ufb01rst year of life learning how to\ninfer three-dimensional s", "r e v i e w s\n\nprimary visual cortex and\nvisual awareness\n\nfrank tong\n\nthe primary visual cortex (v1) is probably the best characterized area of primate cortex, but\nwhether this region contributes directly to conscious visual experience is controversial. early\nneurophysiological and neuroimaging studies found that visual awareness was best correlated\nwith neural activity in extrastriate visual areas, but recent studies have found similarly powerful\neffects in v1. lesion and inactivation studies have provided further evidence that v1 might be\nnecessary for conscious perception. whereas hierarchical models propose that damage to v1\nsimply disrupts the flow of information to extrastriate areas that are crucial for awareness,\ninteractive models propose that recurrent connections between v1 and higher areas form\nfunctional circuits that support awareness. further investigation into v1 and its interactions with\nhigher areas might uncover fundamental aspects of the neural basis of visual awar", "j\no\nu\nr\nm\nd\n \n~\n4\nn\ne\nu\nr\no\n,\nw\ni\ne\nn\nc\ne\nm\ne\nt\nh\no\nd\n~\n,\n \n1\n1\n \n(\n1\n9\n8\n4\n)\n \n4\n7\n-\n6\n0\n \n4\n7\n \ne\nl\ns\ne\nv\ni\ne\nr\n \nn\ns\nm\n \n0\n0\n3\n8\n4\n \nd\ne\nv\ne\nl\no\np\nm\ne\nn\nt\ns\n \no\nf\n \na\n \nw\na\nt\ne\nr\n-\nm\na\nz\ne\n \np\nr\no\nc\ne\nd\nu\nr\ne\n \nf\no\nr\n \ns\nt\nu\nd\ny\ni\nn\ng\n \ns\np\na\nt\ni\na\nl\n \nl\ne\na\nr\nn\ni\nn\ng\n \ni\nn\n \nt\nh\ne\n \nr\na\nt\n \nr\ni\nc\nh\na\nr\nd\n \nm\no\nr\nr\ni\ns\n \nm\nr\nc\n \nc\no\ng\nn\ni\nt\ni\nv\ne\n \nn\ne\nu\nr\no\ns\nc\ni\ne\nn\nc\ne\n \ng\nr\no\nu\np\n,\n \np\n.\nw\nc\nh\no\nl\no\ng\ni\nc\na\nl\n \nl\na\nb\no\nr\na\nt\no\nr\ny\n,\n \nu\nn\ni\nt\n~\ne\nr\ns\ni\no\n'\n \no\n \nf\n \ns\nt\n.\n \na\nn\nd\nr\ne\nw\ns\n,\n \ns\nt\n.\n \na\nn\nd\nr\ne\nw\ns\n,\n \nf\ni\nf\n~\n,\n \n(\nu\n.\nk\n.\n)\n \n(\nr\ne\nc\ne\ni\nv\ne\nd\n \nd\ne\nc\ne\nm\nb\ne\nr\n \n2\n2\nn\nd\n,\n \n1\n9\n8\n3\n)\n \n(\nr\ne\nv\ni\ns\ne\nd\n \nm\na\nr\nc\nh\n \n2\n8\nt\nh\n,\n \n1\n9\n8\n4\n)\n \n(\na\nc\nc\ne\np\nt\ne\nd\n \na\np\nr\ni\nl\n \n3\nr\nd\n,\n \n1\n9\n8\n4\n)\n \nk\ne\ny\n \nw\no\nr\nd\ns\n:\n \nw\na\nt\ne\nr\n-\nm\na\nz\ne\n-\n-\ns\np\na\nt\ni\na\nl\n \nm\ne\nm\no\nr\ny\n \nd\ne\nv\ne\nl\no\np\nm\ne\nn\nt\ns\n \no\nf\n \na\nn\n \no\np\ne\nn\n-\nf\ni\ne\nl\nd\n \nw\na\nt\ne\nr\n-\nm\na\nz\ne\n \np\nr\no\nc\ne\nd\nu\nr\ne\n \ni\nn\n \nw\nh\ni\nc\nh\n \nr\na\nt\ns\n \nl\ne\na\nr\nn\n \nt\no\n \ne\ns\nc\na\np\ne\n \nf\nr\no\nm\n \no\np\na\nq\nu\ne\n \nw\na\nt\ne\nr\n \n", "movinets: mobile video networks for ef\ufb01cient video recognition\n\ndan kondratyuk*, liangzhe yuan, yandong li, li zhang, mingxing tan, matthew brown, boqing gong\n\n{dankondratyuk,lzyuan,yandongli,zhl,tanmingxing,mtbr,bgong}@google.com\n\ngoogle research\n\n1\n2\n0\n2\n\n \nr\np\na\n8\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n1\n1\n5\n1\n1\n\n.\n\n3\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe present mobile video networks (movinets), a fam-\nily of computation and memory ef\ufb01cient video networks\nthat can operate on streaming video for online inference.\n3d convolutional neural networks (cnns) are accurate at\nvideo recognition but require large computation and mem-\nory budgets and do not support online inference, making\nthem dif\ufb01cult to work on mobile devices. we propose a\nthree-step approach to improve computational ef\ufb01ciency\nwhile substantially reducing the peak memory usage of 3d\ncnns. first, we design a video network search space\nand employ neural architecture search to generate ef\ufb01cient\nand diverse 3d cnn architectures. second, w", "1\n2\n0\n2\n\n \nt\nc\no\n5\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n2\n6\n8\n1\n0\n\n.\n\n6\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nself-supervised learning of event-based\noptical flow with spiking neural networks\n\njesse j. hagenaars\u2217\n\nj.j.hagenaars@tudelft.nl\n\nfederico paredes-vall\u00e9s\u2217\n\nf.paredesvalles@tudelft.nl\n\nguido c. h. e. de croon\n\ng.c.h.e.decroon@tudelft.nl\n\nmicro air vehicle laboratory\n\ndelft university of technology, the netherlands\n\nabstract\n\nthe \ufb01eld of neuromorphic computing promises extremely low-power and low-\nlatency sensing and processing. challenges in transferring learning algorithms\nfrom traditional arti\ufb01cial neural networks (anns) to spiking neural networks\n(snns) have so far prevented their application to large-scale, complex regression\ntasks. furthermore, realizing a truly asynchronous and fully neuromorphic pipeline\nthat maximally attains the abovementioned bene\ufb01ts involves rethinking the way in\nwhich this pipeline takes in and accumulates information. in the case of perception,\nspikes would be passed as-", "7\n1\n0\n2\n\n \n\ny\na\nm\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n7\n4\n8\n8\n0\n\n.\n\n4\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nparseval networks: improving robustness to adversarial examples\n\nmoustapha cisse 1 piotr bojanowski 1 edouard grave 1 yann dauphin 1 nicolas usunier 1\n\nabstract\n\nwe introduce parseval networks, a form of deep\nneural networks in which the lipschitz constant\nof linear, convolutional and aggregation layers\nis constrained to be smaller than 1. parseval\nnetworks are empirically and theoretically mo-\ntivated by an analysis of the robustness of the\npredictions made by deep neural networks when\ntheir input is subject to an adversarial perturba-\ntion. the most important feature of parseval net-\nworks is to maintain weight matrices of linear\nand convolutional layers to be (approximately)\nparseval tight frames, which are extensions of\northogonal matrices to non-square matrices. we\ndescribe how these constraints can be maintained\nef\ufb01ciently during sgd. we show that parse-\nval networks match the state-of-the-", "greedy layerwise learning can scale to imagenet\n\neugene belilovsky 1 michael eickenberg 2 edouard oyallon 3\n\nabstract\n\nshallow supervised 1-hidden layer neural net-\nworks have a number of favorable properties that\nmake them easier to interpret, analyze, and opti-\nmize than their deep counterparts, but lack their\nrepresentational power. here we use 1-hidden\nlayer learning problems to sequentially build deep\nnetworks layer by layer, which can inherit proper-\nties from shallow networks. contrary to previous\napproaches using shallow networks, we focus on\nproblems where deep learning is reported as crit-\nical for success. we thus study cnns on image\nclassi\ufb01cation tasks using the large-scale imagenet\ndataset and the cifar-10 dataset. using a simple\nset of ideas for architecture and training we \ufb01nd\nthat solving sequential 1-hidden-layer auxiliary\nproblems lead to a cnn that exceeds alexnet\nperformance on imagenet. extending this train-\ning methodology to construct individual layers\nby solving", "recovery guarantees for one-hidden-layer neural networks\u2217\n\nkai zhong 1 zhao song 2 prateek jain 3 peter l. bartlett 4 inderjit s. dhillon 5\n\nabstract\n\nin this paper, we consider regression problems\nwith one-hidden-layer neural networks (1nns).\nwe distill some properties of activation func-\ntions that lead to local strong convexity in the\nneighborhood of the ground-truth parameters for\nthe 1nn squared-loss objective and most popu-\nlar nonlinear activation functions satisfy the dis-\ntilled properties, including recti\ufb01ed linear units\n(relus), leaky relus, squared relus and sig-\nmoids. for activation functions that are also\nsmooth, we show local linear convergence guar-\nantees of gradient descent under a resampling\nrule. for homogeneous activations, we show ten-\nsor methods are able to initialize the parameters\nto fall into the local strong convexity region. as\na result, tensor initialization followed by gradient\ndescent is guaranteed to recover the ground truth\nwith sample complexity d \u00b7 ", "mice alternate between discrete strategies during \nperceptual decision-making\n\u200a1,2\u2009\u2709, nicholas a. roy2, iris r. stone\u200a\n\n\u200a2, the international brain laboratory*, \n\n\u200a3, anne k. churchland\u200a\n\n\u200a4, alexandre pouget\u200a\n\n\u200a5 and jonathan w. pillow\u200a\n\n\u200a2,6\u2009\u2709\n\nzoe c. ashwood\u200a\nanne e. urai\u200a\n\nclassical models of perceptual decision-making assume that subjects use a single, consistent strategy to form decisions, or \nthat decision-making strategies evolve slowly over time. here we present new analyses suggesting that this common view is \nincorrect. we analyzed data from mouse and human decision-making experiments and found that choice behavior relies on an \ninterplay among multiple interleaved strategies. these strategies, characterized by states in a hidden markov model, persist \nfor tens to hundreds of trials before switching, and often switch multiple times within a session. the identified decision-making \nstrategies were highly consistent across mice and comprised a single \u2018engaged\u2019 state, in which ", "article\n\ncomputing by robust transience: how the fronto-\nparietal network performs sequential, category-\nbased decisions\n\nhighlights\nd recurrent networks trained to perform dmc tasks exhibit\n\nrobust transience dynamics\n\nd dynamics consist of stable and slow states connected by\n\nrobust trajectory tunnels\n\nd models\u2019 neural activities are remarkably similar to recordings\n\nfrom lip and pfc\n\nd trained rnns replicate categorization studies with multiple\n\ncategories\n\nauthors\n\nwarasinee chaisangmongkon,\nsruthi k. swaminathan,\ndavid j. freedman, xiao-jing wang\n\ncorrespondence\nxjwang@nyu.edu\n\nin brief\nchaisangmongkon et al. present a\nrecurrent neural network model of\nprimate fronto-parietal network that can\ncapture various phenomena from\nneurophysiological experiments in\ndelayed match-to-category tasks.\n\nchaisangmongkon et al., 2017, neuron 93, 1504\u20131517\nmarch 22, 2017 published by elsevier inc.\nhttp://dx.doi.org/10.1016/j.neuron.2017.03.002\n\n\f", "article\n\nhttps://doi.org/10.1038/s41467-022-34938-7\n\nsleep-like unsupervised replay reduces\ncatastrophic forgetting in arti\ufb01cial neural\nnetworks\n\nreceived: 24 may 2021\n\naccepted: 10 november 2022\n\ntimothy tadros\nmaxim bazhenov 1,2\n\n1,2, giri p. krishnan2, ramyaa ramyaa3 &\n\ncheck for updates\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\narti\ufb01cial neural networks are known to suffer from catastrophic forgetting:\nwhen learning multiple tasks sequentially, they perform well on the most\nrecent task at the expense of previously learned tasks. in the brain, sleep is\nknown to play an important role in incremental learning by replaying recent\nand old con\ufb02icting memory traces. here we tested the hypothesis that\nimplementing a sleep-like phase in arti\ufb01cial neural networks can protect old\nmemories during new training and alleviate catastrophic forgetting. sleep was\nimplemented as off-line training with local unsupervised hebbian plasticity\nrules and noisy input. in an incrementa", "university of pennsylvania \nuniversity of pennsylvania \nscholarlycommons \nscholarlycommons \n\ngeneral robotics, automation, sensing and \nperception laboratory \n\nlab papers (grasp) \n\n1-2008 \n\nbarcodes: the persistent topology of data \nbarcodes: the persistent topology of data \n\nrobert ghrist \nuniversity of pennsylvania, ghrist@seas.upenn.edu \n\nfollow this and additional works at: https://repository.upenn.edu/grasp_papers \n\nrecommended citation \nrecommended citation \nrobert ghrist, \"barcodes: the persistent topology of data\", . january 2008. \n\nreprinted from: \nghrist, r. barcodes: the persistent topology of data. bulletin of the american mathematical society (new series) \n45, 1 (2008), 61\u201375. \n\nthis paper is posted at scholarlycommons. https://repository.upenn.edu/grasp_papers/1 \nfor more information, please contact repository@pobox.upenn.edu. \n\n\f", "\f", "distributional soft actor-critic: off-policy\n\nreinforcement learning for addressing value\n\nestimation errors\n\njingliang duan, yang guan, shengbo eben li*, yangang ren, qi sun, and bo cheng\n\n1\n\n1\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n1\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n1\n1\n8\n2\n0\n\n.\n\n1\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\u2014in reinforcement learning (rl), function approxi-\nmation errors are known to easily lead to the q-value overesti-\nmations, thus greatly reducing policy performance. this paper\npresents a distributional soft actor-critic (dsac) algorithm,\nwhich is an off-policy rl method for continuous control setting,\nto improve the policy performance by mitigating q-value overes-\ntimations. we \ufb01rst discover in theory that learning a distribution\nfunction of state-action returns can effectively mitigate q-value\noverestimations because it is capable of adaptively adjusting the\nupdate stepsize of the q-value function. then, a distributional soft\npolicy iteration (dspi) framework is developed by embedding the\nreturn dis", "a r t i c l e s\n\nsignals in inferotemporal and perirhinal cortex suggest \nan untangling of visual target information\nmarino pagan, luke s urban, margot p wohl & nicole c rust\nfinding sought visual targets requires our brains to flexibly combine working memory information about what we are looking for \nwith visual information about what we are looking at. to investigate the neural computations involved in finding visual targets, \nwe recorded neural responses in inferotemporal cortex (it) and perirhinal cortex (prh) as macaque monkeys performed a task \nthat required them to find targets in sequences of distractors. we found similar amounts of total task-specific information in both \nareas; however, information about whether a target was in view was more accessible using a linear read-out or, equivalently, was \nmore untangled in prh. consistent with the flow of information from it to prh, we also found that task-relevant information \narrived earlier in it. prh responses were well-describe", "9\n1\n0\n2\n\n \n\nv\no\nn\n2\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n1\n2\n3\n1\n\n.\n\n5\n0\n9\n1\n:\nv\ni\nx\nr\na\n\ngeneralization bounds of stochastic gradient descent\n\nfor wide and deep neural networks\n\nyuan cao\u2217 and quanquan gu\u2020\n\nabstract\n\nwe study the training and generalization of deep neural networks (dnns) in the over-\nparameterized regime, where the network width (i.e., number of hidden nodes per layer) is\nmuch larger than the number of training data points. we show that, the expected 0-1 loss\nof a wide enough relu network trained with stochastic gradient descent (sgd) and random\ninitialization can be bounded by the training loss of a random feature model induced by the\nnetwork gradient at initialization, which we call a neural tangent random feature (ntrf) model.\nfor data distributions that can be classi\ufb01ed by ntrf model with su\ufb03ciently small error, our\n\nresult yields a generalization error bound in the order of ropn\u00b41{2q that is independent of the\n\nnetwork width. our result is more general and sharper ", "biologically inspired modular neural networks\n\nfarooq azam\n\ndissertation submitted to the faculty of the virginia polytechnic institute and state university\n\nin partial ful\ufb01llment of the requirements for the degree of\n\ndoctor of philosophy\n\nin\n\nelectrical and computer engineering\n\ndr. hugh f. vanlandingham, chair\n\ndr. william t. baumann\n\ndr. john s. bay\n\ndr. peter m. athanas\n\ndr. william r. saunders\n\nmay, 2000\n\nblacksburg, virginia\n\nkeywords: biologically inspired neural networks, modular neural networks, principle of divide\n\nand conquer, a priori expert knowledge, arti\ufb01cial neural networks, robustness, accuracy, and\n\ngeneralization.\n\nc(cid:13)farooq azam, 2000\n\n\f", "28\n\nbatch and online learning algorithms for nonconvex\nneyman-pearson classi\ufb01cation\n\ngilles gasso, litis insa\naristidis pappaioannou, marina spivak, and l \u00b4eon bottou, nec labs\n\nwe describe and evaluate two algorithms for neyman-pearson (np) classi\ufb01cation problem which has been\nrecently shown to be of a particular importance for bipartite ranking problems. np classi\ufb01cation is a non-\nconvex problem involving a constraint on false negatives rate. we investigated batch algorithm based on\ndc programming and stochastic gradient method well suited for large-scale datasets. empirical evidences\nillustrate the potential of the proposed methods.\ncategories and subject descriptors: i.5.2 [computing methodologies]: pattern recognition\u2014classi\ufb01er\ndesign and evaluation\ngeneral terms: algorithms\nadditional key words and phrases: neyman-pearson, nonconvex svm, dc algorithm, online learning\nacm reference format:\ngasso, g., pappaioannou, a., spivak, m., and bottou, l. 2011. batch and online learning algo", "the tale of the neuroscientists and the computer: why\nmechanistic theory matters\njoshua w. brown*\n\nopinion article\npublished: 31 october 2014\ndoi: 10.3389/fnins.2014.00349\n\npsychological and brain sciences, indiana university, bloomington, in, usa\n*correspondence: jwmbrown@indiana.edu\n\nedited by:\nangela r. laird, florida international university, usa\nreviewed by:\nalexander j. shackman, university of maryland, usa\nkimberly louise ray, university of california, davis, usa\n\nkeywords: computational modeling, methods, theory, fmri, eeg, neurophysiology, neuropsychology\n\nintroduction\na little over a decade ago, a biologist asked\nthe question \u201ccan a biologist \ufb01x a radio?\u201d\n(lazebnik, 2002). that question framed\nan amusing yet profound discussion of\nwhich methods are most appropriate to\nunderstand the inner workings of a sys-\ntem, such as a radio. for the engineer, the\nanswer is straightforward: you trace out\nthe transistors, resistors, capacitors etc.,\nand then draw an electrical circuit dia-\n", "foundation models for decision making:\nproblems, methods, and opportunities\n\nsherry yang\u22171,2 ofir nachum1 yilun du3\n\njason wei1 pieter abbeel2 dale schuurmans1,4\n\n1google research, brain team, 2uc berkeley, 3mit, 4university of alberta\n\nfoundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in\na wide range of vision and language tasks. when such models are deployed in real world environments,\nthey inevitably interface with other entities and agents. for example, language models are often\nused to interact with human beings through dialogue, and visual perception models are used to\nautonomously navigate neighborhood streets. in response to these developments, new paradigms are\nemerging for training foundation models to interact with other agents and perform long-term reasoning.\nthese paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask,\nand generalist interaction. research at the intersection of foundation ", "vol 458 | 19 march 2009 | doi:10.1038/nature07842\n\narticles\n\nactivation of camkii in single dendritic\nspines during long-term potentiation\n\nseok-jin r. lee1, yasmin escobedo-lozoya1, erzsebet m. szatmari1 & ryohei yasuda1\n\ncalcium/calmodulin-dependent kinase ii (camkii) plays a central part in long-term potentiation (ltp), which underlies some\nforms of learning and memory. here we monitored the spatiotemporal dynamics of camkii activation in individual dendritic\nspines during ltp using two-photon fluorescence lifetime imaging microscopy, in combination with two-photon glutamate\nuncaging. induction of ltp and associated spine enlargement in single spines triggered transient (,1 min) camkii activation\nrestricted to the stimulated spines. camkii in spines was specifically activated by nmda receptors and l-type voltage-sensitive\ncalcium channels, presumably by nanodomain ca21 near the channels, in response to glutamate uncaging and depolarization,\nrespectively. the high degree of compartme", "stable population coding for working memory\ncoexists with heterogeneous neural\ndynamics in prefrontal cortex\n\njohn d. murraya, alberto bernacchiab, nicholas a. royc, christos constantinidisd, ranulfo romoe,f,1,\nand xiao-jing wangg,h,1\n\nadepartment of psychiatry, yale university school of medicine, new haven, ct 06510; bdepartment of engineering, university of cambridge, cambridge\ncb2 1pz, united kingdom; cprinceton neuroscience institute, princeton university, princeton, nj 08544; ddepartment of neurobiology and anatomy, wake\nforest university school of medicine, winston-salem, nc 27157; einstituto de fisiolog\u00b4\u0131a celular-neurociencias, universidad nacional aut \u00b4onoma de m \u00b4exico,\n04510 mexico d.f., mexico;\nand hnew york university-east china normal university institute of brain and cognitive science, nyu-shanghai, shanghai 200122, china\n\nfel colegio nacional, 06020 mexico d.f., mexico; gcenter for neural science, new york university, new york, ny 10012;\n\ncontributed by ranulfo romo, no", "journal  of  neuroscience  methods  263  (2016)  36\u201347\n\ncontents  lists  available  at  sciencedirect\n\njournal\n\n \n\nof\n\n \n\nneuroscience\n\n \n\nmethods\n\nj o  u r  n a l  h o m e  p  a g e :  w w w . e l s e v i e r . c o m / l o c a t e / j n e u m e t h\n\ncomputational   neuroscience\na   bayesian   nonparametric   approach   for   uncovering   rat   hippocampal\npopulation   codes   during   spatial   navigation\nscott   w.   linderman a,   matthew   j.   johnson a,b,   matthew   a.   wilson c,   zhe   chen d,\u2217\n\na paulson  school  of  engineering  and  applied  sciences,  harvard  university,  cambridge,  ma  02138,  usa\nb department  of  neurobiology,  harvard  medical  school,  boston,  ma  02115,  usa\nc picower  institute  for  learning  and  memory,  massachusetts  institute  of  technology,  cambridge,  ma  02139,  usa\nd department  of  psychiatry,  department  of  neuroscience  and  physiology,  new  york  university  school  of  medicine,  new  york,  ny  10016,  usa\n\nh   i  g   h   l ", "vol 457 | 8 january 2009 | doi:10.1038/nature07467\n\nletters\n\nneural processing of auditory feedback during vocal\npractice in a songbird\ngeorg b. keller1 & richard h. r. hahnloser1\n\nsongbirds are capable of vocal learning and communication1,2 and\nare ideally suited to the study of neural mechanisms of complex\nsensory and motor processing. vocal communication in a noisy bird\ncolony and vocal learning of a specific song template both require\nthe ability to monitor auditory feedback3,4 to distinguish self-\ngenerated vocalizations from external sounds and to identify mis-\nmatches between the developing song and a memorized template\nacquired from a tutor5. however, neurons that respond to auditory\nfeedback from vocal output have not been found in song-control\nareas despite intensive searching6\u20138. here we investigate feedback\nprocessing outside the traditional song system, in single auditory\nforebrain neurons of juvenile zebra finches that were in a late develo-\npmental stage of song learning", "9\n1\n0\n2\n\n \n\nn\na\nj\n \n\n2\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n8\n4\n7\n3\n0\n\n.\n\n7\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nrepresentation learning with\ncontrastive predictive coding\n\naaron van den oord\n\ndeepmind\n\navdnoord@google.com\n\noriol vinyals\n\ndeepmind\n\nvinyals@google.com\n\nyazhe li\ndeepmind\n\nyazhe@google.com\n\nabstract\n\nwhile supervised learning has enabled great progress in many applications, unsu-\npervised learning has not seen such widespread adoption, and remains an important\nand challenging endeavor for arti\ufb01cial intelligence. in this work, we propose a\nuniversal unsupervised learning approach to extract useful representations from\nhigh-dimensional data, which we call contrastive predictive coding. the key in-\nsight of our model is to learn such representations by predicting the future in latent\nspace by using powerful autoregressive models. we use a probabilistic contrastive\nloss which induces the latent space to capture information that is maximally useful\nto predict future samples. it also makes the model t", "analyzing the weight dynamics of recurrent\n\nlearning algorithms\n\nulf d. schiller and jochen j. steil\n\nneuroinformatics group, faculty of technology, bielefeld university\n\nabstract\n\nwe provide insights into the organization and dynamics of recurrent online train-\ning algorithms by comparing real time recurrent learning (rtrl) with a new\ncontinuous-time online algorithm. the latter is derived in the spirit of a recent\napproach introduced by atiya and parlos [1], which leads to non-gradient search di-\nrections. we refer to this approach as atiya-parlos learning (aprl) and interpret\nit with respect to its strategy to minimize the standard quadratic error. simulations\nshow that the di(cid:11)erent approaches of rtrl and aprl lead to qualitatively di(cid:11)er-\nent weight dynamics. a formal analysis of the one-output behavior of aprl further\nreveals that the weight dynamics favor a functional partition of the network into\na fast output layer and a slower dynamical reservoir, whose rates of w", "y\nr\na\nt\nn\ne\nm\nm\no\nc\n\ne\ne\ns\n\ne\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\n\na triplet spike-timing\u2013dependent plasticity model\ngeneralizes the bienenstock\u2013cooper\u2013munro rule\nto higher-order spatiotemporal correlations\n\njulijana gjorgjievaa,1,2, claudia clopathb, juliette audetc, and jean-pascal p\ufb01sterd,e\n\nadepartment of applied mathematics and theoretical physics, university of cambridge, cambridge cb3 0wa, united kingdom; blaboratory of neurophysics\nand physiology, universit\u00e9 paris descartes, 75270 paris, france; claboratory of computational neuroscience, ecole polytechnique f\u00e9d\u00e9rale de lausanne,\nch-1015 lausanne, switzerland; dcomputational neuroscience laboratory, department of physiology, university of bern, ch-3012 bern, switzerland; and\nedepartment of engineering, university of cambridge, cambridge cb2 1pz, united kingdom\n\nedited* by leon n. cooper, brown university, providence, ri, and approved september 30, 2011 (received for review april 14, 2011)\n\nsynaptic strength depresses for low and potentiates f", "mobilenetv2: inverted residuals and linear bottlenecks\n\nmark sandler andrew howard menglong zhu andrey zhmoginov liang-chieh chen\n\n{sandler, howarda, menglong, azhmogin, lcchen}@google.com\n\ngoogle inc.\n\n9\n1\n0\n2\n\n \nr\na\n\n \n\nm\n1\n2\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n4\nv\n1\n8\n3\n4\n0\n\n.\n\n1\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\napplications.\n\nin this paper we describe a new mobile architecture,\nmobilenetv2, that improves the state of the art perfor-\nmance of mobile models on multiple tasks and bench-\nmarks as well as across a spectrum of different model\nsizes. we also describe ef\ufb01cient ways of applying these\nmobile models to object detection in a novel framework\nwe call ssdlite. additionally, we demonstrate how\nto build mobile semantic segmentation models through\na reduced form of deeplabv3 which we call mobile\ndeeplabv3.\n\nis based on an inverted residual structure where\nthe shortcut connections are between the thin bottle-\nneck layers. the intermediate expansion layer uses\nlightweight depthwise convolutions to \ufb01", "progressive neural networks\n\nandrei a. rusu*, neil c. rabinowitz*, guillaume desjardins*, hubert soyer,\n\njames kirkpatrick, koray kavukcuoglu, razvan pascanu, raia hadsell\n\n* these authors contributed equally to this work\n\n2\n2\n0\n2\n\n \nt\nc\no\n2\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n1\n7\n6\n4\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\n{andreirusu, ncr, gdesjardins, soyer, kirkpatrick, korayk, razp, raia}@google.com\n\ngoogle deepmind\n\nlondon, uk\n\nabstract\n\nlearning to solve complex sequences of tasks\u2014while both leveraging transfer and\navoiding catastrophic forgetting\u2014remains a key obstacle to achieving human-level\nintelligence. the progressive networks approach represents a step forward in this\ndirection: they are immune to forgetting and can leverage prior knowledge via\nlateral connections to previously learned features. we evaluate this architecture\nextensively on a wide variety of reinforcement learning tasks (atari and 3d maze\ngames), and show that it outperforms common baselines based on pretraining and\n\ufb01net", "corrected: publisher correction\n\na diverse range of factors affect the nature  \nof neural representations underlying  \nshort-term memory\n\na.\u00a0emin\u00a0orhan\u200a\n\n\u200a1* and wei\u00a0ji\u00a0ma2,3\n\nsequential and persistent activity models are two prominent models of short-term memory in neural circuits. in persistent \nactivity models, memories are represented in persistent or nearly persistent activity patterns across a population of neurons, \nwhereas in sequential models, memories are represented dynamically by a sequential activity pattern across the population. \nexperimental evidence for both models has been reported previously. however, it has been unclear under what conditions these \ntwo qualitatively different types of solutions emerge in neural circuits. here, we address this question by training recurrent \nneural networks on several short-term memory tasks under a wide range of circuit and task manipulations. we show that both \nsequential and nearly persistent solutions are part of a spectrum that ", "the journal of neuroscience, december 6, 2006 \u2022 26(49):12717\u201312726 \u2022 12717\n\ncellular/molecular\n\nplasticity compartments in basal dendrites of neocortical\npyramidal neurons\n\nurit gordon,* alon polsky,* and jackie schiller\ndepartment of physiology, technion medical school, haifa 31096, israel\n\nsynaptic plasticity rules widely determine how cortical networks develop and store information. using confocal imaging and dual site\nfocal synaptic stimulation, we show that basal dendrites, which receive the majority of synapses innervating neocortical pyramidal\nneurons, contain two compartments with respect to plasticity rules. synapses innervating the proximal basal tree are easily modified\nwhen paired with the global activity of the neuron. in contrast, synapses innervating the distal basal tree fail to change in response to\nglobal suprathreshold activity or local dendritic spikes. these synapses can undergo long-term potentiation under unusual conditions\nwhen local nmda spikes, which evoke lar", "neuron\n\narticle\n\nperceptual learning reduces interneuronal\ncorrelations in macaque visual cortex\n\nyong gu,1 sheng liu,1 christopher r. fetsch,1 yun yang,1 sam fok,1 adhira sunkara,1 gregory c. deangelis,2\nand dora e. angelaki1,*\n1department of anatomy and neurobiology, washington university school of medicine, st. louis, mo 63110, usa\n2department of brain and cognitive sciences, university of rochester, rochester, ny 14627, usa\n*correspondence: angelaki@pcg.wustl.edu\ndoi 10.1016/j.neuron.2011.06.015\n\nsummary\n\nresponses of neurons in early visual cortex change\nlittle with training and appear insuf\ufb01cient to account\nfor perceptual\nlearning. behavioral performance,\nhowever, relies on population activity, and the accu-\nracy of a population code is constrained by corre-\nlated noise among neurons. we tested whether\ntraining changes interneuronal correlations in the\ndorsal medial superior\ntemporal area, which is\ninvolved in multisensory heading perception. pairs\nof single units were recorded s", "accepted to the 1st ieee european symposium on security & privacy, ieee 2016. saarbrucken, germany.\n\nthe limitations of deep learning\n\nin adversarial settings\n\nnicolas papernot\u2217, patrick mcdaniel\u2217, somesh jha\u2020, matt fredrikson\u2021, z. berkay celik\u2217, ananthram swami\u00a7\n\n\u2217department of computer science and engineering, penn state university\n\n\u2020computer sciences department, university of wisconsin-madison\n\n\u2021school of computer science, carnegie mellon university\n\u00a7united states army research laboratory, adelphi, maryland\n\n{ngp5056,mcdaniel}@cse.psu.edu, {jha,mfredrik}@cs.wisc.edu, zbc102@cse.psu.edu, ananthram.swami.civ@mail.mil\n\n5\n1\n0\n2\n\n \n\nv\no\nn\n4\n2\n\n \n\n \n \n]\n\nr\nc\n.\ns\nc\n[\n \n \n\n1\nv\n8\n2\n5\n7\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014deep learning takes advantage of large datasets\nand computationally ef\ufb01cient training algorithms to outperform\nother approaches at various machine learning tasks. however,\nimperfections in the training phase of deep neural networks\nmake them vulnerable to adversarial samples", "neuron, vol. 27, 205\u2013218, august, 2000, copyright \u00aa 2000 by cell press\n\nfunctions of the primate temporal\nlobe cortical visual areas in invariant\nvisual object and face recognition\n\nreview\n\nedmund t. rolls*\nuniversity of oxford\ndepartment of experimental psychology\nsouth parks road\noxford, ox1 3ud\nunited kingdom\n\nthere is now good evidence that neural systems in tem-\nporal cortical visual areas process information about\nfaces. because a large number of neurons are devoted\nto this class of stimuli, these systems have proved ame-\nnable to experimental analysis. face recognition and\nthe identification of face expression are important in\nprimate social behavior, and analysis of the neural sys-\ntems involved is important for understanding the effects\nof damage to these systems in humans. damage to\nthese or related systems can lead to prosopagnosia, an\nimpairment in recognizing individuals from the sight of\ntheir faces, or to difficulty in identifying the expression\non a face. it turns out t", "research article\n\nmechanisms of distributed working \nmemory in a large- scale network of \nmacaque\u00a0neocortex\njorge f mej\u00edas1, xiao- jing wang2*\n\n1swammerdam institute for life sciences, university of amsterdam, amsterdam, \nnetherlands; 2center for neural science, new york university, new york, united \nstates\n\nabstract: neural activity underlying working memory is not a local phenomenon but distributed \nacross multiple brain regions. to elucidate the circuit mechanism of such distributed activity, we \ndeveloped an anatomically constrained computational model of large- scale macaque cortex. we \nfound that mnemonic internal states may emerge from inter- areal reverberation, even in a regime \nwhere none of the isolated areas is capable of generating self- sustained activity. the mnemonic \nactivity pattern along the cortical hierarchy indicates a transition in space, separating areas engaged \nin working memory and those which do not. a host of spatially distinct attractor states is found, \np", "2\n2\n0\n2\n\n \n\nv\no\nn\n9\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n7\n1\n5\n9\n0\n\n.\n\n3\n0\n2\n2\n:\nv\ni\nx\nr\na\n\ntensorf: tensorial radiance fields\n\nanpei chen1\u22c6 zexiang xu2\u22c6 andreas geiger3\n\njingyi yu1 hao su4\n\n1shanghaitech university\n\n2adobe research\n\n3university of t\u00a8ubingen and mpi-is, t\u00a8ubingen\n\n4uc san diego\n\nhttps://apchenstu.github.io/tensorf/\n\nabstract. we present tensorf, a novel approach to model and recon-\nstruct radiance fields. unlike nerf that purely uses mlps, we model the\nradiance field of a scene as a 4d tensor, which represents a 3d voxel grid\nwith per-voxel multi-channel features. our central idea is to factorize the\n4d scene tensor into multiple compact low-rank tensor components. we\ndemonstrate that applying traditional candecomp/parafac (cp)\ndecomposition \u2013 that factorizes tensors into rank-one components with\ncompact vectors \u2013 in our framework leads to improvements over vanilla\nnerf. to further boost performance, we introduce a novel vector-matrix\n(vm) decomposition that relaxes the l", "siam review\nvol. 60, no. 2, pp. 223\u2013311\n\nc(cid:2) 2018 society for industrial and applied mathematics\n\noptimization methods for\n\u2217\nlarge-scale machine learning\n\n\u2020\nl\u00b4eon bottou\n\u2021\nfrank e. curtis\n\u00a7\njorge nocedal\n\nabstract. this paper provides a review and commentary on the past, present, and future of numeri-\ncal optimization algorithms in the context of machine learning applications. through case\nstudies on text classi\ufb01cation and the training of deep neural networks, we discuss how op-\ntimization problems arise in machine learning and what makes them challenging. a major\ntheme of our study is that large-scale machine learning represents a distinctive setting in\nwhich the stochastic gradient (sg) method has traditionally played a central role while\nconventional gradient-based nonlinear optimization techniques typically falter. based on\nthis viewpoint, we present a comprehensive theory of a straightforward, yet versatile sg\nalgorithm, discuss its practical behavior, and highlight opportuni", "data-dependence of plateau phenomenon in\nlearning with neural network \u2014 statistical\n\nmechanical analysis\n\ndepartment of complexity science and engineering, graduate school of frontier sciences,\n\nyuki yoshida\n\nmasato okada\n\nthe university of tokyo\n\n5-1-5 kashiwanoha, kashiwa, chiba 277-8561, japan\n{yoshida@mns, okada@edu}.k.u-tokyo.ac.jp\n\nabstract\n\nthe plateau phenomenon, wherein the loss value stops decreasing during the\nprocess of learning, has been reported by various researchers. the phenomenon is\nactively inspected in the 1990s and found to be due to the fundamental hierarchical\nstructure of neural network models. then the phenomenon has been thought as\ninevitable. however, the phenomenon seldom occurs in the context of recent\ndeep learning. there is a gap between theory and reality. in this paper, using\nstatistical mechanical formulation, we clari\ufb01ed the relationship between the plateau\nphenomenon and the statistical property of the data learned. it is shown that the\ndata whose co", "large scale distributed deep networks\n\njeffrey dean, greg s. corrado, rajat monga, kai chen,\n\nmatthieu devin, quoc v. le, mark z. mao, marc\u2019aurelio ranzato,\n\nandrew senior, paul tucker, ke yang, andrew y. ng\n\n{jeff, gcorrado}@google.com\n\ngoogle inc., mountain view, ca\n\nabstract\n\nrecent work in unsupervised feature learning and deep learning has shown that be-\ning able to train large models can dramatically improve performance. in this paper,\nwe consider the problem of training a deep network with billions of parameters\nusing tens of thousands of cpu cores. we have developed a software framework\ncalled distbelief that can utilize computing clusters with thousands of machines to\ntrain large models. within this framework, we have developed two algorithms for\nlarge-scale distributed training: (i) downpour sgd, an asynchronous stochastic\ngradient descent procedure supporting a large number of model replicas, and (ii)\nsandblaster, a framework that supports a variety of distributed batch opti", "presented at the iclr ai for social good workshop 2019\n\nreducing leakage in distributed deep learn-\ning for sensitive health data\n\npraneeth vepakomma, otkrist gupta, abhimanyu dubey, ramesh raskar\nmassachusetts institute of technology\ncambridge, ma 02139, usa\n{vepakom,otkrist,dubeya,raskar}@mit.edu\n\nabstract\n\nfor distributed machine learning with health data we demonstrate how minimizing\ndistance correlation between raw data and intermediary representations (smashed\ndata) reduces leakage of sensitive raw data patterns during client communications\nwhile maintaining model accuracy. leakage (measured using kl divergence\nbetween input and intermediate representation) is the risk associated with the in-\nvertibility from intermediary representations, can prevent resource poor health or-\nganizations from using distributed deep learning services. we demonstrate that\nour method reduces leakage in terms of distance correlation between raw data\nand communication payloads from an order of 0.95 to ", "review\npublished: 22 august 2017\ndoi: 10.3389/fnana.2017.00071\n\na laminar organization for selective\ncortico-cortical communication\n\nrinaldo d. d\u2019souza * and andreas burkhalter\n\ndepartment of neuroscience, washington university school of medicine, st. louis, mo, united states\n\nthe neocortex is central to mammalian cognitive ability, playing critical roles in sensory\nperception, motor skills and executive function. this thin, layered structure comprises\ndistinct, functionally specialized areas that communicate with each other through the\naxons of pyramidal neurons. for the hundreds of such cortico-cortical pathways to\nunderlie diverse functions, their cellular and synaptic architectures must differ so that they\nresult in distinct computations at the target projection neurons. in what ways do these\npathways differ? by originating and terminating in different laminae, and by selectively\ntargeting speci\ufb01c populations of excitatory and inhibitory neurons, these \u201cinterareal\u201d\npathways can dif", "ieee transactions on neural networks and learning systems, vol. 29, no. 11, november 2018\n\n5475\n\ndeep cascade learning\n\nenrique s. marquez , jonathon s. hare, and mahesan niranjan\n\nabstract\u2014 in this paper, we propose a novel approach for\nef\ufb01cient training of deep neural networks in a bottom-up fashion\nusing a layered structure. our algorithm, which we refer to as\ndeep cascade learning, is motivated by the cascade correlation\napproach of fahlman and lebiere, who introduced it in the con-\ntext of perceptrons. we demonstrate our algorithm on networks\nof convolutional layers, though its applicability is more general.\nsuch training of deep networks in a cascade directly circum-\nvents the well-known vanishing gradient problem by ensuring\nthat the output is always adjacent to the layer being trained.\nwe present empirical evaluations comparing our deep cascade\ntraining with standard end\u2013end training using back propagation\nof two convolutional neural network architectures on benchmark\nimage cla", "research | reports\n\nnew zealand; inra and agence nationale de la recherche project\nsheepsnpqtl, france; european union through the seventh\nframework programme quantomics (kbbe222664) and 3sr\n(kbbe245140) projects; the ole r\u00f8mer grant from danish natural\nscience research council, bgi-shenzhen, china; the earmarked\nfund for modern china wool & cashmere technology research\nsystem (no.nycytx-40-3); and the australian department of\nagriculture food and fisheries, filling the research gap project,\n\u201chost control of methane emissions.\u201d we thank b. freking\n(usda-ars-u.s. meat animal research center) for provision of\ntexel ram tissue samples for dna extraction and sequencing.\nwe thank the sequencing teams and other contributors; full\ndetails are in the acknowledgements section of the supplementary\nmaterials. we thank sheepgenomics and utah state university\nfor access to the genotyping data for the sheepgenomics and\nlouisiana state university flocks, respectively. we thank\nl. goodman for help wit", "article\n\nreceived 7 oct 2014 | accepted 5 nov 2014 | published 18 dec 2014\n\ndoi: 10.1038/ncomms6768\n\nopen\n\ncompetition between items in working\nmemory leads to forgetting\njarrod a. lewis-peacock1 & kenneth a. norman2\n\nswitching attention from one thought to the next propels our mental lives forward. however,\nit is unclear how this thought-juggling affects our ability to remember these thoughts.\nhere we show that competition between the neural representations of pictures in working\nmemory can impair subsequent recognition of those pictures. we use pattern classi\ufb01ers to\ndecode functional magnetic resonance imaging (fmri) data from a retro-cueing task where\nparticipants juggle two pictures in working memory. trial-by-trial \ufb02uctuations in neural\ndynamics are predictive of performance on a surprise recognition memory test: trials that\nelicit similar levels of classi\ufb01er evidence for both pictures (indicating close competition) are\nassociated with worse memory performance than trials where pa", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/224929937\n\ndissociable roles of ventral and dorsal striatum in instrumental conditioning\n\narticle\u00a0\u00a0in\u00a0\u00a0science \u00b7 may 2004\n\ndoi: 10.1126/science.1094285\n\ncitations\n1,930\n\n6 authors, including:\n\njohn p o'doherty\ncalifornia institute of technology\n\n234 publications\u00a0\u00a0\u00a049,176 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nkarl j friston\nuniversity college london\n\n1,228 publications\u00a0\u00a0\u00a0263,738 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n1,597\n\njohannes schultz\nuniversity of bonn - medical center\n\n83 publications\u00a0\u00a0\u00a03,454 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nraymond j dolan\nuniversity college london\n\n1,184 publications\u00a0\u00a0\u00a0150,105 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nall content following this page was uploaded by karl j friston on 11 march 2014.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "6\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n2\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n4\nv\n3\n8\n9\n1\n0\n\n.\n\n8\n0\n5\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2016\n\ndigging deep into the layers of cnns:\nin\nsearch of how cnns achieve view invariance\n\namrbakry@cs.rutgers.edu\n\namr bakry\u2217\ntarek el-gaaly\u2217\n\ntgaaly@cs.rutgers.edu\n\nmohamed elhoseiny\u2217\n\nm.elhoseiny@cs.rutgers.edu\n\nahmed elgammal\n\nelgammal@cs.rutgers.edu\n\n* indicates co-\ufb01rst authors\n\ncomputer science department, rutgers university\n\nabstract\n\nthis paper is focused on studying the view-manifold structure in the feature spaces\nimplied by the different layers of convolutional neural networks (cnn). there\nare several questions that this paper aims to answer: does the learned cnn rep-\nresentation achieve viewpoint invariance? how does it achieve viewpoint invari-\nance? is it achieved by collapsing the view manifolds, or separating them while\npreserving them? at which layer is view invariance achieved? how can the struc-\nture of the view manifold at each layer of a ", "convergent block coordinate descent for training\n\ntikhonov regularized deep neural networks\n\nziming zhang and matthew brand\n\nmitsubishi electric research laboratories (merl)\n\ncambridge, ma 02139-1955\n\n{zzhang, brand}@merl.com\n\nabstract\n\nby lifting the relu function into a higher dimensional space, we develop a smooth\nmulti-convex formulation for training feed-forward deep neural networks (dnns).\nthis allows us to develop a block coordinate descent (bcd) training algorithm\nconsisting of a sequence of numerically well-behaved convex optimizations. using\nideas from proximal point methods in convex analysis, we prove that this bcd\nalgorithm will converge globally to a stationary point with r-linear convergence\nrate of order one. in experiments with the mnist database, dnns trained with\nthis bcd algorithm consistently yielded better test-set error rates than identical\ndnn architectures trained via all the stochastic gradient descent (sgd) variants in\nthe caffe toolbox.\n\n1\n\nintroduction\n\nfee", "image-to-image translation with conditional adversarial networks\n\nphillip isola\n\njun-yan zhu\n\ntinghui zhou\n\nalexei a. efros\n\nberkeley ai research (bair) laboratory, uc berkeley\n{isola,junyanz,tinghuiz,efros}@eecs.berkeley.edu\n\n8\n1\n0\n2\n\n \n\nv\no\nn\n6\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n3\nv\n4\n0\n0\n7\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nfigure 1: many problems in image processing, graphics, and vision involve translating an input image into a corresponding output image.\nthese problems are often treated with application-speci\ufb01c algorithms, even though the setting is always the same: map pixels to pixels.\nconditional adversarial nets are a general-purpose solution that appears to work well on a wide variety of these problems. here we show\nresults of the method on several. in each case we use the same architecture and objective, and simply train on different data.\n\nabstract\n\n1. introduction\n\nwe investigate conditional adversarial networks as a\ngeneral-purpose solution to image-to-image translation\nproblems. the", "an overview of gradient descent optimization\n\nalgorithms\u2217\n\nsebastian ruder\n\ninsight centre for data analytics, nui galway\n\naylien ltd., dublin\n\nruder.sebastian@gmail.com\n\nabstract\n\ngradient descent optimization algorithms, while increasingly popular, are often\nused as black-box optimizers, as practical explanations of their strengths and\nweaknesses are hard to come by. this article aims to provide the reader with\nintuitions with regard to the behaviour of different algorithms that will allow her\nto put them to use. in the course of this overview, we look at different variants of\ngradient descent, summarize challenges, introduce the most common optimization\nalgorithms, review architectures in a parallel and distributed setting, and investigate\nadditional strategies for optimizing gradient descent.\n\n1\n\nintroduction\n\ngradient descent is one of the most popular algorithms to perform optimization and by far the\nmost common way to optimize neural networks. at the same time, every state-of-th", "metastable attractors explain the variable timing of\nstable behavioral action sequences\n\narticle\n\nhighlights\nd behavioral sequences in freely moving rats revealed large\n\nvariability in action timing\n\nd actions were preceded by onset of speci\ufb01c neural patterns in\n\nsecondary motor cortex\n\nd metastable attractors in a network model can explain the\n\norigin of timing variability\n\nd transitions between attractors are driven by low-dimensional\n\ncorrelated variability\n\nauthors\n\nstefano recanatesi,\nulises pereira-obilinovic,\nmasayoshi murakami, zachary mainen,\nluca mazzucato\n\ncorrespondence\nzmainen@neuro.fchampalimaud.org\n(z.m.),\nlmazzuca@uoregon.edu (l.m.)\n\nin brief\nself-initiated actions in freely moving rats\ncan be predicted by speci\ufb01c ensemble\nactivity patterns in the secondary motor\ncortex (m2). variability in action timing\ncan be explained by metastable\nattractors in a network model of m2.\ntransitions between attractors are\ngenerated by low-dimensional correlated\nvariability, empirically ", "1\n2\n0\n2\n\n \n\nn\na\nj\n \n\n4\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n6\n3\n5\n5\n0\n\n.\n\n1\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nscaling equilibrium propagation to deep convnets\nby drastically reducing its gradient estimator bias\n\naxel laborieux 1,\u2217, maxence ernoult1,2,\u2217, benjamin scellier3,\u2020,\n\nyoshua bengio3,4, julie grollier2, damien querlioz1\n\n1universit\u00e9 paris-saclay, cnrs, c2n, 91120, palaiseau, france\n2unit\u00e9 mixte de physique, cnrs, thales, universit\u00e9 paris-saclay\n\n3mila, universit\u00e9 de montr\u00e9al\n\n4canadian institute for advanced research\n\n\u2020 currently at google\n\n* corresponding authors: {axel.laborieux, maxence.ernoult}@c2n.upsaclay.fr\n\nabstract\n\nequilibrium propagation (ep) is a biologically-inspired counterpart of backprop-\nagation through time (bptt) which, owing to its strong theoretical guarantees\nand the locality in space of its learning rule, fosters the design of energy-ef\ufb01cient\nhardware dedicated to learning. in practice, however, ep does not scale to visual\ntasks harder than mnist. in this work, we show that a ", "m\no\nc\n\n.\n\ne\nr\nu\nt\na\nn\n\n.\ni\n\nc\ns\no\nr\nu\ne\nn\n\n/\n/\n:\np\nt\nt\nh\n \n\u2022\n \n.\n\nc\nn\n\ni\n \n\n \n\na\nc\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n0\n0\n0\n2\n\u00a9\n\n \n\n \n\n\u00a9 2000 nature america inc. \u2022 http://neurosci.nature.com\n\narticles\n\ndirect cortical control of muscle\nactivation in voluntary arm\nmovements: a model\n\nemanuel todorov\n\ngatsby computational neuroscience unit, university college london, 17 queen square london wc1n 3 ar, uk\n\ncorrespondence should be directed to e.t. (emo@gatsby.ucl.ac.uk)\n\nwhat neural activity in motor cortex represents and how it controls ongoing movement remain\nunclear. suggestions that cortex generates low-level muscle control are discredited by correlations\nwith higher-level parameters of hand movement, but no coherent alternative exists. i argue that the\nview of low-level control is in principle correct, and that seeming contradictions result from\noverlooking known properties of the motor periphery. assuming direct motor cortical activation of\nmuscle groups and taking into account the state dependen", "7546 \u2022 the journal of neuroscience, july 13, 2016 \u2022 36(28):7546 \u20137556\n\nsystems/circuits\n\nstimulus dependence of correlated variability across\ncortical areas\n\nx douglas a. ruff and x marlene r. cohen\ndepartment of neuroscience and center for the neural basis of cognition, university of pittsburgh, pittsburgh, pennsylvania 15213\n\nthe way that correlated trial-to-trial variability between pairs of neurons in the same brain area (termed spike count or noise correlation,\nrsc) depends on stimulus or task conditions can constrain models of cortical circuits and of the computations performed by networks of\nneurons (cohen and kohn, 2011). in visual cortex, rsc tends not to depend on stimulus properties (kohn and smith, 2005; huang and\nlisberger, 2009) but does depend on cognitive factors like visual attention (cohen and maunsell, 2009; mitchell et al., 2009). however,\nneurons across visual areas respond to any visual stimulus or contribute to any perceptual decision, and the way that informatio", "ne39ch07-freedman\n\nari\n\n28 may 2016\n\n9:18\n\nneuronal mechanisms of visual\ncategorization: an abstract\nview on decision making\n\ndavid j. freedman1,2 and john a. assad3,4\n1department of neurobiology, university of chicago, chicago, illinois 60637;\nemail: dfreedman@uchicago.edu\n2the grossman institute for neuroscience, quantitative biology, and human behavior,\nuniversity of chicago, chicago, illinois 60637\n3department of neurobiology, harvard medical school, boston, massachusetts 02115;\nemail: jassad@hms.harvard.edu\n4istituto italiano di tecnologia, 16163 genova, italy\n\nkeywords\ncategorization, decision making, vision, parietal cortex, prefrontal cortex,\nlearning and memory, recognition, perception\n\nabstract\ncategorization is our ability to \ufb02exibly assign sensory stimuli into discrete,\nbehaviorally relevant groupings. categorical decisions can be used to study\ndecision making more generally by dissociating category identity of stim-\nuli from the actions subjects use to signal their decisio", "0\n2\n0\n2\n\n \n\nn\na\nj\n \n7\n\n \n \n]\n\n.\n\nc\no\nh\nt\na\nm\n\n[\n \n \n\n5\nv\n6\n5\n9\n7\n0\n\n.\n\n2\n1\n8\n1\n:\nv\ni\nx\nr\na\n\non lazy training in differentiable programming\n\nl\u00e9na\u00efc chizat\n\ncnrs, universit\u00e9 paris-sud\n\norsay, france\n\nlenaic.chizat@u-psud.fr\n\nedouard oyallon\n\ncentralesupelec, inria\ngif-sur-yvette, france\n\nedouard.oyallon@centralesupelec.fr\n\nfrancis bach\n\ninria, ens, psl research university\n\nparis, france\n\nfrancis.bach@inria.fr\n\nabstract\n\ntheoretical works,\n\nit was shown that strongly over-\nin a series of recent\nparameterized neural networks trained with gradient-based methods could converge\nexponentially fast to zero training loss, with their parameters hardly varying. in\nthis work, we show that this \u201clazy training\u201d phenomenon is not speci\ufb01c to over-\nparameterized neural networks, and is due to a choice of scaling, often implicit,\nthat makes the model behave as its linearization around the initialization, thus\nyielding a model equivalent to learning with positive-de\ufb01nite kernels. through a\ntheoretical ana", "https://doi.org/10.1038/s41593-019-0480-6\n\nthe next generation of approaches to investigate \nthe link between synaptic plasticity and learning\n\nyann humeau\u200a\n\n\u200a1,2 and daniel choquet\u200a\n\n\u200a1,2,3\n\nactivity-dependent synaptic plasticity has since long been proposed to represent the subcellular substrate of learning and \nmemory, one of the most important behavioral processes through which we adapt to our environment. despite the undisputed \nimportance of synaptic plasticity for brain function, its exact contribution to learning processes in the context of cellular and \nconnectivity modifications remains obscure. causally bridging synaptic and behavioral modifications indeed remains limited \nby the available tools to measure and control synaptic strength and plasticity in\u00a0vivo under behaviorally relevant conditions. \nafter a brief summary of the current state of knowledge of the links between synaptic plasticity and learning, we will review and \ndiscuss the available and desired tools to progr", "beyond plasticity: the dynamic impact \nof electrical synapses on neural circuits\n\npepe\u00a0alcam\u00ed1,2,3 and alberto\u00a0e.\u00a0pereda3,4*\nabstract | electrical synapses are found in vertebrate and invertebrate nervous systems. \nthe\u00a0cellular basis of these synapses is the gap junction, a group of intercellular channels that \nmediate direct communication between adjacent neurons. similar to chemical synapses, \nelectrical connections are modifiable and their variations in strength provide a mechanism for \nreconfiguring neural circuits. in addition, electrical synapses dynamically regulate neural circuits \nthrough properties without equivalence in chemical transmission. because of their continuous \nnature and bidirectionality , electrical synapses allow electrical currents underlying changes \nin\u00a0membrane potential to leak to \u2018coupled\u2019 partners, dampening neuronal excitability and \naltering their integrative properties. remarkably , this effect can be transiently alleviated when \ncomparable changes in m", "a simple normative network approximates\nlocal non-hebbian learning in the cortex\n\nsiavash golkar 1\n\ndavid lipshutz 1\n\nyanis bahroun 1\n\nanirvan m. sengupta 1,2\n\ndmitri b. chklovskii 1,3\n\n1 center for computational neuroscience, flatiron institute\n2 department of physics and astronomy, rutgers university\n\n3 neuroscience institute, nyu medical center\n\n{sgolkar,dlipshutz,ybahroun,mitya}@flatironinstitute.org\n\nanirvans.physics@gmail.com\n\nabstract\n\nto guide behavior, the brain extracts relevant features from high-dimensional data\nstreamed by sensory organs. neuroscience experiments demonstrate that the pro-\ncessing of sensory inputs by cortical neurons is modulated by instructive signals\nwhich provide context and task-relevant information. here, adopting a norma-\ntive approach, we model these instructive signals as supervisory inputs guiding\nthe projection of the feedforward data. mathematically, we start with a family\nof reduced-rank regression (rrr) objective functions which include reduce", "review\n\ncommunicated by steven nowlan\n\na unifying review of linear gaussian models\n\n\u2217\nsam roweis\ncomputation and neural systems, california institute of technology, pasadena, ca\n91125, u.s.a.\n\n\u2217\nzoubin ghahramani\ndepartment of computer science, university of toronto, toronto, canada\n\nfactor analysis, principal component analysis, mixtures of gaussian clus-\nters, vector quantization, kalman \ufb01lter models, and hidden markov mod-\nels can all be uni\ufb01ed as variations of unsupervised learning under a single\nbasic generative model. this is achieved by collecting together disparate\nobservations and derivations made by many previous authors and intro-\nducing a new way of linking discrete and continuous state models using\na simple nonlinearity. through the use of other nonlinearities, we show\nhow independent component analysis is also a variation of the same basic\ngenerative model. we show that factor analysis and mixtures of gaussians\ncan be implemented in autoencoder neural networks and learned", "m\na\nc\nh\ni\nn\ne\n \nl\ne\na\nr\nn\ni\nn\ng\n,\n \n8\n,\n \n2\n2\n9\n-\n2\n5\n6\n \n(\n1\n9\n9\n2\n)\n \n\u00a9\n \n1\n9\n9\n2\n \nk\nl\nu\nw\ne\nr\n \na\nc\na\nd\ne\nm\ni\nc\n \np\nu\nb\nl\ni\ns\nh\ne\nr\ns\n,\n \nb\no\ns\nt\no\nn\n.\n \nm\na\nn\nu\nf\na\nc\nt\nu\nr\ne\nd\n \ni\nn\n \nt\nh\ne\n \nn\ne\nt\nh\ne\nr\nl\na\nn\nd\ns\n.\n \ns\ni\nm\np\nl\ne\n \ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n \ng\nr\na\nd\ni\ne\nn\nt\n-\nf\no\nl\nl\no\nw\ni\nn\ng\n \na\nl\ng\no\nr\ni\nt\nh\nm\ns\n \nf\no\nr\n \nc\no\nn\nn\ne\nc\nt\ni\no\nn\ni\ns\nt\n \nr\ne\ni\nn\nf\no\nr\nc\ne\nm\ne\nn\nt\n \nl\ne\na\nr\nn\ni\nn\ng\n \nr\no\nn\na\nl\nd\n \nj\n.\n \nw\ni\nl\nl\ni\na\nm\ns\n \nr\nj\nw\n@\nc\no\nr\nw\ni\nn\n.\nc\nc\ns\n.\nn\no\nr\nt\nh\ne\na\ns\nt\ne\nr\nn\n.\ne\nd\nu\n \nc\no\nl\nl\ne\ng\ne\n \no\nf\n \nc\no\nm\np\nu\nt\ne\nr\n \ns\nc\ni\ne\nn\nc\ne\n,\n \n1\n6\n1\n \nc\nn\n,\n \nn\no\nr\nt\nh\ne\na\ns\nt\ne\nr\nn\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n,\n \n3\n6\n0\n \nh\nu\nn\nt\ni\nn\ng\nt\no\nn\n \na\nv\ne\n.\n,\n \nb\no\ns\nt\no\nn\n,\n \nm\na\n \n0\n2\n1\n1\n5\n \na\nb\ns\nt\nr\na\nc\nt\n.\n \nt\nh\ni\ns\n \na\nr\nt\ni\nc\nl\ne\n \np\nr\ne\ns\ne\nn\nt\ns\n \na\n \ng\ne\nn\ne\nr\na\nl\n \nc\nl\na\ns\ns\n \no\nf\n \na\ns\ns\no\nc\ni\na\nt\ni\nv\ne\n \nr\ne\ni\nn\nf\no\nr\nc\ne\nm\ne\nn\nt\n \nl\ne\na\nr\nn\ni\nn\ng\n \na\nl\ng\no\nr\ni\nt\nh\nm\ns\n \nf\no\nr\n \nc\no\nn\nn\ne\nc\nt\ni\no\nn\ni\ns\nt\n \nn\ne\nt\nw\no\nr\nk\ns\n \nc\no\nn\nt\na\ni\nn\ni\nn\ng\n \ns\nt\no\nc\nh\na\ns\nt\ni\nc\n \nu\n", "report\n\na role for the locus coeruleus in hippocampal ca1\nplace cell reorganization during spatial reward\nlearning\n\nhighlights\nd lc-ca1 projections exhibit increased activity near a new\n\nreward location\n\nauthors\n\nalexandra mansell kaufman,\ntristan geiller, attila losonczy\n\nd activation of lc-ca1 near the reward induces place cell\n\noverrepresentation\n\ncorrespondence\nal2856@columbia.edu\n\nd inhibition of lc-ca1 axons suppresses place cell\n\noverrepresentation\n\nin brief\nkaufman et al. imaged and\noptogenetically manipulated the activity\nof locus coeruleus axons to the mouse\nhippocampal area ca1 in vivo. they show\nthat this projection is a key player in\ninducing hippocampal place cell\nreorganization during goal-directed\nspatial learning.\n\nkaufman et al., 2020, neuron 105, 1018\u20131026\nmarch 18, 2020 \u00aa 2020 elsevier inc.\nhttps://doi.org/10.1016/j.neuron.2019.12.029\n\n\f", "adaptive optimal training of animal behavior\n\nji hyun bak1,4 jung yoon choi2,3 athena akrami3,5 ilana witten2,3 jonathan w. pillow2,3\n\n1department of physics, 2department of psychology, princeton university\n\n3princeton neuroscience institute, princeton university\n\n4school of computational sciences, korea institute for advanced study\n\n5howard hughes medical institute\n\njhbak@kias.re.kr, {jungchoi,aakrami,iwitten,pillow}@princeton.edu\n\nabstract\n\nneuroscience experiments often require training animals to perform tasks designed\nto elicit various sensory, cognitive, and motor behaviors. training typically involves\na series of gradual adjustments of stimulus conditions and rewards in order to bring\nabout learning. however, training protocols are usually hand-designed, relying\non a combination of intuition, guesswork, and trial-and-error, and often require\nweeks or months to achieve a desired level of task performance. here we combine\nideas from reinforcement learning and adaptive optimal expe", "towards evaluating the robustness\n\nof neural networks\n\nnicholas carlini\n\ndavid wagner\nuniversity of california, berkeley\n\nabstract\n\noriginal adversarial\n\noriginal adversarial\n\n7\n1\n0\n2\n\n \nr\na\n\n \n\nm\n2\n2\n\n \n \n]\n\nr\nc\n.\ns\nc\n[\n \n \n\n2\nv\n4\n4\n6\n4\n0\n\n.\n\n8\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nneural networks provide state-of-the-art results for most\nmachine learning tasks. unfortunately, neural networks are\nvulnerable to adversarial examples: given an input x and any\ntarget classi\ufb01cation t, it is possible to \ufb01nd a new input x(cid:48)\nthat is similar to x but classi\ufb01ed as t. this makes it dif\ufb01cult\nto apply neural networks in security-critical areas. defensive\ndistillation is a recently proposed approach that can take an\narbitrary neural network, and increase its robustness, reducing\nthe success rate of current attacks\u2019 ability to \ufb01nd adversarial\nexamples from 95% to 0.5%.\n\nin this paper, we demonstrate that defensive distillation does\nnot signi\ufb01cantly increase the robustness of neural networks\nby introducing three ", "t\nr\na\nn\ns\nf\no\nr\nm\ne\nr\n \nc\ni\nr\nc\nu\ni\nt\ns\n \nt\nh\nr\ne\na\nd\na\nu\nt\nh\no\nr\ns\nc\na\nt\nh\ne\nr\ni\nn\ne\n \no\nl\ns\ns\no\nn\n,\nn\ne\nl\ns\no\nn\n \ne\nl\nh\na\ng\ne\n,\nn\ne\ne\nl\n \nn\na\nn\nd\na\n,\nn\ni\nc\nh\no\nl\na\ns\n \nj\no\ns\ne\np\nh\n,\nn\no\nv\na\n \nd\na\ns\ns\na\nr\nm\na\n,\nt\no\nm\n \nh\ne\nn\ni\ng\nh\na\nn\n,\nb\ne\nn\n \nm\na\nn\nn\n,\na\nm\na\nn\nd\na\n \na\ns\nk\ne\nl\nl\n,\ny\nu\nn\nt\na\no\n \nb\na\ni\n,\na\nn\nn\na\n \nc\nh\ne\nn\n,\nt\no\nm\n \nc\no\nn\ne\nr\nl\ny\n,\nd\na\nw\nn\n \nd\nr\na\ni\nn\n,\nd\ne\ne\np\n \ng\na\nn\ng\nu\nl\ni\n,\nz\na\nc\n \nh\na\nt\nf\ni\ne\nl\nd\n-\nd\no\nd\nd\ns\n,\nd\na\nn\nn\ny\n \nh\ne\nr\nn\na\nn\nd\ne\nz\n,\ns\nc\no\nt\nt\n \nj\no\nh\nn\ns\nt\no\nn\n,\na\nn\nd\ny\n \nj\no\nn\ne\ns\n,\nj\na\nc\nk\ns\no\nn\n \nk\ne\nr\nn\ni\no\nn\n,\nl\ni\na\nn\ne\n \nl\no\nv\ni\nt\nt\n,\nk\na\nm\na\nl\n \nn\nd\no\nu\ns\ns\ne\n,\nd\na\nr\ni\no\n \na\nm\no\nd\ne\ni\n,\nt\no\nm\n \nb\nr\no\nw\nn\n,\nj\na\nc\nk\n \nc\nl\na\nr\nk\n,\nj\na\nr\ne\nd\n \nk\na\np\nl\na\nn\n,\ns\na\nm\n \nm\nc\nc\na\nn\nd\nl\ni\ns\nh\n,\nc\nh\nr\ni\ns\n \no\nl\na\nh\na\nf\nf\ni\nl\ni\na\nt\ni\no\nn\na\nn\nt\nh\nr\no\np\ni\nc\np\nu\nb\nl\ni\ns\nh\ne\nd\nm\na\nr\n \n8\n,\n \n2\n0\n2\n2\n*\n \nc\no\nr\ne\n \nr\ne\ns\ne\na\nr\nc\nh\n \nc\no\nn\nt\nr\ni\nb\nu\nt\no\nr\n;\n\u2020\n \nc\no\nr\ne\n \ni\nn\nf\nr\na\ns\nt\nr\nu\nc\nt\nu\nr\ne\n \nc\no\nn\nt\nr\ni\nb\nu\nt\no\nr\n;\n\u2021\n \nc\no\nr\nr\ne\ns\np\no\nn\nd\ne\nn\nc\ne\n \nt\no\n \nc\no\nl\n", "3\n2\n0\n2\n\n \n\ny\na\nm\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n1\n6\n6\n5\n1\n\n.\n\n1\n1\n2\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2023\n\nwhat learning algorithm is in-context learn-\ning? investigations with linear models\n\nekin aky\u00a8urek1,2,a. dale schuurmans1\n\njacob andreas\u22172 tengyu ma\u22171,3,b denny zhou\u22171\n\n1google research\n\n2mit csail\n\n3 stanford university\n\n\u2217collaborative advising\n\nabstract\n\nneural sequence models, especially transformers, exhibit a remarkable capacity\nfor in-context learning. they can construct new predictors from sequences of\nlabeled examples (x, f (x)) presented in the input without further parameter up-\ndates. we investigate the hypothesis that transformer-based in-context learners\nimplement standard learning algorithms implicitly, by encoding smaller models\nin their activations, and updating these implicit models as new examples appear\nin the context. using linear regression as a prototypical problem, we offer three\nsources of evidence for this hypothesis. first, we ", "research article\nhuman inferences about sequences: a\nminimal transition probability model\n\nflorent meyniel1*, maxime maheu1,2, stanislas dehaene1,3\n\n1 cognitive neuroimaging unit, cea drf/i2bm, inserm, universite\u00b4 paris-sud, universite\u00b4 paris-saclay,\nneurospin center, gif-sur-yvette, france, 2 universite\u00b4 paris descartes, sorbonne paris cite\u00b4, paris, france,\n3 coll\u00e8ge de france, paris, france\n\na11111\n\n* florent.meyniel@cea.fr\n\nabstract\n\nthe brain constantly infers the causes of the inputs it receives and uses these inferences to\ngenerate statistical expectations about future observations. experimental evidence for\nthese expectations and their violations include explicit reports, sequential effects on reaction\ntimes, and mismatch or surprise signals recorded in electrophysiology and functional mri.\nhere, we explore the hypothesis that the brain acts as a near-optimal inference device that\nconstantly attempts to infer the time-varying matrix of transition probabilities between the sti-\nm", "exploiting compositionality to explore a large space of model structures\n\nroger b. grosse\n\nruslan salakhutdinov\n\ncomp. sci. & ai lab\n\nmit\n\ncambridge, ma 02139\n\ndept. of statistics\n\nuniversity of toronto\n\ntoronto, ontario, canada\n\nwilliam t. freeman\ncomp. sci. & ai lab\n\njoshua b. tenenbaum\n\nbrain and cognitive sciences\n\nmit\n\nmit\n\ncambridge, ma 02139\n\ncambridge, ma 02193\n\nabstract\n\nthe recent proliferation of richly structured prob-\nabilistic models raises the question of how to au-\ntomatically determine an appropriate model for a\ndataset. we investigate this question for a space\nof matrix decomposition models which can ex-\npress a variety of widely used models from unsu-\npervised learning. to enable model selection, we\norganize these models into a context-free gram-\nmar which generates a wide variety of structures\nthrough the compositional application of a few\nsimple rules. we use our grammar to generically\nand ef\ufb01ciently infer latent components and esti-\nmate predictive likelihood for ", "neuron\n\nperspective\n\ndirect fit to nature: an evolutionary\nperspective on biological and\narti\ufb01cial neural networks\n\nuri hasson,1,2,* samuel a. nastase,1 and ariel goldstein1\n1princeton neuroscience institute, princeton university, princeton, nj, usa\n2department of psychology, princeton university, princeton, nj, usa\n*correspondence: hasson@princeton.edu\nhttps://doi.org/10.1016/j.neuron.2019.12.002\n\nevolution is a blind \ufb01tting process by which organisms become adapted to their environment. does the brain\nuse similar brute-force \ufb01tting processes to learn how to perceive and act upon the world? recent advances in\narti\ufb01cial neural networks have exposed the power of optimizing millions of synaptic weights over millions of\nobservations to operate robustly in real-world contexts. these models do not learn simple, human-interpret-\nable rules or representations of the world; rather, they use local computations to interpolate over task-rele-\nvant manifolds in a high-dimensional parameter space. ", "letter\n\ncommunicated by erkki oja\n\ncanonical correlation analysis: an overview with\napplication to learning methods\n\ndavid r. hardoon\ndrh@ecs.soton.ac.uk\nsandor szedmak\nss03v@ecs.soton.ac.uk\njohn shawe-taylor\njst@ecs.soton.ac.uk\nschool of electronics and computer science, image, speech and intelligent systems\nresearch group, university of southampton, southampton s017 1bj, u.k.\n\nwe present a general method using kernel canonical correlation analy-\nsis to learn a semantic representation to web images and their associated\ntext. the semantic space provides a common representation and enables\na comparison between the text and images. in the experiments, we look\nat two approaches of retrieving images based on only their content from a\ntext query. we compare orthogonalization approaches against a standard\ncross-representation retrieval technique known as the generalized vector\nspace model.\n\n1 introduction\n\nduring recent years, there have been advances in data learning using kernel\nmethods. k", "9\n1\n0\n2\n\n \n\nv\no\nn\n6\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n2\n8\n6\n1\n1\n\n.\n\n1\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nexperience replay for continual learning\n\ndavid rolnick\n\nuniversity of pennsylvania\n\nphiladelphia, pa usa\n\ndrolnick@seas.upenn.edu\n\narun ahuja\ndeepmind\nlondon, uk\n\narahuja@google.com\n\njonathan schwarz\n\ndeepmind\nlondon, uk\n\nschwarzjn@google.com\n\ntimothy p. lillicrap\n\ndeepmind\nlondon, uk\n\ncountzero@google.com\n\ngreg wayne\ndeepmind\nlondon, uk\n\ngregwayne@google.com\n\nabstract\n\ninteracting with a complex world involves continual learning, in which tasks and\ndata distributions change over time. a continual learning system should demon-\nstrate both plasticity (acquisition of new knowledge) and stability (preservation of\nold knowledge). catastrophic forgetting is the failure of stability, in which new\nexperience overwrites previous experience. in the brain, replay of past experience\nis widely believed to reduce forgetting, yet it has been largely overlooked as a\nsolution to forgetting in deep reinforcement l", "7\n1\n0\n2\n\n \nt\nc\no\n \n4\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n6\n4\n1\n1\n1\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nsuperspike: supervised learning in multi-layer spik-\ning neural networks\n\nfriedemann zenke1, 2 & surya ganguli1\n1department of applied physics\nstanford university\nstanford, ca 94305\nunited states of america\n\n2centre for neural circuits and behaviour\nuniversity of oxford\noxford\nunited kingdom\n\nkeywords: spiking neural networks, multi-layer networks, supervised learning, tempo-\nral coding, synaptic plasticity, feedback alignment, credit assignment\n\nabstract\n\na vast majority of computation in the brain is performed by spiking neural networks. de-\nspite the ubiquity of such spiking, we currently lack an understanding of how biological\nspiking neural circuits learn and compute in-vivo, as well as how we can instantiate such\ncapabilities in arti\ufb01cial spiking circuits in-silico. here we revisit the problem of super-\nvised learning in temporally coding multi-layer spiking neural networks. first, by us", "mapreduce for parallel reinforcement learning\n\nyuxi li1 and dale schuurmans2\n\n1 college of computer science and engineering\n\nuniv. of electronic science and technology of china\n\nchengdu, china\n\n2 department of computing science\n\nuniversity of alberta\n\nedmonton, alberta, canada\n\nabstract. we investigate the parallelization of reinforcement learning\nalgorithms using mapreduce, a popular parallel computing framework.\nwe present parallel versions of several dynamic programming algorithms,\nincluding policy evaluation, policy iteration, and o\ufb00-policy updates. fur-\nthermore, we design parallel reinforcement learning algorithms to deal\nwith large scale problems using linear function approximation, includ-\ning model-based projection, least squares policy iteration, temporal dif-\nference learning and recent gradient temporal di\ufb00erence learning algo-\nrithms. we give time and space complexity analysis of the proposed algo-\nrithms. this study demonstrates how parallelization opens new avenues\nfor s", "8\n1\n0\n2\n\n \nr\np\na\n3\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n7\n1\n6\n8\n0\n\n.\n\n4\n0\n8\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2018\n\ndistributed distributional deterministic\npolicy gradients\n\ngabriel barth-maron\u02da, matthew w. hoffman\u02da, david budden, will dabney,\ndan horgan, dhruva tb, alistair muldal, nicolas heess, timothy lillicrap\ndeepmind\nlondon, uk\n{gabrielbm, mwhoffman, budden, wdabney, horgan, dhruvat,\nalimuldal, heess, countzero}@google.com\n\nabstract\n\nthis work adopts the very successful distributional perspective on reinforcement\nlearning and adapts it to the continuous control setting. we combine this within a\ndistributed framework for off-policy learning in order to develop what we call the\ndistributed distributional deep deterministic policy gradient algorithm, d4pg.\nwe also combine this technique with a number of additional, simple improvements\nsuch as the use of n-step returns and prioritized experience replay. experimen-\ntally we examine the contribution of each of these", "bayesian learning and inference in\n\nrecurrent switching linear dynamical systems\n\nscott w. linderman\u2217\ncolumbia university\n\nmatthew j. johnson\u2217\nharvard and google brain\n\nandrew c. miller\nharvard university\n\nryan p. adams\n\nharvard and google brain\n\ndavid m. blei\n\ncolumbia university\n\nliam paninski\n\ncolumbia university\n\nabstract\n\nmany natural systems, such as neurons \ufb01ring\nin the brain or basketball teams traversing a\ncourt, give rise to time series data with com-\nplex, nonlinear dynamics. we can gain in-\nsight into these systems by decomposing the\ndata into segments that are each explained by\nsimpler dynamic units. building on switch-\ning linear dynamical systems (slds), we de-\nvelop a model class and bayesian inference\nalgorithms that not only discover these dy-\nnamical units but also, by learning how tran-\nsition probabilities depend on observations or\ncontinuous latent states, explain their switch-\ning behavior. our key innovation is to de-\nsign these recurrent slds models to enable\nr", "research article\n\nneuroscience\npsychological and cognitive sciences\n\na hierarchy of linguistic predictions during natural language\ncomprehension\n\nmicha heilbrona,b,1\n\n, kristijan armenia, jan-mathijs scho\ufb00elena\n\n, peter hagoorta,b\n\n, and floris p. de langea\n\nedited by stanislas dehaene, commissariat a l\u2019 \u00b4energie atomique et aux \u00b4energies alternatives, gif-sur-yvette, france; received february 11, 2022;\naccepted june 28, 2022\n\nunderstanding spoken language requires transforming ambiguous acoustic streams into\na hierarchy of representations, from phonemes to meaning. it has been suggested that\nthe brain uses prediction to guide the interpretation of incoming input. however, the\nrole of prediction in language processing remains disputed, with disagreement about\nboth the ubiquity and representational nature of predictions. here, we address both\nissues by analyzing brain recordings of participants listening to audiobooks, and using\na deep neural network (gpt-2) to precisely quantify contex", "lifted neural networks\n\narmin askari 1 geoffrey negiar 1 rajiv sambharya 1 laurent el ghaoui 1 2\n\n8\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n1\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n2\n3\n5\n1\n0\n\n.\n\n5\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe describe a novel family of models of multi-\nlayer feedforward neural networks in which the\nactivation functions are encoded via penalties in\nthe training problem. our approach is based on\nrepresenting a non-decreasing activation function\nas the argmin of an appropriate convex optimiza-\ntion problem. the new framework allows for algo-\nrithms such as block-coordinate descent methods\nto be applied, in which each step is composed of\na simple (no hidden layer) supervised learning\nproblem that is parallelizable across data points\nand/or layers. experiments indicate that the pro-\nposed models provide excellent initial guesses for\nweights for standard neural networks. in addi-\ntion, the model provides avenues for interesting\nextensions, such as robustness against noisy in-\nputs and optimizing over", "information selection in noisy environments with large action spaces\n\npedro tsividis (tsividis@mit.edu), samuel j. gershman (sjgershm@mit.edu),\n\njoshua b. tenenbaum (jbt@mit.edu), laura schulz (lshulz@mit.edu)\n\nmassachusetts institute of technology, department of brain and cognitive sciences,\n\n77 massachusetts ave., cambridge, ma 02139 usa\n\nabstract\n\na critical aspect of human cognition is the ability to effec-\ntively query the environment for information. the \u2018real\u2019\nworld is large and noisy, and therefore designing effec-\ntive queries involves prioritizing both scope \u2013 the range\nof hypotheses addressed by the query \u2013 and reliability \u2013\nthe likelihood of obtaining a correct answer. here we de-\nsigned a simple information-search game in which partic-\nipants had to select an informative query from a large set\nof queries, trading off scope and reliability. we \ufb01nd that\nadults are effective information-searchers even in large,\nnoisy environments, and that their information search is\nbest exp", "neuron\n\nprimer\n\nimaging calcium in neurons\n\nchristine grienberger1 and arthur konnerth1,*\n1institute of neuroscience, technical university munich, biedersteinerstr. 29, 80802 munich, germany\n*correspondence: arthur.konnerth@lrz.tum.de\ndoi 10.1016/j.neuron.2012.02.011\n\ncalcium ions generate versatile intracellular signals that control key functions in all types of neurons. imaging\ncalcium in neurons is particularly important because calcium signals exert their highly speci\ufb01c functions in\nwell-de\ufb01ned cellular subcompartments. in this primer, we brie\ufb02y review the general mechanisms of neuronal\ncalcium signaling. we then introduce the calcium imaging devices, including confocal and two-photon\nmicroscopy as well as miniaturized devices that are used in freely moving animals. we provide an overview\nof the classical chemical \ufb02uorescent calcium indicators and of the protein-based genetically encoded\ncalcium indicators. using application examples, we introduce new developments in the \ufb01eld, such", "on the spectral bias of neural networks\n\nnasim rahaman * 1 2 aristide baratin * 1 devansh arpit 1 felix draxler 2 min lin 1 fred a. hamprecht 2\n\nyoshua bengio 1 aaron courville 1\n\n9\n1\n0\n2\n\n \n\ny\na\nm\n1\n3\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n4\n3\n7\n8\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nneural networks are known to be a class of highly\nexpressive functions able to \ufb01t even random input-\noutput mappings with 100% accuracy.\nin this\nwork we present properties of neural networks\nthat complement this aspect of expressivity. by\nusing tools from fourier analysis, we highlight a\nlearning bias of deep networks towards low fre-\nquency functions \u2013 i.e. functions that vary glob-\nally without local \ufb02uctuations \u2013 which manifests\nitself as a frequency-dependent learning speed.\nintuitively, this property is in line with the ob-\nservation that over-parameterized networks pri-\noritize learning simple patterns that generalize\nacross data samples. we also investigate the role\nof the shape of the data manifold b", "inferring spike-timing-dependent plasticity from\n\nspike train data\n\nian h. stevenson and konrad p. kording\n\ndepartment of physical medicine and rehabilitation\n{i-stevenson, kk}@northwestern.edu\n\nnorthwestern university\n\nabstract\n\nsynaptic plasticity underlies learning and is thus central for development, mem-\nory, and recovery from injury. however, it is often dif\ufb01cult to detect changes in\nsynaptic strength in vivo, since intracellular recordings are experimentally chal-\nlenging. here we present two methods aimed at inferring changes in the coupling\nbetween pairs of neurons from extracellularly recorded spike trains. first, using\na generalized bilinear model with poisson output we estimate time-varying cou-\npling assuming that all changes are spike-timing-dependent. this approach allows\nmodel-based estimation of stdp modi\ufb01cation functions from pairs of spike trains.\nthen, using recursive point-process adaptive \ufb01ltering methods we estimate more\ngeneral variation in coupling strength ove", "neurally plausible reinforcement learning of\n\nworking memory tasks\n\njaldert o. rombouts, sander m. bohte\n\ncwi, life sciences\n\namsterdam, the netherlands\n\n{j.o.rombouts, s.m.bohte}@cwi.nl\n\npieter r. roelfsema\n\nnetherlands institute for neuroscience\n\namsterdam, the netherlands\n\np.r.roelfsema@nin.knaw.nl\n\nabstract\n\na key function of brains is undoubtedly the abstraction and maintenance of in-\nformation from the environment for later use. neurons in association cortex play\nan important role in this process: by learning these neurons become tuned to rel-\nevant features and represent the information that is required later as a persistent\nelevation of their activity [1]. it is however not well known how such neurons\nacquire these task-relevant working memories. here we introduce a biologically\nplausible learning scheme grounded in reinforcement learning (rl) theory [2]\nthat explains how neurons become selective for relevant information by trial and\nerror learning. the model has memory units w", "1\n2\n0\n2\n\n \nt\nc\no\n5\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n5\nv\n2\n6\n2\n8\n0\n\n.\n\n0\n1\n0\n2\n:\nv\ni\nx\nr\na\n\nlocal plasticity rules can learn deep representations\n\nusing self-supervised contrastive predictions\n\nbernd illing\n\nguillaume bellec\u2217\n\njean ventura\n\nwulfram gerstner\u2217\n\n{firstname.lastname}@epfl.ch\n\ndepartment of computer science & department of life sciences\n\n\u00e9cole polytechnique f\u00e9d\u00e9rale de lausanne\n\n1015 switzerland\n\nabstract\n\nlearning in the brain is poorly understood and learning rules that respect biological\nconstraints, yet yield deep hierarchical representations, are still unknown. here,\nwe propose a learning rule that takes inspiration from neuroscience and recent\nadvances in self-supervised deep learning. learning minimizes a simple layer-\nspeci\ufb01c loss function and does not need to back-propagate error signals within or\nbetween layers. instead, weight updates follow a local, hebbian, learning rule that\nonly depends on pre- and post-synaptic neuronal activity, predictive dendritic input\nand wi", "european journal of neuroscience, vol. 18, pp. 2011\u00b12024, 2003\n\n\u00df federation of european neuroscience societies\n\nretrospective and prospective persistent activity induced\nby hebbian learning in a recurrent cortical network\n\ngianluigi mongillo,1 daniel j. amit2,3 and nicolas brunel4\n1dipartimento di fisiologia umana, universita\u00e1 di roma la sapienza, rome, italy\n2infm, dipartimento di fisica, universita\u00e1 di roma la sapienza, rome, italy\n3racah institute of physics, hebrew university, jerusalem, israel\n4cnrs, neurophysique et physiologie du syste\u00e1 me moteur, universite\u00e2 rene\u00e2 descartes, paris, france\n\nkeywords: integrate-and-\u00aere neuron, network model, perirhinal cortex, prefrontal cortex\n\nabstract\n\nrecordings from cells in the associative cortex of monkeys performing visual working memory tasks link persistent neuronal activity,\nlong-term memory and associative memory. in particular, delayed pair-associate tasks have revealed neuronal correlates of long-term\nmemory of associations between", "research article\n\nopposite initialization to novel cues in\ndopamine signaling in ventral and\nposterior striatum in mice\nwilliam menegas, benedicte m babayan, naoshige uchida,\nmitsuko watabe-uchida*\n\ndepartment of molecular and cellular biology, center for brain science, harvard\nuniversity, cambridge, united states\n\nabstract dopamine neurons are thought to encode novelty in addition to reward prediction\nerror (the discrepancy between actual and predicted values). in this study, we compared dopamine\nactivity across the striatum using fiber fluorometry in mice. during classical conditioning, we\nobserved opposite dynamics in dopamine axon signals in the ventral striatum (\u2018vs dopamine\u2019) and\nthe posterior tail of the striatum (\u2018ts dopamine\u2019). ts dopamine showed strong excitation to novel\ncues, whereas vs dopamine showed no responses to novel cues until they had been paired with a\nreward. ts dopamine cue responses decreased over time, depending on what the cue predicted.\nadditionally, ts dopa", "7\n1\n0\n2\n\n \n\nn\na\nj\n \n\n0\n3\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n9\nv\n0\n8\n9\n6\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\npublishedasaconferencepaperaticlr2015adam:amethodforstochasticoptimizationdiederikp.kingma*universityofamsterdam,openaidpkingma@openai.comjimmyleiba\u2217universityoftorontojimmy@psi.utoronto.caabstractweintroduceadam,analgorithmfor\ufb01rst-ordergradient-basedoptimizationofstochasticobjectivefunctions,basedonadaptiveestimatesoflower-ordermo-ments.themethodisstraightforwardtoimplement,iscomputationallyef\ufb01cient,haslittlememoryrequirements,isinvarianttodiagonalrescalingofthegradients,andiswellsuitedforproblemsthatarelargeintermsofdataand/orparameters.themethodisalsoappropriatefornon-stationaryobjectivesandproblemswithverynoisyand/orsparsegradients.thehyper-parametershaveintuitiveinterpre-tationsandtypicallyrequirelittletuning.someconnectionstorelatedalgorithms,onwhichadamwasinspired,arediscussed.wealsoanalyzethetheoreticalcon-vergencepropertiesofthealgorithmandprovidearegretboundontheconver-genceratethatiscompar", "clustering appearances of objects under varying illumination conditions\n\n\u2020\n\njeffrey ho\n\njongwoo lim\njho@cs.ucsd.edu myang@honda-ri.com jlim1@uiuc.edu\n\nming-hsuan yang(cid:1)\n\n\u2020\n\ncomputer science & engineering\nuniversity of california at san diego\n\n(cid:1) honda research institute\n\n\u2021\n\nkuang-chih lee\nklee10@uiuc.edu\n\u2021\n\n\u2021\n\ndavid kriegman\n\n\u2020\n\nkriegman@cs.ucsd.edu\n\ncomputer science\n\n800 california street\n\nuniversity of illinois at urbana-champaign\n\nla jolla, ca 92093\n\nmountain view, ca 94041\n\nurbana, il 61801\n\nabstract\n\nwe introduce two appearance-based methods for clustering\na set of images of 3-d objects, acquired under varying il-\nlumination conditions, into disjoint subsets corresponding\nto individual objects. the \ufb01rst algorithm is based on the\nconcept of illumination cones. according to the theory, the\nclustering problem is equivalent to \ufb01nding convex polyhe-\ndral cones in the high-dimensional image space. to ef\ufb01-\nciently determine the conic structures hidden in the image\ndata, we intr", "invariant scattering convolution networks\n\njoan bruna and st\u00b4ephane mallat\n\ncmap, ecole polytechnique, palaiseau, france\n\n1\n\n2\n1\n0\n2\n\n \nr\na\n\nm\n8\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n3\n1\n5\n1\n\n.\n\n3\n0\n2\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014a wavelet scattering network computes a translation invari-\nant image representation, which is stable to deformations and preserves\nhigh frequency information for classi\ufb01cation. it cascades wavelet trans-\nform convolutions with non-linear modulus and averaging operators. the\n\ufb01rst network layer outputs sift-type descriptors whereas the next layers\nprovide complementary invariant information which improves classi\ufb01ca-\ntion. the mathematical analysis of wavelet scattering networks explain\nimportant properties of deep convolution networks for classi\ufb01cation.\n\na scattering representation of stationary processes incorporates\nhigher order moments and can thus discriminate textures having same\nfourier power spectrum. state of the art classi\ufb01cation results are ob-\ntained for handwr", "3\n2\n0\n2\n\n \n\nv\no\nn\n1\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n5\nv\n0\n8\n7\n7\n0\n\n.\n\n4\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nshortcut learning in deep neural networks\n\nrobert geirhos1,2,\u2217,\u00a7, j\u00a8orn-henrik jacobsen3,\u2217, claudio michaelis1,2,\u2217,\n\nrichard zemel\u2020,3, wieland brendel\u2020,1, matthias bethge\u2020,1 & felix a. wichmann\u2020,1\n\n2international max planck research school for intelligent systems, germany\n\n1university of t\u00a8ubingen, germany\n\n3university of toronto, vector institute, canada\n\n\u2217joint first / \u2020 joint senior authors\n\n\u00a7to whom correspondence should be addressed: robert.geirhos@wichmannlab.org\n\nabstract\n\ndeep learning has triggered the current rise of artificial intelligence and is the workhorse\nof today\u2019s machine intelligence. numerous success stories have rapidly spread all over\nscience, industry and society, but its limitations have only recently come into focus. in this\nperspective we seek to distill how many of deep learning\u2019s problems can be seen as differ-\nent symptoms of the same underlying problem: shortcut learni", "consistent cross-modal identification of cortical \nneurons with coupled autoencoders\n\nrohan gala\u200a\nanton arkhipov, gabe murphy, bosiljka tasic, hongkui zeng, michael hawrylycz and uygar s\u00fcmb\u00fcl\u200a\n\n\u200a, fahimeh baftizadeh, jeremy miller\u200a\n\n\u200a, nathan gouwens\u200a\n\n\u200a, \n\n\u200a\u2009\u2709, agata budzillo\u200a\n\n\u200a\u2009\u2709\n\nconsistent identification of neurons in different experimental \nmodalities is a key problem in neuroscience. although meth-\nods  to  perform  multimodal  measurements  in  the  same  set \nof  single  neurons  have  become  available,  parsing  complex \nrelationships  across  different  modalities  to  uncover  neuro-\nnal identity is a growing challenge. here we present an opti-\nmization framework to learn coordinated representations of \nmultimodal  data  and  apply  it  to  a  large  multimodal  dataset \nprofiling  mouse  cortical  interneurons.  our  approach  reveals \nstrong  alignment  between  transcriptomic  and  electrophysi-\nological characterizations, enables accurate cross-modal data \nprediction, ", "5\n1\n0\n2\n\n \n\ng\nu\na\n9\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n8\n5\n4\n0\n\n.\n\n8\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nlearning to predict independent of span\n\nhado van hasselt \u2217\n\nrichard s. sutton \u2020\n\noctober 18, 2018\n\nabstract\n\nwe consider how to learn multi-step predictions e\ufb03ciently. conventional algo-\nrithms wait until observing actual outcomes before performing the computations to\nupdate their predictions.\nif predictions are made at a high rate or span over a\nlarge amount of time, substantial computation can be required to store all relevant\nobservations and to update all predictions when the outcome is \ufb01nally observed.\nwe show that the exact same predictions can be learned in a much more compu-\ntationally congenial way, with uniform per-step computation that does not depend\non the span of the predictions. we apply this idea to various settings of increasing\ngenerality, repeatedly adding desired properties and each time deriving an equiva-\nlent span-independent algorithm for the conventional algorithm that sat", "j neurophysiol 97: 4235\u2013 4257, 2007.\nfirst published march 21, 2007; doi:10.1152/jn.00095.2007.\n\ntemporal complexity and heterogeneity of single-neuron activity in\npremotor and motor cortex\n\nmark m. churchland and krishna v. shenoy\nneurosciences program and department of electrical engineering, stanford university, stanford, california\n\nsubmitted 29 january 2007; accepted in \ufb01nal form 15 march 2007\n\nchurchland mm, shenoy kv. temporal complexity and heteroge-\nneity of single-neuron activity in premotor and motor cortex. j\nneurophysiol 97: 4235\u2013 4257, 2007. first published march 21, 2007;\ndoi:10.1152/jn.00095.2007. the relationship between neural activity\nin motor cortex and movement is highly debated. although many\nstudies have examined the spatial\ntuning (e.g., for direction) of\ncortical responses,\nless attention has been paid to the temporal\nproperties of individual neuron responses. we developed a novel task,\nemploying two instructed speeds, that allows meaningful averaging of\nneural", "reward prediction error computation\nin the pedunculopontine tegmental\nnucleus neurons\n\nyasushi kobayashi a,b and ken-ichi okadaa\nagraduate school of frontier biosciences, osaka university, machikaneyama,\ntoyonaka, japan\nbatr computational neuroscience laboratories, hikaridai, seika-cho,\nsoraku-gun, kyoto, japan\n\nabstract: in this article, we address the role of neuronal activity in the\npathways of the brainstem\u2013midbrain circuit in reward and the basis for\nbelieving that this circuit provides advantages over previous reinforce-\nment learning theory. several lines of evidence support the reward-based\nlearning theory proposing that midbrain dopamine (da) neurons send\na teaching signal (the reward prediction error signal) to control synap-\ntic plasticity of the projection area. however, the underlying mechanism\nof where and how the reward prediction error signal is computed still\nremains unclear. since the pedunculopontine tegmental nucleus (pptn)\nin the brainstem is one of the strongest e", "194 \n\nieee  transactions on image  processing, vol. 4,  no.  2, february  1995 \n\nlikelihood calculation for a  class of \nmultiscale stochastic models, with \napplication to texture discrimination \n\nmark  r.  luettgen,  member, ieee, and alan  s. willsky,  fellow, ieee \n\nabstruct-  a  class  of  multiscale  stochastic  models  based  on \nscale-recursive dynamics on trees has recently been  introduced. \ntheoretical and experimental results have shown that these mod- \nels provide  an extremely rich  framework  for  representing both \nprocesses which are intrinsically multiscale, e.g., llf processes, as \nwell  as  1-d markov  processes and  2-d  markov  random  fields. \nmoreover, efficient optimal  estimation algorithms have been de- \nveloped for these models by exploiting their scale-recursive struc- \nture. in this paper, we exploit this structure in order to develop a \ncomputationally efficient and parallelizable algorithm for  likeli- \nhood calculation. we  illustrate one possible applic", "gradient target propagation\n\ntiago de souza farias\u2217 and jonas maziero\u2020\n\ndepartamento de f\u00edsica, centro de ci\u00eancias naturais e exatas, universidade\nfederal de santa maria, avenida roraima 1000, santa maria, rs, 97105-900,\n\nbrazil\n\nabstract\n\nwe report a learning rule for neural networks that computes how much each neuron should\ncontribute to minimize a giving cost function via the estimation of its target value. by theoret-\nical analysis, we show that this learning rule contains backpropagation, hebbian learning, and\nadditional terms. we also give a general technique for weights initialization. our results are\nat least as good as those obtained with backpropagation. the neural networks are trained and\ntested in three problems: mnist, fashion-mnist, and cifar-10 datasets. the associated\ncode is available at https://github.com/tiago939/target.\n\n1 introduction\n\nthe search for an e\ufb03cient learning rule for arti\ufb01cial neural networks is an active area of research.\na learning rule is a set of eq", "ieee transactions on pattern analysis and machine intelligence, vol. 43, no. 11, november 2021\n\n4037\n\nself-supervised visual feature learning with\n\ndeep neural networks: a survey\n\nlonglong jing , student member, ieee and yingli tian , fellow, ieee\n\nabstract\u2014large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual\nfeature learning from images or videos for computer vision applications. to avoid extensive cost of collecting and annotating large-\nscale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image\nand video features from large-scale unlabeled data without using any human-annotated labels. this paper provides an extensive\nreview of deep learning-based self-supervised general visual feature learning methods from images or videos. first, the motivation,\ngeneral pipeline, and terminologies of this \ufb01eld are described. then the common deep neural ne", "neuron\n\nreport\n\nlearning by the dendritic\nprediction of somatic spiking\n\nrobert urbanczik1 and walter senn1,*\n1department of physiology, and center for learning, cognition and memory, university of bern, bu\u00a8 hlplatz 5, ch-3012 bern, switzerland\n*correspondence: senn@pyl.unibe.ch\nhttp://dx.doi.org/10.1016/j.neuron.2013.11.030\n\nsummary\n\nrecent modeling of spike-timing-dependent plas-\nticity indicates that plasticity involves as a third\nfactor a local dendritic potential, besides pre- and\npostsynaptic \ufb01ring times. we present a simple\ncompartmental neuron model together with a non-\nhebbian, biologically plausible learning rule for den-\ndritic synapses where plasticity is modulated by\nthese three factors.\nin functional terms, the rule\nseeks to minimize discrepancies between somatic\n\ufb01rings and a local dendritic potential. such predic-\ntion errors can arise in our model from stochastic\n\ufb02uctuations as well as from synaptic input, which\ndirectly targets the soma. depending on the nature\nof this", "synaptic plasticity rules with physiological\ncalcium levels\n\nyanis ingleberta,1\ue840, johnatan aljadeffb,c,d,1, nicolas brunelb,c,e,f,2\ue840, and dominique debannea,2\n\naunit\u00e9 de neurobiologie des canaux ionique et de la synapse, umr1072, inserm, aix-marseille universit\u00e9, 13015 marseille, france; bdepartment of\nneurobiology, university of chicago, chicago, il 60637; cdepartment of statistics, university of chicago, chicago, il 60637; dneurobiology section, division\nof biological sciences, university of california san diego, la jolla, ca 92093; edepartment of neurobiology, duke university, durham, nc 27710;\nand fdepartment of physics, duke university, durham, nc 27710\n\nedited by mu-ming poo, chinese academy of sciences, shanghai, china, and approved october 28, 2020 (received for review july 13, 2020)\n\nspike-timing\u2013dependent plasticity (stdp) is considered as a pri-\nmary mechanism underlying formation of new memories during\nlearning. despite the growing interest in activity-dependent plas-\nticit", "1\n2\n0\n2\n\n \nr\na\n\n \n\nm\n1\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n1\n7\n4\n1\n1\n\n.\n\n7\n0\n0\n2\n:\nv\ni\nx\nr\na\n\ngeometric compression of invariant manifolds in neural nets\n\njonas paccolata, leonardo petrinia, mario geigera, kevin tylooa, and matthieu wyarta\n\nainstitute of physics, \u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne, 1015 lausanne,\n\nswitzerland\n\nmarch 12, 2021\n\nabstract\n\nwe study how neural networks compress uninformative input space in models where data lie in d\ndimensions, but whose label only vary within a linear manifold of dimension d(cid:107) < d. we show that for a\none-hidden layer network initialized with in\ufb01nitesimal weights (i.e. in the feature learning regime) trained\nwith gradient descent, the \ufb01rst layer of weights evolve to become nearly insensitive to the d\u22a5 = d \u2212 d(cid:107)\nuninformative directions. these are e\ufb00ectively compressed by a factor \u03bb \u223c \u221a\np, where p is the size of the\ntraining set. we quantify the bene\ufb01t of such a compression on the test error \u0001. for large initialization\nof t", "statistics and samples in distributional reinforcement learning\n\nmark rowland 1 robert dadashi 2 saurabh kumar 2 r\u00b4emi munos 1 marc g. bellemare 2 will dabney 1\n\nabstract\n\nwe present a unifying framework for designing\nand analysing distributional reinforcement learn-\ning (drl) algorithms in terms of recursively es-\ntimating statistics of the return distribution. our\nkey insight is that drl algorithms can be de-\ncomposed as the combination of some statistical\nestimator and a method for imputing a return dis-\ntribution consistent with that set of statistics. with\nthis new understanding, we are able to provide\nimproved analyses of existing drl algorithms as\nwell as construct a new algorithm (edrl) based\nupon estimation of the expectiles of the return\ndistribution. we compare edrl with existing\nmethods on a variety of mdps to illustrate con-\ncrete aspects of our analysis, and develop a deep\nrl variant of the algorithm, er-dqn, which we\nevaluate on the atari-57 suite of games.\n\n1. introduct", "theoretical aspects of group\nequivariant neural networks\n\ncarlos esteves\n\ndepartment of computer and information science\n\nuniversity of pennsylvania\n\nphiladelphia, pa\n\nabstract\n\ngroup equivariant neural networks have been explored in the past few years\nand are interesting from theoretical and practical standpoints. they lever-\nage concepts from group representation theory, non-commutative harmonic\nanalysis and differential geometry that do not often appear in machine\nlearning. in practice, they have been shown to reduce sample and model\ncomplexity, notably in challenging tasks where input transformations such\nas arbitrary rotations are present. we begin this work with an exposition\nof group representation theory and the machinery necessary to de\ufb01ne and\nevaluate integrals and convolutions on groups. then, we show applications\nto recent so(3) and se(3) equivariant networks, namely the spherical cnns,\nclebsch-gordan networks, and 3d steerable cnns. we proceed to discuss\ntwo recent theoret", "new research\n\nsensory and motor systems\n\nsharpening of hierarchical visual feature\nrepresentations of blurred images\n\nmohamed abdelhack,1,2 and yukiyasu kamitani1,2\n\ndoi:http://dx.doi.org/10.1523/eneuro.0443-17.2018\n\n1graduate school of informatics, kyoto university, yoshida-honmachi, sakyo-ku, kyoto 606-8501, japan and 2atr\ncomputational neuroscience laboratories, 2-2-2 hikaridai, seika, soraku, kyoto 619-0288, japan\n\nvisual abstract\n\nthe robustness of the visual system lies in its ability to perceive degraded images. this is achieved through\ninteracting bottom-up, recurrent, and top-down pathways that process the visual input in concordance with\nstored prior information. the interaction mechanism by which they integrate visual input and prior information is\nstill enigmatic. we present a new approach using deep neural network (dnn) representation to reveal the effects\nof such integration on degraded visual inputs. we transformed measured human brain activity resulting from\n\nsignifican", "8\n1\n0\n2\n\n \n\nn\na\nj\n \n7\n1\n\n \n \n]\n\n.\n\no\nh\nh\nt\na\nm\n\n[\n \n \n\n1\nv\n4\n9\n8\n5\n0\n\n.\n\n1\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ndeep learning: an introduction for applied\n\nmathematicians\n\ncatherine f. higham\u2217\n\ndesmond j. higham\u2020\n\njanuary 19, 2018\n\nabstract\n\nmultilayered arti\ufb01cial neural networks are becoming a pervasive tool\nin a host of application \ufb01elds. at the heart of this deep learning revolution\nare familiar concepts from applied and computational mathematics; no-\ntably, in calculus, approximation theory, optimization and linear algebra.\nthis article provides a very brief introduction to the basic ideas that un-\nderlie deep learning from an applied mathematics perspective. our target\naudience includes postgraduate and \ufb01nal year undergraduate students in\nmathematics who are keen to learn about the area. the article may also\nbe useful for instructors in mathematics who wish to enliven their classes\nwith references to the application of deep learning techniques. we focus\non three fundamental questions: what is a dee", "published as a conference paper at iclr 2019\n\nuniversal transformers\nmostafa dehghani\u2217\u2020\nuniversity of amsterdam deepmind\ndehghani@uva.nl\n\nstephan gouws\u2217\n\nsgouws@google.com\n\noriol vinyals\ndeepmind\nvinyals@google.com\n\n9\n1\n0\n2\n\n \nr\na\n\nm\n5\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n3\nv\n9\n1\n8\n3\n0\n\n.\n\n7\n0\n8\n1\n:\nv\ni\nx\nr\na\n\njakob uszkoreit\ngoogle brain\nusz@google.com\n\n\u0142ukasz kaiser\ngoogle brain\nlukaszkaiser@google.com\n\nabstract\n\nrecurrent neural networks (rnns) sequentially process data by updating their\nstate with each new data point, and have long been the de facto choice for sequence\nmodeling tasks. however, their inherently sequential computation makes them\nslow to train. feed-forward and convolutional architectures have recently been\nshown to achieve superior results on some sequence modeling tasks such as machine\ntranslation, with the added advantage that they concurrently process all inputs in\nthe sequence, leading to easy parallelization and faster training times. despite these\nsuccesses, however, pop", "deep linear neural networks:\n\na theory of learning in the brain and mind\n\na dissertation\n\nsubmitted to the department of electrical\n\nengineering\n\nand the committee on graduate studies\n\nof stanford university\n\nin partial fulfillment of the requirements\n\nfor the degree of\n\ndoctor of philosophy\n\nandrew michael saxe\n\njune 2015\n\n\f", "2\n1\n0\n2\n\n \n\nb\ne\nf\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n0\n9\n8\n3\n\n.\n\n2\n0\n2\n1\n:\nv\ni\nx\nr\na\n\npac bounds for discounted mdps\n\ntor lattimore1 and marcus hutter1,2,3\n\nresearch school of computer science\n\n1australian national university and 2eth z\u00a8urich and 3nicta\n\n{tor.lattimore,marcus.hutter}@anu.edu.au\n\njanuary 2012\n\nabstract\n\nwe study upper and lower bounds on the sample-complexity of learning near-\noptimal behaviour in \ufb01nite-state discounted markov decision processes (mdps). for\nthe upper bound we make the assumption that each action leads to at most two\npossible next-states and prove a new bound for a ucrl-style algorithm on the number\nof time-steps when it is not probably approximately correct (pac). the new lower\nbound strengthens previous work by being both more general (it applies to all policies)\nand tighter. the upper and lower bounds match up to logarithmic factors.\n\ncontents\n\n1 introduction\n2 notation\n3 estimation\n4 upper con\ufb01dence reinforcement learning algorithm\n5 upper pac bound", "a direct adaptive method for faster backpropagation learning: \n\nthe rprop algorithm \n\nmartin  riedmiller \n\nheinrich braun \n\ninstitut fur logik, komplexitat  und  deduktionssyteme \n\nuniversity of  karlsruhe \n\nw-7500  karlsruhe \n\nfrg \n\nriedml@ira.uka.de \n\nabstract-  a  new learning algorithm for multi- \nlayer feedforward networks, rprop, is proposed. \nto  overcome  the  inherent  disadvantages of  pure \ngradient-descent, rprop  performs  a local  adap- \ntation of the weight-updates according  to the be- \nhaviour of the errorfunction. in substantial differ- \nence to other adaptive techniques, the effect of the \nrprop adaptation process is  not  blurred by  the \nunforseeable influence of the size of the derivative \nbut only dependent on the temporal behaviour of \nits sign.  this leads to an efficient and transparent \nadaptation process.  the promising capabilities of \nrprop  are  shown in  comparison to  other  well- \nknown adaptive techniques. \n\n1.  introduction \n\na .   backpropagation  l", "5\n1\n0\n2\n\n \nr\na\n\n \n\nm\n0\n2\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n2\n7\n5\n6\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2015\n\nexplaining and harnessing\nadversarial examples\n\nian j. goodfellow, jonathon shlens & christian szegedy\ngoogle inc., mountain view, ca\n{goodfellow,shlens,szegedy}@google.com\n\nabstract\n\nseveral machine learning models, including neural networks, consistently mis-\nclassify adversarial examples\u2014inputs formed by applying small but intentionally\nworst-case perturbations to examples from the dataset, such that the perturbed in-\nput results in the model outputting an incorrect answer with high con\ufb01dence. early\nattempts at explaining this phenomenon focused on nonlinearity and over\ufb01tting.\nwe argue instead that the primary cause of neural networks\u2019 vulnerability to ad-\nversarial perturbation is their linear nature. this explanation is supported by new\nquantitative results while giving the \ufb01rst explanation of the most intriguing fact\nabout them: their generaliza", "which neural net architectures give rise to\n\nexploding and vanishing gradients?\n\nboris hanin\n\ndepartment of mathematics\n\ntexas a& m university\ncollege station, tx, usa\nbhanin@math.tamu.edu\n\nabstract\n\nwe give a rigorous analysis of the statistical behavior of gradients in a randomly\ninitialized fully connected network n with relu activations. our results show\nthat the empirical variance of the squares of the entries in the input-output jacobian\nof n is exponential in a simple architecture-dependent constant \u0000, given by the\nsum of the reciprocals of the hidden layer widths. when \u0000 is large, the gradients\ncomputed by n at initialization vary wildly. our approach complements the mean\n\ufb01eld theory analysis of random networks. from this point of view, we rigorously\ncompute \ufb01nite width corrections to the statistics of gradients at the edge of chaos.\n\n1\n\nintroduction\n\na fundamental obstacle in training deep neural nets using gradient based optimization is the exploding\nand vanishing gradient pr", "a fine-grained spectral perspective on neural networks\n\ngreg yang 1 hadi salman 1 2\n\n0\n2\n0\n2\n\n \nr\np\na\n9\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n9\n9\n5\n0\n1\n\n.\n\n7\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nare neural networks biased toward simple func-\ntions? does depth always help learn more com-\nplex features? is training the last layer of a net-\nwork as good as training all layers? how to set the\nrange for learning rate tuning? these questions\nseem unrelated at face value, but in this work we\ngive all of them a common treatment from the\nspectral perspective. we will study the spectra of\nthe conjugate kernel, ck, (also called the neu-\nral network-gaussian process kernel), and the\nneural tangent kernel, ntk. roughly, the ck\nand the ntk tell us respectively \u201cwhat a network\nlooks like at initialization\u201d and \u201cwhat a network\nlooks like during and after training.\u201d their spectra\nthen encode valuable information about the initial\ndistribution and the training and generalization\nproperties of neural networks. by anal", "original research\npublished: 18 february 2021\ndoi: 10.3389/fnins.2021.633674\n\nscaling equilibrium propagation to\ndeep convnets by drastically\nreducing its gradient estimator bias\n\naxel laborieux 1*, maxence ernoult 1,2,3*, benjamin scellier 3\u2020, yoshua bengio 3,4,\njulie grollier 2 and damien querlioz 1\n\n1 universit\u00e9 paris-saclay, cnrs, centre de nanosciences et de nanotechnologies, palaiseau, france, 2 unit\u00e9 mixte de\nphysique, cnrs, thales, universit\u00e9 paris-saclay, palaiseau, france, 3 mila, universit\u00e9 de montr\u00e9al, montreal, qc, canada,\n4 canadian institute for advanced research, toronto, on, canada\n\nequilibrium propagation is a biologically-inspired algorithm that\ntrains convergent\nrecurrent neural networks with a local learning rule. this approach constitutes a major\nlead to allow learning-capable neuromophic systems and comes with strong theoretical\nguarantees. equilibrium propagation operates in two phases, during which the network\nis let to evolve freely and then \u201cnudged\u201d toward a ", "free from bellman completeness: trajectory stitching via\n\nmodel-based return-conditioned supervised learning\n\nzhaoyi zhou1, chuning zhu2, runlong zhou2, qiwen cui2,\n\nabhishek gupta2, simon s. du2\n\n1tsinghua university, 2university of washington\n\nzhouzhao20@mails.tsinghua.edu.cn\n\noctober 31, 2023\n\nabstract\n\noff-policy dynamic programming (dp) techniques such as q-learning have\nproven to be an important technique for solving sequential decision-making\nproblems. however, in the presence of function approximation such algo-\nrithms are not guaranteed to converge, often diverging due to the absence of\nbellman-completeness in the function classes considered, a crucial condi-\ntion for the success of dp-based methods. in this paper, we show how off-\npolicy learning techniques based on return-conditioned supervised learning\n(rcsl) are able to circumvent these challenges of bellman completeness,\nconverging under significantly more relaxed assumptions inherited from su-\npervised learning. we prove", "feedforward and feedback connections between areas v1 and v2\nof the monkey have similar rapid conduction velocities\n\nrapid communication\n\np. girard, j. m. hupe\u00b4 , and j. bullier\ninstitut national de la sante\u00b4 et de la recherche me\u00b4dicale u371, 69500 bron, france\n\nreceived 7 august 2000; accepted in \ufb01nal form 2 november 2000\n\ngirard, p., j. m. hupe\u00b4, and j. bullier. feedforward and feedback\nconnections between areas v1 and v2 of the monkey have similar\nrapid conduction velocities. j neurophysiol 85: 1328 \u20131331, 2001. it\nis often assumed that the action of cortical feedback connections is\nslow and modulatory, whereas feedforward connections carry a rapid\ndrive to their target neurons. recent results from our laboratory\nshowed a very rapid effect of feedback connections on the visual\nresponses of neurons in lower order areas. we wanted to determine\nwhether such a rapid action is mediated by fast conducting axons.\nusing electrical stimulation, we compared the conduction velocities\nalong fe", "a hebbian/anti-hebbian network derived from \nonline non-negative matrix factorization can \n\ncluster and discover sparse features \n\ncengiz pehlevan1,2 and dmitri b. chklovskii2 \n\n1 janelia farm research campus \nhoward hughes medical institute \n\nashburn, va 20147 \n\npehlevanc@janelia.hhmi.org \n\n \n\n2 simons center for data analysis  \n\nsimons foundation \nnew york, ny 10010 \n\nmitya@simonsfoundation.org \n\nabstract:  despite  our  extensive  knowledge  of  biophysical \nproperties of neurons, there is no commonly accepted algorithmic \ntheory of neuronal function.  here we explore the hypothesis that \nsingle-layer  neuronal  networks  perform  online  symmetric  non-\nnegative matrix factorization (snmf) of the similarity matrix of \nthe streamed data. by starting with the snmf cost function we \nderive  an  online  algorithm,  which  can  be  implemented  by  a \nbiologically  plausible  network  with  local  learning  rules.  we \ndemonstrate  that  such  network  performs  soft  clustering  of  th", "opinion\n\nstability of the \ufb01ttest: organizing\nlearning through retroaxonal signals\nkenneth d. harris1,2\n\n1 center for molecular and behavioral neuroscience, rutgers university, 197 university avenue, newark, nj 07102, usa\n2 smilow neuroscience program and department of otolaryngology, new york university school of medicine, 550 first avenue,\nnew york, ny 10016, usa\n\nclassically, neurons communicate by anterograde\nconduction of action potentials. however, information\ncan also pass backward along axons, a process that is\nessential during the development of the nervous system.\nhere we propose a role for such \u2018retroaxonal\u2019 signals in\nadult learning. we hypothesize that strengthening of a\nneuron\u2019s output synapses stabilizes recent changes in\nthe same neuron\u2019s inputs. during learning, the input\nsynapses of many neurons undergo transient changes,\nresulting in altered spiking activity. if this in turn pro-\nmotes strengthening of output synapses, the recent\nsynaptic changes will be stabilized; o", "computational neuroscience\ninternal representation of task rules by recurrent dynamics: \nthe importance of the diversity of neural responses\n\noriginal research article\npublished: 04 october 2010\ndoi: 10.3389/fncom.2010.00024\n\nmattia rigotti1,2, daniel ben dayan rubin1,2, xiao-jing wang 3 and stefano fusi1,2*\n\n1  center for theoretical neuroscience, college of physicians and surgeons, columbia university, new york, ny, usa\n2  institute of neuroinformatics, university of zurich and swiss federal institute of technology zurich, zurich, switzerland\n3  department of neurobiology, kavli institute for neuroscience, yale university school of medicine, new haven, ct, usa\n\nedited by:\nklaus r. pawelzik, university of \nbremen, germany\nreviewed by:\nalessandro treves, scuola \ninternazionale superiore di studi \navanzati (sissa), italy\nchristian leibold, ludwig maximilians \nuniversity, germany\n*correspondence:\nstefano fusi, center for theoretical \nneuroscience, columbia university, \n1051 riverside dri", "research article\n\ndemixed principal component analysis of\nneural population data\ndmitry kobak1\u2020, wieland brendel1,2,3\u2020, christos constantinidis4,\nclaudia e feierstein1, adam kepecs5, zachary f mainen1, xue-lian qi4,\nranulfo romo6,7, naoshige uchida8, christian k machens1*\n\n1champalimaud neuroscience program, champalimaud centre for the unknown,\nlisbon, portugal; 2e\u00b4 cole normale supe\u00b4 rieure, paris, france; 3centre for integrative\nneuroscience, university of tu\u00a8 bingen, tu\u00a8 bingen, germany; 4wake forest university\nschool of medicine, winston-salem, united states; 5cold spring harbor laboratory,\ncold spring harbor, united states; 6instituto de fisiolog\u0131\u00b4a celular-neurociencias,\nuniversidad nacional auto\u00b4 noma de me\u00b4 xico, mexico city, mexico; 7el colegio\nnacional, mexico city, mexico; 8harvard university, cambridge, united states\n\nabstract neurons in higher cortical areas, such as the prefrontal cortex, are often tuned to a\nvariety of sensory and motor variables, and are therefore said ", "rapid adaptation with conditionally shifted neurons\n\ntsendsuren munkhdalai 1 xingdi yuan 1 soroush mehri 1 adam trischler 1\n\n8\n1\n0\n2\n\n \nl\nu\nj\n \n\n3\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n6\n2\n9\n9\n0\n\n.\n\n2\n1\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe describe a mechanism by which arti\ufb01cial neu-\nral networks can learn rapid adaptation \u2013 the abil-\nity to adapt on the \ufb02y, with little data, to new\ntasks \u2013 that we call conditionally shifted neurons.\nwe apply this mechanism in the framework of\nmetalearning, where the aim is to replicate some\nof the \ufb02exibility of human learning in machines.\nconditionally shifted neurons modify their activa-\ntion values with task-speci\ufb01c shifts retrieved from\na memory module, which is populated rapidly\nbased on limited task experience. on metalearn-\ning benchmarks from the vision and language\ndomains, models augmented with conditionally\nshifted neurons achieve state-of-the-art results.\n\n1. introduction\nthe ability to adapt our behavior rapidly in response to\nexternal or internal feedbac", "5\n1\n0\n2\n\n \n\nb\ne\nf\n6\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n2\n5\n8\n1\n0\n\n.\n\n2\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nsurpassing human-level performance on imagenet classi\ufb01cation\n\ndelving deep into recti\ufb01ers:\n\nkaiming he\n\nxiangyu zhang\n\nshaoqing ren\n\njian sun\n\nmicrosoft research\n\n{kahe, v-xiangz, v-shren, jiansun}@microsoft.com\n\nabstract\n\nrecti\ufb01ed activation units (recti\ufb01ers) are essential for\nstate-of-the-art neural networks.\nin this work, we study\nrecti\ufb01er neural networks for image classi\ufb01cation from two\naspects. first, we propose a parametric recti\ufb01ed linear\nunit (prelu) that generalizes the traditional recti\ufb01ed unit.\nprelu improves model \ufb01tting with nearly zero extra com-\nputational cost and little over\ufb01tting risk. second, we de-\nrive a robust initialization method that particularly consid-\ners the recti\ufb01er nonlinearities. this method enables us to\ntrain extremely deep recti\ufb01ed models directly from scratch\nand to investigate deeper or wider network architectures.\nbased on our prelu networks (prelu-nets), we achi", "int j comput vis (2015) 113:54\u201366\ndoi 10.1007/s11263-014-0788-3\n\nspiking deep convolutional neural networks for energy-ef\ufb01cient\nobject recognition\nyongqiang cao \u00b7 yang chen \u00b7 deepak khosla\n\nreceived: 9 february 2014 / accepted: 10 november 2014 / published online: 23 november 2014\n\u00a9 springer science+business media new york 2014\n\nabstract deep-learning neural networks such as convo-\nlutional neural network (cnn) have shown great potential\nas a solution for dif\ufb01cult vision problems, such as object\nrecognition. spiking neural networks (snn)-based architec-\ntures have shown great potential as a solution for realizing\nultra-low power consumption using spike-based neuromor-\nphic hardware. this work describes a novel approach for con-\nverting a deep cnn into a snn that enables mapping cnn\nto spike-based hardware architectures. our approach \ufb01rst tai-\nlors the cnn architecture to \ufb01t the requirements of snn,\nthen trains the tailored cnn in the same way as one would\nwith cnn, and \ufb01nally applies t", "evidence that recurrent circuits are critical to \nthe ventral stream\u2019s execution of core object \nrecognition behavior\n\nkohitij kar\u200a\n\n\u200a1,2*, jonas kubilius1,3, kailyn schmidt1, elias b. issa1,4 and james j. dicarlo\u200a\n\n\u200a1,2\n\nnon-recurrent deep convolutional neural networks (cnns) are currently the best at modeling core object recognition, a behavior \nthat is supported by the densely recurrent primate ventral stream, culminating in the inferior temporal (it) cortex. if recurrence \nis critical to this behavior, then primates should outperform feedforward-only deep cnns for images that require additional \nrecurrent processing beyond the feedforward it response. here we first used behavioral methods to discover hundreds of these \n\u2018challenge\u2019 images. second, using large-scale electrophysiology, we observed that behaviorally sufficient object identity solu-\ntions emerged ~30\u2009ms later in the it cortex for challenge images compared with primate performance-matched \u2018control\u2019 images. \nthird, these ", "review\n\ndialogues on prediction errors\nyael niv1 and geoffrey schoenbaum2\n\n1 center for the study of brain, mind and behavior and department of psychology, green hall, princeton university, princeton,\nnj 08544, usa\n2 departments of anatomy and neurobiology, and psychiatry, university of maryland school of medicine, 20 penn street,\nbaltimore, md 21201, usa\n\nideas\n\ncomputational\n\nrecognition that\n\nthe\nfrom\nreinforcement learning are relevant to the study of\nneural circuits has taken the cognitive neuroscience\ncommunity by storm. a central tenet of these models\nis that discrepancies between actual and expected out-\ncomes can be used for learning. neural correlates of such\nprediction-error signals have been observed now in mid-\nbrain dopaminergic neurons, striatum, amygdala and\neven prefrontal cortex, and models incorporating pre-\ndiction errors have been invoked to explain complex\nphenomena such as the transition from goal-directed\nto habitual behavior. yet, like any revolution, the fast-", "3\n2\n0\n2\n\n \n\ny\na\nm\n8\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n2\n5\n2\n1\n1\n\n.\n\n5\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nbrain-inspired learning in arti\ufb01cial neural\n\nnetworks: a review\n\nsamuel schmidgall1, \u0000, jascha achterberg2,3, thomas miconi4, louis kirsch5, rojin ziaei6, s. pardis hajiseyedrazi6, and\n\njason eshraghian7\n\n1johns hopkins university\n2university of cambridge\n\n3intel labs\n\n4ml collective\n\n5the swiss ai lab idsia\n\n6university of maryland, college park\n7university of california, santa cruz\n\narti\ufb01cial neural networks (anns) have emerged as an essential\ntool in machine learning, achieving remarkable success across\ndiverse domains, including image and speech generation, game\nplaying, and robotics. however, there exist fundamental dif-\nferences between anns\u2019 operating mechanisms and those of\nthe biological brain, particularly concerning learning processes.\nthis paper presents a comprehensive review of current brain-\ninspired learning representations in arti\ufb01cial neural networks.\nwe investigate the integration", "special  issue:  time  in  the  brain\nopinion\nspace\nand\nsequence\n\n \n\n \n\n \n\ntime:\nthe\ngenerator\n\n \n\nhippocampus\n\n \n\nas\n\n \n\n \n\na\n\ngy\u00f6rgy  buzs\u00e1ki1,2,3,* and  david  tingley1\n\nneural  computations  are  often  compared  to  instrument-measured  distance  or\nduration,  and  such  relationships  are  interpreted  by  a  human  observer.  how-\never,  neural  circuits  do  not  depend  on  human-made  instruments  but  perform\ncomputations  relative  to  an  internally  de\ufb01ned  rate-of-change.  while  neuronal\ncorrelations  with  external  measures,  such  as  distance  or  duration,  can  be\nobserved  in  spike  rates  or  other  measures  of  neuronal  activity,  what  matters\nfor  the  brain  is  how  such  activity  patterns  are  utilized  by  downstream  neural\nobservers.  we  suggest  that  hippocampal  operations  can  be  described  by  the\nsequential  activity  of  neuronal  assemblies  and  their  internally  de\ufb01ned  rate  of\nchange  without  resorting  to  the  concept  of  space ", "letter\nhuman-level control through deep reinforcement\nlearning\n\ndoi:10.1038/nature14236\n\nvolodymyr mnih1*, koray kavukcuoglu1*, david silver1*, andrei a. rusu1, joel veness1, marc g. bellemare1, alex graves1,\nmartin riedmiller1, andreas k. fidjeland1, georg ostrovski1, stig petersen1, charles beattie1, amir sadik1, ioannis antonoglou1,\nhelen king1, dharshan kumaran1, daan wierstra1, shane legg1 & demis hassabis1\n\nthe theory of reinforcement learning provides a normative account1,\ndeeply rooted in psychological2 and neuroscientific3 perspectives on\nanimal behaviour, of how agents may optimize their control of an\nenvironment. to use reinforcement learning successfully in situations\napproaching real-world complexity, however, agents are confronted\nwith a difficult task: they must derive efficient representations of the\nenvironment from high-dimensional sensory inputs, and use these\nto generalize past experience to new situations. remarkably, humans\nand other animals seem to solve this pro", "m\no\nc\n\n.\n\ne\nr\nu\nt\na\nn\n\n.\ni\n\nc\ns\no\nr\nu\ne\nn\n\n/\n/\n:\np\nt\nt\nh\n\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n1\n0\n0\n2\n\u00a9\n\n \n\n\u00a9 2001 nature publishing group  http://neurosci.nature.com\n\narticles\n\ndendritic spine geometry is critical\nfor ampa receptor expression in\nhippocampal ca1 pyramidal neurons\n\nmasanori matsuzaki1,2,3, graham c. r. ellis-davies4, tomomi nemoto1,2,3, yasushi miyashita3,\nmasamitsu iino2,3 and haruo kasai1,2\n\n1 department of cell physiology, national institute for physiological sciences, and the graduate university for advanced studies, \n\nmyodaiji, okazaki 444-8585, japan\n\n2 crest, japan science and technology corporation, hongo, bunkyo-ku, tokyo 113-0033, japan \n3 university of tokyo school of medicine, hongo, bunkyo-ku, tokyo 113-0033, japan \n4 department of pharmacology and physiology, mcp hahnemann university, philadelphia, pennsylvania 19102, usa \ncorrespondence should be addressed to h.k. (hkasai@nips.ac.jp)\n\npublished online: 22 october 2001, doi: 10.1038/nn7", "a computational model of limb impedance control\nbased on principles of internal model uncertainty\n\ndjordje mitrovic1*, stefan klanke1, rieko osu2,3, mitsuo kawato2, sethu vijayakumar1\n\n1 ipab, school of informatics, university of edinburgh, edinburgh, united kingdom, 2 atr computational neuroscience laboratories, keihanna science city, kyoto, japan,\n3 national institute of information and communications technology, kyoto, japan\n\nabstract\n\nefficient human motor control is characterized by an extensive use of joint impedance modulation, which is achieved by co-\ncontracting antagonistic muscles in a way that is beneficial to the specific task. while there is much experimental evidence\navailable that the nervous system employs such strategies, no generally-valid computational model of impedance control\nderived from first principles has been proposed so far. here we develop a new impedance control model for antagonistic\nlimb systems which is based on a minimization of uncertainties in the i", "ieee transactions on information theory, vol. 67, no. 5, may 2021\n\n2581\n\ndeep neural network approximation theory\n\ndennis elbr\u00e4chter, dmytro perekrestenko , philipp grohs, and helmut b\u00f6lcskei\n\n, fellow, ieee\n\n(invited paper)\n\nabstract\u2014 this paper develops fundamental\n\nlimits of deep\nneural network learning by characterizing what is possible if\nno constraints are imposed on the learning algorithm and on the\namount of training data. concretely, we consider kolmogorov-\noptimal approximation through deep neural networks with the\nguiding theme being a relation between the complexity of\nthe function (class) to be approximated and the complexity of\nthe approximating network in terms of connectivity and memory\nrequirements for storing the network topology and the associated\nquantized weights. the theory we develop establishes that deep\nnetworks are kolmogorov-optimal approximants for markedly\ndifferent function classes, such as unit balls in besov spaces\nand modulation spaces. in addition, dee", "the journal of neuroscience, november 24, 2010 \u2022 30(47):15747\u201315759 \u2022 15747\n\nbehavioral/systems/cognitive\n\ncaudate encodes multiple computations for perceptual\ndecisions\n\nlong ding and joshua i. gold\ndepartment of neuroscience, university of pennsylvania, philadelphia, pennsylvania 19104-6074\n\nperceptual decision making is a complex process that requires multiple computations, including the accumulation of sensory evidence\nand an ongoing evaluation of the accumulation process to use for prediction and adjustment. implementing these computations likely\ninvolves interactions among many brain regions. for perceptual decisions linked to oculomotor actions, neural correlates of sensory\nevidence accumulation have been identified in several cortical areas, including the frontal eye field and lateral intraparietal area, and one\nof their direct, subcortical targets, the superior colliculus. these structures are also connected indirectly, via the basal ganglia. the basal\nganglia pathway has been", "neuron, vol. 39, 991\u20131004, september 11, 2003, copyright \uf8e92003 by cell press\n\nintensity versus identity coding\nin an olfactory system\n\nmark stopfer,1,2 vivek jayaraman,1\nand gilles laurent*\ncalifornia institute of technology\ndivision of biology\ncomputation and neural systems program\npasadena, california 91125\n\nsummary\n\nwe examined the encoding and decoding of odor iden-\ntity and intensity by neurons in the antennal lobe and\nthe mushroom body, first and second relays, respec-\ntively, of the locust olfactory system. increased odor\nconcentration led to changes in the firing patterns\nof individual antennal lobe projection neurons (pns),\nsimilar to those caused by changes in odor identity,\nthus potentially confounding representations for iden-\ntity and concentration. however, when these time-\nvarying responses were examined across many pns,\nconcentration-specific patterns clustered by identity,\nresolving the apparent confound. this is because pn\nensemble representations changed relatively c", "downloaded from \n\nhttp://cshperspectives.cshlp.org/\n\n on october 2, 2023 - published by cold spring harbor laboratory press \n\nsynapses and memory storage\n\nmark mayford1, steven a. siegelbaum2, and eric r. kandel2\n\n1the scripps research institute, department of cell biology, la jolla, california 92037\n2columbia university, kavli institute for brain science, howard hughes medical institute,\ndepartment of neuroscience, new york, new york 10032\n\ncorrespondence: erk5@columbia.edu\n\nthe synapse is the functional unit of the brain. during the last several decades we have\nacquired a great deal of information on its structure, molecular components, and physio-\nlogical function. it is clear that synapses are morphologically and molecularly diverse and\nthat this diversity is recruited to different functions. one of the most intriguing \ufb01ndings is that\nthe size of the synaptic response in not invariant, but can be altered by a variety of homo-\nand heterosynaptic factors such as past patterns of use ", "review\npublished: 31 july 2018\ndoi: 10.3389/fncir.2018.00053\n\neligibility traces and plasticity on\nbehavioral time scales:\nexperimental support of neohebbian\nthree-factor learning rules\n\nwulfram gerstner*, marco lehmann, vasiliki liakoni, dane corneil and johanni brea\n\nschool of computer science and school of life sciences, \u00e9cole polytechnique f\u00e9d\u00e9rale de lausanne, lausanne, switzerland\n\nmost elementary behaviors such as moving the arm to grasp an object or walking into\nthe next room to explore a museum evolve on the time scale of seconds; in contrast,\nneuronal action potentials occur on the time scale of a few milliseconds. learning rules\nof the brain must therefore bridge the gap between these two different time scales.\nmodern theories of synaptic plasticity have postulated that the co-activation of pre- and\npostsynaptic neurons sets a \ufb02ag at the synapse, called an eligibility trace, that leads to a\nweight change only if an additional factor is present while the \ufb02ag is set. this thir", "the journal of neuroscience, october 15, 1998, 18(20):8455\u20138466\n\nspatial firing properties of hippocampal ca1 populations in an\nenvironment containing two visually identical regions\n\nwilliam e. skaggs and bruce l. mcnaughton\narizona research laboratories, division of neural systems, memory and aging, university of arizona, tucson,\narizona 85724\n\npopulations of 10-39 ca1 pyramidal cells were recorded from\nfour rats foraging for food reward in an environment consisting\nof two nearly identical boxes connected by a corridor. for each\nrat, a higher-than-chance fraction of cells had similarly shaped\nspatial \ufb01ring \ufb01elds in both boxes, but other cells had completely\ndifferent \ufb01elds in the two boxes. the level of correlation of \ufb01elds\nin the two boxes differed greatly across rats and, for three of the\nfour rats, across recording sessions. thus, the factors control-\nling the level of correlation are likely to be subtle. two control\nmanipulations were performed. first, the two boxes were phys-\nica", "4\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n5\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n5\nv\n0\n5\n8\n0\n\n.\n\n8\n0\n3\n1\n:\nv\ni\nx\nr\na\n\ngenerating sequences with\nrecurrent neural networks\n\nalex graves\n\ndepartment of computer science\n\nuniversity of toronto\n\ngraves@cs.toronto.edu\n\nabstract\n\nthis paper shows how long short-term memory recurrent neural net-\nworks can be used to generate complex sequences with long-range struc-\nture, simply by predicting one data point at a time. the approach is\ndemonstrated for text (where the data are discrete) and online handwrit-\ning (where the data are real-valued). it is then extended to handwriting\nsynthesis by allowing the network to condition its predictions on a text\nsequence. the resulting system is able to generate highly realistic cursive\nhandwriting in a wide variety of styles.\n\n1\n\nintroduction\n\nrecurrent neural networks (rnns) are a rich class of dynamic models that have\nbeen used to generate sequences in domains as diverse as music [6, 4], text [30]\nand motion capture data [29]. rnns can be t", "0\n2\n0\n2\n\n \n\nb\ne\nf\n7\n1\n\n \n\n \n \n]\nt\ns\nh\nt\na\nm\n\n.\n\n[\n \n \n\n3\nv\n1\n9\n1\n2\n1\n\n.\n\n4\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nlinearized two-layers neural networks in high dimension\n\nbehrooz ghorbani\u2217, song mei\u2020, theodor misiakiewicz\u2021, andrea montanari\u00a7\n\nfebruary 18, 2020\n\nabstract\n\nwe consider the problem of learning an unknown function f(cid:63) on the d-dimensional sphere\nwith respect to the square loss, given i.i.d. samples {(yi, xi)}i\u2264n where xi is a feature vector\nuniformly distributed on the sphere and yi = f(cid:63)(xi) + \u03b5i. we study two popular classes of\nmodels that can be regarded as linearizations of two-layers neural networks around a random\ninitialization: the random features model of rahimi-recht (rf); the neural tangent kernel model\nof jacot-gabriel-hongler (nt). both these approaches can also be regarded as randomized\napproximations of kernel ridge regression (with respect to di\ufb00erent kernels), and enjoy universal\napproximation properties when the number of neurons n diverges, for a \ufb01xed dimension d", "neural tuning and representational \ngeometry\n\nnikolaus\u00a0kriegeskorte \n\n  and xue- xin\u00a0wei \n\n \n\nabstract | a central goal of neuroscience is to understand the representations \nformed by brain activity patterns and their connection to behaviour. the classic \napproach is to investigate how individual neurons encode stimuli and how their \ntuning determines the fidelity of the neural representation. tuning analyses often \nuse the fisher information to characterize the sensitivity of neural responses to \nsmall changes of the stimulus. in recent decades, measurements of large \npopulations of neurons have motivated a complementary approach, which focuses \non the information available to linear decoders. the decodable information is \ncaptured by the geometry of the representational patterns in the multivariate \nresponse space. here we review neural tuning and representational geometry with \nthe goal of clarifying the relationship between them. the tuning induces the \ngeometry, but different sets", "new types of deep neural network learning for speech recognition \n\nand related applications: an overview  \n\n \n\nli deng1, geoffrey hinton2, and brian kingsbury3 \n\n1microsoft research, redmond, wa, usa  \n2university of toronto, ontario, canada  \n\n3ibm t. j. watson research center, yorktown heights, ny, usa \n\nabstract \n\n \nin  this  paper,  we  provide  an  overview  of  the  invited  and \ncontributed  papers  presented  at  the  special  session  at  icassp-\n2013,  entitled  \u201cnew  types  of  deep  neural  network  learning  for \nspeech  recognition  and  related  applications,\u201d  as  organized  by \nthe  authors.  we  also  describe  the  historical  context  in  which \nacoustic  models  based  on  deep  neural  networks  have  been \ndeveloped. \n     the  technical  overview  of  the  papers  presented  in  our  special \nsession  is  organized  into  five  ways  of  improving  deep  learning \nmethods:  (1)  better  optimization;  (2)  better  types  of  neural \nactivation  function  and  be", "opportunities for neuromorphic computing \nalgorithms and applications\n\ncatherine d. schuman\u200a\nand bill kay1\n\n\u200a1,2\u2009\u2709, shruti r. kulkarni1, maryam parsa1,3, j. parker mitchell1, prasanna date\u200a\n\n\u200a1  \n\nneuromorphic computing technologies will be important for the future of computing, but much of the work in neuromorphic \ncomputing has focused on hardware development. here, we review recent results in neuromorphic computing algorithms and \napplications. we highlight characteristics of neuromorphic computing technologies that make them attractive for the future of \ncomputing and we discuss opportunities for future development of algorithms and applications on these systems.\n\nwith  the  end  of  moore\u2019s  law  approaching  and  dennard \n\nscaling  ending,  the  computing  community  is  increas-\ningly looking at new technologies to enable continued \nperformance  improvements.  neuromorphic  computers  are  one \nsuch  new  computing  technology.  the  term  neuromorphic  was \ncoined by carver mea", "vision research 49 (2009) 1295\u20131306\n\ncontents lists available at sciencedirect\n\nvision research\n\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / v i s r e s\n\nbayesian surprise attracts human attention\nlaurent itti a,*, pierre baldi b,1\na computer science department and neuroscience graduate program, university of southern california, hedco neuroscience building, 3641 watt way,\nhnb-30a, los angeles, ca 90089, usa\nb computer science department and institute for genomics and bioinformatics, university of california, irvine, irvine, ca 92697-3425, usa\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 3 october 2007\nreceived in revised form 2 september 2008\n\nkeywords:\nattention\nsurprise\nbayes theorem\ninformation theory\neye movements\nnatural vision\nfree viewing\nsaliency\nnovelty\n\nwe propose a formal bayesian de\ufb01nition of surprise to capture subjective aspects of sensory information.\nsurprise measures how data affects an observer, in terms of d", "elifesciences.org\n\nresearch article\n\nreconceiving the hippocampal map as a \ntopological template\nyuri dabaghian1,2*, vicky l brandt1,2, loren m frank3,4\n\n1the jan and dan duncan neurological research institute at texas children's \nhospital, houston, united states; 2baylor college of medicine, houston, united \nstates; 3sloan-swartz center for theoretical neurobiology, w.m. keck center for \nintegrative neuroscience, university of california, san francisco, san francisco, \nunited states; 4department of physiology, university of california, san francisco, \nsan francisco, united states\n\nabstract the role of the hippocampus in spatial cognition is incontrovertible yet controversial. \nplace cells, initially thought to be location-specifiers, turn out to respond promiscuously to a wide \nrange of stimuli. here we test the idea, which we have recently demonstrated in a computational \nmodel, that the hippocampal place cells may ultimately be interested in a space's topological \nqualities (its con", "0\n2\n0\n2\n\n \n\nb\ne\nf\n0\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n2\n7\n5\n7\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nneural tangent kernel:\n\nconvergence and generalization in neural networks\n\narthur jacot\n\n\u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\narthur.jacot@netopera.net\n\nimperial college london and \u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\nfranck gabriel\n\nfranckrgabriel@gmail.com\n\ncl\u00b4ement hongler\n\n\u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\nclement.hongler@gmail.com\n\nabstract\n\nat initialization, arti\ufb01cial neural networks (anns) are equivalent to gaussian\nprocesses in the in\ufb01nite-width limit (16; 4; 7; 13; 6), thus connecting them to\nkernel methods. we prove that the evolution of an ann during training can also\nbe described by a kernel: during gradient descent on the parameters of an ann,\nthe network function f\u03b8 (which maps input vectors to output vectors) follows the\nkernel gradient of the functional cost (which is convex, in contrast to the parameter\ncost) w.r.t. a new kernel: the neural tangent kerne", "article\n\nreceived 7 jan 2016 | accepted 16 sep 2016 | published 8 nov 2016\n\ndoi: 10.1038/ncomms13276\n\nopen\n\nrandom synaptic feedback weights support error\nbackpropagation for deep learning\ntimothy p. lillicrap1,2, daniel cownden3, douglas b. tweed4,5 & colin j. akerman1\n\nthe brain processes information through multiple layers of neurons. this deep architecture is\nrepresentationally powerful, but complicates learning because it is dif\ufb01cult to identify the\nresponsible neurons when a mistake is made. in machine learning, the backpropagation\nalgorithm assigns blame by multiplying error signals with all the synaptic weights on each\nneuron\u2019s axon and further downstream. however, this involves a precise, symmetric backward\nconnectivity pattern, which is thought to be impossible in the brain. here we demonstrate\nthat this strong architectural constraint is not required for effective error propagation.\nwe present a surprisingly simple mechanism that assigns blame by multiplying errors by\neven r", "biorxiv preprint \nthe copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\nhttps://doi.org/10.1101/2021.11.08.467806\n; \n\nthis version posted september 25, 2022. \n\ndoi: \n\navailable under a\n\ncc-by-nc-nd 4.0 international license\n.\n\nparametric control of \ufb02exible timing through low-dimensional neural\nmanifolds\nmanuel beiran1, 2,\u2021, nicolas meirhaeghe3, 4,\u2021, hansem sohn5, mehrdad jazayeri5, 6,\u2217,\nsrdjan ostojic1,\u2217\n1 laboratoire de neurosciences cognitives et computationnelles, inserm u960, ecole normale su-\nperieure - psl university, 75005 paris, france.\n2 zuckerman mind brain behavior institute, columbia university, new york, ny 10027, usa.\n3 harvard-mit division of health sciences and technology, massachusetts institute of technology,\ncambridge, ma 02139, usa.\n4 institut de neurosciences de la timone (int), umr 7289, cnrs, aix-marseille universit\u00b4e, mar-\nseille ", "predictive coding: a fresh view of inhibition in the retina \nauthor(s): m. v. srinivasan, s. b. laughlin and a. dubs \nsource: proceedings of the royal society of london. series b, biological sciences, nov. \n22, 1982, vol. 216, no. 1205 (nov. 22, 1982), pp. 427-459\npublished by: royal society \n\n \n\nstable url: https://www.jstor.org/stable/35861\n\njstor is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide \nrange of content in a trusted digital archive. we use information technology and tools to increase productivity and \nfacilitate new forms of scholarship. for more information about jstor, please contact support@jstor.org. \n \nyour use of the jstor archive indicates your acceptance of the terms & conditions of use, available at \nhttps://about.jstor.org/terms\n\nroyal society is collaborating with jstor to digitize, preserve and extend access to \nproceedings of the royal society of london. series b, biological sciences\n\nthis content d", "multi-manifold clustering\n\nyong wang1,2, yuan jiang2, yi wu1, and zhi-hua zhou2\n\n1 department of mathematics and systems science\n\nnational university of defense technology, changsha 410073, china\n\nyongwang82@gmail.com, wuyi work@sina.com\n\n2 national key laboratory for novel software technology\n\nnanjing university, nanjing 210093, china\n\n{jiangy, zhouzh}@lamda.nju.edu.cn\n\nabstract. manifold clustering, which regards clusters as groups of points around\ncompact manifolds, has been realized as a promising generalization of traditional\nclustering. a number of linear or nonlinear manifold clustering approaches have\nbeen developed recently. although they have attained better performances than\ntraditional clustering methods in many scenarios, most of these approaches suffer\nfrom two weaknesses. first, when the data are drawn from hybrid modeling, i.e.,\nsome data manifolds are separated but some are intersected, existing approaches\ncould not work well although hybrid modeling often appears in r", "the journal of neuroscience, 2000, vol. 20 rc61 1 of 6\n\nselectivity for complex shapes in primate visual area v2\n\njay hegde\u00b4 and david c. van essen\ndepartment of anatomy and neurobiology, washington university school of medicine, st. louis, missouri 63110\n\nto explore the role of visual area v2 in shape analysis, we\nstudied the responses of neurons in area v2 of the alert ma-\ncaque using a set of 128 grating and geometric line stimuli that\nvaried in their shape characteristics and geometric complexity.\nsimple stimuli included oriented bars and sinusoidal gratings;\ncomplex stimuli included angles, arcs, circles, and intersecting\nlines, plus hyperbolic and polar gratings. we found that most v2\ncells responded well to at least some of the complex stimuli,\nand in many v2 cells the most effective complex stimulus\nelicited a signi\ufb01cantly larger response than the most effective\n\nbar or sinusoid. approximately one-third of the v2 cells showed\nsigni\ufb01cant differential responsiveness to various co", "m\no\nc\n\n.\n\ne\nr\nu\nt\na\nn\n\n.\ni\n\nc\ns\no\nr\nu\ne\nn\n\n/\n/\n:\np\nt\nt\nh\n\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n1\n0\n0\n2\n\u00a9\n\n \n\narticles\n\n\u00a9 2001 nature publishing group  http://neurosci.nature.com\n\nin vivo intracellular recording and\nperturbation of persistent activity\nin a neural integrator\n\ne. aksay1,2, g. gamkrelidze1,2, h. s. seung1,3, r. baker2 and d. w. tank1\n\n1 biological computation research department, bell laboratories, lucent technologies, 700 mountain avenue, murray hill, new jersey 07974, usa\n\n2 department of physiology and neuroscience, new york university school of medicine, new york, new york 10016, usa\n\n3 howard hughes medical institute, brain and cognitive sciences department, massachusetts institute of technology, cambridge, massachusetts 02139, usa \n\ncorrespondence should be addressed to d.w.t. (dwtank@lucent.com) or r.b. (bakerr01@endeavor.med.nyu.edu)\n\nto investigate the mechanisms of persistent neural activity, we obtained in vivo intracellular recordings\nfrom ne", "research article summary \u25e5\n\nneural computation\n\nspiking neurons can discover\npredictive features by\naggregate-label learning\n\nrobert g\u00fctig\n\nintroduction: opportunities and dangers\ncan often be predicted on the basis of sensory\nclues. the attack of a predator, for example,\nmay be preceded by the sounds of breaking\ntwigs or whiffs of odor. life is easier if one\nlearns these clues. however, this is difficult\nwhen clues are hidden within distracting\nstreams of unrelated sensory activity. even\nworse, they can be separated from the events\nthat they predict by long and variable delays.\nto discover those clues, a learning procedure\nmust bridge the gap between the short epochs\nwithin which clues occur and the time when\nfeedback arrives. this \u201ctemporal credit-assignment\nproblem\u201d is a core challenge in biological and\nmachine learning.\n\nrationale: a neural detector of a sensory\nclue should fire whenever the clue occurs but\n\nremain silent otherwise. hence, the number of\noutput spikes of this neuron", "journal of computational neuroscience (2021) 49:107\u2013127\nhttps://doi.org/10.1007/s10827-021-00780-x\n\noriginal article\n\npredictive coding models  for\u00a0pain perception\n\nyuru\u00a0song1,2\u00a0\u00b7 mingchen\u00a0yao1,3 \namrita\u00a0singh6\u00a0\u00b7 jing\u00a0wang6,7,8 \n\n\u00a0\u00b7 helen\u00a0kemprecos4\u00a0\u00b7 aine\u00a0byrne5 \n\u00a0\u00b7 zhe\u00a0s.\u00a0chen1,7,8 \n\n\u00a0\u00b7 zhengdong\u00a0xiao1\u00a0\u00b7 qiaosheng\u00a0zhang6 \n\n\u00a0\u00b7 \n\nreceived: 1 september 2020 / revised: 14 december 2020 / accepted: 29 january 2021 \n\u00a9 the author(s), under exclusive licence to springer science+business media, llc part of springer nature 2021\n\n/ published online: 17 february 2021\n\nabstract\npain is a complex, multidimensional experience that involves dynamic interactions between sensory-discriminative and \naffective-emotional processes. pain experiences have a high degree of variability depending on their context and prior \nanticipation. viewing pain perception as a perceptual inference problem, we propose a predictive coding paradigm to \ncharacterize evoked and non-evoked pain. we record the local field pote", "article\n\nreceived 17 mar 2016 | accepted 6 jan 2017 | published 20 feb 2017\n\ndoi: 10.1038/ncomms14531\n\nopen\n\nplace cells are more strongly tied to landmarks\nin deep than in super\ufb01cial ca1\ntristan geiller1,2, mohammad fattahi1,3, june-seek choi2 & se\u00b4bastien royer1,3\n\nenvironmental cues affect place cells responses, but whether this information is integrated\nversus segregated in distinct hippocampal cell populations is unclear. here, we show that, in\nmice running on a treadmill enriched with visual-tactile landmarks, place cells are\nmore strongly controlled by landmark-associated sensory inputs in deeper regions of\nca1 pyramidal layer (ca1d). many cells in ca1d display several \ufb01ring \ufb01elds correlated with\nlandmarks, mapping positions slightly before or within the landmarks. supporting direct\ninvolvement of sensory inputs, their \ufb01ring \ufb01elds show instantaneous responses to landmark\nmanipulations, persist through change of context, and encode landmark identity and saliency.\nin contrast, cel", "minimum sharpness: scale-invariant parameter-robustness of neural networks\n\n1\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n6\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n2\n1\n6\n2\n1\n\n.\n\n6\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nhikaru ibayashi 1 takuo hamaguchi 2 masaaki imaizumi 2\n\nabstract\n\ntoward achieving robust and defensive neural\nnetworks, the robustness against the weight pa-\nrameters perturbations, i.e., sharpness, attracts\nattention in recent years (sun et al., 2020). how-\never, sharpness is known to remain a critical is-\nsue, \u201cscale-sensitivity.\u201d in this paper, we propose\na novel sharpness measure, minimum sharpness.\nit is known that nns have a speci\ufb01c scale transfor-\nmation that constitutes equivalent classes where\nfunctional properties are completely identical, and\nat the same time, their sharpness could change\nunlimitedly. we de\ufb01ne our sharpness through a\nminimization problem over the equivalent nns\nbeing invariant to the scale transformation. we\nalso develop an ef\ufb01cient and exact technique to\nmake the sharpness tractable, which redu", "12978 \u2022 the journal of neuroscience, september 29, 2010 \u2022 30(39):12978 \u201312995\n\nbehavioral/systems/cognitive\n\nselectivity and tolerance (\u201cinvariance\u201d) both increase as\nvisual information propagates from cortical area v4 to it\n\nnicole c. rust1,2,3 and james j. dicarlo1,2\n1mcgovern institute for brain research and 2department brain and cognitive, sciences, massachusetts institute of technology, cambridge, massachusetts\n02139, and 3department psychology, university of pennsylvania, philadelphia, pennsylvania 19104\n\nour ability to recognize objects despite large changes in position, size, and context is achieved through computations that are\nthought to increase both the shape selectivity and the tolerance (\u201cinvariance\u201d) of the visual representation at successive stages of\nthe ventral pathway [visual cortical areas v1, v2, and v4 and inferior temporal cortex (it)]. however, these ideas have proven\ndifficult to test. here, we consider how well population activity patterns at two stages of the", "published as a conference paper at iclr 2021\n\nan image is worth 16x16 words:\ntransformers for image recognition at scale\n\nalexey dosovitskiy\u2217,\u2020, lucas beyer\u2217, alexander kolesnikov\u2217, dirk weissenborn\u2217,\nxiaohua zhai\u2217, thomas unterthiner, mostafa dehghani, matthias minderer,\n\ngeorg heigold, sylvain gelly, jakob uszkoreit, neil houlsby\u2217,\u2020\n\n\u2217equal technical contribution, \u2020equal advising\n\n1\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n3\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n9\n2\n9\n1\n1\n\n.\n\n0\n1\n0\n2\n:\nv\ni\nx\nr\na\n\ngoogle research, brain team\n\n{adosovitskiy, neilhoulsby}@google.com\n\nabstract\n\nwhile the transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. in\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. we show that this reliance on cnns is not necessary\nand a pure transformer applied directly to", "managing competing goals \u2014 a key \nrole for the frontopolar cortex\n\nfarshad alizadeh mansouri1\u20133*, etienne koechlin4*, marcello g.\u00a0p.\u00a0rosa1,2  \nand mark j.\u00a0buckley5\nabstract | humans are set apart from other animals by many elements of advanced cognition and \nbehaviour, including language, judgement and reasoning. what is special about the human brain \nthat gives rise to these abilities? could the foremost part of the prefrontal cortex (the frontopolar \ncortex), which has become considerably enlarged in humans during evolution compared with \nother animals, be important in this regard, especially as, in primates, it contains a unique \ncytoarchitectural field, area 10? the first studies of the function of the frontopolar cortex in \nmonkeys have now provided critical new insights about its precise role in monitoring the \nsignificance of current and alternative goals. in human evolution, the frontopolar cortex may \nhave acquired a further role in enabling the monitoring of the significance ", "learning and planning in average-reward markov decision processes\n\nyi wan * 1 abhishek naik * 1 richard s. sutton 1 2\n\nabstract\n\nwe introduce learning and planning algorithms\nfor average-reward mdps, including 1) the \ufb01rst\ngeneral proven-convergent off-policy model-free\ncontrol algorithm without reference states, 2) the\n\ufb01rst proven-convergent off-policy model-free pre-\ndiction algorithm, and 3) the \ufb01rst off-policy learn-\ning algorithm that converges to the actual value\nfunction rather than to the value function plus\nan offset. all of our algorithms are based on us-\ning the temporal-difference error rather than the\nconventional error when updating the estimate of\nthe average reward. our proof techniques are a\nslight generalization of those by abounadi, bert-\nsekas, and borkar (2001). in experiments with an\naccess-control queuing task, we show some of\nthe dif\ufb01culties that can arise when using methods\nthat rely on reference states and argue that our\nnew algorithms can be signi\ufb01cantly easie", "article\n\nreceived 31 may 2016 | accepted 3 aug 2016 | published 20 sep 2016\n\ndoi: 10.1038/ncomms12815\n\nopen\n\na dendritic disinhibitory circuit mechanism for\npathway-speci\ufb01c gating\nguangyu robert yang1, john d. murray1,2 & xiao-jing wang1,3\n\nwhile reading a book in a noisy cafe\u00b4, how does your brain \u2018gate in\u2019 visual information while\n\ufb01ltering out auditory stimuli? here we propose a mechanism for such \ufb02exible routing of\ninformation \ufb02ow in a complex brain network (pathway-speci\ufb01c gating), tested using a\nnetwork model of pyramidal neurons and three classes of interneurons with connection\nprobabilities constrained by data. we \ufb01nd that if inputs from different pathways cluster on a\npyramidal neuron dendrite, a pathway can be gated-on by a disinhibitory circuit motif. the\nbranch-speci\ufb01c disinhibition can be achieved despite dense interneuronal connectivity, even\nwith random connections. moreover, clustering of input pathways on dendrites can naturally\nemerge through synaptic plasticity regula", "the functional organization of cortical feedback \ninputs to primary visual cortex\n\ntiago marques\u200a\n\n\u200a1,2, julia nguyen\u200a\n\n\u200a1,2, gabriela fioreze1 and leopoldo petreanu\u200a\n\n\u200a1*\n\ncortical feedback is thought to mediate cognitive processes like attention, prediction, and awareness. understanding its function \nrequires identifying the organizational logic of feedback axons relaying different signals. we measured retinotopic specificity \nin inputs from the lateromedial visual area in mouse primary visual cortex (v1) by mapping receptive fields in feedback boutons \nand relating them to those of neurons in their vicinity. lateromedial visual area inputs in layer 1 targeted, on average, retinotopi-\ncally matched locations in v1, but many of them relayed distal visual information. orientation-selective axons overspread around \nthe retinotopically matched location perpendicularly to their preferred orientation. direction-selective axons were biased to \nvisual areas shifted from the retinotopically m", "learning universal policies via text-guided video generation\n\nyilun du * 1 2 mengjiao yang * 3 2 bo dai 2 hanjun dai 2 o\ufb01r nachum 2\n\njoshua b. tenenbaum 1 dale schuurmans 2 4 pieter abbeel 3\n\n3\n2\n0\n2\n\n \n\nb\ne\nf\n2\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n2\nv\n1\n1\n1\n0\n0\n\n.\n\n2\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\na goal of arti\ufb01cial intelligence is to construct\nan agent that can solve a wide variety of tasks.\nrecent progress in text-guided image synthesis\nhas yielded models with an impressive ability to\ngenerate complex novel images, exhibiting combi-\nnatorial generalization across domains. motivated\nby this success, we investigate whether such tools\ncan be used to construct more general-purpose\nagents. speci\ufb01cally, we cast the sequential deci-\nsion making problem as a text-conditioned video\ngeneration problem, where, given a text-encoded\nspeci\ufb01cation of a desired goal, a planner synthe-\nsizes a set of future frames depicting its planned\nactions in the future, after which control actions\nare extracted from the ", "article\n\ncommunicated by christian machens\n\nsequential optimal design of neurophysiology experiments\n\njeremy lewi\njeremy@lewi.us\nbioengineering graduate program, wallace h. coulter department of biomedical\nengineering, laboratory for neuroengineering, georgia institute of technology,\natlanta, ga 30332, u.s.a. http://www.lewilab.org\n\nrobert butera\nrbutera@ece.gatech.edu\nschool of electrical and computer engineering, laboratory for neuroengineering,\ngeorgia institute of technology, atlanta, ga 30332, u.s.a.\n\nliam paninski\nliam@stat.columbia.edu\ndepartment of statistics and center for neurotheory, columbia university,\nnew york, ny 10027, u.s.a. http://www.stat.columbia.edu/\u223cliam\nadaptively optimizing experiments has the potential to signi\ufb01cantly\nreduce the number of trials needed to build parametric statistical models\nof neural systems. however, application of adaptive methods to neuro-\nphysiology has been limited by severe computational challenges. since\nmost neurons are high-dimensional", "7\n1\n0\n2\n\n \n\nn\na\nj\n \n\n3\n1\n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n3\nv\n3\n7\n6\n3\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nunder review as a conference paper at iclr 2017\n\nlearning to navigate\nin complex environments\n\npiotr mirowski\u2217, razvan pascanu\u2217, fabio viola, hubert soyer, andrew j. ballard,\nandrea banino, misha denil, ross goroshin, laurent sifre, koray kavukcuoglu,\ndharshan kumaran, raia hadsell\n\ndeepmind\nlondon, uk\n\n{piotrmirowski, razp, fviola, soyer, aybd, abanino, mdenil, goroshin, sifre,\nkorayk, dkumaran, raia} @google.com\n\nabstract\n\nlearning to navigate in complex environments with dynamic elements is an impor-\ntant milestone in developing ai agents. in this work we formulate the navigation\nquestion as a reinforcement learning problem and show that data ef\ufb01ciency and task\nperformance can be dramatically improved by relying on additional auxiliary tasks\nleveraging multimodal sensory inputs. in particular we consider jointly learning\nthe goal-driven reinforcement learning problem with auxiliary depth predict", "an analysis of reinforcement learning with function approximation\n\nfrancisco s. melo\ncarnegie mellon university, pittsburgh, pa 15213, usa\n\nsean p. meyn\ncoordinated science lab, urbana, il 61801, usa\n\nm. isabel ribeiro\ninstitute for systems and robotics, 1049-001 lisboa, portugal\n\nfmelo@cs.cmu.edu\n\nmeyn@control.csl.uiuc.edu\n\nmir@isr.ist.utl.pt\n\nabstract\n\nwe address the problem of computing the\noptimal q-function in markov decision prob-\nlems with in\ufb01nite state-space. we analyze\nthe convergence properties of several vari-\nations of q-learning when combined with\nfunction approximation, extending the anal-\nysis of td-learning in (tsitsiklis & van roy,\n1996a) to stochastic control settings. we\nidentify conditions under which such approx-\nimate methods converge with probability 1.\nwe conclude with a brief discussion on the\ngeneral applicability of our results and com-\npare them with several related works.\n\n1. introduction\nconvergence of q-learning with function approxima-\ntion has been a lo", "functional network reorganization during learning in\na brain-computer interface paradigm\n\nbeata jarosiewicza,b,1,2, steven m. chasea,b,c,1, george w. frasera,b, meel vellistea,b, robert e. kassb,c,\nand andrew b. schwartza,b,3\n\nadepartment of neurobiology, university of pittsburgh, pittsburgh, pa 15213; cdepartment of statistics, carnegie mellon university, pittsburgh, pa 15213;\nand bcenter for the neural basis of cognition, university of pittsburgh and carnegie mellon university\n\nedited by j. anthony movshon, new york university, new york, ny, and approved october 17, 2008 (received for review august 15, 2008)\n\nefforts to study the neural correlates of learning are hampered\nby the size of the network in which learning occurs. to under-\nstand the importance of learning-related changes in a network\nof neurons, it is necessary to understand how the network acts\nas a whole to generate behavior. here we introduce a paradigm\nin which the output of a cortical network can be perturbed\ndirectly", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/083857\n; \n\nthis version posted august 9, 2017. \n\nthe copyright holder for this preprint (which was not\n\ncertified by peer review) is the author/funder. all rights reserved. no reuse allowed without permission. \n\n1\t\n\n\t\n\n \n\n \n \n \n \n \npredictive  representations  can  link  model-based  reinforcement  learning  to  model-free \nmechanisms \n \nevan  m.  russek1*\u00b6,  ida  momennejad2\u00b6,  matthew  m.  botvinick3,  samuel  j.  gershman4, \nnathaniel d. daw2 \n \n \n1 center for neural science, new york university, new york, ny, united states of america \n \n2  princeton  neuroscience  institute  and  department  of  psychology,  princeton  university, \nprinceton, nj, united states of america \n \n3 google deepmind, london, united kingdom \n \n4 department of psychology and center for brain science, harvard university, cambridge, ma, \nunited states of america  \n \n* corresponding author \ne-mail: emr443@nyu.edu \n \n\u00b6 emr and im contributed equally to this work ", "8\n1\n0\n2\n\n \nt\nc\no\n3\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n6\n3\n1\n2\n0\n\n.\n\n0\n1\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\na baseline for detecting misclassified and\nout-of-distribution examples\nin neural networks\n\ndan hendrycks\u2217\nuniversity of california, berkeley\nhendrycks@berkeley.edu\n\nkevin gimpel\ntoyota technological institute at chicago\nkgimpel@ttic.edu\n\nabstract\n\nwe consider the two related problems of detecting if an example is misclassi\ufb01ed or\nout-of-distribution. we present a simple baseline that utilizes probabilities from\nsoftmax distributions. correctly classi\ufb01ed examples tend to have greater maxi-\nmum softmax probabilities than erroneously classi\ufb01ed and out-of-distribution ex-\namples, allowing for their detection. we assess performance by de\ufb01ning sev-\neral tasks in computer vision, natural language processing, and automatic speech\nrecognition, showing the effectiveness of this baseline across all. we then show\nthe baseline can sometimes be surpassed, demonstrating t", "recurrent orthogonal networks and long-memory tasks\n\nmikael henaff\nnew york university, facebook ai research\narthur szlam\nfacebook ai research\nyann lecun\nnew york university, facebook ai research\n\nabstract\n\nalthough rnns have been shown to be power-\nful tools for processing sequential data, \ufb01nding\narchitectures or optimization strategies that al-\nlow them to model very long term dependencies\nis still an active area of research. in this work,\nwe carefully analyze two synthetic datasets orig-\ninally outlined in (hochreiter & schmidhuber,\n1997) which are used to evaluate the ability of\nrnns to store information over many time steps.\nwe explicitly construct rnn solutions to these\nproblems, and using these constructions, illumi-\nnate both the problems themselves and the way in\nwhich rnns store different types of information\nin their hidden states. these constructions fur-\nthermore explain the success of recent methods\nthat specify unitary initializations or constraints\non the transition mat", "on the di\ufb03culty of training recurrent neural networks\n\nrazvan pascanu\nuniversit\u00b4e de montr\u00b4eal, 2920, chemin de la tour, montr\u00b4eal, qu\u00b4ebec, canada, h3t 1j8\n\npascanur@iro.umontreal.ca\n\ntomas mikolov\nspeech@fit, brno university of technology, brno, czech republic\n\nt.mikolov@gmail.com\n\nyoshua bengio\nuniversit\u00b4e de montr\u00b4eal, 2920, chemin de la tour, montr\u00b4eal, qu\u00b4ebec, canada, h3t 1j8\n\nyoshua.bengio@umontreal.ca\n\nabstract\n\nthere are two widely known issues with prop-\nerly training recurrent neural networks, the\nvanishing and the exploding gradient prob-\nlems detailed in bengio et al. (1994).\nin\nthis paper we attempt to improve the under-\nstanding of the underlying issues by explor-\ning these problems from an analytical, a geo-\nmetric and a dynamical systems perspective.\nour analysis is used to justify a simple yet ef-\nfective solution. we propose a gradient norm\nclipping strategy to deal with exploding gra-\ndients and a soft constraint for the vanishing\ngradients problem. we validate emp", "understanding contrastive representation learning through\n\nalignment and uniformity on the hypersphere\n\ntongzhou wang 1 phillip isola 1\n\nabstract\n\ncontrastive representation learning has been out-\nstandingly successful in practice. in this work,\nwe identify two key properties related to the con-\ntrastive loss: (1) alignment (closeness) of features\nfrom positive pairs, and (2) uniformity of the in-\nduced distribution of the (normalized) features on\nthe hypersphere. we prove that, asymptotically,\nthe contrastive loss optimizes these properties,\nand analyze their positive effects on downstream\ntasks. empirically, we introduce an optimizable\nmetric to quantify each property. extensive exper-\niments on standard vision and language datasets\ncon\ufb01rm the strong agreement between both met-\nrics and downstream task performance. directly\noptimizing for these two metrics leads to repre-\nsentations with comparable or better performance\nat downstream tasks than contrastive learning.\n\nproject page: ss", "6\n1\n0\n2\n\n \nr\np\na\n3\n2\n\n \n\n \n \n]\n\n.\n\nc\no\nh\nt\na\nm\n\n[\n \n \n\n2\nv\n6\n9\n0\n6\n0\n\n.\n\n0\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nwhen are nonconvex problems not scary?\n\nju sun, qing qu, and john wright\n\n{js4038, qq2105, jw2966}@columbia.edu\n\ndepartment of electrical engineering, columbia university, new york, usa\n\noctober 20, 2015 revised: april 26, 2016\n\nabstract\n\nin this note, we focus on smooth nonconvex optimization problems that obey: (1) all local\nminimizers are also global; and (2) around any saddle point or local maximizer, the objective has\na negative directional curvature. concrete applications such as dictionary learning, generalized\nphase retrieval, and orthogonal tensor decomposition are known to induce such structures. we\ndescribe a second-order trust-region algorithm that provably converges to a global minimizer\ne\ufb03ciently, without special initializations. finally we highlight alternatives, and open problems\nin this direction.\n\n1 introduction\ngeneral nonconvex optimization problems (henceforth \u201cncvx proble", "9\n1\n0\n2\n\n \nr\na\n\n \n\nm\n6\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n9\n9\n5\n1\n0\n\n.\n\n3\n0\n9\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2019\n\nlearning dynamics model in reinforcement\nlearning by incorporating the long term fu-\nture\n\nnan rosemary ke+ \u2021 \u2663, amanpreet singh\u2665, ahmed touati+ \u2665, anirudh goyal+,\nyoshua bengio+\u00b6, devi parikh\u2665\u2666 & dhruv batra\u2665\u2666\n\nabstract\n\nin model-based reinforcement learning, the agent interleaves between model\nlearning and planning. these two components are inextricably intertwined.\nif\nthe model is not able to provide sensible long-term prediction, the executed plan-\nner would exploit model \ufb02aws, which can yield catastrophic failures. this paper\nfocuses on building a model that reasons about the long-term future and demon-\nstrates how to use this for ef\ufb01cient planning and exploration. to this end, we\nbuild a latent-variable autoregressive model by leveraging recent ideas in varia-\ntional inference. we argue that forcing latent variables to carry future information", "contractive auto-encoders:\n\nexplicit invariance during feature extraction\n\nsalah rifai(1)\npascal vincent(1)\nxavier muller(1)\nxavier glorot(1)\nyoshua bengio(1)\n(1) dept. iro, universit\u00b4e de montr\u00b4eal. montr\u00b4eal (qc), h3c 3j7, canada\n\nrifaisal@iro.umontreal.ca\nvincentp@iro.umontreal.ca\nmullerx@iro.umontreal.ca\nglorotxa@iro.umontreal.ca\nbengioy@iro.umontreal.ca\n\nabstract\n\n1. introduction\n\nwe present in this paper a novel approach\nfor training deterministic auto-encoders. we\nshow that by adding a well chosen penalty\nterm to the classical reconstruction cost func-\ntion, we can achieve results that equal or sur-\npass those attained by other regularized auto-\nencoders as well as denoising auto-encoders\non a range of datasets. this penalty term\ncorresponds to the frobenius norm of the\njacobian matrix of the encoder activations\nwith respect to the input. we show that\nthis penalty term results in a localized space\ncontraction which in turn yields robust fea-\ntures on the activation layer. furthe", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/266254052\n\na critical time window for dopamine actions on the structural plasticity of\ndendritic spines\n\narticle\u00a0\u00a0in\u00a0\u00a0science \u00b7 september 2014\n\ndoi: 10.1126/science.1255514\u00a0\u00b7\u00a0source: pubmed\n\ncitations\n462\n\n6 authors, including:\n\nakiko hayashi-takagi\nriken\n\n63 publications\u00a0\u00a0\u00a04,698 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n930\n\nhidetoshi urakubo\nkyoto university\n\n34 publications\u00a0\u00a0\u00a0809 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nall content following this page was uploaded by akiko hayashi-takagi on 10 september 2015.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nmind  the  last  spike  \u2014  \ufb01ring  rate  models  for  mesoscopic\npopulations  of  spiking  neurons\ntilo  schwalger1,2 and  anton  v  chizhov3,4\n\nthe  dominant  modeling  framework  for  understanding  cortical\ncomputations  are  heuristic  \ufb01ring  rate  models.  despite  their\nsuccess,  these  models  fall  short  to  capture  spike\nsynchronization  effects,  to  link  to  biophysical  parameters  and\nto  describe  \ufb01nite-size  \ufb02uctuations.  in  this  opinion  article,  we\npropose  that  the  refractory  density  method  (rdm),  also  known\nas  age-structured  population  dynamics  or  quasi-renewal\ntheory,  yields  a  powerful  theoretical  framework  to  build  rate-\nbased  models  for  mesoscopic  neural  populations  from  realistic\nneuron  dynamics  at  the  microscopic  level.  we  review  recent\nadvances  achieved  by  the  rdm  to  obtain  ef\ufb01cient  population\ndensity  equations  for  networks  of  generalized  i", "supplementary information for \u201cthe hippocampus as a predictive map\u201d\n\nkimberly l. stachenfeld1,2,*, matthew m. botvinick1,3, samuel j. gershman4\n1deepmind, london, uk\n2princeton neuroscience institute, princeton university, princeton, nj, usa\n3gatsby computational neuroscience unit, university college london, london, uk\n4department of psychology and center for brain science, harvard university, cambridge, ma, usa\n*stachenfeld@google.com\n\n1 predictive temporal codes and the sr\nmany models of prospective coding in the hippocampus have drawn inspiration from the well-documented\nordered temporal structure of \ufb01ring in hippocampus relative to the theta phase1\u20133, and considered the\nmany ways in which replaying hippocampal sweeps during sharp wave ripple events might be used for\nplanning4\u20139. the \ufb01ring of cells in hippocampus is aligned to theta such that cells encoding more distant\nplaces \ufb01re later during a theta cycle than immediately upcoming states (a phenomenon referred to as theta\nprecessi", "large-scale neural recordings call for new insights \nto link brain and behavior\n\nanne e. urai\u200a\n\n\u200a1,2, brent doiron3, andrew m. leifer\u200a\n\n\u200a4 and anne k. churchland\u200a\n\n\u200a1,5\u2009\u2709\n\nneuroscientists today can measure activity from more neurons than ever before, and are facing the challenge of connecting \nthese brain-wide neural recordings to computation and behavior. in the present review, we first describe emerging tools and \ntechnologies being used to probe large-scale brain activity and new approaches to characterize behavior in the context of such \nmeasurements. we next highlight insights obtained from large-scale neural recordings in diverse model systems, and argue that \nsome of these pose a challenge to traditional theoretical frameworks. finally, we elaborate on existing modeling frameworks to \ninterpret these data, and argue that the interpretation of brain-wide neural recordings calls for new theoretical approaches that \nmay depend on the desired level of understanding. these advances i", "research article\n\nneural modularity helps organisms evolve to\nlearn new skills without forgetting old skills\nkai olav ellefsen1, jean-baptiste mouret2,3, jeff clune4*\n\n1 department of computer and information science, norwegian university of science and technology,\ntrondheim, norway, 2 sorbonne universit\u00e9s, upmc univ paris 06, umr 7222, isir, paris, france, 3 cnrs,\numr 7222, isir, paris, france, 4 computer science department, university of wyoming, laramie,\nwyoming, united states of america\n\n* jeffclune@uwyo.edu\n\nabstract\n\nopen access\n\ncitation: ellefsen ko, mouret j-b, clune j (2015)\nneural modularity helps organisms evolve to learn\nnew skills without forgetting old skills. plos\ncomput biol 11(4): e1004128. doi:10.1371/journal.\npcbi.1004128\n\neditor: josh c. bongard, university of vermont,\nunited states\n\nreceived: september 17, 2014\n\naccepted: january 14, 2015\n\npublished: april 2, 2015\n\ncopyright: \u00a9 2015 ellefsen et al. this is an open\naccess article distributed under the terms of the\n", "taming transformers for high-resolution image synthesis\n\npatrick esser*\n\nrobin rombach*\n\nbj\u00a8orn ommer\n\nheidelberg collaboratory for image processing, iwr, heidelberg university, germany\n\n*both authors contributed equally to this work\n\nfigure 1. our approach enables transformers to synthesize high-resolution images like this one, which contains 1280x460 pixels.\n\nabstract\n\ndesigned to learn long-range interactions on sequential\ndata, transformers continue to show state-of-the-art results\non a wide variety of tasks. in contrast to cnns, they contain\nno inductive bias that prioritizes local interactions. this\nmakes them expressive, but also computationally infeasi-\nble for long sequences, such as high-resolution images. we\ndemonstrate how combining the effectiveness of the induc-\ntive bias of cnns with the expressivity of transformers en-\nables them to model and thereby synthesize high-resolution\nimages. we show how to (i) use cnns to learn a context-\nrich vocabulary of image constituents,", "neuron, vol. 36, 955\u2013968, december 5, 2002, copyright \uf8e92002 by cell press\n\nprobabilistic decision making\nby slow reverberation in cortical circuits\n\nxiao-jing wang1\nvolen center for complex systems\nbrandeis university\nwaltham, massachusetts 02254\n\nsummary\n\nrecent physiological studies of alert primates have\nrevealed cortical neural correlates of key steps in a\nperceptual decision-making process. to elucidate\nsynaptic mechanisms of decision making, i investi-\ngated a biophysically realistic cortical network model\nfor a visual discrimination experiment. in the model,\nslow recurrent excitation and feedback inhibition pro-\nduce attractor dynamics that amplify the difference\nbetween conflicting inputs and generates a binary\nchoice. the model is shown to account for salient\ncharacteristics of the observed decision-correlated\nneural activity, as well as the animal\u2019s psychometric\nfunction and reaction times. these results suggest\nthat recurrent excitation mediated by nmda receptors\nprovides a ", "1\n2\n0\n2\n\n \nc\ne\nd\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n4\n3\n0\n0\n0\n\n.\n\n1\n1\n1\n2\n:\nv\ni\nx\nr\na\n\npreprint\n\nneural networks as kernel learners: the\nsilent alignment effect\n\nalexander atanasov\u2217\u00a7\u2021, blake bordelon\u2217\u2020\u2021 & cengiz pehlevan\u2020\u2021\n\u00a7department of physics\n\u2020john a. paulson school of engineering and applied sciences\n\u2021center for brain science\nharvard university\ncambridge, ma 02138, usa\n{atanasov,blake bordelon,cpehlevan}@g.harvard.edu\n\nabstract\n\nneural networks in the lazy training regime converge to kernel machines. can\nneural networks in the rich feature learning regime learn a kernel machine with\na data-dependent kernel? we demonstrate that this can indeed happen due to a\nphenomenon we term silent alignment, which requires that the tangent kernel of\na network evolves in eigenstructure while small and before the loss appreciably\ndecreases, and grows only in overall scale afterwards. we empirically show that\nsuch an effect takes place in homogenous neural networks with small initialization\nand ", "0\n2\n0\n2\n\n \n\np\ne\ns\n8\n1\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n6\n2\n4\n3\n0\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\numap: uniform manifold\n\napproximation and projection for\n\ndimension reduction\n\nleland mcinnes\n\ntu(cid:138)e institute for mathematics and computing\n\nleland.mcinnes@gmail.com\n\njohn healy\n\ntu(cid:138)e institute for mathematics and computing\n\njchealy@gmail.com\n\njames melville\n\njlmelville@gmail.com\n\nseptember 21, 2020\n\nabstract\n\numap (uniform manifold approximation and projection) is a novel\nmanifold learning technique for dimension reduction. umap is constructed\nfrom a theoretical framework based in riemannian geometry and algebraic\ntopology. (cid:140)e result is a practical scalable algorithm that is applicable to\nreal world data. (cid:140)e umap algorithm is competitive with t-sne for visu-\nalization quality, and arguably preserves more of the global structure with\nsuperior run time performance. furthermore, umap has no computational\nrestrictions on embedding dimension, making it viable as a ge", "7\n1\n0\n2\n\n \nr\np\na\n \n6\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n3\n6\n4\n6\n0\n\n.\n\n2\n0\n7\n1\n:\nv\ni\nx\nr\na\n\npredicting non-linear dynamics by stable local learning in\n\na recurrent spiking neural network\n\naditya gilra1,*\n\nwulfram gerstner1\n\n1school of computer and communication sciences, and brain-mind institute, school of life\nsciences, \u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne (epfl), lausanne 1015, switzerland.\n\n\u2217correspondence: aditya.gilra@ep\ufb02.ch\n\nabstract\n\nbrains need to predict how the body reacts to motor commands. it is an open question\nhow networks of spiking neurons can learn to reproduce the non-linear body dynamics\ncaused by motor commands, using local, online and stable learning rules. here, we present\na supervised learning scheme for the feedforward and recurrent connections in a network of\nheterogeneous spiking neurons. the error in the output is fed back through \ufb01xed random\nconnections with a negative gain, causing the network to follow the desired dynamics, while\nan online and loc", "r e v i e w s\n\n*department of brain and \ncognitive sciences and center \nfor visual science, university \nof rochester, rochester, \nnew york 14627, usa. \n\u2021gatsby computational \nneuroscience unit, \nuniversity college london, \n17 queen square, \nlondon wc1n 3ar, uk. \ncorrespondence to a.p. \ne-mail: \nalex@bcs.rochester.edu\ndoi:10.1038/nrn1888\n\n358 | may 2006 | volume 7 \n\nneural correlations, population coding \nand computation\n\nbruno b. averbeck*, peter e. latham\u2021 and alexandre pouget*\nabstract | how the brain encodes information in population activity, and how it combines and \nmanipulates that activity as it carries out computations, are questions that lie at the heart of \nsystems neuroscience. during the past decade, with the advent of multi-electrode recording \nand improved theoretical models, these questions have begun to yield answers. however, a \ncomplete understanding of neuronal variability, and, in particular, how it affects population \ncodes, is missing. this is because variability ", "deeply-supervised nets\n\nchen-yu lee \u2217\n\ndept. of eecs, ucsd\nchl260@ucsd.edu\n\nsaining xie \u2217\n\ndept. of cse and cogsci, ucsd\n\ns9xie@ucsd.edu\n\n4\n1\n0\n2\n\n \n\np\ne\ns\n5\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n5\n8\n1\n5\n\n.\n\n9\n0\n4\n1\n:\nv\ni\nx\nr\na\n\npatrick gallagher\n\ndept. of cogsci, ucsd\nrexaran@gmail.com\n\nzhengyou zhang\nmicrosoft research\n\nzhang@microsoft.com\n\nzhuowen tu \u2020\n\ndept. of cogsci, ucsd\n\nztu@ucsd.edu\n\nabstract\n\nour proposed deeply-supervised nets (dsn) method simultaneously minimizes\nclassi\ufb01cation error while making the learning process of hidden layers direct and\ntransparent. we make an attempt to boost the classi\ufb01cation performance by study-\ning a new formulation in deep networks. three aspects in convolutional neural\nnetworks (cnn) style architectures are being looked at: (1) transparency of the\nintermediate layers to the overall classi\ufb01cation; (2) discriminativeness and robust-\nness of learned features, especially in the early layers; (3) effectiveness in training\ndue to the presence of the", "the limits of color awareness during active,\nreal-world vision\n\nmichael a. cohena,b,1, thomas l. botchc\ue840, and caroline e. robertsonc,1\n\nadepartment of psychology, program in neuroscience, amherst college, amherst, ma 01002; bmcgovern institute for brain research, department of brain\nand cognitive sciences, massachusetts institute of technology, cambridge, ma, 02139; and cdepartment of psychological and brain sciences, dartmouth\ncollege, hanover, nh 03755;\n\nedited by dale purves, duke university, durham, nc, and approved april 21, 2020 (received for review december 18, 2019)\n\ncolor ignites visual experience, imbuing the world with meaning,\nemotion, and richness. as soon as an observer opens their eyes,\nthey have the immediate impression of a rich, colorful experience\nthat encompasses their entire visual world. here, we show that\nthis impression is surprisingly inaccurate. we used head-mounted\nvirtual reality (vr) to place observers in immersive, dynamic real-\nworld environments, which t", "improved denoising diffusion probabilistic models\n\nalex nichol * 1 prafulla dhariwal * 1\n\n1\n2\n0\n2\n\n \n\nb\ne\nf\n8\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n7\n6\n9\n0\n\n.\n\n2\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\ndenoising diffusion probabilistic models (ddpm)\nare a class of generative models which have re-\ncently been shown to produce excellent sam-\nples. we show that with a few simple modi\ufb01-\ncations, ddpms can also achieve competitive log-\nlikelihoods while maintaining high sample quality.\nadditionally, we \ufb01nd that learning variances of\nthe reverse diffusion process allows sampling with\nan order of magnitude fewer forward passes with\na negligible difference in sample quality, which\nis important for the practical deployment of these\nmodels. we additionally use precision and re-\ncall to compare how well ddpms and gans\ncover the target distribution. finally, we show\nthat the sample quality and likelihood of these\nmodels scale smoothly with model capacity and\ntraining compute, making them easily scalable.\nwe re", "integrated morphoelectric and transcriptomic\nclassi\ufb01cation of cortical gabaergic cells\n\narticle\n\ngraphical abstract\n\nauthors\nnathan w. gouwens, staci a. sorensen,\nfahimeh baftizadeh, ..., jim berg,\ngabe j. murphy, hongkui zeng\n\ncorrespondence\nnathang@alleninstitute.org (n.w.g.),\nstacis@alleninstitute.org (s.a.s.),\ngabem@alleninstitute.org (g.j.m.)\n\nin brief\ngabaergic cortical interneurons of the\nmouse visual cortex can be de\ufb01ned into\n28 types based on their morphological,\nelectrophysiological, and transcriptomic\nproperties and are distinguished by their\nlayer-speci\ufb01c axon innervation patterns.\n\nhighlights\nd patch-seq data obtained from >4,200 gabaergic\n\ninterneurons with >500 morphologies\n\nd comprehensive characterization of morphoelectric features\n\nof transcriptomic types\n\nd 28 interneuron met-types with congruent properties across\n\ndata modalities\n\nd different sst met-types preferentially innervate different\n\ncortical layers\n\ngouwens et al., 2020, cell 183, 935\u2013953\nnovember 12, 2020 ", "a r t i c l e s\n\n , matthew m botvinick1\u20133 \n\ndorsal hippocampus contributes to model-based \nplanning\nkevin j miller1 \nplanning can be defined as action selection that leverages an internal model of the outcomes likely to follow each possible action. \nits neural mechanisms remain poorly understood. here we adapt recent advances from human research for rats, presenting for \nthe first time an animal task that produces many trials of planned behavior per session, making multitrial rodent experimental \ntools available to study planning. we use part of this toolkit to address a perennially controversial issue in planning: the role of \nthe dorsal hippocampus. although prospective hippocampal representations have been proposed to support planning, intact \nplanning in animals with damaged hippocampi has been repeatedly observed. combining formal algorithmic behavioral analysis \nwith muscimol inactivation, we provide causal evidence directly linking dorsal hippocampus with planning behavior. our", "8\n1\n0\n2\n\n \nt\nc\no\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n1\n6\n2\n1\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nrelational inductive biases, deep learning, and graph networks\n\npeter w. battaglia1\u2217, jessica b. hamrick1, victor bapst1,\n\nalvaro sanchez-gonzalez1, vinicius zambaldi1, mateusz malinowski1,\nandrea tacchetti1, david raposo1, adam santoro1, ryan faulkner1,\ncaglar gulcehre1, francis song1, andrew ballard1, justin gilmer2,\n\ngeorge dahl2, ashish vaswani2, kelsey allen3, charles nash4,\n\nvictoria langston1, chris dyer1, nicolas heess1,\n\ndaan wierstra1, pushmeet kohli1, matt botvinick1,\n\noriol vinyals1, yujia li1, razvan pascanu1\n\n1deepmind; 2google brain; 3mit; 4university of edinburgh\n\nabstract\n\narti\ufb01cial intelligence (ai) has undergone a renaissance recently, making major progress in\nkey domains such as vision, language, control, and decision-making. this has been due, in\npart, to cheap data and cheap compute resources, which have \ufb01t the natural strengths of deep\nlearning. however, many de\ufb01ning character", "research article\n\nextracting grid cell characteristics from\nplace cell inputs using non-negative\nprincipal component analysis\nyedidyah dordek1,2\u2020, daniel soudry3,4*\u2020, ron meir1, dori derdikman2*\n\n1faculty of electrical engineering, technion \u2013 israel institute of technology, haifa,\nisrael; 2rappaport faculty of medicine and research institute, technion \u2013 israel\ninstitute of technology, haifa, israel; 3department of statistics, columbia\nuniversity, new york, united states; 4center for theoretical neuroscience,\ncolumbia university, new york, united states\n\nabstract many recent models study the downstream projection from grid cells to place cells,\nwhile recent data have pointed out the importance of the feedback projection. we thus asked how\ngrid cells are affected by the nature of the input from the place cells. we propose a single-layer\nneural network with feedforward weights connecting place-like input cells to grid cell outputs.\nplace-to-grid weights are learned via a generalized hebbi", "original article\ntheory\nwhatdoesthefreeenergyprincipletellusabout\nthebrain?\nsamuelj.gershman1\u2217\n1departmentofpsychologyandcenterfor\nbrainscience,harvarduniversity,\ncambridge,ma,02138,usa\ncorrespondence\nnorthwestlaboratories,52oxfordst.,\nroom295.05,cambridge,ma,02138,usa\nemail: gershman@fas.harvard.edu\nfundinginformation\nthisworkwassupportedbyaresearch\nfellowshipfromthealfredp.sloan\nfoundation.\n\nthefreeenergyprinciplehasbeenproposedasaunifying\naccountofbrainfunction. itiscloselyrelated,andinsome\ncasessubsumes,earlierunifyingideassuchasbayesianin-\nference, predictive coding, and active learning. this arti-\ncleclari\ufb01estheseconnections,teasingapartdistinctiveand\nsharedpredictions.\nkeywords\nbayesianbrain,decisiontheory,variationalinference,predictive\ncoding\n\nintroduction\n\n1 |\nthefreeenergyprinciple(fep)states,inanutshell,thatthebrainseekstominimizesurprise[1]. itisarguablythemost\nambitioustheoryofthebrainavailabletoday,claimingtosubsumemanyotherimportantideas,suchaspredictivecoding,\nef\ufb01cient", "1002 \u2022 the journal of neuroscience, january 26, 2005 \u2022 25(4):1002\u20131014\n\nbehavioral/systems/cognitive\n\nangular path integration by moving \u201chill of activity\u201d: a\nspiking neuron model without recurrent excitation of the\nhead-direction system\n\npengcheng song and xiao-jing wang\nvolen center for complex systems, brandeis university, waltham, massachusetts 02454\n\nduring spatial navigation, the head orientation of an animal is encoded internally by neural persistent activity in the head-direction (hd)\nsystem. in computational models, such a bell-shaped \u201chill of activity\u201d is commonly assumed to be generated by recurrent excitation in a\ncontinuous attractor network. recent experimental evidence, however, indicates that hd signal in rodents originates in a reciprocal loop\nbetween the lateral mammillary nucleus (lmn) and the dorsal tegmental nucleus (dtn), which is characterized by a paucity of local\nexcitatory axonal collaterals. moreover, when the animal turns its head to a new direction, the hea", "journal of computer and system sciences (cid:21) ss1504\n\njournal of computer and system sciences 55, 119(cid:21)139 (1997)\narticle no. ss971504\n\na decision-theoretic generalization of on-line learning\n\nand an application to boosting*\n\nyoav freund and robert e. schapire-\n\nat6t labs, 180 park avenue, florham park, new jersey 07932\n\nreceived december 19, 1996\n\nin the first part of the paper we consider the problem of dynamically\napportioning resources among a set of options in a worst-case on-line\nframework. the model we study can be interpreted as a broad, abstract\nextension of the well-studied on-line prediction model to a general\ndecision-theoretic setting. we show that the multiplicative weight-\nupdate littlestone(cid:21)warmuth rule can be adapted to this model, yielding\nbounds that are slightly weaker in some cases, but applicable to a con-\nsiderably more general class of learning problems. we show how the\nresulting learning algorithm can be applied to a variety of problems,\nincludi", "on linear identi\ufb01ability of learned representations\n\ngeoffrey roeder\u2217\nprinceton university\n\ngoogle brain\n\nroeder@princeton.edu\n\nluke metz\ngoogle brain\n\nlmetz@google.com\n\n0\n2\n0\n2\n\n \nl\nu\nj\n \n\n8\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n0\n1\n8\n0\n0\n\n.\n\n7\n0\n0\n2\n:\nv\ni\nx\nr\na\n\ndiederik p. kingma\u2217\n\ngoogle brain\n\ndurk@google.com\n\nabstract\n\nidenti\ufb01ability is a desirable property of a statistical model: it implies that the\ntrue model parameters may be estimated to any desired precision, given suf\ufb01cient\ncomputational resources and data. we study identi\ufb01ability in the context of repre-\nsentation learning: discovering nonlinear data representations that are optimal with\nrespect to some downstream task. when parameterized as deep neural networks,\nsuch representation functions typically lack identi\ufb01ability in parameter space, be-\ncause they are overparameterized by design. in this paper, building on recent\nadvances in nonlinear ica, we aim to rehabilitate identi\ufb01ability by showing that\na large family of discrimi", "\f", "phil. trans. r. soc. b (2009) 364, 1211\u20131221\ndoi:10.1098/rstb.2008.0300\n\npredictive coding under the free-energy principle\n\nkarl friston* and stefan kiebel\n\nthe wellcome trust centre of neuroimaging, institute of neurology, university college london,\n\nqueen square, london wc1n 3bg, uk\n\nthis paper considers prediction and perceptual categorization as an inference problem that is solved\nby the brain. we assume that the brain models the world as a hierarchy or cascade of dynamical\nsystems that encode causal structure in the sensorium. perception is equated with the optimization or\ninversion of these internal models, to explain sensory data. given a model of how sensory data are\ngenerated, we can invoke a generic approach to model inversion, based on a free energy bound on the\nmodel\u2019s evidence. the ensuing free-energy formulation furnishes equations that prescribe the\nprocess of recognition, i.e. the dynamics of neuronal activity that represent the causes of sensory\ninput. here, we focus o", "physical review x 5, 021028 (2015)\n\nmacroscopic description for networks of spiking neurons\n\n1center for brain and cognition, department of information and communication technologies,\n\nernest montbri\u00f3,1 diego paz\u00f3,2 and alex roxin3\n\nuniversitat pompeu fabra, 08018 barcelona, spain\n\n2instituto de f\u00edsica de cantabria (ifca), csic-universidad de cantabria, 39005 santander, spain\n\n3centre de recerca matem\u00e0tica, campus de bellaterra, edifici c, 08193 bellaterra, spain\n\n(received 30 december 2014; published 19 june 2015)\n\na major goal of neuroscience, statistical physics, and nonlinear dynamics is to understand how brain\nfunction arises from the collective dynamics of networks of spiking neurons. this challenge has been\nchiefly addressed through large-scale numerical simulations. alternatively, researchers have formulated\nmean-field theories to gain insight into macroscopic states of large neuronal networks in terms of the\ncollective firing activity of the neurons, or the firing rate. howeve", "7\n1\n0\n2\n\n \nr\na\n\nm\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n7\n8\n8\n0\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nhow to escape saddle points e\ufb03ciently\n\nchi jin\u2217\n\nrong ge\u2020\n\npraneeth netrapalli\u2021\n\nsham m. kakade\u00a7\n\nmichael i. jordan\u00b6\n\nmarch 3, 2017\n\nabstract\n\nthis paper shows that a perturbed form of gradient descent converges to a second-order\nstationary point in a number iterations which depends only poly-logarithmically on dimension\n(i.e., it is almost \u201cdimension-free\u201d). the convergence rate of this procedure matches the well-\nknown convergence rate of gradient descent to \ufb01rst-order stationary points, up to log factors.\nwhen all saddle points are non-degenerate, all second-order stationary points are local minima,\nand our result thus shows that perturbed gradient descent can escape saddle points almost for\nfree.\n\nour results can be directly applied to many machine learning applications, including deep\nlearning. as a particular concrete example of such an application, we show that our results\ncan be used directly", "journal of machine learning research 3 (2003) 993-1022\n\nsubmitted 2/02; published 1/03\n\nlatent dirichlet allocation\n\ndavid m. blei\ncomputer science division\nuniversity of california\nberkeley, ca 94720, usa\nandrew y. ng\ncomputer science department\nstanford university\nstanford, ca 94305, usa\n\nmichael i. jordan\ncomputer science division and department of statistics\nuniversity of california\nberkeley, ca 94720, usa\n\neditor: john lafferty\n\nblei@cs.berkeley.edu\n\nang@cs.stanford.edu\n\njordan@cs.berkeley.edu\n\nabstract\n\nwe describe latent dirichlet allocation (lda), a generative probabilistic model for collections of\ndiscrete data such as text corpora. lda is a three-level hierarchical bayesian model, in which each\nitem of a collection is modeled as a \ufb01nite mixture over an underlying set of topics. each topic is, in\nturn, modeled as an in\ufb01nite mixture over an underlying set of topic probabilities. in the context of\ntext modeling, the topic probabilities provide an explicit representation of a doc", "1\n\ntraining deep architectures without end-to-end\n\nbackpropagation:\n\na survey on the provably optimal methods\n\nshiyu duan, jos\u00b4e c. pr\u00b4\u0131ncipe\n\n2\n2\n0\n2\n\n \n\ng\nu\na\n9\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n9\n1\n4\n3\n0\n\n.\n\n1\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\u2014this tutorial paper surveys provably optimal al-\nternatives to end-to-end backpropagation (e2ebp) \u2014 the de\nfacto standard for training deep architectures. modular training\nrefers to strictly local training without both the forward and\nthe backward pass, i.e., dividing a deep architecture into several\nnonoverlapping modules and training them separately without\nany end-to-end operation. between the fully global e2ebp and\nthe strictly local modular training, there are weakly modular\nhybrids performing training without the backward pass only.\nthese alternatives can match or surpass the performance of\ne2ebp on challenging datasets such as imagenet, and are gain-\ning increasing attention primarily because they offer practical\nadvantages over e2ebp, which wi", "b r i e f c o m m u n i c at i o n s\n\ncocaine-induced decision-\nmaking de\ufb01cits are mediated\nby miscoding in\nbasolateral amygdala\nthomas a stalnaker1, matthew r roesch1, theresa m franz1,\ndonna j calu2, teghpal singh2 & geoffrey schoenbaum1,3,4\n\naddicts and drug-experienced animals have decision-making\nde\ufb01cits in reversal-learning tasks and more complex \u2018gambling\u2019\nvariants. here we show evidence that these de\ufb01cits are\nmediated by persistent encoding of outdated associative\ninformation in the basolateral amygdala. cue-selective neurons\nin the basolateral amygdala, recorded in cocaine-treated rats,\nfailed to change cue preference during reversal learning.\nfurther, the presence of these neurons was critical to\nthe expression of the reversal-learning de\ufb01cit in the\ncocaine-treated rats.\n\naddicts make poor decisions. these de\ufb01cits have been modeled in\naddicts and drug-experienced animals using reversal-learning tasks and\nmore complex \u2018gambling\u2019 variants. in these settings, subjects \ufb01rst learn", "biol cybern (2009) 101:379\u2013385\ndoi 10.1007/s00422-009-0341-6\n\noriginal paper\n\nbiologically plausible learning in neural networks:\na lesson from bacterial chemotaxis\n\nyury p. shimansky\n\nreceived: 10 september 2009 / accepted: 2 october 2009 / published online: 21 october 2009\n\u00a9 springer-verlag 2009\n\nabstract learning processes in the brain are usually asso-\nciated with plastic changes made to optimize the strength\nof connections between neurons. although many details\nrelated to biophysical mechanisms of synaptic plasticity\nhave been discovered, it is unclear how the concurrent per-\nformance of adaptive modi\ufb01cations in a huge number of\nspatial locations is organized to minimize a given objec-\ntive function. since direct experimental observation of even\na relatively small subset of such changes is not feasible,\ncomputational modeling is an indispensable investigation\ntool for solving this problem. however, the conventional\nmethod of error back-propagation (ebp) employed for opti-\nmizing s", "5\n1\n0\n2\n\n \n\nv\no\nn\n0\n3\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\nv\n9\n4\n2\n9\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\non learning to think: algorithmic information theory\n\nfor novel combinations of reinforcement learning\n\ncontrollers and recurrent neural world models\n\ntechnical report\n\nj\u00a8urgen schmidhuber\nthe swiss ai lab\n\nistituto dalle molle di studi sull\u2019intelligenza arti\ufb01ciale (idsia)\n\nuniversit`a della svizzera italiana (usi)\n\nscuola universitaria professionale della svizzera italiana (supsi)\n\ngalleria 2, 6928 manno-lugano, switzerland\n\n30 november 2015\n\nabstract\n\nthis paper addresses the general problem of reinforcement learning (rl) in partially observable\nenvironments. in 2013, our large rl recurrent neural networks (rnns) learned from scratch to\ndrive simulated cars from high-dimensional video input. however, real brains are more powerful\nin many ways. in particular, they learn a predictive model of their initially unknown environment,\nand somehow use it for abstract (e.g., hierarchical) planning and reas", "arti\ufb01cialintelligence112(1999)181\u2013211betweenmdpsandsemi-mdps:aframeworkfortemporalabstractioninreinforcementlearningrichards.suttona,\u2217,doinaprecupb,satindersinghaaat&tlabs.-research,180parkavenue,florhampark,nj07932,usabcomputersciencedepartment,universityofmassachusetts,amherst,ma01003,usareceived1december1998abstractlearning,planning,andrepresentingknowledgeatmultiplelevelsoftemporalabstractionarekey,longstandingchallengesforai.inthispaperweconsiderhowthesechallengescanbeaddressedwithinthemathematicalframeworkofreinforcementlearningandmarkovdecisionprocesses(mdps).weextendtheusualnotionofactioninthisframeworktoincludeoptions\u2014closed-looppoliciesfortakingactionoveraperiodoftime.examplesofoptionsincludepickingupanobject,goingtolunch,andtravelingtoadistantcity,aswellasprimitiveactionssuchasmuscletwitchesandjointtorques.overall,weshowthatoptionsenabletemporallyabstractknowledgeandactiontobeincludedinthereinforcementlearningframeworkinanaturalandgeneralway.inparticular,weshowthatoptionsmay", "6\n1\n0\n2\n\n \nt\nc\no\n8\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n2\nv\n4\n4\n1\n8\n0\n\n.\n\n9\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ngoogle\u2019s neural machine translation system: bridging the gap\n\nbetween human and machine translation\n\nyonghui wu, mike schuster, zhifeng chen, quoc v. le, mohammad norouzi\n\nyonghui,schuster,zhifengc,qvl,mnorouzi@google.com\n\nwolfgang macherey, maxim krikun, yuan cao, qin gao, klaus macherey,\nje\ufb00 klingner, apurva shah, melvin johnson, xiaobing liu, \u0142ukasz kaiser,\nstephan gouws, yoshikiyo kato, taku kudo, hideto kazawa, keith stevens,\n\ngeorge kurian, nishant patil, wei wang, cli\ufb00 young, jason smith, jason riesa,\n\nalex rudnick, oriol vinyals, greg corrado, macdu\ufb00 hughes, je\ufb00rey dean\n\nabstract\n\nneural machine translation (nmt) is an end-to-end learning approach for automated translation,\nwith the potential to overcome many of the weaknesses of conventional phrase-based translation systems.\nunfortunately, nmt systems are known to be computationally expensive both in training and in translation\ninference \u2013 som", "developmental brain research 107 1998 159\u2013163\n\n(cid:14)\n\n.\n\nshort communication\n\nquantified distribution of serotonin transporter and receptors during the\n\npostnatal development of the rat barrel field cortex\n\nsonia mansour-robaey, naguib mechawar, fatiha radja, clermont beaulieu,\n\nlaurent descarries )\n\ndepartement de pathologie et biologie cellulaire and centre de recherche en sciences neurologiques, uni\u02ddersite de montreal, montreal,\n\n\u00b4\n\n\u00b4\n\n\u00b4\n\n\u00b4\n\nquebec, canada h3c 3j7\n\n\u00b4\n\naccepted 16 december 1997\n\nabstract\n\n1b\n\nserotonin membrane transporter and 5-ht and 5-ht\n(cid:14)\n\nreceptors were visualized and measured by autoradiography in the rat barrel\nfield cortex at postnatal days 4, 8, 12, 16 and in adult )p60 . h citalopram binding, reflecting the presence of 5-ht transporter on\nthalamocortical fibers, produced a clearcut barrel pattern from p4 to p16 peak at p8 , and decreased to a dispersed, low density in the\nadult. the patterning and temporal profile of 5-ht\n2a\nreceptor binding\ni doi", "proceedings of the thirtieth aaai conference on artificial intelligence (aaai-16)\n\ndeep reinforcement learning with double q-learning\n\nhado van hasselt , arthur guez, and david silver\n\ngoogle deepmind\n\nabstract\n\nthe popular q-learning algorithm is known to overestimate\naction values under certain conditions. it was not previously\nknown whether, in practice, such overestimations are com-\nmon, whether they harm performance, and whether they can\ngenerally be prevented. in this paper, we answer all these\nquestions af\ufb01rmatively. in particular, we \ufb01rst show that the\nrecent dqn algorithm, which combines q-learning with a\ndeep neural network, suffers from substantial overestimations\nin some games in the atari 2600 domain. we then show that\nthe idea behind the double q-learning algorithm, which was\nintroduced in a tabular setting, can be generalized to work\nwith large-scale function approximation. we propose a spe-\nci\ufb01c adaptation to the dqn algorithm and show that the re-\nsulting algorithm not", "2\n2\n0\n2\n\n \nc\ne\nd\n0\n1\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\n1\nv\n4\n1\n1\n6\n\n.\n\n2\n1\n3\n1\n:\nv\ni\nx\nr\na\n\nauto-encoding variational bayes\n\ndiederik p. kingma\n\nmachine learning group\nuniversiteit van amsterdam\ndpkingma@gmail.com\n\nmax welling\n\nmachine learning group\nuniversiteit van amsterdam\n\nwelling.max@gmail.com\n\nabstract\n\nhow can we perform ef\ufb01cient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable posterior\ndistributions, and large datasets? we introduce a stochastic variational inference\nand learning algorithm that scales to large datasets and, under some mild differ-\nentiability conditions, even works in the intractable case. our contributions are\ntwo-fold. first, we show that a reparameterization of the variational lower bound\nyields a lower bound estimator that can be straightforwardly optimized using stan-\ndard stochastic gradient methods. second, we show that for i.i.d. datasets with\ncontinuous latent variables per datap", "what does dopamine mean?\n\njoshua d. berke\n\ndopamine is a critical modulator of both learning and motivation. this presents a problem: how can target cells know whether \nincreased dopamine is a signal to learn or to move? it is often presumed that motivation involves slow (\u2018tonic\u2019) dopamine \nchanges,  while  fast  (\u2018phasic\u2019)  dopamine  fluctuations  convey  reward  prediction  errors  for  learning.  yet  recent  studies  have \nshown that dopamine conveys motivational value and promotes movement even on subsecond timescales. here i describe \nan alternative account of how dopamine regulates ongoing behavior. dopamine release related to motivation is rapidly and \nlocally sculpted by receptors on dopamine terminals, independently from dopamine cell firing. target neurons abruptly switch \nbetween learning and performance modes, with striatal cholinergic interneurons providing one candidate switch mechanism. \nthe behavioral impact of dopamine varies by subregion, but in each case dopamine pr", "journal of machine learning research 5 (2004) 1471\u20131530\n\nsubmitted 10/03; revised 10/04; published 11/04\n\nvariance reduction techniques for gradient estimates in\n\nreinforcement learning\n\nevan greensmith\nresearch school of information sciences and engineering\naustralian national university\ncanberra 0200, australia\npeter l. bartlett\ncomputer science division & department of statistics\nuc berkeley\nberkeley, ca 94720, usa\n\njonathan baxter\npanscient pty. ltd.\n10 gawler terrace\nwalkerville, sa 5081, australia\n\neditor: michael littman\n\nevan@csl.anu.edu.au\n\nbartlett@stat.berkeley.edu\n\njbaxter@panscient.com\n\nabstract\n\npolicy gradient methods for reinforcement learning avoid some of the undesirable properties of\nthe value function approaches, such as policy degradation (baxter and bartlett, 2001). however,\nthe variance of the performance gradient estimates obtained from the simulation is sometimes ex-\ncessive.\nin this paper, we consider variance reduction methods that were developed for monte\nca", "r e v i e w s\n\nthe origin of extracellular fields and \ncurrents \u2014 eeg, ecog, lfp and spikes\n\ngy\u00f6rgy buzs\u00e1ki1,2,3, costas a. anastassiou4 and christof koch4,5\n\nabstract | neuronal activity in the brain gives rise to transmembrane currents that can be \nmeasured in the extracellular medium. although the major contributor of the extracellular \nsignal is the synaptic transmembrane current, other sources \u2014 including na+ and ca2+ \nspikes, ionic fluxes through voltage- and ligand-gated channels, and intrinsic membrane \noscillations \u2014 can substantially shape the extracellular field. high-density recordings of \nfield activity in animals and subdural grid recordings in humans, combined with recently \ndeveloped data processing tools and computational modelling, can provide insight into \nthe cooperative behaviour of neurons, their average synaptic input and their spiking \noutput, and can increase our understanding of how these processes contribute to the \nextracellular signal.\n\nelectric current con", "t e c h n i c a l   r e p o r t s\n\n \n\n  & john p cunningham1,3,4 \n\nstructure in neural population recordings: an expected \nbyproduct of simpler phenomena?\ngamaleldin f elsayed1\u20133 \nneuroscientists increasingly analyze the joint activity  \nof multineuron recordings to identify population-level \nstructures believed to be significant and scientifically novel. \nclaims of significant population structure support hypotheses \nin many brain areas. however, these claims require first \ninvestigating the possibility that the population structure in \nquestion is an expected byproduct of simpler features known \nto exist in data. classically, this critical examination can \nbe either intuited or addressed with conventional controls. \nhowever, these approaches fail when considering population \ndata, raising concerns about the scientific merit of population-\nlevel studies. here we develop a framework to test the  \nnovelty of population-level findings against simpler features \nsuch as correlations across", "research article\n\npredicting non-linear dynamics by stable\nlocal learning in a recurrent spiking neural\nnetwork\naditya gilra1,2*, wulfram gerstner1,2\n\n1brain-mind institute, school of life sciences, e\u00b4 cole polytechnique fe\u00b4 de\u00b4 rale de\nlausanne, lausanne, switzerland; 2school of computer and communication\nsciences, e\u00b4 cole polytechnique fe\u00b4 de\u00b4 rale de lausanne, lausanne, switzerland\n\nabstract the brain needs to predict how the body reacts to motor commands, but how a\nnetwork of spiking neurons can learn non-linear body dynamics using local, online and stable\nlearning rules is unclear. here, we present a supervised learning scheme for the feedforward and\nrecurrent connections in a network of heterogeneous spiking neurons. the error in the output is\nfed back through fixed random connections with a negative gain, causing the network to follow the\ndesired dynamics. the rule for feedback-based online local learning of weights (follow) is\nlocal in the sense that weight changes depend on th", "optimal architectures in a solvable model of deep\n\nnetworks\n\njonathan kadmon\n\nthe racah institute of physics and elsc\n\nthe hebrew university, israel\n\njonathan.kadmon@mail.huji.ac.il\n\nhaim sompolinsky\n\nthe racah institute of physics and elsc\n\nthe hebrew university, israel\n\nand\n\ncenter for brain science\n\nharvard university\n\nabstract\n\ndeep neural networks have received a considerable attention due to the success\nof their training for real world machine learning applications. they are also\nof great interest to the understanding of sensory processing in cortical sensory\nhierarchies. the purpose of this work is to advance our theoretical understanding of\nthe computational bene\ufb01ts of these architectures. using a simple model of clustered\nnoisy inputs and a simple learning rule, we provide analytically derived recursion\nrelations describing the propagation of the signals along the deep network. by\nanalysis of these equations, and de\ufb01ning performance measures, we show that\nthese model networks ", "published as a conference paper at iclr 2019\n\nrecurrent experience replay in\ndistributed reinforcement learning\n\nsteven kapturowski, georg ostrovski, john quan, r\u00b4emi munos, will dabney\ndeepmind, london, uk\n{skapturowski,ostrovski,johnquan,munos,wdabney}@google.com\n\nabstract\n\nbuilding on the recent successes of distributed training of rl agents, in this paper\nwe investigate the training of rnn-based rl agents from distributed prioritized\nexperience replay. we study the effects of parameter lag resulting in represen-\ntational drift and recurrent state staleness and empirically derive an improved\ntraining strategy. using a single network architecture and \ufb01xed set of hyper-\nparameters, the resulting agent, recurrent replay distributed dqn, quadruples\nthe previous state of the art on atari-57, and matches the state of the art on\ndmlab-30. it is the \ufb01rst agent to exceed human-level performance in 52 of the\n57 atari games.\n\n1\n\nintroduction\n\nreinforcement learning (rl) has seen a rejuvenation", "on the sample complexity of reinforcement learning with a\n\ngenerative model\n\nmohammad gheshlaghi azar\ndepartment of biophysics, radboud university nijmegen, 6525 ez nijmegen, the netherlands\n\nm.azar@science.ru.nl\n\nr\u00b4emi munos\ninria lille, sequel project, 40 avenue, halley 59650, villeneuve dascq, france\n\nremi.munos@inria.fr\n\nhilbert j. kappen\ndepartment of biophysics, radboud university nijmegen, 6525 ez nijmegen, the netherlands\n\nb.kappen@science.ru.nl\n\nabstract\n\nwe consider the problem of learning the opti-\nmal action-value function in the discounted-\nreward markov decision processes (mdps).\nwe prove a new pac bound on the sample-\ncomplexity of model-based value iteration al-\ngorithm in the presence of the generative\nmodel, which indicates that for an mdp with\nn state-action pairs and the discount factor\n\n\u03b3 \u2208 [0, 1) only o(cid:0)n log(n/\u03b4)/(cid:0)(1 \u2212 \u03b3)3\u03b52(cid:1)(cid:1)\nlower bound of \u03b8(cid:0)n log(n/\u03b4)/(cid:0)(1 \u2212 \u03b3)3\u03b52(cid:1)(cid:1)\n\nsamples are required to \ufb01nd an \u03b5-optimal es-\nti", "r e v i e w s\n\nmultiple reward signals \nin the brain\n\nwolfram schultz\n\nthe fundamental biological importance of rewards has created an increasing interest in the\nneuronal processing of reward information. the suggestion that the mechanisms underlying\ndrug addiction might involve natural reward systems has also stimulated interest. this article\nfocuses on recent neurophysiological studies in primates that have revealed that neurons in a\nlimited number of brain structures carry specific signals about past and future rewards. this\nresearch provides the first step towards an understanding of how rewards influence behaviour\nbefore they are received and how the brain might use reward information to control learning and\ngoal-directed behaviour.\n\ngoal-directed behaviour\nbehaviour controlled by\nrepresentation of a goal or an\nunderstanding of a causal\nrelationship between behaviour\nand attainment of a goal.\n\nreinforcers\npositive reinforcers (rewards)\nincrease the frequency of\nbehaviour leading t", "8\n1\n0\n2\n\n \nt\nc\no\n3\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n9\n5\n7\n5\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ninsights on representational similarity in neural\n\nnetworks with canonical correlation\n\nari s. morcos\u2217\u2021\n\ndeepmind\u2020\n\narimorcos@gmail.com\n\nmaithra raghu\u2217\u2021\n\ngoogle brain, cornell university\n\nmaithrar@gmail.com\n\nsamy bengio\ngoogle brain\n\nbengio@google.com\n\nabstract\n\ncomparing different neural network representations and determining how repre-\nsentations evolve over time remain challenging open questions in our understand-\ning of the function of neural networks. comparing representations in neural net-\nworks is fundamentally dif\ufb01cult as the structure of representations varies greatly,\neven across groups of networks trained on identical tasks, and over the course\nof training. here, we develop projection weighted cca (canonical correlation\nanalysis) as a tool for understanding neural networks, building off of svcca,\na recently proposed method [22]. we \ufb01rst improve the core method, showing\nhow to differ", "assessing the scalability of biologically-motivated\n\ndeep learning algorithms and architectures\n\nsergey bartunov\n\ndeepmind\n\nadam santoro\n\ndeepmind\n\nblake a. richards\nuniversity of toronto\n\nluke marris\ndeepmind\n\ngeoffrey e. hinton\n\ngoogle brain\n\ntimothy p. lillicrap\n\ndeepmind, university college london\n\nabstract\n\nthe backpropagation of error algorithm (bp) is impossible to implement in a\nreal brain. the recent success of deep networks in machine learning and ai,\nhowever, has inspired proposals for understanding how the brain might learn\nacross multiple layers, and hence how it might approximate bp. as of yet, none\nof these proposals have been rigorously evaluated on tasks where bp-guided deep\nlearning has proved critical, or in architectures more structured than simple fully-\nconnected networks. here we present results on scaling up biologically motivated\nmodels of deep learning on datasets which need deep networks with appropriate\narchitectures to achieve good performance. we present r", "article\n\nreceived 21 mar 2016 | accepted 12 jul 2016 | published 13 sep 2016 | updated 23 nov 2017\n\ndoi: 10.1038/ncomms12554 open\n\na dynamic code for economic object valuation\nin prefrontal cortex neurons\nken-ichiro tsutsui1,*,w\n\n, fabian grabenhorst1,*, shunsuke kobayashi1,w\n\n& wolfram schultz1\n\nneuronal reward valuations provide the physiological basis for economic behaviour. yet, how\nsuch valuations are converted to economic decisions remains unclear. here we show that the\ndorsolateral prefrontal cortex (dlpfc) implements a \ufb02exible value code based on\nobject-speci\ufb01c valuations by single neurons. as monkeys perform a reward-based foraging\ntask,\nindividual dlpfc neurons signal the value of speci\ufb01c choice objects derived from\nrecent experience. these neuronal object values satisfy principles of competitive choice\nmechanisms, track performance \ufb02uctuations and follow predictions of a classical behavioural\nmodel (herrnstein\u2019s matching law). individual neurons dynamically encode both, the ", "research\n\nresearch article summary \u25e5\n\nneuroscience\n\nneural population control via deep\nimage synthesis\n\npouya bashivan*, kohitij kar*, james j. dicarlo\u2020\n\nintroduction: the pattern of light that\nstrikes the eyes is processed and re-represented\nvia patterns of neural activity in a \u201cdeep\u201d series\nof six interconnected cortical brain areas called\nthe ventral visual stream. visual neuroscience\nresearch has revealed that these patterns of\nneural activity underlie our ability to recog-\nnize objects and their relationships in the\nworld. recent advances have enabled neuro-\nscientists to build ever more precise models\nof this complex visual processing. currently, the\nbest such models are particular deep artificial\nneural network (ann) models in which each\nbrain area has a corresponding model layer\nand each brain neuron has a corresponding mod-\nel neuron. such models are quite good at pre-\ndicting the responses of brain neurons, but their\ncontribution to an understanding of primate\nvisual processi", "8\n1\n0\n2\n\n \n\nv\no\nn\n0\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n7\n8\n5\n4\n0\n\n.\n\n7\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nassessing the scalability of biologically-motivated\n\ndeep learning algorithms and architectures\n\nsergey bartunov\n\ndeepmind\n\nadam santoro\n\ndeepmind\n\nblake a. richards\nuniversity of toronto\n\nluke marris\ndeepmind\n\ngeoffrey e. hinton\n\ngoogle brain\n\ntimothy p. lillicrap\n\ndeepmind, university college london\n\nabstract\n\nthe backpropagation of error algorithm (bp) is impossible to implement in a real\nbrain. the recent success of deep networks in machine learning and ai, however,\nhas inspired proposals for understanding how the brain might learn across multiple\nlayers, and hence how it might approximate bp. as of yet, none of these proposals\nhave been rigorously evaluated on tasks where bp-guided deep learning has proved\ncritical, or in architectures more structured than simple fully-connected networks.\nhere we present results on scaling up biologically motivated models of deep learn-\ning on datasets which n", "biorxiv preprint \nthe copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is \n\nhttps://doi.org/10.1101/2023.06.27.546656\nthis version posted june 30, 2023. \n; \n\ndoi: \n\nmade available under a\n\ncc-by-nd 4.0 international license\n.\n\nneuronal wiring diagram of an adult brain\n\nsven dorkenwald1,2, arie matsliah1, amy r sterling1,3, philipp schlegel4,5, szi-chieh yu1, claire e.\nmckellar1, albert lin1,6, marta costa5, katharina eichler5, yijie yin5, will silversmith1, casey\nschneider-mizell7, chris s. jordan1, derrick brittain7, akhilesh halageri1, kai kuehner1,\noluwaseun ogedengbe1, ryan morey1, jay gager1, krzysztof kruk3, eric perlman8, runzhe\nyang1,2, david deutsch1,9, doug bland1, marissa sorek1,3, ran lu1, thomas macrina1,2, kisuk\nlee1,10, j. alexander bae1,11, shang mu1, barak nehoran1,2, eric mitchell1, sergiy popovych1,2,\njingpeng wu1, zhen jia1, manuel castro1, ni", "limitations of lazy training of\ntwo-layers neural networks\n\nbehrooz ghorbani\n\ndepartment of electrical engineering\n\nstanford university\n\nsong mei\n\nicme\n\nstanford university\n\nghorbani@stanford.edu\n\nsongmei@stanford.edu\n\ntheodor misiakiewicz\ndepartment of statistics\n\nstanford university\n\nmisiakie@stanford.edu\n\nandrea montanari\n\ndepartment of electrical engineering\n\nand department of statistics\n\nstanford university\n\nmontanar@stanford.edu\n\nabstract\n\nwe study the supervised learning problem under either of the following two models:\n(1) feature vectors xi are d-dimensional gaussians and responses are yi = f\u2217(xi)\n\nfor f\u2217 an unknown quadratic function;\n\n(2) feature vectors xi are distributed as a mixture of two d-dimensional centered\n\ngaussians, and yi\u2019s are the corresponding class labels.\n\nwe use two-layers neural networks with quadratic activations, and compare three\ndifferent learning regimes: the random features (rf) regime in which we only\ntrain the second-layer weights; the neural tangen", "vision research 51 (2011) 1484\u20131525\n\ncontents lists available at sciencedirect\n\nvision research\n\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / v i s r e s\n\nreview\nvisual attention: the past 25 years\n\nmarisa carrasco\n\npsychology and neural science, new york university, united states\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\nthis review focuses on covert attention and how it alters early vision. i explain why attention is consid-\nered a selective process, the constructs of covert attention, spatial endogenous and exogenous attention,\nand feature-based attention. i explain how in the last 25 years research on attention has characterized the\neffects of covert attention on spatial \ufb01lters and how attention in\ufb02uences the selection of stimuli of inter-\nest. this review includes the effects of spatial attention on discriminability and appearance in tasks med-\niated by contrast sensitivity and spatial resolution; the effects of feature-based attention on basic v", "article\n\ncommunicated by manuel beiran\n\nheterogeneity in neuronal dynamics is learned by gradient\ndescent for temporal processing tasks\n\nchloe n. winston\nwincnw@gmail.com\ndepartments of neuroscience and computer science, university of washington,\nseattle, wa 98195, u.s.a., and university of washington computational\nneuroscience center, seattle, wa 98195, u.s.a.\n\ndana mastrovito\ndana.mastrovito@alleninstitute.org\nallen institute for brain science, seattle, wa 98109, u.s.a.\n\neric shea-brown\netsb@uw.edu\nstefan mihalas\nstefanm@alleninstitute.org\nuniversity of washington computational neuroscience center, seattle, wa 98195,\nu.s.a.; allen institute for brain science, seattle, wa 98109, u.s.a.; and department\nof applied mathematics, university of washington, seattle, wa 98195, u.s.a.\n\nindividual neurons in the brain have complex intrinsic dynamics that\nare highly diverse. we hypothesize that the complex dynamics produced\nby networks of complex and heterogeneous neurons may contribute to\nthe b", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nphasic  dopamine  signals:  from  subjective  reward  value\nto  formal  economic  utility\nwolfram  schultz1,  regina  m  carelli2 and  r  mark  wightman3\n\nalthough  rewards  are  physical  stimuli  and  objects,  their  value\nfor  survival  and  reproduction  is  subjective.  the  phasic,\nneurophysiological  and  voltammetric  dopamine  reward\nprediction  error  response  signals  subjective  reward  value.\nthe  signal  incorporates  crucial  reward  aspects  such  as\namount,  probability,  type,  risk,  delay  and  effort.  differences\nof  dopamine  release  dynamics  with  temporal  delay  and  effort\nin  rodents  may  derive  from  methodological  issues  and\nrequire  further  study.  recent  designs  using  concepts  and\nbehavioral  tools  from  experimental  economics  allow  to\nformally  characterize  the  subjective  value  signal  as  economic\nutility  and  thus  to  establish  a  neuronal  value  function.  with\nthes", "chain-of-thought prompting elicits reasoning\n\nin large language models\n\njason wei\nbrian ichter\n\nxuezhi wang\n\ndale schuurmans\n\nfei xia\n\ned h. chi\n\nquoc v. le\n\nmaarten bosma\ndenny zhou\n\ngoogle research, brain team\n\n{jasonwei,dennyzhou}@google.com\n\nabstract\n\nwe explore how generating a chain of thought\u2014a series of intermediate reasoning\nsteps\u2014signi\ufb01cantly improves the ability of large language models to perform\ncomplex reasoning. in particular, we show how such reasoning abilities emerge\nnaturally in suf\ufb01ciently large language models via a simple method called chain-of-\nthought prompting, where a few chain of thought demonstrations are provided as\nexemplars in prompting.\nexperiments on three large language models show that chain-of-thought prompting\nimproves performance on a range of arithmetic, commonsense, and symbolic\nreasoning tasks. the empirical gains can be striking. for instance, prompting a\npalm 540b with just eight chain-of-thought exemplars achieves state-of-the-art\naccuracy on", "a r t i c l e s\n\ncomputational principles of synaptic memory \nconsolidation\nmarcus k benna1 & stefano fusi1,2\nmemories are stored and retained through complex, coupled processes operating on multiple timescales. to understand the \ncomputational principles behind these intricate networks of interactions, we construct a broad class of synaptic models that \nefficiently harness biological complexity to preserve numerous memories by protecting them against the adverse effects of \noverwriting. the memory capacity scales almost linearly with the number of synapses, which is a substantial improvement over \nthe square root scaling of previous models. this was achieved by combining multiple dynamical processes that initially store \nmemories in fast variables and then progressively transfer them to slower variables. notably, the interactions between fast and \nslow variables are bidirectional. the proposed models are robust to parameter perturbations and can explain several properties of \nbiologic", "\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\n\u0003\u0006\u0005\t\u0005\b\u0001\u000b\n\r\f\t\u000e\u0010\u000f\n\n\u0003\u0012\u0011\u0013\u000e\u0014\n\u0016\u0015\u0018\u0017\u0019\u0011\u001a\n\n\n\u001b\u0016\u001c\u001d\n\u0018\u001e\u001f\u001c \u0017\u0013\u000e\"! \u0017#\n\u0016\u0015\n\n\u001c$\u000f\n\n%&\u001c$\u001e'\u001e(\u001c$\u001e\n\n\u0003)\u0015\t\u000e+*,\n\u0016-.\u001e\u001f\u0017\n\n/1032547698:2<;\u0013=?>@8a0b;\rced\b0b;gfh8jie0lk3mn4hc\n\noqpqr\u0013;ts\tuwvx4n;hyqz[p\\2<f\rchoqpq=]re^w4_6(u\u0013`\u00128a;h8a;cbd4\\^\u0013egf\n\nh(8aij8aekbl8a2\u001fmnnhclmnnqnqn\n\npcqsr\u0013t\u0013u\u001fvxwxy\u001dr\u0013z{u|q\n\n}\u007f~\u0081\u0080\u0083\u0082{\u0084\u000b\u0085\u0004\u0086{~a\u0086{~j\u0087\u0089\u00885\u008a\u008c\u008b(\u008d\u0089\u008e\u008c\u0086\u0006\u008fa\u0090\u0092\u0091\u0092\u0093@\u0088\u0094\u00885~a\u008e'\u0095c\u00885~a\u00965\u0097\u0099\u0098,\u009a+\u008e\u0018\u009b\u0010\u0098,~\u008c\u009c\u009d\u0090\u009e\u008e\u008c\u0086\u0010\u009a\u00a0\u009f\u00a2\u00a1\u008c\u0086\u00a4\u00a3\u008c\u009a\u009d\u0098,\u0087\u008c\u0091\u0092\u0086{\u008b\u00a5\u00985\u0097\u0004~\u008c\u0098,~a\u00a6\u00a7\u0091\u009e\u0090\u009e~a\u0086?\u00885\u009a\n\u008e\u008c\u0090\u009e\u008b|\u0086{~\u008c\u009c\u009d\u0090\u009e\u0098,~\u008c\u00885\u0091\u009e\u0090\u00a8\u009f\u007f\u00a9\r\u009a+\u0086{\u008ea\u008a\u008c\u009b\u0010\u009f+\u0090\u009e\u0098,~q\u00aa\u000b\u008ea\u0090\u009e\u009c+\u009b\u0010\u0098\u00ab\u0093,\u0086{\u009a\u009d\u0090\u009e~\u008c\u0096\r\u0090\u0092~j\u009f+\u009a+\u0090\u0092~\u008c\u009c+\u0090\u0092\u009b?\u00885\u0091\u0092\u0091\u0092\u00a9\r\u0091\u009e\u0098\u00ab\u00ac \u00a6\u00a7\u008e\u008c\u0090\u009e\u008b|\u0086{~\u008c\u009c\u009d\u0090\u009e\u0098,~\u008c\u00885\u0091@\u009c[\u009f\u00a2\u009a\u009d\u008a\u008c\u009b\u0010\u009f+\u008a\u008c\u009a+\u0086\u0010\u009c\n\u0086{\u008b\u0094\u0087\u00ad\u0086{\u008e\u008c\u008ea\u0086{\u008e\u0018\u0090\u009e~\u00ae\u00a1\u008c\u0090\u009e\u0096,\u00a1:\u00a6\u007f\u008e\u008c\u0090\u0092\u008b\u00af\u0086\u0010~\u008c\u009c+\u0090\u0092\u0098,~\u0089\u00885\u0091\u008c\u008e\u008c\u0088@\u009f\"\u0088\u0006\u009c+\u0086\u0010\u009f+\u009c{\u00b0w\u0085\u001d\u00a1\u008c\u0086.\u00a9s\u008e\u008c\u0086\u0010\u009c+\u009b{\u009a\u009d\u0090\u009e\u0087\u00ad\u0086\u0012\u00885~s\u00885\u0091\u009e\u0096,\u00985\u009a+\u0090\u0092\u009f+\u00a1\u008c\u008b(\u008d,\u009b?\u00885\u0091\u0092\u0091\u009e\u0086\u0010\u008e\n}\u007f\u009c+\u0098,\u008b\u00af\u00885\u00a3q\u008dc\u00885~a\u008e\u00b1\u008ea\u0086{\u008b|\u0098,~\u008c\u009c\u009d\u009f+\u009a\u00a2\u0088@\u009f+\u00869\u0090\u0092\u009f+\u009c\u0012\u009c+\u008a\u008c\u009b\u0010\u009b{\u0086\u0010\u009c+\u009c\u009d\u0097\u0099\u008aa\u0091\u00a0\u00885\u00a3a\u00a3\u008c\u0091\u009e\u0090\u0092\u009b?\u0088@\u009f+\u0090\u009e\u0098,~(\u009f\u00a2\u0098\u001f\u009c+\u0086.\u0093,\u0086{\u009a+\u00885\u0091w\u009a+\u0086{\u00885\u0091\u0013\u00885~\u008c\u008e\u00b2\u009c[\u00a9a~:\u00a6\n\u009f\u00a2\u00a1\u008c\u0086.\u009f\u00a2\u0090\u009e\u009b\u00a4\u008e\u0089\u0088<\u009f\"\u00889\u009c+\u0086.\u009f\u00a2\u009c{\u00b0\n}\u007f~\t\u009f\u00a2\u00a1\u008c\u0090\u0092\u009c\u00b3\u00a3\u008c\u00885\u00a3n\u0086\u0010\u009a{\u008d\u0089\u00acl\u0086\u00b4\u008e\u008c\u0090\u0092\u009c+\u009b{\u008aa\u009c+\u009c\u0012\u009c\u009d\u0098,\u008b\u00af\u0086\u0006\u00985\u0097e\u009f\u00a2\u00a1\u008c\u0086\u0006\u009f\u00a2\u00a1\u008c\u0086\u0010\u0098,\u009a+\u0086.\u009f\u00a2\u0090\u009e\u009b{\u00885\u0091c\u009b{\u0091\u009e\u00885\u0090\u009e\u008b|\u009c \u0097\u0099\u0098,\u009a\r}\u007f\u009c+\u0098,\u008b\u00af\u00885\u00a3'\u008b\u00ae\u00885\u008ea\u0086)\u0090\u0092~\n\u0080\u00b5\u0082{\u0084l\u00b0h}\u007f~|\u00a3\u0089\u00885\u009a[\u009f\u00a2\u0090\u009e\u009b\u0010\u008a\u008c\u0091\u00b6\u0088@\u009a{\u008d,\u00acl\u0086", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/2023.02.28.530327\n; \n\nthis version posted march 1, 2023. \n\nthe copyright holder for this preprint\n\n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\navailable under a\n\ncc-by-nc-nd 4.0 international license\n.\n\nfebruary 27, 2023\n\n1/22\n\n\f", "published as a conference paper at iclr 2018\n\ndeep neural networks as gaussian processes\n\njaehoon lee\u2217\u2020, yasaman bahri\u2217\u2020, roman novak , samuel s. schoenholz,\njeffrey pennington, jascha sohl-dickstein\n\ngoogle brain\n{jaehlee, yasamanb, romann, schsam, jpennin, jaschasd}@google.com\n\nabstract\n\nit has long been known that a single-layer fully-connected neural network with an\ni.i.d. prior over its parameters is equivalent to a gaussian process (gp), in the limit\nof in\ufb01nite network width. this correspondence enables exact bayesian inference\nfor in\ufb01nite width neural networks on regression tasks by means of evaluating the\ncorresponding gp. recently, kernel functions which mimic multi-layer random\nneural networks have been developed, but only outside of a bayesian framework.\nas such, previous work has not identi\ufb01ed that these kernels can be used as co-\nvariance functions for gps and allow fully bayesian prediction with a deep neural\nnetwork.\nin this work, we derive the exact equivalence between ", "letter\n\ncommunicated by bard ermentrout\n\nsynchrony of neuronal oscillations controlled by gabaergic\nreversal potentials\n\nho young jeong\njeonghy@cns.nyu.edu\ncenter for neural science, new york university, new york, ny 10003, u.s.a.\n\nboris gutkin\nboris.gutkin@ens.fr\ngroup for neural theory, dec, ens-paris; coll\u00b4ege de france, and ura 2169\nrecepteurs et cognition, institut pasteur, paris, france\n\ngabaergic synapse reversal potential is controlled by the concentration\nof chloride. this concentration can change signi\ufb01cantly during develop-\nment and as a function of neuronal activity. thus, gaba inhibition can\nbe hyperpolarizing, shunting, or partially depolarizing. previous results\npinpointed the conditions under which hyperpolarizing inhibition (or\ndepolarizing excitation) can lead to synchrony of neural oscillators. here\nwe examine the role of the gabaergic reversal potential in generation of\nsynchronous oscillations in circuits of neural oscillators. using weakly\ncoupled oscillator analy", "neural networks 61 (2015) 85\u2013117\n\ncontents lists available at sciencedirect\n\nneural networks\n\njournal homepage: www.elsevier.com/locate/neunet\n\nreview\ndeep learning in neural networks: an overview\nj\u00fcrgen schmidhuber\nthe swiss ai lab idsia, istituto dalle molle di studi sull\u2019intelligenza artificiale, university of lugano & supsi, galleria 2, 6928 manno-lugano, switzerland\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\nin recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in\npattern recognition and machine learning. this historical survey compactly summarizes relevant work,\nmuch of it from the previous millennium. shallow and deep learners are distinguished by the depth\nof their credit assignment paths, which are chains of possibly learnable, causal links between actions\nand effects. i review deep supervised learning (also recapitulating the history of backpropagation),\nunsupervised learning, reinforcement learning & evolutionary computation, a", "neuron\n\narticle\n\nsparseness and expansion\nin sensory representations\n\nbaktash babadi1 and haim sompolinsky1,2,*\n1swartz program in theoretical neuroscience, center for brain science, harvard university, cambridge, ma 02138, usa\n2edmond and lily safra center for brain sciences, hebrew university, jerusalem 91904, israel\n*correspondence: haim@\ufb01z.huji.ac.il\nhttp://dx.doi.org/10.1016/j.neuron.2014.07.035\n\nsummary\n\nin several sensory pathways, input stimuli project\nto sparsely active downstream populations that\nhave more neurons than incoming axons. here, we\naddress the computational bene\ufb01ts of expansion\nand sparseness for clustered inputs, where different\nclusters represent behaviorally distinct stimuli and\nintracluster variability represents sensory or neu-\nronal noise. through analytical calculations and\nnumerical simulations, we show that expansion\nimplemented by feed-forward random synaptic\nweights ampli\ufb01es variability in the incoming stimuli,\nand this noise enhancement increases with ", "6\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n4\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n0\n6\n4\n4\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nmodel-free episodic control\n\ncharles blundell\ngoogle deepmind\n\ncblundell@google.com\n\nbenigno uria\n\ngoogle deepmind\nburia@google.com\n\nalexander pritzel\ngoogle deepmind\n\napritzel@google.com\n\nyazhe li\n\ngoogle deepmind\nyazhe@google.com\n\navraham ruderman\n\ngoogle deepmind\n\naruderman@google.com\n\njoel z leibo\n\ngoogle deepmind\njzl@google.com\n\njack rae\n\ngoogle deepmind\njwrae@google.com\n\ndaan wierstra\ngoogle deepmind\n\nwierstra@google.com\n\ndemis hassabis\ngoogle deepmind\n\ndemishassabis@google.com\n\nabstract\n\nstate of the art deep reinforcement learning algorithms take many millions of inter-\nactions to attain human-level performance. humans, on the other hand, can very\nquickly exploit highly rewarding nuances of an environment upon \ufb01rst discovery.\nin the brain, such rapid learning is thought to depend on the hippocampus and\nits capacity for episodic memory. here we investigate whether a simple model\nof hippo", "distinct eligibility traces for ltp and ltd in cortical\nsynapses\n\narticle\n\nhighlights\nd hebbian conditioning induces eligibility traces for ltp and\n\nltd in cortical synapses\n\nd b2ars and 5-ht2crs convert the traces into ltp and ltd,\n\nrespectively\n\nd anchoring of b2ars and 5-ht2c is key for trace conversion\n\nd temporal properties of the ltp/d traces allow reward-timing\n\nprediction\n\nauthors\n\nkaiwen he, marco huertas, su z. hong,\nxiaoxiu tie, johannes w. hell, harel\nshouval, alfredo kirkwood\n\ncorrespondence\nkirkwood@jhu.edu\n\nin brief\nhow is stimulus-evoked activity\nassociated with a time-delayed reward in\nreinforcement learning? he et al. report\non the existence of silent and transient\nsynaptic tags (eligibility traces) that can\nbe converted into long-term changes in\nsynaptic strength by reward-linked\nneuromodulators.\n\nhe et al., 2015, neuron 88, 528\u2013538\nnovember 4, 2015 \u00aa2015 elsevier inc.\nhttp://dx.doi.org/10.1016/j.neuron.2015.09.037\n\n\f", "4\n1\n0\n2\n\n \nc\ne\nd\n0\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n1\n0\n4\n5\n\n.\n\n0\n1\n4\n1\n:\nv\ni\nx\nr\na\n\nneural turing machines\n\nalex graves\ngreg wayne\nivo danihelka\n\ngravesa@google.com\ngregwayne@google.com\ndanihelka@google.com\n\ngoogle deepmind, london, uk\n\nabstract\n\nwe extend the capabilities of neural networks by coupling them to external memory re-\nsources, which they can interact with by attentional processes. the combined system is\nanalogous to a turing machine or von neumann architecture but is differentiable end-to-\nend, allowing it to be ef\ufb01ciently trained with gradient descent. preliminary results demon-\nstrate that neural turing machines can infer simple algorithms such as copying, sorting,\nand associative recall from input and output examples.\n\n1\n\nintroduction\n\ncomputer programs make use of three fundamental mechanisms: elementary operations\n(e.g., arithmetic operations), logical \ufb02ow control (branching), and external memory, which\ncan be written to and read from in the course of computation (vo", "self-organization in a \nperceptual network \n\nralph linsker \nibm research \n\na young animal or child perceives \n\nand identifies features in its envi- \n, ronment in an apparently effort- \nless way. no presently known algorithms \neven  approach  this  flexible,  general- \npurpose perceptual capability. discover- \ning the principles that may underlie per- \nceptual processing is important both for \nneuroscience and for the development of \nsynthetic perceptual systems. \n\ntwo important aspects of the mystery of \n\nperception are \n\nwhat processing functions does the \nneural  \u201cmachinery\u201d  perform  on \nperceptual  input, and what  is the \ncircuitry  that  implements  these \nfunctions? \nhow does this \u201cmachinery\u201d  come \nto be? \n\nunlike  conventional  computer  hard- \nware, neural circuitry is not hard-wired or \nspecified as an explicit set of point-to-point \nconnections. instead it develops under the \ninfluence  of  a  genetic specification  and. \nepigenetic  factors,  such  as  electrical \nactivity", "the journal of neuroscience, may 15, 1998, 18(10):3870\u20133896\n\nthe variable discharge of cortical neurons: implications for\nconnectivity, computation, and information coding\n\nmichael n. shadlen1 and william t. newsome2\n1department of physiology and biophysics and regional primate research center, university of washington, seattle,\nwashington 98195-7290, and 2howard hughes medical institute and department of neurobiology, stanford university\nschool of medicine, stanford, california 94305\n\ncortical neurons exhibit tremendous variability in the number\nand temporal distribution of spikes in their discharge patterns.\nfurthermore, this variability appears to be conserved over large\nregions of the cerebral cortex, suggesting that it is neither\nreduced nor expanded from stage to stage within a processing\npathway. to investigate the principles underlying such statisti-\ncal homogeneity, we have analyzed a model of synaptic inte-\ngration incorporating a highly simpli\ufb01ed integrate and \ufb01re\nmechanism ", "mini review article\npublished: 22 october 2014\ndoi: 10.3389/fncom.2014.00135\n\nwhy vision is not both hierarchical and feedforward\nmichael h. herzog* and aaron m. clarke\n\nlaboratory of psychophysics, brain, mind institute, \u00e9cole polytechnique f\u00e9d\u00e9rale de lausanne, lausanne, switzerland\n\nedited by:\nantonio j. rodriguez-sanchez,\nuniversity of innsbruck, austria\nreviewed by:\nmichael zillich, vienna university of\ntechnology, austria\nnorbert k\u00fcger, the maersk\nmc-kinney moller institute,\ndenmark\n*correspondence:\nmichael h. herzog, laboratory of\npsychophysics, brain, mind\ninstitute, \u00e9cole polytechnique\nf\u00e9d\u00e9rale de lausanne, epfl sv\nbmi lpsy sv-2807, station 19,\nch-1015 lausanne, switzerland\ne-mail: michael.herzog@ep\ufb02.ch\n\nin classical models of object recognition, \ufb01rst, basic features (e.g., edges and lines) are\nanalyzed by independent \ufb01lters that mimic the receptive \ufb01eld pro\ufb01les of v1 neurons. in a\nfeedforward fashion, the outputs of these \ufb01lters are fed to \ufb01lters at the next processing\nstage,", "learnable latent embeddings for joint \nbehavioural and neural analysis\n\nin the format provided by the \nauthors and unedited\n\nnature | www.nature.com/naturesupplementary informationhttps://doi.org/10.1038/s41586-023-06031-6\f", "8\n1\n0\n2\n\n \n\ng\nu\na\n7\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n5\nv\n6\n3\n1\n5\n0\n\n.\n\n1\n1\n7\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2018\n\ndeep rewiring: training very sparse deep net-\nworks\n\nguillaume bellec, david kappel, wolfgang maass & robert legenstein\ninstitute for theoretical computer science\ngraz university of technology\naustria\n{bellec,kappel,maass,legenstein}@igi.tugraz.at\n\nabstract\n\nneuromorphic hardware tends to pose limits on the connectivity of deep networks\nthat one can run on them. but also generic hardware and software implementa-\ntions of deep learning run more ef\ufb01ciently for sparse networks. several methods\nexist for pruning connections of a neural network after it was trained without con-\nnectivity constraints. we present an algorithm, deep r, that enables us to train\ndirectly a sparsely connected neural network. deep r automatically rewires the\nnetwork during supervised training so that connections are there where they are\nmost needed for the task, while its total number", "decoding the organization of spinal \ncircuits that control locomotion\n\nole kiehn\nabstract | unravelling the functional operation of neuronal networks and linking cellular activity \nto specific behavioural outcomes are among the biggest challenges in neuroscience. in this \nbroad\u00a0field of research, substantial progress has been made in studies of the spinal networks \nthat\u00a0control locomotion. through united efforts using electrophysiological and molecular \ngenetic network approaches and behavioural studies in phylogenetically diverse experimental \nmodels, the organization of locomotor networks has begun to be decoded. the emergent themes \nfrom this research are that the locomotor networks have a modular organization with distinct \ntransmitter and molecular codes and that their organization is reconfigured with changes to the \nspeed of locomotion or changes in gait.\n\ngait\na description of the pattern  \nof limb movements. different \ngaits have different patterns  \nof movements and are often", "pathnet: evolution channels gradient descent in super\n\nneural networks\n\nchrisantha fernando, dylan banarse, charles blundell, yori zwols, david ha\u2020, andrei\n\na. rusu, alexander pritzel, daan wierstra\n\ngoogle deepmind, london, uk. \u2020google brain\n\nchrisantha@google.com\n\n7\n1\n0\n2\n\n \n\nn\na\nj\n \n\n0\n3\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n4\n3\n7\n8\n0\n\n.\n\n1\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\nfor arti\ufb01cial general intelligence (agi) it would be e\ufb03cient\nif multiple users trained the same giant neural network, per-\nmitting parameter reuse, without catastrophic forgetting.\npathnet is a \ufb01rst step in this direction. it is a neural net-\nwork algorithm that uses agents embedded in the neural net-\nwork whose task is to discover which parts of the network to\nre-use for new tasks. agents are pathways (views) through\nthe network which determine the subset of parameters that\nare used and updated by the forwards and backwards passes\nof the backpropogation algorithm. during learning, a tour-\nnament selection genetic algorithm is u", "neuropsychologia 51 (2013) 2371\u20132388\n\ncontents lists available at sciencedirect\n\nneuropsychologia\n\njournal homepage: www.elsevier.com/locate/neuropsychologia\n\nmoderate levels of activation lead to forgetting\nin the think/no-think paradigm\n\ngreg j. detre 1, annamalai natarajan 1, samuel j. gershman, kenneth a. norman n\n\ndepartment of psychology and princeton neuroscience institute, princeton university, princeton, nj 08540, usa\n\na r t i c l e i n f o\n\na b s t r a c t\n\navailable online 7 march 2013\n\nkeywords:\nfmri\nmemory\ninhibition\nplasticity\n\nusing the think/no-think paradigm (anderson & green, 2001), researchers have found that suppressing\nretrieval of a memory (in the presence of a strong retrieval cue) can make it harder to retrieve that\nmemory on a subsequent test. this effect has been replicated numerous times, but the size of the effect\nis highly variable. also, it is unclear from a neural mechanistic standpoint why preventing recall of a\nmemory now should impair your ability to r", "control of synaptic plasticity in deep \ncortical networks\n\npieter r.\u00a0roelfsema1,2,3* and anthony holtmaat4\nabstract | humans and many other animals have an enormous capacity to learn about sensory \nstimuli and to master new skills. however, many of the mechanisms that enable us to learn remain \nto be understood. one of the greatest challenges of systems neuroscience is to explain how \nsynaptic connections change to support maximally adaptive behaviour. here, we provide an \noverview of factors that determine the change in the strength of synapses, with a focus on \nsynaptic plasticity in sensory cortices. we review the influence of neuromodulators and feedback \nconnections in synaptic plasticity and suggest a specific framework in which these factors can \ninteract to improve the functioning of the entire network.\n\nreward-prediction errors\n(rpes). differences between \nthe amount of reward that was \nexpected and the amount that \nwas obtained.\n\nreinforcement learning\ntrial-and-error learnin", "7\n1\n0\n2\n\n \n\nv\no\nn\n2\n2\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n4\nv\n2\n3\n7\n3\n0\n\n.\n\n4\n0\n7\n1\n:\nv\ni\nx\nr\na\n\ndeep q-learning from demonstrations\n\ntodd hester\n\ngoogle deepmind\n\nmatej vecerik\ngoogle deepmind\n\nolivier pietquin\ngoogle deepmind\n\ntoddhester@google.com\n\nmatejvecerik@google.com\n\npietquin@google.com\n\nmarc lanctot\ngoogle deepmind\nlanctot@google.com\n\ntom schaul\n\ngoogle deepmind\nschaul@google.com\n\nbilal piot\n\ngoogle deepmind\npiot@google.com\n\ndan horgan\n\ngoogle deepmind\nhorgan@google.com\n\njohn quan\n\ngoogle deepmind\n\njohnquan@google.com\n\nandrew sendonaris\n\ngoogle deepmind\nsendos@yahoo.com\n\nian osband\n\ngoogle deepmind\n\niosband@google.com\n\ngabriel dulac-arnold\n\ngoogle deepmind\n\ngabe@squirrelsoup.net\n\njohn agapiou\ngoogle deepmind\n\njagapiou@google.com\n\njoel z. leibo\ngoogle deepmind\njzl@google.com\n\naudrunas gruslys\n\ngoogle deepmind\n\naudrunas@google.com\n\nabstract\n\ndeep reinforcement\nlearning (rl) has achieved several\nhigh pro\ufb01le successes in dif\ufb01cult decision-making problems.\nhowever, these algorithms typica", "b\ni\no\nl\n.\n \nc\ny\nb\ne\nr\nn\n.\n \n6\n6\n,\n \n2\n4\n1\n-\n2\n5\n1\n \n(\n1\n9\n9\n2\n)\n \nb\ni\no\nl\no\ng\ni\nc\na\nl\n \nc\ny\nb\ne\nr\nn\ne\nt\ni\nc\ns\n \n(cid:14)\n9\n \ns\np\nr\ni\nn\ng\ne\nr\n-\nv\ne\nr\nl\na\ng\n \n1\n9\n9\n2\n \no\nn\n \nt\nh\ne\n \nc\no\nm\np\nu\nt\na\nt\ni\no\nn\na\nl\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n \no\nf\n \nt\nh\ne\n \nn\ne\no\nc\no\nr\nt\ne\nx\n \ni\ni\n \nt\nh\ne\n \nr\no\nl\ne\n \no\nf\n \nc\no\nr\nt\ni\nc\no\n-\nc\no\nr\nt\ni\nc\na\nl\n \nl\no\no\np\ns\n \nd\n.\n \nm\nu\nm\nf\no\nr\nd\n \nm\na\nt\nh\ne\nm\na\nt\ni\nc\ns\n \nd\ne\np\na\nr\nt\nm\ne\nn\nt\n,\n \nh\na\nr\nv\na\nr\nd\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n,\n \n1\n \no\nx\nf\no\nr\nd\n \ns\nt\nr\ne\ne\nt\n,\n \nc\na\nm\nb\nr\ni\nd\ng\ne\n,\n \nm\na\n \n0\n2\n1\n3\n8\n,\n \nu\ns\na\n \nr\ne\nc\ne\ni\nv\ne\nd\n \nj\nu\nn\ne\n \n2\n9\n,\n \n1\n9\n9\n1\n/\na\nc\nc\ne\np\nt\ne\nd\n \nj\nu\nl\ny\n \n1\n2\n,\n \n1\n9\n9\n1\n \na\nb\ns\nt\nr\na\nc\nt\n.\n \nt\nh\ni\ns\n \np\na\np\ne\nr\n \ni\ns\n \na\n \ns\ne\nq\nu\ne\nl\n \nt\no\n \na\nn\n \ne\na\nr\nl\ni\ne\nr\n \np\na\np\ne\nr\n \nw\nh\ni\nc\nh\n \np\nr\no\np\no\ns\ne\nd\n \na\nn\n \na\nc\nt\ni\nv\ne\n \nr\no\nl\ne\n \nf\no\nr\n \nt\nh\ne\n \nt\nh\na\nl\na\nm\nu\ns\n,\n \ni\nn\nt\ne\n-\n \ng\nr\na\nt\ni\nn\ng\n \nm\nu\nl\nt\ni\np\nl\ne\n \nh\ny\np\no\nt\nh\ne\ns\ne\ns\n \nf\no\nr\nm\ne\nd\n \ni\nn\n \nt\nh\ne\n \nc\no\nr\nt\ne\nx\n \nv\ni\na\n \nt\nh\ne\n \nt\nh\na\nl\na\nm\no\n-\nc\no\nr\nt\ni\nc\na\nl\n \nl\no\no\np", "solving the problem of negative synaptic weights in\n\ncortical models\n\nchristopher parisien,1 charles h. anderson,2 chris eliasmith3\u2217\n\n1dept. of computer science, university of toronto,\n\ntoronto, on m5s 3g4, canada\n\n2dept. of anatomy and neurobiology, washington university school of medicine,\n\nst. louis, mo 63110, u.s.a.\n\n3centre for theoretical neuroscience, university of waterloo,\n\nwaterloo, on n2l 3g1, canada\n\n\u2217\n\nto whom correspondence should be addressed; e-mail: celiasmith@uwaterloo.ca\n\nin cortical neural networks, connections from a given neuron are either in-\n\nhibitory or excitatory, but not both. this constraint is often ignored by the-\n\noreticians who build models of these systems. there is currently no general\n\nsolution to the problem of converting such unrealistic network models into\n\nbiologically plausible models that respect this constraint. we demonstrate a\n\nconstructive transformation of models that solves this problem for both feed-\n\nforward and dynamic recurrent network", "research advance\n\nsequential neuromodulation of hebbian\nplasticity offers mechanism for effective\nreward-based navigation\nzuzanna brzosko1\u2020, sara zannone2\u2020, wolfram schultz1, claudia clopath2*\u2021,\nole paulsen1*\u2021\n\n1department of physiology, development and neuroscience, physiological\nlaboratory, cambridge, united kingdom; 2department of bioengineering, imperial\ncollege london, south kensington campus, london, united kingdom\n\nabstract spike timing-dependent plasticity (stdp) is under neuromodulatory control, which is\ncorrelated with distinct behavioral states. previously, we reported that dopamine, a reward signal,\nbroadens the time window for synaptic potentiation and modulates the outcome of hippocampal\nstdp even when applied after the plasticity induction protocol (brzosko et al., 2015). here, we\ndemonstrate that sequential neuromodulation of stdp by acetylcholine and dopamine offers an\nefficacious model of reward-based navigation. specifically, our experimental data in mouse\nhippocampa", "dueling network architectures for deep reinforcement learning\n\n6\n1\n0\n2\n\n \nr\np\na\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n1\n8\n5\n6\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nziyu wang\ntom schaul\nmatteo hessel\nhado van hasselt\nmarc lanctot\nnando de freitas\ngoogle deepmind, london, uk\n\nabstract\n\nin recent years there have been many successes\nof using deep representations in reinforcement\nlearning. still, many of these applications use\nconventional architectures, such as convolutional\nnetworks, lstms, or auto-encoders. in this pa-\nper, we present a new neural network architec-\nture for model-free reinforcement learning. our\ndueling network represents two separate estima-\ntors: one for the state value function and one for\nthe state-dependent action advantage function.\nthe main bene\ufb01t of this factoring is to general-\nize learning across actions without imposing any\nchange to the underlying reinforcement learning\nalgorithm. our results show that this architec-\nture leads to better policy evaluation in the pres-\nence", "neuron\n\nreview\n\norienting and reorienting: the locus coeruleus\nmediates cognition through arousal\n\nsusan j. sara1,* and sebastien bouret2,*\n1laboratoire de physiologie de la perception et de l\u2019action, cnrs, umr-7152, colle` ge de france, 11 pl marcelin berthelot, 75005 paris,\nfrance and memolife laboratory of excellence, paris science & lettres research university, 75005 paris, france\n2team motivation brain & behavior, institut du cerveau et de la moelle e\u00b4 pinie` re, ho\u02c6 pital pitie\u00b4 -salpe\u02c6 trie` re, 47 boulevard de l\u2019ho\u02c6 pital,\n75013 paris, france\n*correspondence: susan.sara@college-de-france.fr (s.j.s.), sebastien.bouret@icm-institute.org (s.b.)\nhttp://dx.doi.org/10.1016/j.neuron.2012.09.011\n\nmood, motivation, attention, and arousal are behavioral states having a profound impact on cognition.\nbehavioral states are mediated though the peripheral nervous system and neuromodulatory systems in\nthe brainstem. the noradrenergic nucleus locus coeruleus is activated in parallel with the au", "reports\n\nfor helpful discussions and comments on the manu-\nscript. supported by nih grants ns09482 (j.c.m.)\nand ns11535, mh44754, and mh48432 (d.j.).\n\n17 september 1996; accepted 5 november 1996\n\nwas further depolarized by current injection\nto produce a burst of aps during the epsps,\nthen a persistent increase (\u2b0e20%) was ob-\nserved in 8 of 11 connections (fig. 1, c and\nd; 94 \u2afe 23% increase) (8, 9).\n\nto establish whether the occurrence of\npostsynaptic aps during epsps was indeed\ncritical for the induction of the increase in\nepsp amplitude, a number of control ex-\nperiments were performed. pairing of indi-\nvidual postsynaptic aps with epsps and\nwithout a sustained postsynaptic depolariza-\ntion (fig. 2a) induced a persistent increase\nin epsp amplitudes (38 \u2afe 9%; n \u2afd 21; 20\nhz; fig. 2b) that was not associated with\nmeasurable changes in input resistance, cur-\nrent\u2013ap discharge relation, or ap thresh-\nold. neither bursts of postsynaptic aps\nalone nor high-frequency bursts of presyn-\naptic a", "communicated by michael jordan \n\nthe helmholtz machine \n\npeter dayan \ngeoffrey e.  hinton \nradford m.  neal \ndepartment  of  computer science, university of  toronto, \n6 king\u2019s college road, toronto, ontario m5s 1a4, canada \n\nrichard s. zemel \ncnl, the salk institute, po box 85800, san diego, c a  92186-5800 usa \n\ndiscovering the structure inherent in a set of  patterns is a fundamen- \ntal aim of  statistical inference or learning.  one fruitful approach is to \nbuild a parameterized stochastic generative model, independent draws \nfrom  which  are  likely  to  produce  the  patterns.  for  all but  the  sim- \nplest generative models, each pattern can be generated in exponentially \nmany ways. it is thus intractable to adjust the parameters to maximize \nthe probability  of  the observed patterns.  we  describe a way of  finess- \ning  this  combinatorial  explosion by  maximizing an  easily  computed \nlower bound  on the probability of  the observations.  our method  can \nbe viewed as a fo", "biorxiv preprint \nthe copyright holder for this preprint (which was\nnot certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available \n\nthis version posted november 5, 2017. \n\nhttps://doi.org/10.1101/214262\n; \n\ndoi: \n\nunder a\n\ncc-by-nc-nd 4.0 international license\n.\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\na theory of multineuronal dimensionality, dynamics and\n\nmeasurement\n\npeiran gao1, eric trautmann2, byron yu3, gopal santhanam4, stephen ryu5,4, krishna\n\nshenoy4,1,6,7,8,9 and surya ganguli\u221710,2,4,8,9\n\n1department of bioengineering, stanford university, stanford, ca 94305, usa\n\n2neurosciences program, school of medicine, stanford university, stanford, ca 94305,\n\nusa\n\n3department of electrical computer engineering and biomedical engineering, carnegie\n\nmellon university, pittsburgh, pa 15213, usa\n\n4department of electrical engineering, st", "6\n1\n0\n2\n\n \nt\nc\no\n \n5\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n4\n5\n4\n8\n0\n\n.\n\n5\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nlinear dynamical neural population models through\n\nnonlinear embeddings\n\nyuanjun gao\u2217 1 , evan archer\u221712, liam paninski12, john p. cunningham12\n\ndepartment of statistics1 and grossman center2\n\ncolumbia university\n\nnew york, ny, united states\n\nyg2312@columbia.edu, evan@stat.columbia.edu,\nliam@stat.columbia.edu, jpc2181@columbia.edu\n\nabstract\n\na body of recent work in modeling neural activity focuses on recovering low-\ndimensional latent features that capture the statistical structure of large-scale neural\npopulations. most such approaches have focused on linear generative models,\nwhere inference is computationally tractable. here, we propose flds, a general\nclass of nonlinear generative models that permits the \ufb01ring rate of each neuron\nto vary as an arbitrary smooth function of a latent, linear dynamical state. this\nextra \ufb02exibility allows the model to capture a richer set of neural variability", "on fast deep nets for agi vision\n\nj\u00a8urgen schmidhuber, dan cires\u00b8an, ueli meier, jonathan masci, alex graves\n\nthe swiss ai lab idsia\n\nuniversity of lugano & supsi, switzerland\n\nmay 5, 2011\n\nabstract\n\narti\ufb01cial general intelligence will not be general without computer vision.\nbiologically inspired adaptive vision models have started to outperform traditional\npre-programmed methods: our fast deep / recurrent neural networks recently col-\nlected a string of 1st ranks in many important visual pattern recognition bench-\nmarks: ijcnn traf\ufb01c sign competition, norb, cifar10, mnist, three icdar\nhandwriting competitions. we greatly pro\ufb01t from recent advances in computing\nhardware, complementing recent progress in the agi theory of mathematically\noptimal universal problem solvers.\n\nkeywords: agi, fast deep neural nets, computer vision, hardware advances vs\ntheoretical progress.\n\n1 introduction\n\ncomputer vision is becoming essential for thousands of practical applications. for ex-\nample, the futur", "extending the effects of spike-timing-dependent\nplasticity to behavioral timescales\n\npatrick j. drew*\u2020 and l. f. abbott\u2021\n\n*neurobiology section, division of biology, university of california at san diego, 9500 gilman drive, la jolla, ca 92093-0357; and \u2021center of neurobiology\nand behavior, department of physiology and cellular biophysics, columbia university college of physicians and surgeons, kolb research annex,\n1051 riverside drive, new york, ny 10032-2695\n\nedited by charles f. stevens, the salk institute for biological studies, la jolla, ca, and approved april 19, 2006 (received for review january 26, 2006)\n\nactivity-dependent modi\ufb01cation of synaptic strengths due to\nspike-timing-dependent plasticity (stdp) is sensitive to correla-\ntions between pre- and postsynaptic \ufb01ring over timescales of tens\nof milliseconds. temporal associations typically encountered in\nbehavioral tasks involve times on the order of seconds. to relate\nthe learning of such temporal associations to stdp, we mus", "denoising diffusion probabilistic models\n\njonathan ho\nuc berkeley\n\njonathanho@berkeley.edu\n\najay jain\nuc berkeley\n\najayj@berkeley.edu\n\npieter abbeel\nuc berkeley\n\npabbeel@cs.berkeley.edu\n\nabstract\n\nwe present high quality image synthesis results using diffusion probabilistic models,\na class of latent variable models inspired by considerations from nonequilibrium\nthermodynamics. our best results are obtained by training on a weighted variational\nbound designed according to a novel connection between diffusion probabilistic\nmodels and denoising score matching with langevin dynamics, and our models nat-\nurally admit a progressive lossy decompression scheme that can be interpreted as a\ngeneralization of autoregressive decoding. on the unconditional cifar10 dataset,\nwe obtain an inception score of 9.46 and a state-of-the-art fid score of 3.17. on\n256x256 lsun, we obtain sample quality similar to progressivegan. our imple-\nmentation is available at https://github.com/hojonathanho/diffusion.\n\n", "neuroresource\n\nunsupervised discovery of demixed, low-\ndimensional neural dynamics across multiple\ntimescales through tensor component analysis\n\ngraphical abstract\n\nauthors\n\nalex h. williams, tony hyun kim,\nforea wang, ..., mark schnitzer,\ntamara g. kolda, surya ganguli\n\ncorrespondence\nahwillia@stanford.edu (a.h.w.),\nsganguli@stanford.edu (s.g.)\n\nin brief\nwilliams et al. describe an unsupervised\nmethod to uncover simple structure in\nlarge-scale recordings by extracting\ndistinct cell assemblies with rapid within-\ntrial dynamics, re\ufb02ecting interpretable\naspects of perceptions, actions, and\nthoughts, and slower across-trial\ndynamics re\ufb02ecting learning and internal\nstate changes.\n\nhighlights\nd tensor component analysis (tca) allows single-trial neural\n\ndimensionality reduction\n\nd tca reveals structure spanning multiple timescales from\n\ncognition to learning\n\nd tca demixes data: features learned from neural data alone\n\ndirectly match behavior\n\nd tca uncovers cell types with common dynamics ", "\f", "s c i e n c e a d va n c e s | r e s e a r c h a r t i c l e\n\nc o m p u t e r s c i e n c e\nstudent of games: a unified learning algorithm for both\nperfect and imperfect information games\nmartin schmid1,2, matej morav\u010d\u00edk1,2, neil burch2,3,4, rudolf kadlec1,2, josh davidson2,3,\nkevin waugh2,3, nolan bard2,3, finbarr timbers2,5, marc lanctot2,6, g. zacharias holland2,3,\nelnaz davoodi2,6, alden christianson2,7, michael bowling2,4,7*\n\ngames have a long history as benchmarks for progress in artificial intelligence. approaches using search and\nlearning produced strong performance across many perfect information games, and approaches using game-\ntheoretic reasoning and learning demonstrated strong performance for specific imperfect information poker\nvariants. we introduce student of games, a general-purpose algorithm that unifies previous approaches, com-\nbining guided search, self-play learning, and game-theoretic reasoning. student of games achieves strong em-\npirical performance in large p", "0\n2\n0\n2\n\n \n\nb\ne\nf\n4\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n6\n7\n6\n0\n\n.\n\n6\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nfinding the needle in the haystack with\n\nconvolutions: on the bene\ufb01ts of architectural bias\n\nst\u00e9phane d\u2019ascoli\n\nstephane.dascoli@ens.fr\n\nlaboratoire de physique de l\u2019ecole normale sup\u00e9rieure ens, universit\u00e9 psl,\n\ncnrs, sorbonne universit\u00e9, universit\u00e9 paris-diderot, sorbonne paris cit\u00e9, paris, france\n\nlevent sagun\n\nleventsagun@fb.com\nfacebook ai research\nfacebook, paris, france\n\njoan bruna\n\nbruna@cims.nyu.edu\n\ncourant institute of mathematical sciences and center for data science\n\nnew york university, new york city, united states\n\ngiulio biroli\n\ngiulio.biroli@lps.ens.fr\n\nlaboratoire de physique de l\u2019ecole normale sup\u00e9rieure ens, universit\u00e9 psl,\n\ncnrs, sorbonne universit\u00e9, universit\u00e9 paris-diderot, sorbonne paris cit\u00e9, paris, france\n\nabstract\n\ndespite the phenomenal success of deep neural networks in a broad range of\nlearning tasks, there is a lack of theory to understand the way they work. in\nparticul", "\f", "reinforcement learning improves \nbehaviour from evaluative feedback\n\nmichael l. littman1\n\nreinforcement learning is a branch of machine learning concerned with using experience gained through interacting \nwith the world and evaluative feedback to improve a system\u2019s ability to make behavioural decisions. it has been called the \nartificial intelligence problem in a microcosm because learning algorithms must act autonomously to perform well and \nachieve their goals. partly driven by the increasing availability of rich data, recent years have seen exciting advances in \nthe theory and practice of reinforcement learning, including developments in fundamental technical areas such as gen-\neralization, planning, exploration and empirical methodology, leading to increasing applicability to real-life problems.\n\nreinforcement-learning algorithms1,2 are inspired by our under-\n\nstanding of decision making in humans and other animals in \nwhich learning is supervised through the use of reward signals ", "articles\n\nvirtual reality for freely moving animals\njohn r stowers1,2, maximilian hofbauer1\u20134, renaud bastien5,6, johannes griessner1 \nsarfarazhussain farooqui3,4,7, ruth m fischer3, karin nowikovsky7, wulf haubensak1 \nkristin tessmar-raible3,4 \n\n  & andrew d straw1,8 \n\n \n\n , peter higgins1, \n\n , iain d couzin5,6, \n\nstandard animal behavior paradigms incompletely mimic \nnature and thus limit our understanding of behavior and brain \nfunction. virtual reality (vr) can help, but it poses challenges. \ntypical vr systems require movement restrictions but disrupt \nsensorimotor experience, causing neuronal and behavioral \nalterations. we report the development of freemovr, a vr \nsystem for freely moving animals. we validate immersive \nvr for mice, flies, and zebrafish. freemovr allows instant, \ndisruption-free environmental reconfigurations and interactions \nbetween real organisms and computer-controlled agents. using \nthe freemovr platform, we established a height-aversion \nassay in mice and", "research article\npredictive representations can link model-\nbased reinforcement learning to model-free\nmechanisms\n\nevan m. russek1\u262f*, ida momennejad2\u262f, matthew m. botvinick3, samuel j. gershman4,\nnathaniel d. daw2\n\n1 center for neural science, new york university, new york, ny, united states of america, 2 princeton\nneuroscience institute and department of psychology, princeton university, princeton, nj, united states of\namerica, 3 deepmind, london, united kingdom and gatsby computational neuroscience unit, university\ncollege london, united kingdom, 4 department of psychology and center for brain science, harvard\nuniversity, cambridge, ma, united states of america\n\n\u262f these authors contributed equally to this work.\n* emr443@nyu.edu\n\nabstract\n\nhumans and animals are capable of evaluating actions by considering their long-run future\nrewards through a process described using model-based reinforcement learning (rl) algo-\nrithms. the mechanisms by which neural circuits perform the computation", "a r t i c l e s\n\nregulation of neuronal input transformations by \ntunable dendritic inhibition\nmatthew lovett-barron1, gergely f turi1, patrick kaifosh1, peter h lee2, fr\u00e9d\u00e9ric bolze3, xiao-hua sun3,  \njean-fran\u00e7ois nicoud3, boris v zemelman4, scott m sternson2 & attila losonczy1\n\ntransforming synaptic input into action potential output is a fundamental function of neurons. the pattern of action potential \noutput from principal cells of the mammalian hippocampus encodes spatial and nonspatial information, but the cellular and \ncircuit mechanisms by which neurons transform their synaptic input into a given output are unknown. using a combination \nof optical activation and cell type\u2013specific pharmacogenetic silencing in vitro, we found that dendritic inhibition is the \nprimary regulator of input-output transformations in mouse hippocampal ca1 pyramidal cells, and acts by gating the dendritic \nelectrogenesis driving burst spiking. dendrite-targeting interneurons are themselves modulated b", "vol 466 | 22 july 2010 | doi:10.1038/nature09263\n\narticles\n\nstart/stop signals emerge in nigrostriatal\ncircuits during sequence learning\n\nxin jin1 & rui m. costa1,2\n\nlearning new action sequences subserves a plethora of different abilities such as escaping a predator, playing the piano, or\nproducing fluent speech. proper initiation and termination of each action sequence is critical for the organization of\nbehaviour, and is compromised in nigrostriatal disorders like parkinson\u2019s and huntington\u2019s diseases. using a self-paced\noperant task in which mice learn to perform a particular sequence of actions to obtain an outcome, we found neural activity in\nnigrostriatal circuits specifically signalling the initiation or the termination of each action sequence. this start/stop activity\nemerged during sequence learning, was specific for particular actions, and did not reflect interval timing, movement speed or\naction value. furthermore, genetically altering the function of striatal circuits disr", "6\n1\n0\n2\n\n \n\ny\na\nm\n7\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n8\n7\n7\n6\n0\n\n.\n\n4\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nbenchmarking deep reinforcement learning for continuous control\n\nyan duan\u2020\nxi chen\u2020\nrein houthooft\u2020\u2021\njohn schulman\u2020\u00a7\npieter abbeel\u2020\n\u2020 university of california, berkeley, department of electrical engineering and computer sciences\n\u2021 ghent university - iminds, department of information technology\n\u00a7 openai\n\nrockyduan@eecs.berkeley.edu\nc.xi@eecs.berkeley.edu\nrein.houthooft@ugent.be\njoschu@eecs.berkeley.edu\npabbeel@cs.berkeley.edu\n\nabstract\n\nrecently,\nresearchers have made signi\ufb01cant\nprogress combining the advances in deep learn-\ning for learning feature representations with rein-\nforcement learning. some notable examples in-\nclude training agents to play atari games based\non raw pixel data and to acquire advanced ma-\nnipulation skills using raw sensory inputs. how-\never, it has been dif\ufb01cult to quantify progress\nin the domain of continuous control due to the\nlack of a commonly adopted benchmark. in this", "7\n1\n0\n2\n\n \nl\nu\nj\n \n\n1\n1\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n4\n1\n9\n7\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nwhy do similarity matching objectives lead to\n\nhebbian/anti-hebbian networks?\n\ncengiz pehlevan1, anirvan m. sengupta1,2, and dmitri b.\n\nchklovskii1,3\n\n1center for computational biology, flatiron institute, new york,\n\nny\n\n2physics and astronomy department, rutgers university, new\n\nbrunswick, nj\n\n3nyu langone medical center, new york, ny\n\nabstract\n\nmodeling self-organization of neural networks for unsupervised learning\nusing hebbian and anti-hebbian plasticity has a long history in neuro-\nscience. yet, derivations of single-layer networks with such local learning\nrules from principled optimization objectives became possible only recently,\nwith the introduction of similarity matching objectives. what explains the\nsuccess of similarity matching objectives in deriving neural networks with\nlocal learning rules? here, using dimensionality reduction as an example,\nwe introduce several variable substit", "7\n1\n0\n2\n\n \n\nn\na\nj\n \n\n1\n2\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n6\n9\n5\n5\n0\n\n.\n\n2\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nneuromorphic deep learning machines\n\nemre neftci1, charles augustine3, somnath paul3, and georgios detorakis1\n\n1department of cognitive sciences, uc irvine, irvine, ca, usa,\n3circuit research lab, intel corporation, hilsboro, or, usa,\n\njanuary 24, 2017\n\nabstract\n\nan ongoing challenge in neuromorphic computing is to devise general and computationally e\ufb03cient\nmodels of inference and learning which are compatible with the spatial and temporal constraints of the\nbrain. one increasingly popular and successful approach is to take inspiration from inference and learning\nalgorithms used in deep neural networks. however, the workhorse of deep learning, the gradient descent\nback propagation (bp) rule, often relies on the immediate availability of network-wide information stored\nwith high-precision memory, and precise operations that are di\ufb03cult to realize in neuromorphic hardware.\nremarkably, recent work show", "e\u0000cient planning in mdps by small backups\n\nharm van seijen\nrichard s. sutton\ndepartment of computing science, university of alberta, edmonton, alberta, t6g 2e8, canada\n\nharm.vanseijen@ualberta.ca\nsutton@cs.ualberta.ca\n\nabstract\n\ne\u0000cient planning plays a crucial role in\nmodel-based reinforcement learning. tradi-\ntionally, the main planning operation is a\nfull backup based on the current estimates of\nthe successor states. consequently, its com-\nputation time is proportional to the num-\nber of successor states.\nin this paper, we\nintroduce a new planning backup that uses\nonly the current value of a single successor\nstate and has a computation time indepen-\ndent of the number of successor states. this\nnew backup, which we call a small backup,\nopens the door to a new class of model-based\nreinforcement learning methods that exhibit\nmuch \ufb01ner control over their planning process\nthan traditional methods. we empirically\ndemonstrate that this increased \ufb02exibility al-\nlows for more e\u0000cient plannin", " \n\na hebbian/anti-hebbian network for online sparse \ndictionary learning derived from symmetric matrix \n\nfactorization \n\ntao hu1, cengiz pehlevan2,3, and dmitri b. chklovskii3 \n\n1 texas a&m university \n\nms 3128 tamus  \n\ncollege station, tx 77843 \n\ntaohu@tees.tamus.edu \n\n2 janelia farm research campus \nhoward hughes medical institute \n\nashburn, va 20147 \n\npehlevanc@janelia.hhmi.org \n\n3 simons center for data analysis \n\nsimons foundation \nnew york, ny 10010 \n\nmitya@simonsfoundation.org \n\nimage  ensemble \n\nabstract\u2014olshausen  and  field  (of)  proposed  that  neural \ncomputations in the primary visual cortex (v1) can be partially \nmodelled  by  sparse  dictionary  learning.  by  minimizing  the \nregularized  representation  error \nthey  derived  an  online \nalgorithm,  which  learns  gabor-filter  receptive  fields  from  a \nnatural \nin  agreement  with  physiological \nexperiments. whereas the of algorithm can be mapped onto the \ndynamics  and  synaptic  plasticity \nin  a  single-layer  n", "insight review articles\n\ncomputational roles for dopamine \nin behavioural control\n\np. read montague1,2, steven e. hyman3 & jonathan d. cohen4,5\n\n1department of neuroscience and 2menninger department of psychiatry and behavioral sciences, baylor college of medicine, 1 baylor plaza,\nhouston, texas 77030, usa (e-mail: read@bcm.tmc.edu)\n3harvard university, cambridge, massachusetts 02138, usa (e-mail: seh@harvard.edu) \n4department of psychiatry, university of pittsburgh and 5department of psychology, center for the study of brain, mind & behavior, \ngreen hall, princeton university, princeton, new jersey 08544, usa (e-mail: jdc@princeton.edu)\n\nneuromodulators such as dopamine have a central role in cognitive disorders. in the past decade, biological\nfindings on dopamine function have been infused with concepts taken from computational theories of\nreinforcement learning. these more abstract approaches have now been applied to describe the biological\nalgorithms at play in our brains when we f", "replay, the default mode \nnetwork and the cascaded memory \nsystems model\n\nkarola\u00a0kaefer \nfrancesco\u00a0p.\u00a0battaglia \n\n \n\n , federico\u00a0stella \n\n , bruce\u00a0l.\u00a0mcnaughton \n\n  and \n\nabstract | the spontaneous replay of patterns of activity related to past experiences \nand memories is a striking feature of brain activity, as is the coherent activation of \nsets of brain areas \u2014 particularly those comprising the default mode network \n(dmn) \u2014 during rest. we propose that these two phenomena are strongly \nintertwined and that their potential functions overlap. in the \u2018cascaded memory \nsystems model\u2019 that we outline here, we hypothesize that the dmn forms  \nthe backbone for the propagation of replay, mediating interactions between the  \nhippocampus and the neocortex that enable the consolidation of new memories. \nthe dmn may also independently ignite replay cascades, which support \nreactivation of older memories or high-level semantic representations. we suggest \nthat transient cortical activations, in", "letter\n\ncommunicated by douglas tweed\n\nequivalence of equilibrium propagation\nand recurrent backpropagation\n\nbenjamin scellier\nbenjamin.scellier@polytechnique.edu\nuniversity of montreal, montreal, quebec, h3t 1n8, canada\n\nyoshua bengio\nyoshua.bengio@mila.quebec\nuniversity of montreal, montreal, quebec, h3t 1n8, canada, and cifar\n\nrecurrent backpropagation and equilibrium propagation are supervised\nlearning algorithms for fixed-point recurrent neural networks, which dif-\nfer in their second phase. in the first phase, both algorithms converge to\na fixed point that corresponds to the configuration where the prediction\nis made. in the second phase, equilibrium propagation relaxes to another\nnearby fixed point corresponding to smaller prediction error, whereas re-\ncurrent backpropagation uses a side network to compute error derivatives\niteratively. in this work, we establish a close connection between these\ntwo algorithms. we show that at every moment in the second phase, the\ntemporal deriv", "4\n1\n0\n2\n\n \n\np\ne\ns\n7\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n2\n4\n8\n4\n\n.\n\n9\n0\n4\n1\n:\nv\ni\nx\nr\na\n\ngoing deeper with convolutions\n\nchristian szegedy\n\nwei liu\n\ngoogle inc.\n\nuniversity of north carolina, chapel hill\n\nyangqing jia\ngoogle inc.\n\npierre sermanet\n\nscott reed\n\ndragomir anguelov\n\ndumitru erhan\n\ngoogle inc.\n\nuniversity of michigan\n\ngoogle inc.\n\ngoogle inc.\n\nvincent vanhoucke\n\ngoogle inc.\n\nandrew rabinovich\n\ngoogle inc.\n\nabstract\n\nwe propose a deep convolutional neural network architecture codenamed incep-\ntion, which was responsible for setting the new state of the art for classi\ufb01cation\nand detection in the imagenet large-scale visual recognition challenge 2014\n(ilsvrc14). the main hallmark of this architecture is the improved utilization\nof the computing resources inside the network. this was achieved by a carefully\ncrafted design that allows for increasing the depth and width of the network while\nkeeping the computational budget constant. to optimize quality, the architectural\ndecisions w", "16494 \u2022 the journal of neuroscience, november 16, 2011 \u2022 31(46):16494 \u201316506\n\nbehavioral/systems/cognitive\n\nsynaptic properties of corticocortical connections between\nthe primary and secondary visual cortical areas in the\nmouse\n\nroberto de pasquale and s. murray sherman\ndepartment of neurobiology, university of chicago, chicago, illinois 60637\n\ndespite the importance of corticocortical connections, few published studies have investigated the functional, synaptic properties of such\nconnections in any species, because most studies have been purely anatomical or aimed at functional features other than synaptic\nproperties. we recently published a study of synaptic properties of connections between the primary and secondary cortical auditory\nareas in brain slices from the mouse, and, in the present study, we aimed to extend this by performing analogous studies of the primary\nand secondary visual areas (v1 and v2). we found effectively the same results. that is, connections between v1 and v2", "7\n1\n0\n2\n\n \n\nn\na\nj\n \n\n3\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n3\n6\n7\n5\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nlearning to reinforcement learn\n\njx wang1, z kurth-nelson1, d tirumala1, h soyer1, jz leibo1,\nr munos1, c blundell1, d kumaran1,3, m botvinick1,2\n1deepmind, london, uk\n2gatsby computational neuroscience unit, ucl, london, uk\n3institute of cognitive neuroscience, ucl, london, uk\n\n{wangjane, zebk, dhruvat, soyer, jzl, munos, cblundell,\ndkumaran, botvinick} @google.com\n\nabstract\n\nin recent years deep reinforcement learning (rl) systems have attained superhuman\nperformance in a number of challenging task domains. however, a major limitation\nof such applications is their demand for massive amounts of training data. a critical\npresent objective is thus to develop deep rl methods that can adapt rapidly to new\ntasks. in the present work we introduce a novel approach to this challenge, which\nwe refer to as deep meta-reinforcement learning. previous work has shown that\nrecurrent networks can support meta-lea", "neurobiology of learning and memory 115 (2014) 68\u201377\n\ncontents lists available at sciencedirect\n\nneurobiology of learning and memory\n\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / y n l m e\n\nreview\nmodulation of learning and memory by cytokines: signaling\nmechanisms and long term consequences\n\nelissa j. donzis, natalie c. tronson\n\n\u21d1\n\ndepartment of psychology, university of michigan, ann arbor, mi 48109, usa\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 7 april 2014\nrevised 12 august 2014\naccepted 13 august 2014\navailable online 21 august 2014\n\nkeywords:\ncytokine\nmemory modulation\nlearning\nsignal transduction\nepigenetic modi\ufb01cation\nneurogenesis\n\nthis review describes the role of cytokines and their downstream signaling cascades on the modulation of\nlearning and memory. immune proteins are required for many key neural processes and dysregulation of\nthese functions by systemic in\ufb02ammation can result in impairments of memory that pers", "psychological  review\n2000, vol.  107, no.  2, 358-367\n\ncopyright 2000 by the american  psychological  association, inc.\n0033-295x/00/s5.0q  doi:  10.1037//0033-295x.107.2.358\n\ntheoretical notes\n\nhow  persuasive  is  a good  fit?  a comment on theory testing\n\nseth roberts\n\nuniversity  of  california,  berkeley\n\nharold pashler\n\nuniversity  of  california,  san  diego\n\nquantitative  theories  with  free  parameters  often  gain  credence  when they  closely  fit  data.  this  is  a\nmistake.  a  good  fit  reveals  nothing  about  the  flexibility  of  the  theory  (how much  it  cannot  fit), the\nvariability of the data (how firmly  the data rule out what the theory cannot fit), or the likelihood  of other\noutcomes  (perhaps  the  theory  could  have  fit  any plausible  result), and a  reader  needs  all  3  pieces of\ninformation  to  decide  how  much  the  fit  should  increase  belief  in the  theory.  the  use  of  good  fits  as\nevidence is not  supported by philosophers  of scienc", "review\n\ntrends in neurosciences vol.27 no.8 august 2004\n\nputting a spin on the dorsal \u2013ventral\ndivide of the striatum\npieter voorn1, louk j.m.j. vanderschuren2, henk j. groenewegen1,\ntrevor w. robbins3 and cyriel m.a. pennartz4\n\n1department of anatomy, research institute neurosciences, vu university medical center, mf-g-102, po box 7057, 1007 mb,\namsterdam, the netherlands\n2rudolf magnus institute of neuroscience, department of pharmacology and anatomy, university medical center utrecht,\n3584 cg utrecht, the netherlands\n3department of experimental psychology, university of cambridge, downing street, cambridge cb2 3eb, uk\n4department of animal physiology and cognitive neuroscience, neurobiology section of swammerdam institute for life sciences,\nuniversity of amsterdam, faculty of science, 1098 sm amsterdam, the netherlands\n\nsince its conception three decades ago, the idea that\nthe striatum consists of a dorsal sensorimotor part and\na ventral portion processing limbic information has\nspa", "communicated by david  haussler \n\nbayesian interpolation \n\ndavid j. c. mackay\u2019 \ncomputation and neural systems, california institute of  technology 139-74, \npasadena, ca 91225 usa \n\nalthough bayesian analysis has been in use since laplace, the bayesian \nmethod  of  model-comparison  has  only  recently  been  developed  in \ndepth.  in this  paper,  the  bayesian  approach  to  regularization  and \nmodel-comparison  is demonstrated  by  studying  the  inference  prob- \nlem of  interpolating noisy data.  the concepts and methods described \nare  quite  general  and  can  be  applied  to  many  other  data  modeling \nproblems.  regularizing constants are set by examining their posterior \nprobability distribution.  alternative regularizers (priors) and alterna- \ntive basis sets are objectively compared by evaluating the evidence for \nthem.  \u201doccam\u2019s razor\u201d is automatically embodied by this process.  the \nway  in  which  bayes infers  the  values  of  regularizing constants  and \nnoise level", "article\n\ncommunicated by jonathan victor\n\nestimation of entropy and mutual information\n\nliam paninski\nliam@cns.nyu.edu\ncenter for neural science, new york university, new york, ny 10003, u.s.a.\n\nwe present some new results on the nonparametric estimation of entropy\nand mutual information. first, we use an exact local expansion of the\nentropy function to prove almost sure consistency and central limit the-\norems for three of the most commonly used discretized information esti-\nmators. the setup is related to grenander\u2019s method of sieves and places\nno assumptions on the underlying probability measure generating the\ndata. second, we prove a converse to these consistency theorems, demon-\nstrating that a misapplication of the most common estimation techniques\nleads to an arbitrarily poor estimate of the true information, even given\nunlimited data. this \u201cinconsistency\u201d theorem leads to an analytical ap-\nproximation of the bias, valid in surprisingly small sample regimes and\nmore accurate tha", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/9042578\n\nindependent rate and temporal coding in hippocampal pyramidal cells\n\narticle\u00a0\u00a0in\u00a0\u00a0nature \u00b7 november 2003\n\ndoi: 10.1038/nature02058\u00a0\u00b7\u00a0source: pubmed\n\ncitations\n536\n\n3 authors, including:\n\nneil burgess\nuniversity college london\n\n363 publications\u00a0\u00a0\u00a033,711 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n393\n\njohn o'keefe\nuniversity of toronto\n\n86 publications\u00a0\u00a0\u00a017,217 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nall content following this page was uploaded by neil burgess on 20 may 2014.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "gtm: the generative\ntopographic mapping\n\nchristopher m. bishop,\n\nmarkus svens(cid:19)en\n\nmicrosoft research\n\n7 j j thomson avenue\n\ncambridge, cb3 0fb, u.k.\n\nfcmbishop,markussvg@microsoft.com\n\nhttp://research.microsoft.com/f(cid:24)cmbishop,(cid:24)markussvg\n\nchristopher k. i. williams\n\ninstitute for adaptive and neural computation\ndivision of informatics, university of edinburgh\n\n5 forrest hill, edinburgh, eh1 2ql, scotland, u.k.\n\nckiw@dai.ed.ac.uk\n\npublished as: \"the generative topographic mapping, neural computation 10, no. 1, 215{\n234 (1998)\n\nabstract\n\nlatent variable models represent the probability density of data in a space of several\ndimensions in terms of a smaller number of latent, or hidden, variables. a familiar\nexample is factor analysis which is based on a linear transformations between the\nlatent space and the data space. in this paper we introduce a form of non-linear\nlatent variable model called the generative topographic mapping for which the pa-\nrameters of the model ", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\ntowards  the  neural  population  doctrine\nshreya  saxena  and  john  p  cunningham\n\nacross  neuroscience,  large-scale  data  recording  and\npopulation-level  analysis  methods  have  experienced  explosive\ngrowth.  while  the  underlying  hardware  and  computational\ntechniques  have  been  well  reviewed,  we  focus  here  on  the\nnovel  science  that  these  technologies  have  enabled.  we  detail\nfour  areas  of  the  \ufb01eld  where  the  joint  analysis  of  neural\npopulations  has  signi\ufb01cantly  furthered  our  understanding  of\ncomputation  in  the  brain:  correlated  variability,  decoding,\nneural  dynamics,  and  arti\ufb01cial  neural  networks.  together,  these\n\ufb01ndings  suggest  an  exciting  trend  towards  a  new  era  where\nneural  populations  are  understood  to  be  the  essential  unit  of\ncomputation  in  many  brain  regions,  a  classic  idea  that  has  been\ngiven  new  life.\n\naddress\ndepartment  of ", "journal of machine learning research 3 (2003) 1137\u20131155\n\nsubmitted 4/02; published 2/03\n\na neural probabilistic language model\n\nyoshua bengio\nr\u00e9jean ducharme\npascal vincent\nchristian jauvin\nd\u00e9partement d\u2019informatique et recherche op\u00e9rationnelle\ncentre de recherche math\u00e9matiques\nuniversit\u00e9 de montr\u00e9al, montr\u00e9al, qu\u00e9bec, canada\n\nbengioy@iro.umontreal.ca\nducharme@iro.umontreal.ca\nvincentp@iro.umontreal.ca\njauvinc@iro.umontreal.ca\n\neditors: jaz kandola, thomas hofmann, tomaso poggio and john shawe-taylor\n\nabstract\n\na goal of statistical language modeling is to learn the joint probability function of sequences of\nwords in a language. this is intrinsically dif\ufb01cult because of the curse of dimensionality: a word\nsequence on which the model will be tested is likely to be different from all the word sequences seen\nduring training. traditional but very successful approaches based on n-grams obtain generalization\nby concatenating very short overlapping sequences seen in the training set. we propo", "generation of stable heading \nrepresentations in diverse visual scenes\n\nhttps://doi.org/10.1038/s41586-019-1767-1\nreceived: 29 december 2018\naccepted: 7 october 2019\npublished online: 20 november 2019\n\nsung soo kim1,3,4*, ann m. hermundstad1, sandro romani1, l. f. abbott1,2 & vivek jayaraman1*\n\nmany animals rely on an internal heading representation when navigating in varied \nenvironments1\u201310. how this representation is linked to the sensory cues that define \ndifferent surroundings is unclear. in the fly brain, heading is represented by \u2018compass\u2019 \nneurons that innervate a ring-shaped structure known as the ellipsoid body3,11,12. each \ncompass neuron receives inputs from \u2018ring\u2019 neurons that are selective for particular \nvisual features13\u201316; this combination provides an ideal substrate for the extraction of \ndirectional information from a visual scene. here we combine two-photon calcium \nimaging and optogenetics in tethered flying flies with circuit modelling, and show how \nthe correlat", "ieee transactions on neural networks, vol. 8, no. 6, november 1997\n\n1321\n\nnonlinear backpropagation: doing backpropagation\n\nwithout derivatives of the activation function\n\njohn hertz, anders krogh, benny lautrup, and torsten lehmann\n\nabstract\u2014the conventional linear backpropagation algorithm\nis replaced by a nonlinear version, which avoids the necessity for\ncalculating the derivative of the activation function. this may be\nexploited in hardware realizations of neural processors. in this\npaper we derive the nonlinear backpropagation algorithms in\nthe framework of recurrent backpropagation and present some\nnumerical simulations of feedforward networks on the nettalk\nproblem. a discussion of implementation in analog very large\nscale integration (vlsi) electronics concludes the paper.\n\nindex terms\u2014 backpropagation, neural-network implementa-\n\ntion, neural networks, recurrent backpropagation.\n\ngation and present some numerical simulations of feedforward\nnetworks on the nettalk problem [4]. ", "journal of physiology - paris 97 (2003) 683\u2013694\n\nwww.elsevier.com/locate/jphysparis\n\noptimal computation with attractor networks\n\npeter e. latham a,*, sophie deneve b, alexandre pouget c\n\na department of neurobiology, university of california at los angeles, los angeles, ca 90095-1763, usa\n\nb gatsby computational neuroscience unit, university college london, london wc1 3ar, uk\n\nc department of brain and cognitive sciences, university of rochester, rochester, ny 14627, usa\n\nabstract\n\nwe investigate the ability of multi-dimensional attractor networks to perform reliable computations with noisy population codes.\nwe show that such networks can perform computations as reliably as possible\u2013\u2013meaning they can reach the cram\u0013er-rao bound\u2013\u2013\nso long as the noise is small enough. \u2018\u2018small enough\u2019\u2019 depends on the properties of the noise, especially its correlational structure.\nfor many correlational structures, noise in the range of what is observed in the cortex is su\ufb03ciently small that biologicall", "_na~tu_r_e_v_o_l_. 3_2_3 _9_0_ct_o_b_e_r_1_98_6 __ ____ __ _  letterstonature - - - - - - - - - - - - - - - - - -=533  \n\ndelineating  the  absolute  indigeneity  of amino  acids  in  fossils. \nas  ams  techniques  are  refined  to  handle  smaller  samples,  it \nmay also become possible to date individual amino acid enan \ntiomers by the 14c method. if one enantiomer is entirely derived \nfrom the other by racemization during diagenesis, the individual \nd- and  l-enantiomers  for  a  given  amino  acid  should  have \nidentical  14c  ages. \n\nolder,  more  poorly preserved  fossils  may  not always  prove \namenable to the determination of amino acid indigeneity by the \nstable  isotope  method,  as  the  prospects  for  complete  replace \nment  of indigenous  amino  acids  with  non-indigenous  amino \nacids  increases  with  time.  as  non-indigenous  amino  acids \nundergo  racemization,  the  enantiomers  may  have  identical \nisotopic  compositions  and  still  not  be  related  to  the  ", "understanding batch normalization\n\njohan bjorck, carla gomes, bart selman, kilian q. weinberger\n\ncornell university\n\n{njb225,gomes,selman,kqw4} @cornell.edu\n\nabstract\n\nbatch normalization (bn) is a technique to normalize activations in intermediate\nlayers of deep neural networks.\nits tendency to improve accuracy and speed\nup training have established bn as a favorite technique in deep learning. yet,\ndespite its enormous success, there remains little consensus on the exact reason\nand mechanism behind these improvements. in this paper we take a step towards a\nbetter understanding of bn, following an empirical approach. we conduct several\nexperiments, and show that bn primarily enables training with larger learning rates,\nwhich is the cause for faster convergence and better generalization. for networks\nwithout bn we demonstrate how large gradient updates can result in diverging loss\nand activations growing uncontrollably with network depth, which limits possible\nlearning rates. bn avoids ", "vol 440|30 march 2006|doi:10.1038/nature04587\n\nletters\n\nreverse replay of behavioural sequences in\nhippocampal place cells during the awake state\ndavid j. foster1 & matthew a. wilson1\n\nthe hippocampus has long been known to be involved in spatial\nnavigational learning in rodents1,2, and in memory for events in\nrodents3,4, primates5 and humans6. a unifying property of both\nnavigation and event memory is a requirement for dealing with\ntemporally sequenced information. reactivation of temporally\nsequenced memories for previous behavioural experiences has\nbeen reported in sleep in rats7,8. here we report that sequential\nreplay occurs in the rat hippocampus during awake periods\nimmediately after spatial experience. this replay has a unique\nform, in which recent episodes of spatial experience are replayed\nin a temporally reversed order. this replay is suggestive of a role in\nthe evaluation of event sequences in the manner of reinforcement\nlearning models. we propose that such replay might co", "physical review x 8, 031003 (2018)\n\nclassification and geometry of general perceptual manifolds\n\nsueyeon chung,1,2,6 daniel d. lee,2,3 and haim sompolinsky2,4,5\n\n1program in applied physics, school of engineering and applied sciences, harvard university,\n\ncambridge, massachusetts 02138, usa\n\n2center for brain science, harvard university, cambridge, massachusetts 02138, usa\n\n3school of engineering and applied science, university of pennsylvania,\n\nphiladelphia, pennsylvania 19104, usa\n\n4racah institute of physics, hebrew university, jerusalem 91904, israel\n\n5edmond and lily safra center for brain sciences, hebrew university, jerusalem 91904, israel\n\n6department of brain and cognitive sciences, massachusetts institute of technology,\n\ncambridge, massachusetts 02139, usa\n\n \n\n(received 17 october 2017; published 5 july 2018)\n\nperceptual manifolds arise when a neural population responds to an ensemble of sensory signals\nassociated with different physical features (e.g., orientation, pose, sca", "the journal of neuroscience, november 1, 2002, 22(21):9475\u20139489\n\nresponse of neurons in the lateral intraparietal area during a\ncombined visual discrimination reaction time task\n\njamie d. roitman1 and michael n. shadlen2\n1program in neurobiology and behavior, and 2howard hughes medical institute, department of physiology and\nbiophysics, and regional primate research center, university of washington, seattle, washington 98195-7290\n\ndecisions about the visual world can take time to form, espe-\ncially when information is unreliable. we studied the neural\ncorrelate of gradual decision formation by recording activity\nfrom the lateral intraparietal cortex (area lip) of rhesus monkeys\nduring a combined motion-discrimination reaction-time task.\nmonkeys reported the direction of random-dot motion by mak-\ning an eye movement to one of two peripheral choice targets,\none of which was within the response \ufb01eld of the neuron. we\nvaried the dif\ufb01culty of the task and measured both the accuracy\nof direc", "original research\npublished: 31 august 2018\ndoi: 10.3389/fnins.2018.00608\n\ndeep supervised learning using\nlocal errors\n\nhesham mostafa 1*, vishwajith ramesh 2 and gert cauwenberghs 1,2\n\n1 institute for neural computation, university of california, san diego, san diego, ca, united states, 2 department of\nbioengineering, university of california, san diego, san diego, ca, united states\n\nerror backpropagation is a highly effective mechanism for\nlearning high-quality\nhierarchical features in deep networks. updating the features or weights in one layer,\nhowever, requires waiting for the propagation of error signals from higher layers. learning\nusing delayed and non-local errors makes it hard to reconcile backpropagation with\nthe learning mechanisms observed in biological neural networks as it requires the\nneurons to maintain a memory of the input long enough until the higher-layer errors\narrive. in this paper, we propose an alternative learning mechanism where errors are\ngenerated locally i", "10\n\ndistributedoptimizationofdeeplynestedsystemsmiguel\u00b4a.carreira-perpi\u02dcn\u00b4anweiranwangelectricalengineeringandcomputerscience,schoolofengineering,universityofcalifornia,mercedabstractintelligentprocessingofcomplexsignalssuchasimagesisoftenperformedbyahierarchyofnon-linearprocessinglayers,suchasadeepnetoranobjectrecognitioncascade.jointestimationoftheparametersofallthelayersisadif\ufb01cultnonconvexoptimization.wedescribeageneralstrategytolearntheparametersand,tosomeex-tent,thearchitectureofnestedsystems,whichwecallthemethodofauxiliarycoordinates(mac).thisreplacestheoriginalprobleminvolvingadeeplynestedfunctionwithaconstrainedprob-leminvolvingadifferentfunctioninanaug-mentedspacewithoutnesting.theconstrainedproblemmaybesolvedwithpenalty-basedmeth-odsusingalternatingoptimizationoverthepa-rametersandtheauxiliarycoordinates.machasprovableconvergence,iseasytoimplementreusingexistingalgorithmsforsinglelayers,canbeparallelizedtriviallyandmassively,appliesevenwhenparameterderivativesarenotavail-abl", "\f", "supplementary figure 1 \n\nlow-dimensional structure in the head direction circuit \n\neach column shows results from a different animal. (a) waking manifold visualized using isomap1. (b) waking manifold visualized using \na  variational  autoencoder2,3.  (c)  betti  barcodes  on  data  without  outlier  removal.  top  row  shows  betti-0  (number  of  connected \ncomponents); second row shows betti-1 (number of rings); third row shows betti-2 (number of three-dimensional holes or voids). there\nare a very large number of transient features, so for ease of visualization, features are plotted if they lie in the 99th, 98th and 90th \npercentile of lengths for betti-0, -1 and -2 respectively. for betti-1 rings this corresponds to minimum lengths of 2.22, 1.62, 1.18, 0.39,\n0.57,  1.16,  2.61  \u221ahz  respectively  (left  to  right).  (d)  as  in  (c)  but  for  data  with  outliers  removed  (see  si  s2.3  on  nt-tda).  percentile \nthresholds  are  the  same  as  in  panel  (c).  corresponding  mini", "gaussian-process factor analysis for low-dimensional\n\nsingle-trial analysis of neural population activity\n\nbyron m. yu1,2,4, john p. cunningham1, gopal santhanam1,\n\nstephen i. ryu1,3, krishna v. shenoy1,2\n\n1department of electrical engineering, 2neurosciences program,\n\n3department of neurosurgery, stanford university, stanford, ca 94305\n\n{byronyu,jcunnin,gopals,seoulman,shenoy}@stanford.edu\n\nmaneesh sahani4\n\n4gatsby computational neuroscience unit, ucl\n\nlondon, wc1n 3ar, uk\n\nmaneesh@gatsby.ucl.ac.uk\n\nabstract\n\nwe consider the problem of extracting smooth, low-dimensional neural trajecto-\nries that summarize the activity recorded simultaneously from tens to hundreds of\nneurons on individual experimental trials. current methods for extracting neural\ntrajectories involve a two-stage process: the data are \ufb01rst \u201cdenoised\u201d by smooth-\ning over time, then a static dimensionality reduction technique is applied. we \ufb01rst\ndescribe extensions of the two-stage methods that allow the degree of smooth", "0\n2\n0\n2\n\n \n\nn\na\nj\n \n\n9\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n7\n1\n0\n2\n0\n\n.\n\n1\n1\n8\n1\n:\nv\ni\nx\nr\na\n\na general theory of equivariant cnns on\n\nhomogeneous spaces\n\ntaco s. cohen\n\nqualcomm ai research\u2217\n\nmario geiger\n\npcsl research group\n\nqualcomm technologies netherlands b.v.\n\nepfl\n\ntacos@qti.qualcomm.com\n\nmario.geiger@epfl.ch\n\nmaurice weiler\n\nquva lab\n\nu. of amsterdam\nm.weiler@uva.nl\n\nabstract\n\nwe present a general theory of group equivariant convolutional neural networks\n(g-cnns) on homogeneous spaces such as euclidean space and the sphere. feature\nmaps in these networks represent \ufb01elds on a homogeneous base space, and layers\nare equivariant maps between spaces of \ufb01elds. the theory enables a systematic\nclassi\ufb01cation of all existing g-cnns in terms of their symmetry group, base\nspace, and \ufb01eld type. we also consider a fundamental question: what is the most\ngeneral kind of equivariant linear map between feature spaces (\ufb01elds) of given\ntypes? following mackey, we show that such maps correspond one-to-on", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/2020.06.12.148775\n; \n\nthis version posted may 10, 2023. \n\nthe copyright holder for this preprint (which\n\nwas not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\navailable under a\n\ncc-by-nc-nd 4.0 international license\n.\n\nneurotransmitter classification from electron\n\nmicroscopy images at synaptic sites in drosophila\n\nmelanogaster\n\nnils eckstein1,2*, alexander shakeel bates3,4,5*, andrew champion6, michelle du1, yijie yin6, philipp schlegel6,3,\nalicia kun-yang lu1, thomson rymer1, samantha finley-may1, tyler paterson1, ruchi parekh1, sven dorkenwald7,\n\narie matsliah7, szi-chieh yu7, claire mckellar7, amy sterling7, katharina eichler6, marta costa6, sebastian\n\nseung7, mala murthy7, volker hartenstein8, gregory s.x.e. jefferis3,6, and jan funke1\n\n1hhmi janelia research campus, ashburn, va, usa\n\n2institute of neuroinformatics uzh/ethz, zurich, switzerland\n", "review\ntheories  of  error  back-propagation  in\nthe  brain\n\njames  c.r.  whittington1,2 and  rafal  bogacz1,*\n\nthis  review  article  summarises  recently  proposed  theories  on  how  neural\ncircuits  in  the  brain  could  approximate  the  error  back-propagation  algorithm\nused  by  arti\ufb01cial  neural  networks.  computational  models  implementing  these\ntheories  achieve  learning  as  ef\ufb01cient  as  arti\ufb01cial  neural  networks,  but  they  use\nsimple  synaptic  plasticity  rules  based  on  activity  of  presynaptic  and  postsyn-\naptic  neurons.  the  models  have  similarities,  such  as  including  both  feedfor-\nward  and  feedback  connections,  allowing  information  about  error  to  propagate\nthroughout  the  network.  furthermore,  they  incorporate  experimental  evidence\non  neural  connectivity,  responses,  and  plasticity.  these  models  provide\ninsights  on  how  brain  networks  might  be  organised  such  that  modi\ufb01cation\nof  synaptic  weights  on  multiple  le", "6\n1\n0\n2\n\n \n\nn\na\nj\n \n\n5\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n4\n5\n2\n5\n0\n\n.\n\n6\n0\n5\n1\n:\nv\ni\nx\nr\na\n\ngradient estimation using\n\nstochastic computation graphs\n\njohn schulman1,2\n\njoschu@eecs.berkeley.edu\n\nnicolas heess1\n\nheess@google.com\n\ntheophane weber1\n\ntheophane@google.com\n\npieter abbeel2\n\npabbeel@eecs.berkeley.edu\n\n1 google deepmind\n\n2 university of california, berkeley, eecs department\n\nabstract\n\nin a variety of problems originating in supervised, unsupervised, and reinforce-\nment learning, the loss function is de\ufb01ned by an expectation over a collection\nof random variables, which might be part of a probabilistic model or the exter-\nnal world. estimating the gradient of this loss function, using samples, lies at\nthe core of gradient-based learning algorithms for these problems. we introduce\nthe formalism of stochastic computation graphs\u2014directed acyclic graphs that in-\nclude both deterministic functions and conditional probability distributions\u2014and\ndescribe how to easily and automatically derive ", "neuropharmacology 52 (2007) 24e40\n\nwww.elsevier.com/locate/neuropharm\n\nthe late maintenance of hippocampal ltp: requirements, phases,\n\n\u2018synaptic tagging\u2019, \u2018late-associativity\u2019 and implications\n\nklaus g. reymann, julietta u. frey*\n\ndepartment for neurophysiology, leibniz institute for neurobiology, brenneckestrasse 6, d-39118 magdeburg, germany\n\nreceived 23 june 2006; received in revised form 14 july 2006; accepted 17 july 2006\n\nabstract\n\nour review focuses on the mechanisms which enable the late maintenance of hippocampal long-term potentiation (ltp; >3 h), a phenomenon\nwhich is thought to underlie prolonged memory. about 20 years ago we showed for the \ufb01rst time that the maintenance of ltp e like memory\nstorage e depends on intact protein synthesis and thus, consists of at least two temporal phases. here we concentrate on mechanisms required for\nthe induction of the transient early-ltp and of the protein synthesis-dependent late-ltp. our group has shown that the induction of late-ltp\nr", "article \n\ncommunicated by peter dayan \n\nbiologically plausible error-driven learning \nusing local activation differences: \nthe generalized recirculation algorithm \n\nrandall c.  o\u2019reilly \ndepartment of  psychology, carnegie mellon university, pittsburgh, pa  15213 usa \n\nl\n\nd\no\nw\nn\no\na\nd\ne\nd\n\nthe error backpropagation  learning  algorithm  (bp) is generally  con- \nsidered biologically implausible because it does not use locally avail- \nable,  activation-based  variables.  a  version  of  bp  that  can  be  com- \nputed  locally using bidirectional activation recirculation  (hinton and \nmcclelland 1988) instead of  backpropagated  error derivatives is more \nbiologically  plausible.  this paper  presents  a generalized  version  of \nthe  recirculation algorithm  (generec), which  overcomes several limi- \ntations of  the  earlier  algorithm by using  a generic recurrent network \nwith  sigmoidal units that can learn arbitrary inputloutput mappings. \nhowever, the contrastive hebbian learning a", "report\n\ngrid cells form a global representation of\nconnected environments\n\ngraphical abstract\n\nauthors\n\nfrancis carpenter, daniel manson, ...,\nneil burgess, caswell barry\n\ncorrespondence\nfrancis.carpenter.12@ucl.ac.uk (f.c.),\ncaswell.barry@ucl.ac.uk (c.b.)\n\nin brief\ngrid cells are thought to provide an\nef\ufb01cient metric for spatial navigation.\nhowever, this requires regular,\ncontinuous grid \ufb01ring patterns across the\nenvironment. carpenter et al. show that\ngrid patterns are initially determined by\nsensory cues but self-correct with\nexperience to form the globally coherent\npattern required of a spatial metric.\n\nhighlights\nd grid cells were recorded in connected, perceptually identical\n\ncompartments\n\nd initial grid \ufb01ring patterns replicated between the two\n\ncompartments\n\nd with experience, a single, continuous grid pattern spanning\n\nboth compartments formed\n\nd this globally coherent pattern allows grid cells to act as a\n\nspatial metric\n\ncarpenter et al., 2015, current biology 25, 1176\u20131182\n", "received: 2 january 2020 \ndoi: 10.1111/ejn.14745  \n\n|  revised: 23 march 2020 \n\n|  accepted: 25 march 2020\n\ns p e c i a l   i s s u e   r e v i e w\n\nthe credit assignment problem in cortico-basal ganglia-thalamic \nnetworks: a review, a problem and a possible solution\n\n \n\n|   catalina\u00a0vich2 \n\n|   matthew\u00a0clapp3 \n\njonathan e.\u00a0rubin1\ntimothy\u00a0verstynen3,5\n1department of mathematics, center for the neural basis of cognition, university of pittsburgh, pittsburgh, pa, usa\n2department de matem\u00e0tiques i inform\u00e0tica, institute of applied computing and community code, universitat de les illes balears, palma, spain\n3carnegie mellon neuroscience institute, carnegie mellon university, pittsburgh, pa, usa\n4micron school of materials science and engineering, boise state university, boise, id, usa\n5department of psychology, center for the neural basis of cognition, carnegie mellon university, pittsburgh, pa, usa\n\n|   kendra\u00a0noneman4 \n\n|   \n\ncorrespondence\njonathan e. rubin, department of \nmathematics, ", "a. g. barto, r. s. sutton, and c. w. anderson, \u201cneuronlike adaptive elements that \ncan solve difficult learning control problems,\u201d   \nieee transactions on systems, man, and cybernetics, vol. smc-13, pp. 834\u2013846, \nsept./oct. 1983.\u00a0\n\n\f", "differentiable plasticity: training plastic neural networks with\n\nbackpropagation\n\nthomas miconi 1 jeff clune 1 kenneth o. stanley 1\n\n8\n1\n0\n2\n\n \nl\nu\nj\n \n\n1\n3\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n4\n6\n4\n2\n0\n\n.\n\n4\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nhow can we build agents that keep learning from\nexperience, quickly and ef\ufb01ciently, after their ini-\ntial training? here we take inspiration from the\nmain mechanism of learning in biological brains:\nsynaptic plasticity, carefully tuned by evolution\nto produce ef\ufb01cient lifelong learning. we show\nthat plasticity, just like connection weights, can\nbe optimized by gradient descent in large (mil-\nlions of parameters) recurrent networks with heb-\nbian plastic connections. first, recurrent plastic\nnetworks with more than two million parameters\ncan be trained to memorize and reconstruct sets\nof novel, high-dimensional (1,000+ pixels) nat-\nural images not seen during training. crucially,\ntraditional non-plastic recurrent networks fail to\nsolve this task. furthermore, ", "article\n\nmeta-learning synaptic plasticity and memory\naddressing for continual familiarity detection\n\nhighlights\nd meta-learning is used to discover network architectures and\n\nplasticity rules\n\nauthors\n\ndanil tyulmankov,\nguangyu robert yang, l.f. abbott\n\nd anti-hebbian plasticity emerges as the mechanism for\n\nencoding familiarity\n\nd strong feedforward synapses emerge as an addressing\n\nfunction for storage and retrieval\n\ncorrespondence\ndt2586@columbia.edu (d.t.),\nyanggr@mit.edu (g.r.y.),\nlfa2103@columbia.edu (l.f.a.)\n\nd experimental features such as repetition suppression are\n\nreproduced\n\nin brief\ntyulmankov et al. use meta-learning to\nbuild neural network models for continual\nfamiliarity detection. they show that anti-\nhebbian plasticity is the preferred\nmechanism for optimizing memory\ncapacity and propose strong feedforward\nweights as an explicit addressing\nmechanism for selecting memory\nlocations during storage and retrieval.\n\ntyulmankov et al., 2022, neuron 110, 544\u2013557\nfebruary 2, ", "a unified model of nmda receptor-dependent\nbidirectional synaptic plasticity\n\nharel z. shouval*\u2020, mark f. bear*\u2021\u00a7, and leon n cooper*\u2021\u00b6\n\n*institute for brain and neural systems, departments of \u00b6physics and \u2021neuroscience, and \u00a7howard hughes medical institute, brown university,\nprovidence, ri 02912\n\ncontributed by leon n cooper, june 7, 2002\n\nsynapses in the brain are bidirectionally modi\ufb01able, but the routes of\ninduction are diverse. in various experimental paradigms, n-methyl-\nd-aspartate receptor-dependent long-term depression and long-term\npotentiation have been induced selectively by varying the membrane\npotential of the postsynaptic neurons during presynaptic stimulation\nof a constant frequency, the rate of presynaptic stimulation, and the\ntiming of pre- and postsynaptic action potentials. in this paper, we\npresent a mathematical embodiment of bidirectional synaptic plas-\nticity that is able to explain diverse induction protocols with a \ufb01xed\nset of parameters. the key assumptions a", "sn computer science (2021) 2:420 \nhttps://doi.org/10.1007/s42979-021-00815-1\n\nreview article\n\ndeep learning: a\u00a0comprehensive overview on\u00a0techniques, taxonomy, \napplications and\u00a0research directions\n\niqbal\u00a0h.\u00a0sarker1,2 \n\nreceived: 29 may 2021 / accepted: 7 august 2021 / published online: 18 august 2021 \n\u00a9 the author(s), under exclusive licence to springer nature singapore pte ltd 2021\n\nabstract\ndeep learning (dl), a branch of machine learning (ml) and artificial intelligence (ai) is nowadays considered as a core \ntechnology of today\u2019s fourth industrial revolution (4ir or industry 4.0). due to its learning capabilities from data, dl \ntechnology originated from artificial neural network (ann), has become a hot topic in the context of computing, and is \nwidely applied in various application areas like healthcare, visual recognition, text analytics,\u00a0cybersecurity, and many more. \nhowever, building an appropriate dl model is a challenging task, due to the dynamic nature and variations in real", "5\n1\n0\n2\n\n \nr\np\na\n6\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n4\n1\n6\n6\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\naccepted as a workshop contribution at iclr 2015\n\nin search of the real inductive bias:\non the role of implicit regularization in deep\nlearning\n\nbehnam neyshabur, ryota tomioka & nathan srebro\ntoyota technological institute at chicago\nchicago, il 60637, usa\n{bneyshabur,tomioka,nati}@ttic.edu\n\nabstract\n\nwe present experiments demonstrating that some other form of capacity control,\ndifferent from network size, plays a central role in learning multi-layer feed-\nforward networks. we argue, partially through analogy to matrix factorization,\nthat this is an inductive bias that can help shed light on deep learning.\n\n1\n\nintroduction\n\ncentral to any form of learning is an inductive bias that induces some sort of capacity control (i.e. re-\nstricts or encourages predictors to be \u201csimple\u201d in some way), which in turn allows for generalization.\nthe success of learning then depends on how well the inductive bias cap", "backpropagation and the brain\ntimothy\u00a0p.\u00a0lillicrap   , adam\u00a0santoro, luke\u00a0marris, colin\u00a0j.\u00a0akerman and geoffrey\u00a0hinton \nhttps://doi.org/10.1038/s41583-020-0277-3\n\nperspectivesnature reviews | neurosciencein format as provided by the authorssupplementary information\f", "review\n\n\u2018activity-silent\u2019  working  memory  in\nprefrontal  cortex:  a  dynamic  coding\nframework\n\nmark  g.  stokes\n\noxford  centre  for  human  brain  activity,  university  of  oxford,  oxford,  uk\n\nworking  memory  (wm)  provides  the  functional  back-\nbone  to  high-level  cognition.  maintenance  in  wm  is\noften  assumed  to  depend  on  the  stationary  persistence\nof  neural  activity  patterns  that  represent  memory  con-\ntent.  however,  accumulating  evidence  suggests  that\npersistent  delay  activity  does  not  always  accompany\nwm  maintenance  but  instead  seems  to  wax  and  wane\nas  a  function  of  the  current  task  relevance  of  memoran-\nda.  furthermore,  new  methods  for  measuring  and  ana-\nlysing  population-level  patterns  show  that  activity\nstates  are  highly  dynamic.  at  \ufb01rst  glance,  these  dynam-\nics  seem  at  odds  with  the  very  nature  of  wm.  how  can\nwe  keep  a  stable  thought  in  mind  while  brain  activity  is\nconstantly  chan", "review\n\nmemory  trace  replay:  the  shaping\nof  memory  consolidation  by\nneuromodulation\nlaura  a.  atherton1,  david  dupret2,  and  jack  r.  mellor1\n\n1 school  of  physiology  and  pharmacology,  university  of  bristol,  bristol,  bs8  1td,  uk\n2 medical  research  council  brain  network  dynamics  unit  at  the  university  of  oxford,  department  of  pharmacology,  oxford,  ox1\n3th,  uk\n\nthe  consolidation  of  memories  for  places  and  events  is\nthought  to  rely,  at  the  network  level,  on  the  replay  of\nspatially  tuned  neuronal  \ufb01ring  patterns  representing\ndiscrete  places  and  spatial  trajectories.  this  occurs  in\nthe  hippocampal-entorhinal  circuit  during  sharp  wave\nripple  events  (swrs)  that  occur  during  sleep  or  rest.\nhere,  we  review  theoretical  models  of  lingering  place\ncell  excitability  and  behaviorally  induced  synaptic  plas-\nticity  within  cell  assemblies  to  explain  which  sequences\nor  places  are  replayed.  we  further", "spontaneous travelling cortical waves gate \nperception in behaving primates\n\nhttps://doi.org/10.1038/s41586-020-2802-y\nreceived: 19 february 2019\naccepted: 10 july 2020\npublished online: 7 october 2020\n\n check for updates\n\nzachary w. davis1,7\u2009\u2709, lyle muller1,2,3,4,7, julio martinez-trujillo3,5,6, terrence sejnowski1 &  \njohn h. reynolds1\u2009\u2709\n\nperceptual sensitivity varies from moment to moment. one potential source of this \nvariability is spontaneous fluctuations in cortical activity that can travel as waves1. \nspontaneous travelling waves have been reported during anaesthesia2\u20137, but it is not \nknown whether they have a role during waking perception. here, using newly \ndeveloped analytic techniques to characterize the moment-to-moment dynamics of \nnoisy multielectrode data, we identify spontaneous waves of activity in the \nextrastriate visual cortex of awake, behaving marmosets (callithrix jacchus). in \nmonkeys trained to detect faint visual targets, the timing and position of spontaneo", "6\n1\n0\n2\n\n \nc\ne\nd\n \n3\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n9\n5\n0\n9\n0\n\n.\n\n9\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ntraining recurrent networks to generate hypotheses\nabout how the brain solves hard navigation problems\n\ningmar kanitscheider & ila fiete\n\ndepartment of neuroscience\n\nthe university of texas\n\naustin, tx 78712\n\nikanitscheider, ilafiete @mail.clm.utexas.edu\n\nabstract\n\nself-localization during navigation with noisy sensors in an ambiguous world is\ncomputationally challenging, yet animals and humans excel at it. in robotics, simul-\ntaneous location and mapping (slam) algorithms solve this problem though joint\nsequential probabilistic inference of their own coordinates and those of external\nspatial landmarks. we generate the \ufb01rst neural solution to the slam problem by\ntraining recurrent lstm networks to perform a set of hard 2d navigation tasks\nthat include generalization to completely novel trajectories and environments. the\nhidden unit representations exhibit several key properties of hippocampal ", "communicated by garrison cottrell\n\ndimension reduction by local principal component analysis\n\nnandakishore kambhatla\ntodd k. leen\ndepartment of computer science and engineering, oregon graduate institute\nof science and technology, portland, oregon 97291-1000, u.s.a.\n\nreducing or eliminating statistical redundancy between the components\nof high-dimensional vector data enables a lower-dimensional represen-\ntation without signi\ufb01cant loss of information. recognizing the limita-\ntions of principal component analysis (pca), researchers in the statistics\nand neural network communities have developed nonlinear extensions of\npca. this article develops a local linear approach to dimension reduction\nthat provides accurate representations and is fast to compute. we exercise\nthe algorithms on speech and image data, and compare performance with\npca and with neural network implementations of nonlinear pca. we\n\ufb01nd that both nonlinear techniques can provide more accurate represen-\ntations than pca and ", "8400 \u2022 the journal of neuroscience, june 23, 2010 \u2022 30(25):8400 \u2013 8410\n\ndevelopment/plasticity/repair\n\na reward-modulated hebbian learning rule can explain\nexperimentally observed network reorganization in a brain\ncontrol task\n\nrobert legenstein,1 steven m. chase,2,3,4 andrew b. schwartz,2,3 and wolfgang maass1\n1institute for theoretical computer science, graz university of technology, 8010 graz, austria, 2department of neurobiology, university of pittsburgh,\npittsburgh, pennsylvania 15213, 3center for the neural basis of cognition, university of pittsburgh and carnegie mellon university, and 4department of\nstatistics, carnegie mellon university, pittsburgh, pa 15213\n\nit has recently been shown in a brain\u2013 computer interface experiment that motor cortical neurons change their tuning properties\nselectively to compensate for errors induced by displaced decoding parameters. in particular, it was shown that the three-dimensional\ntuning curves of neurons whose decoding parameters were reass", "article\nmastering the game of go with deep \nneural networks and tree search\n\ndavid silver1*, aja huang1*, chris j. maddison1, arthur guez1, laurent sifre1, george van den driessche1,  \njulian schrittwieser1, ioannis antonoglou1, veda panneershelvam1, marc lanctot1, sander dieleman1, dominik grewe1, \njohn nham2, nal kalchbrenner1, ilya sutskever2, timothy lillicrap1, madeleine leach1, koray kavukcuoglu1,  \nthore graepel1 & demis hassabis1\n\ndoi:10.1038/nature16961\n\nthe game of go has long been viewed as the most challenging of classic games for artificial intelligence owing to its \nenormous search space and the difficulty of evaluating board positions and moves. here we introduce a new approach \nto computer go that uses \u2018value networks\u2019 to evaluate board positions and \u2018policy networks\u2019 to select moves. these deep \nneural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement \nlearning from games of self-play. without any lookahead se", "journal of machine learning research 6 (2005) 695{709\n\nsubmitted 11/04; revised 3/05; published 4/05\n\nestimation of non-normalized statistical models\n\nby score matching\n\naapo hyv(cid:127)arinen\nhelsinki institute for information technology (bru)\ndepartment of computer science\nfin-00014 university of helsinki, finland\n\neditor: peter dayan\n\naapo.hyvarinen@helsinki.fi\n\nabstract\n\none often wants to estimate statistical models where the probability density function is\nknown only up to a multiplicative normalization constant. typically, one then has to resort\nto markov chain monte carlo methods, or approximations of the normalization constant.\nhere, we propose that such models can be estimated by minimizing the expected squared\ndistance between the gradient of the log-density given by the model and the gradient of\nthe log-density of the observed data. while the estimation of the gradient of log-density\nfunction is, in principle, a very di(cid:14)cult non-parametric problem, we prove a surpri", "9\n1\n0\n2\n\n \n\nb\ne\nf\n6\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n2\n6\n1\n6\n0\n\n.\n\n2\n0\n9\n1\n:\nv\ni\nx\nr\na\n\n1\n\nself-supervised visual feature learning with\n\ndeep neural networks: a survey\n\nlonglong jing and yingli tian\u2217, fellow, ieee\n\nabstract\u2014large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual\nfeature learning from images or videos for computer vision applications. to avoid extensive cost of collecting and annotating\nlarge-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general\nimage and video features from large-scale unlabeled data without using any human-annotated labels. this paper provides an extensive\nreview of deep learning-based self-supervised general visual feature learning methods from images or videos. first, the motivation,\ngeneral pipeline, and terminologies of this \ufb01eld are described. then the common deep neural network architectures that used for\n", "lettercommunicatedbytomheskeslearningcurvesforstochasticgradientdescentinlinearfeedforwardnetworksjustinwerfeljkwerfel@mit.edudepartmentofelectricalengineeringandcomputerscience,massachusettsinstituteoftechnology,cambridge,ma02139,u.s.a.xiaohuixiexhxie@mit.edubroadinstituteofmassachusettsinstituteoftechnologyandharvarduniversity,cambridge,ma02141,u.s.a.h.sebastianseungseung@mit.eduhowardhughesmedicalinstitute,departmentofbrainandcognitivesciences,massachusettsinstituteoftechnology,cambridge,ma02139,u.s.a.gradient-followinglearningmethodscanencounterproblemsofimple-mentationinmanyapplications,andstochasticvariantsaresometimesusedtoovercomethesedif\ufb01culties.weanalyzethreeonlinetrainingmethodsusedwithalinearperceptron:directgradientdescent,nodeper-turbation,andweightperturbation.learningspeedisde\ufb01nedastherateofexponentialdecayinthelearningcurves.whenthescalarparameterthatcontrolsthesizeofweightupdatesischosentomaximizelearningspeed,nodeperturbationisslowerthandirectgradientdescentbyafac-to", "j neurophysiol 97: 4296 \u2013 4309, 2007.\nfirst published april 11, 2007; doi:10.1152/jn.00024.2007.\n\nobject category structure in response patterns of neuronal population in\nmonkey inferior temporal cortex\n\nroozbeh kiani,1,3 hossein esteky,1,2 koorosh mirpour,2 and keiji tanaka4,5\n1research group for brain and cognitive sciences, school of medicine, shaheed beheshti university, tehran, iran; 2school of cognitive\nsciences, institute for studies in theoretical physics and mathematics, niavaran, tehran, iran; 3department of neurobiology and\nbehavior, university of washington, seattle, washington; 4cognitive brain mapping laboratory, riken brain science institute, wako,\nsaitama, japan; and 5graduate school of science and engineering, saitama university, saitama, saitama, japan\n\nsubmitted 9 january 2007; accepted in \ufb01nal form 30 march 2007\n\nkiani r, esteky h, mirpour k, tanaka k. object category struc-\nture in response patterns of neuronal population in monkey inferior\ntemporal cortex. j neuro", "6\n1\n0\n2\n\n \n\nb\ne\nf\n8\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n8\n0\n9\n5\n0\n\n.\n\n2\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ne\ufb03cient approaches for escaping higher order saddle\n\npoints in non-convex optimization\n\nanima anandkumar\u2217\n\nrong ge\u2020\n\nfebruary 19, 2016\n\nabstract\n\nlocal search heuristics for non-convex optimizations are popular in applied machine\nlearning. however, in general it is hard to guarantee that such algorithms even converge\nto a local minimum, due to the existence of complicated saddle point structures in high\ndimensions. many functions have degenerate saddle points such that the \ufb01rst and\nsecond order derivatives cannot distinguish them with local optima.\nin this paper\nwe use higher order derivatives to escape these saddle points: we design the \ufb01rst\ne\ufb03cient algorithm guaranteed to converge to a third order local optimum (while existing\ntechniques are at most second order). we also show that it is np-hard to extend this\nfurther to \ufb01nding fourth order local optima.\n\n1\n\nintroduction\n\nrecent trend in applied ", "stochastic neighbor embedding\n\ngeoffrey hinton and sam roweis\n\ndepartment of computer science, university of toronto\n\n10 king\u2019s college road, toronto, m5s 3g5 canada\n\n\u0000 hinton,roweis\n\n@cs.toronto.edu\n\nabstract\n\nwe describe a probabilistic approach to the task of placing objects, de-\nscribed by high-dimensional vectors or by pairwise dissimilarities, in a\nlow-dimensional space in a way that preserves neighbor identities. a\ngaussian is centered on each object in the high-dimensional space and\nthe densities under this gaussian (or the given dissimilarities) are used\nto de\ufb01ne a probability distribution over all the potential neighbors of\nthe object. the aim of the embedding is to approximate this distribu-\ntion as well as possible when the same operation is performed on the\nlow-dimensional \u201cimages\u201d of the objects. a natural cost function is a\nsum of kullback-leibler divergences, one per object, which leads to a\nsimple gradient for adjusting the positions of the low-dimensional im-\nages. un", "neuron\n\narticle\n\nabrupt transitions between prefrontal neural\nensemble states accompany behavioral\ntransitions during rule learning\n\ndaniel durstewitz,1,4,* nicole m. vittoz,2,4 stan b. floresco,3 and jeremy k. seamans2,*\n1rg computational neuroscience, central institute of mental health and interdisciplinary center for neurosciences,\nuniversity of heidelberg, j 5, 68159 mannheim, germany\n2brain research centre, psychiatry, faculty of medicine, university of british columbia, vancouver, bc v6t 2b5, canada\n3psychology department, university of british columbia, vancouver, bc v6t 2b5, canada\n4these authors contributed equally to this work\n*correspondence: daniel.durstewitz@zi-mannheim.de (d.d.), seamans@interchange.ubc.ca (j.k.s.)\ndoi 10.1016/j.neuron.2010.03.029\n\nsummary\n\none of the most intriguing aspects of adaptive\nbehavior involves the inference of regularities and\nrules in ever-changing environments. rules are often\ndeduced through evidence-based learning which\nrelies on the prefro", "j neurophysiol 129: 552\u2013580, 2023.\nfirst published february 8, 2023; doi:10.1152/jn.00454.2022\n\nreview\n\nnow and then\n\nhow our understanding of memory replay evolves\n\nzhe sage chen1,2,3,4 and matthew a. wilson5,6\n\n1department of psychiatry, new york university grossman school of medicine, new york, new york, united states;\n2department of neuroscience and physiology, new york university grossman school of medicine, new york, new york,\nunited states; 3neuroscience institute, new york university grossman school of medicine, new york, new york, united\nstates; 4department of biomedical engineering, new york university tandon school of engineering, brooklyn, new york,\nunited states; 5department of brain and cognitive sciences, massachusetts institute of technology, cambridge,\nmassachusetts, united states; and 6picower institute for learning and memory, massachusetts institute of technology,\ncambridge, massachusetts, united states\n\nabstract\n\nmemory reactivations and replay, widely reported in ", "a r t i c l e s\n\nstimulus onset quenches neural variability:  \na widespread cortical phenomenon\n\nmark m churchland1,2,16, byron m yu1\u20133,16, john p cunningham1, leo p sugrue2,4, marlene r cohen2,4,  \ngreg s corrado2,4, william t newsome2,4,5, andrew m clark6, paymon hosseini6, benjamin b scott6,  \ndavid c bradley6, matthew a smith7, adam kohn8,9, j anthony movshon9, katherine m armstrong2,5,  \ntirin moore2,5, steve w chang10, lawrence h snyder10, stephen g lisberger11, nicholas j priebe12, ian m finn13, \ndavid ferster13, stephen i ryu1,14, gopal santhanam1, maneesh sahani3 & krishna v shenoy1,2,15\n\nneural responses are typically characterized by computing the mean firing rate, but response variability can exist across trials. many \nstudies have examined the effect of a stimulus on the mean response, but few have examined the effect on response variability. \nwe measured neural variability in 13 extracellularly recorded datasets and one intracellularly recorded dataset from seven areas \ns", "5\n1\n0\n2\n\n \nr\np\na\n6\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n1\n5\n7\n2\n\n.\n\n6\n0\n4\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2015\n\nreweighted wake-sleep\n\nj\u00a8org bornschein and yoshua bengio \u2217\ndepartment of computer science and operations research\nuniversity of montreal\nmontreal, quebec, canada\n\nabstract\n\ntraining deep directed graphical models with many hidden variables and perform-\ning inference remains a major challenge. helmholtz machines and deep belief\nnetworks are such models, and the wake-sleep algorithm has been proposed to\ntrain them. the wake-sleep algorithm relies on training not just the directed gen-\nerative model but also a conditional generative model (the inference network) that\nruns backward from visible to latent, estimating the posterior distribution of la-\ntent given visible. we propose a novel interpretation of the wake-sleep algorithm\nwhich suggests that better estimators of the gradient can be obtained by sampling\nlatent variables multiple times from the inferenc", "neuroresource\n\nmapping sub-second structure in mouse behavior\n\nhighlights\nd computational modeling reveals structure in mouse behavior\n\nwithout observer bias\n\nd mouse behavior appears to be composed of stereotyped,\n\nsub-second modules\n\nauthors\n\nalexander b. wiltschko,\nmatthew j. johnson, giuliano iurilli, ...,\nvictoria e. abraira, ryan p. adams,\nsandeep robert datta\n\nd from this perspective, new behaviors result from altering\n\nboth modules and transitions\n\ncorrespondence\nsrdatta@hms.harvard.edu\n\nd unsupervised analysis reveals how genes and neural activity\n\nimpact behavior\n\nin brief\nmouse behavior appears inherently\ndivided into brief modules of 3d motion.\nthis sub-second structure reveals the\nin\ufb02uence of the environment, genes and\nneural activity on action.\n\nwiltschko et al., 2015, neuron 88, 1121\u20131135\ndecember 16, 2015 \u00aa2015 elsevier inc.\nhttp://dx.doi.org/10.1016/j.neuron.2015.11.031\n\n\f", "5\n1\n0\n2\n\n \nr\np\na\n9\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n9\n8\n9\n2\n\n.\n\n6\n0\n4\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2015\n\ntechniques for learning binary\nstochastic feedforward neural networks\n\ntapani raiko & mathias berglund\ndepartment of information and computer science\naalto university\nespoo, finland\n{tapani.raiko,mathias.berglund}@aalto.fi\n\nguillaume alain & laurent dinh\ndepartment of computer science and operations research\nuniversit\u00b4e de montr\u00b4eal\nmontr\u00b4eal, canada\nguillaume.alain.umontreal@gmail.com, dinhlaur@iro.umontreal.ca\n\nabstract\n\nstochastic binary hidden units in a multi-layer perceptron (mlp) network give at\nleast three potential bene\ufb01ts when compared to deterministic mlp networks. (1)\nthey allow to learn one-to-many type of mappings. (2) they can be used in struc-\ntured prediction problems, where modeling the internal structure of the output is\nimportant. (3) stochasticity has been shown to be an excellent regularizer, which\nmakes generalization performance p", "neuron, vol. 39, 807\u2013820, august 28, 2003, copyright \uf8e92003 by cell press\n\ncoincident pre- and postsynaptic activity\nmodifies gabaergic synapses by postsynaptic\nchanges in cl\u2afa transporter activity\n\nmelanie a. woodin,1 karunesh ganguly,1,2\nand mu-ming poo*\ndivision of neurobiology\ndepartment of molecular and cell biology\nuniversity of california\nberkeley, california 94720\n\nsummary\n\ncoincident pre- and postsynaptic activation is known\nto induce long-term modification of glutamatergic syn-\napses. we report here that, in both hippocampal cul-\ntures and acute hippocampal slices, repetitive post-\nsynaptic spiking within 20 ms before and after the\nactivation of gabaergic synapses also led to a persis-\ntent change in synaptic strength. this synaptic modifi-\ncation required ca2\u2d19 influx through postsynaptic\nl-type ca2\u2d19 channels and was due to a local decrease\nin k\u2d19-cl\u2afa cotransport activity, effectively reducing the\nstrength of inhibition. thus, gabaergic synapses can\ndetect and be modified by coi", "torcs: the open racing car simulator\n\nbernhard wymann\n\ny\nandrew sumner\n\n(cid:3)\n\ny\nchristos dimitrakakis\nz\neric espi(cid:19)e\n\nchristophe guionneau\n\nz\n\nmarch 12, 2015\n\n1\n\nintroduction\n\nthe open racing car simulator (torcs [14]), is a modern, modular, highly-\nportable multi-player, multi-agent car simulator. its high degree of modularity\nand portability render it ideal for arti(cid:12)cial intelligence research. indeed, a num-\nber of research-oriented competitions and papers have already appeared that\nmake use of the torcs engine. the purpose of this document is to introduce\nthe structure of torcs to the general arti(cid:12)cial intelligence and machine learn-\ning community and explain how it is possible to tests agents on the platform.\n\ntorcs can be used to develop arti(cid:12)cially intelligent (ai) agents for a va-\nriety of problems. at the car level, new simulation modules can be developed,\nwhich include intelligent control systems for various car components. at the\ndriver level, a ", "journal \n\nof  complexity \n\n4,  216-245 \n\n(1988) \n\ndynamics  and architecture \n\nfor  neural  computation* \n\napplied \n\nphysics \n\nlaboratory, \n\nfernando \n\nj.  pineda \n\njohns \nlaurel, \n\nhopkins \nmaryland \n\nuniversity, \n20707 \n\nreceived  april,  1987 \n\njohns \n\nhopkins \n\nroad, \n\ntransformation \n\nthe  architectural \n\nand  autoassociative \n\nrecall.  backpropagation \n\nrecurrent  backpropagation \n\ncomponents  are  presented \n\ncomponents  which  perform \n\nfor  a  collective  nonlinear  dynamical \n\nuseful  computation  can  be  performed  by  systematically  exploiting \n\nthe  phenom- \nenology  of  nonlinear  dynamical  systems.  two  dynamical  phenomena  are  isolated \nthe  operations  of  continu- \ninto  primitive  architectural \nous  nonlinear \ntech- \nin  a  formal- \nniques  for  programming \nism  appropriate \nthat \nconventional \nis  not  capable  of  storing  multiple  patterns \nin  an  associative  memory  which  starts  out  with  an  insufficient  number  of  point \nattractors. \nit  is  sho", "learning to walk via deep reinforcement learning\n\ntuomas haarnoja\u2217,1,2, sehoon ha\u2217,1, aurick zhou2, jie tan1, george tucker1 and sergey levine1,2\n\n1google brain 2berkeley arti\ufb01cial intelligence research, university of california, berkeley\n\nemail: {tuomash,sehoonha,jietan,gjt}@google.com,{azhou42,svlevine}@berkeley.edu\n\n\u2217the \ufb01rst two authors contributed equally.\n\n9\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n9\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n3\n0\n1\n1\n1\n\n.\n\n2\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014deep reinforcement learning (deep rl) holds the\npromise of automating the acquisition of complex controllers\nthat can map sensory inputs directly to low-level actions. in the\ndomain of robotic locomotion, deep rl could enable learning\nlocomotion skills with minimal engineering and without an\nexplicit model of the robot dynamics. unfortunately, applying\ndeep rl to real-world robotic tasks is exceptionally dif\ufb01cult,\nprimarily due to poor sample complexity and sensitivity to\nhyperparameters. while hyperparameters can be easily tuned\ni", "place cells may simply be memory cells: memory\ncompression leads to spatial tuning and history\ndependence\n\nmarcus k. bennaa,b,c,1,2\n\nand stefano fusia,b,d,1,2\n\nacenter for theoretical neuroscience, columbia university, new york, ny 10027; bmortimer b. zuckerman mind brain behavior institute, columbia\nuniversity, new york, ny 10027; cneurobiology section, division of biological sciences, university of california san diego, la jolla, ca 92093; and dkavli\ninstitute for brain sciences, columbia university, new york, ny 10027\n\nedited by james mcclelland, center for mind, brain and computation, department of psychology, stanford university, stanford, ca; received september 2,\n2020; accepted november 2, 2021\n\nthe observation of place cells has suggested that the hippocam-\npus plays a special role in encoding spatial information. however,\nplace cell responses are modulated by several nonspatial variables\nand reported to be rather unstable. here, we propose a memory\nmodel of the hippocampus tha", "research article\n\ntowards deep learning with segregated\ndendrites\njordan guerguiev1,2, timothy p lillicrap3, blake a richards1,2,4*\n\n1department of biological sciences, university of toronto scarborough, toronto,\ncanada; 2department of cell and systems biology, university of toronto, toronto,\ncanada; 3deepmind, london, united kingdom; 4learning in machines and brains\nprogram, canadian institute for advanced research, toronto, canada\n\nabstract deep learning has led to significant advances in artificial intelligence, in part, by\nadopting strategies motivated by neurophysiology. however, it is unclear whether deep learning\ncould occur in the real brain. here, we show that a deep learning algorithm that utilizes multi-\ncompartment neurons might help us to understand how the neocortex optimizes cost functions.\nlike neocortical pyramidal neurons, neurons in our model receive sensory information and higher-\norder feedback in electrotonically segregated compartments. thanks to this segregation", "3\n2\n0\n2\n\n \nr\np\na\n7\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n2\n3\n5\n0\n\n.\n\n2\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nscalable real-time recurrent learning using\n\nsparse connections and selective learning\n\nkhurram javed1\nhaseeb shah1\nrichard sutton1\nmartha white1\n1 university of alberta, edmonton, canada\n\nkjaved@ualberta.ca\nhshah1@ualberta.ca\nrsutton@ualberta.ca\nwhitem@ualberta.ca\n\nabstract\n\nstate construction from sensory observations is an important component of a reinforcement\nlearning agent. one solution for state construction is to use recurrent neural networks.\nback-propagation through time (bptt), and real-time recurrent learning (rtrl) are two\npopular gradient-based methods for recurrent learning. bptt requires the complete se-\nquence of observations before computing gradients and is unsuitable for online real-time\nupdates. rtrl can do online updates but scales poorly to large networks. in this paper,\nwe propose two constraints that make rtrl scalable. we show that by either decompos-\ning the network into i", "supporting information\nkirkpatrick et al. 10.1073/pnas.1611835114\nrandom patterns\nin this section we show that using ewc it is possible to recover\na power-law decay for the snr of random patterns. the task\nconsists of associating random n-dimensional binary vectors xt\nto a random binary output yt by learning a weight vector w . the\ncontinual-learning aspect of the problem arises from the fact that\nat time step i, only the ith pattern is accessible to the learning\nalgorithm. before providing a detailed derivation of the learning\nbehavior, we provide a sketch of the main ideas. learning con-\nsists of minimizing an objective function at each time step. this\nobjective function contains the square loss for the current pattern\nplus a penalty that minimizes the distance of the weight vector to\nits old value. this corresponds to ewc if the distance metric\nused is the diagonal of the total fisher information matrix. con-\nversely, if a \ufb01xed metric is used, we recover gradient descent. in\nthis pa", "6\n1\n0\n2\n\n \nt\nc\no\n1\n3\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n5\n3\n0\n0\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nfull-capacity unitary recurrent neural networks\n\nscott wisdom1\u2217, thomas powers1\u2217, john r. hershey2, jonathan le roux2, and les atlas1\n\n1 department of electrical engineering, university of washington\n\n{swisdom, tcpowers, atlas}@uw.edu\n\n2 mitsubishi electric research laboratories (merl)\n\n{hershey, leroux}@merl.com\n\nabstract\n\nrecurrent neural networks are powerful models for processing sequential data,\nbut they are generally plagued by vanishing and exploding gradient problems.\nunitary recurrent neural networks (urnns), which use unitary recurrence matri-\nces, have recently been proposed as a means to avoid these issues. however, in\nprevious experiments, the recurrence matrices were restricted to be a product of\nparameterized unitary matrices, and an open question remains: when does such a\nparameterization fail to represent all unitary matrices, and how does this restricted\nrepresentational capaci", "a toy model of universality:\n\nreverse engineering how networks learn group operations\n\nbilal chughtai 1 lawrence chan 2 neel nanda 1\n\n3\n2\n0\n2\n\n \n\ny\na\nm\n4\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n2\n0\n3\n0\n\n.\n\n2\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nuniversality is a key hypothesis in mechanistic\ninterpretability \u2013 that different models learn simi-\nlar features and circuits when trained on similar\ntasks.\nin this work, we study the universality\nhypothesis by examining how small neural net-\nworks learn to implement group composition. we\npresent a novel algorithm by which neural net-\nworks may implement composition for any finite\ngroup via mathematical representation theory. we\nthen show that networks consistently learn this\nalgorithm by reverse engineering model logits\nand weights, and confirm our understanding us-\ning ablations. by studying networks of differing\narchitectures trained on various groups, we find\nmixed evidence for universality: using our algo-\nrithm, we can completely characterize the fam", "research article\n\nhow biological attention mechanisms\nimprove task performance in a large-scale\nvisual system model\ngrace w lindsay1,2*, kenneth d miller1,2,3,4\n\n1center for theoretical neuroscience, college of physicians and surgeons,\ncolumbia university, new york, united states; 2mortimer b. zuckerman mind brain\nbehaviour institute, columbia university, new york, united states; 3swartz program\nin theoretical neuroscience, kavli institute for brain science, new york, united\nstates; 4department of neuroscience, columbia university, new york, united\nstates\n\nabstract how does attentional modulation of neural activity enhance performance? here we\nuse a deep convolutional neural network as a large-scale model of the visual system to address this\nquestion. we model the feature similarity gain model of attention, in which attentional modulation\nis applied according to neural stimulus tuning. using a variety of visual tasks, we show that neural\nmodulations of the kind and magnitude observed e", "internally generated predictions enhance neural\nand behavioral detection of sensory stimuli in an\nelectric fish\n\narticle\n\nhighlights\nd negative images enhance neural coding of prey-like stimuli\n\nd negative images enhance behavioral detection of prey-like\n\nstimuli\n\nd disrupting synaptic plasticity impairs neural and behavioral\n\ndetection performance\n\nauthors\n\narmen g. enikolopov, l.f. abbott,\nnathaniel b. sawtell\n\ncorrespondence\nns2635@columbia.edu\n\nin brief\nstable and accurate perception requires\ncombining sensory input with predictions\nbased on past experience. using electric\n\ufb01sh as a model system, enikolopov et al.\ndemonstrate improvements in neural\ncoding and behavioral detection of\nexternal stimuli due to internally\ngenerated predictions.\n\nenikolopov et al., 2018, neuron 99, 135\u2013146\njuly 11, 2018 \u00aa 2018 elsevier inc.\nhttps://doi.org/10.1016/j.neuron.2018.06.006\n\n\f", "tensor programs iv:\n\nfeature learning in in\ufb01nite-width neural networks\n\ngreg yang 1 edward j. hu 2 3\n\nabstract\n\nas its width tends to in\ufb01nity, a deep neural\nnetwork\u2019s behavior under gradient descent can\nbecome simpli\ufb01ed and predictable (e.g. given\nby the neural tangent kernel (ntk)), if it\nis parametrized appropriately (e.g.\nthe ntk\nparametrization). however, we show that the stan-\ndard and ntk parametrizations of a neural net-\nwork do not admit in\ufb01nite-width limits that can\nlearn features, which is crucial for pretraining and\ntransfer learning such as with bert. we propose\nsimple modi\ufb01cations to the standard parametriza-\ntion to allow for feature learning in the limit. us-\ning the tensor programs technique, we derive\nexplicit formulas for such limits. on word2vec\nand few-shot learning on omniglot via maml,\ntwo canonical tasks that rely crucially on feature\nlearning, we compute these limits exactly. we\n\ufb01nd that they outperform both ntk baselines and\n\ufb01nite-width networks, with the latte", "article\n\ncommunicated by jonathan pillow\n\nextracting low-dimensional latent structure from time\nseries in the presence of delays\n\nkarthik c. lakshmanan\nkarthikl@cs.cmu.edu\nrobotics institute and center for the neural basis of cognition, carnegie mellon\nuniversity, pittsburgh, pa 15213, u.s.a.\n\npatrick t. sadtler\npatrick.t.sadtler@gmail.com\ndepartment of bioengineering, center for the neural basis of cognition, and systems\nneuroscience institute, university of pittsburgh, pittsburgh, pa 15261, u.s.a.\n\nelizabeth c. tyler-kabara\nelizabeth.tyler-kabara@chp.edu\ndepartment of neurological surgery, department of bioengineering, and department\nof physical medicine and rehabilitation, university of pittsburgh, pittsburgh,\npa 15261, u.s.a.\n\naaron p. batista\napb10@pitt.edu\ndepartment of bioengineering, center for the neural basis of cognition, and systems\nneuroscience institute, university of pittsburgh, pittsburgh, pa 15261, u.s.a.\n\nbyron m. yu\nbyronyu@cmu.edu\ndepartment of electrical engineerin", "learning transferable visual models from natural language supervision\n\nalec radford * 1 jong wook kim * 1 chris hallacy 1 aditya ramesh 1 gabriel goh 1 sandhini agarwal 1\n\ngirish sastry 1 amanda askell 1 pamela mishkin 1 jack clark 1 gretchen krueger 1 ilya sutskever 1\n\n1\n2\n0\n2\n\n \n\nb\ne\nf\n6\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n0\n2\n0\n0\n0\n\n.\n\n3\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nstate-of-the-art computer vision systems are\ntrained to predict a \ufb01xed set of predetermined\nobject categories. this restricted form of super-\nvision limits their generality and usability since\nadditional labeled data is needed to specify any\nother visual concept. learning directly from raw\ntext about images is a promising alternative which\nleverages a much broader source of supervision.\nwe demonstrate that the simple pre-training task\nof predicting which caption goes with which im-\nage is an ef\ufb01cient and scalable way to learn sota\nimage representations from scratch on a dataset\nof 400 million (image, text) pairs collected ", "sliced score matching: a scalable approach to\n\ndensity and score estimation\n\n9\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n7\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n8\n8\n0\n7\n0\n\n.\n\n5\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nyang song\u2217\n\nstanford university\n\nsahaj garg\u2217\n\nstanford university\n\njiaxin shi\n\ntsinghua university\n\nstefano ermon\n\nstanford university\n\nabstract\n\nscore matching is a popular method for esti-\nmating unnormalized statistical models. how-\never, it has been so far limited to simple, shal-\nlow models or low-dimensional data, due to\nthe dif\ufb01culty of computing the hessian of log-\ndensity functions. we show this dif\ufb01culty can\nbe mitigated by projecting the scores onto ran-\ndom vectors before comparing them. this ob-\njective, called sliced score matching, only in-\nvolves hessian-vector products, which can be\neasily implemented using reverse-mode auto-\nmatic differentiation. therefore, sliced score\nmatching is amenable to more complex models\nand higher dimensional data compared to score\nmatching. theoretically, we prove the consis-\n", "9\n1\n0\n2\n\n \nl\nu\nj\n \n\n9\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n5\n9\n2\n0\n1\n\n.\n\n6\n0\n7\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2018\n\nnoisy networks for exploration\n\nmeire fortunato\u2217 mohammad gheshlaghi azar\u2217 bilal piot \u2217\n\njacob menick matteo hessel\n\nian osband alex graves volodymyr mnih\n\nremi munos demis hassabis olivier pietquin charles blundell\n\nshane legg\n\ndeepmind {meirefortunato,mazar,piot,\njmenick,mtthss,iosband,gravesa,vmnih,\nmunos,dhcontact,pietquin,cblundell,legg}@google.com\n\nabstract\n\nwe introduce noisynet, a deep reinforcement learning agent with parametric noise\nadded to its weights, and show that the induced stochasticity of the agent\u2019s policy\ncan be used to aid ef\ufb01cient exploration. the parameters of the noise are learned\nwith gradient descent along with the remaining network weights. noisynet is\nstraightforward to implement and adds little computational overhead. we \ufb01nd that\nreplacing the conventional exploration heuristics for a3c, dqn and dueling agents\n(entropy reward ", "opinion\n\ntrends in cognitive sciences vol.11 no.8\n\nuntangling invariant object\nrecognition\n\njames j. dicarlo and david d. cox\n\nmcgovern institute for brain research, and department of brain and cognitive sciences, massachusetts institute of technology,\ncambridge, ma 02139, usa\n\ndespite tremendous variation in the appearance of visual\nobjects, primates can recognize a multitude of objects,\neach in a fraction of a second, with no apparent effort.\nhowever, the brain mechanisms that enable this funda-\nmental ability are not understood. drawing on ideas from\nneurophysiology and computation, we present a grap-\nhical perspective on the key computational challenges of\nobject recognition, and argue that the format of neuronal\npopulation representation and a property that we term\n\u2018object tangling\u2019 are central. we use this perspective to\nshow that the primate ventral visual processing stream\nachieves a particularly effective solution in which single-\nneuron invariance is not the goal. finally, we", "1\n2\n0\n2\n\n \n\nb\ne\nf\n6\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n5\n9\n9\n0\n1\n\n.\n\n1\n0\n9\n1\n:\nv\ni\nx\nr\na\n\ngo-explore: a new approach for hard-exploration\n\nproblems\n\nadrien ecoffet\n\njoost huizinga\n\nkenneth o. stanley*\n\njeff clune*\n\njoel lehman\nuber ai labs\n\nsan francisco, ca 94103\n\n*co-senior authors\n\nadrienecoffet,joost.hui,jclune@gmail.com\n\nauthors\u2019 note: we recommend reading (and citing) our updated paper, \u201cfirst return, then explore\u201d:\necoffet, a., huizinga, j., lehman, j., stanley, k.o. and clune, j. first return, then explore. nature\n590, 580\u2013586 (2021). https://doi.org/10.1038/s41586-020-03157-9\nit can be found at https://tinyurl.com/go-explore-nature.\n\nabstract\n\na grand challenge in reinforcement learning is intelligent exploration, especially\nwhen rewards are sparse or deceptive. two atari games serve as benchmarks for\nsuch hard-exploration domains: montezuma\u2019s revenge and pitfall. on both games,\ncurrent rl algorithms perform poorly, even those with intrinsic motivation, which\nis the dominant me", "r e v i e w s\n\nhomeostatic plasticity in the\ndeveloping nervous system\n\ngina g. turrigiano and sacha b. nelson\n\nactivity has an important role in refining synaptic connectivity during development, in part through\n\u2018hebbian\u2019 mechanisms such as long-term potentiation and long-term depression. however,\nhebbian plasticity is probably insufficient to explain activity-dependent development because it\ntends to destabilize the activity of neural circuits. how can complex circuits maintain stable activity\nstates in the face of such destabilizing forces? an idea that is emerging from recent work is that\naverage neuronal activity levels are maintained by a set of homeostatic plasticity mechanisms that\ndynamically adjust synaptic strengths in the correct direction to promote stability. here we discuss\nevidence from a number of systems that homeostatic synaptic plasticity is crucial for processes\nranging from memory storage to activity-dependent development.\n\nhomeostasis has been a central concept i", "deep learning with elastic averaging sgd\n\nsixin zhang\n\ncourant institute, nyu\nzsx@cims.nyu.edu\n\nanna choromanska\ncourant institute, nyu\n\nachoroma@cims.nyu.edu\n\nyann lecun\n\ncenter for data science, nyu & facebook ai research\n\nyann@cims.nyu.edu\n\nabstract\n\nwe study the problem of stochastic optimization for deep learning in the paral-\nlel computing environment under communication constraints. a new algorithm\nis proposed in this setting where the communication and coordination of work\namong concurrent processes (local workers), is based on an elastic force which\nlinks the parameters they compute with a center variable stored by the parameter\nserver (master). the algorithm enables the local workers to perform more explo-\nration, i.e. the algorithm allows the local variables to \ufb02uctuate further from the\ncenter variable by reducing the amount of communication between local workers\nand the master. we empirically demonstrate that in the deep learning setting, due\nto the existence of many local ", "4\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n4\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n7\n4\n2\n6\n\n.\n\n6\n0\n4\n1\n:\nv\ni\nx\nr\na\n\nrecurrent models of visual attention\n\nvolodymyr mnih nicolas heess alex graves koray kavukcuoglu\n\ngoogle deepmind\n\n{vmnih,heess,gravesa,korayk} @ google.com\n\nabstract\n\napplying convolutional neural networks to large images is computationally ex-\npensive because the amount of computation scales linearly with the number of\nimage pixels. we present a novel recurrent neural network model that is ca-\npable of extracting information from an image or video by adaptively selecting\na sequence of regions or locations and only processing the selected regions at\nhigh resolution. like convolutional neural networks, the proposed model has a\ndegree of translation invariance built-in, but the amount of computation it per-\nforms can be controlled independently of the input image size. while the model\nis non-differentiable, it can be trained using reinforcement learning methods to\nlearn task-speci\ufb01c policies. we evalu", "unsupervised learning of video representations using lstms\n\nnitish srivastava\nelman mansimov\nruslan salakhutdinov\nuniversity of toronto, 6 kings college road, toronto, on m5s 3g4 canada\n\nnitish@cs.toronto.edu\nemansim@cs.toronto.edu\nrsalakhu@cs.toronto.edu\n\n6\n1\n0\n2\n\n \n\nn\na\nj\n \n\n4\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n1\n8\n6\n4\n0\n\n.\n\n2\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe use multilayer long short term memory\n(lstm) networks to learn representations of\nvideo sequences. our model uses an encoder\nlstm to map an input sequence into a \ufb01xed\nlength representation. this representation is de-\ncoded using single or multiple decoder lstms\nto perform different tasks, such as reconstruct-\ning the input sequence, or predicting the future\nsequence. we experiment with two kinds of\ninput sequences \u2013 patches of image pixels and\nhigh-level representations (\u201cpercepts\u201d) of video\nframes extracted using a pretrained convolutional\nnet. we explore different design choices such\nas whether the decoder lstms should condi-\ntion on", "a uni\ufb01ed theory for the origin of grid cells through\n\nthe lens of pattern formation\n\nben sorscher*1, gabriel c. mel*2, surya ganguli1, samuel a. ocko1\n\n1department of applied physics, stanford university\n2neurosciences phd program, stanford university\n\nabstract\n\ngrid cells in the brain \ufb01re in strikingly regular hexagonal patterns across space.\nthere are currently two seemingly unrelated frameworks for understanding these\npatterns. mechanistic models account for hexagonal \ufb01ring \ufb01elds as the result of\npattern-forming dynamics in a recurrent neural network with hand-tuned center-\nsurround connectivity. normative models specify a neural architecture, a learning\nrule, and a navigational task, and observe that grid-like \ufb01ring \ufb01elds emerge due to\nthe constraints of solving this task. here we provide an analytic theory that uni\ufb01es\nthe two perspectives by casting the learning dynamics of neural networks trained\non navigational tasks as a pattern forming dynamical system. this theory pro-\nvides ", "coneur-580; no of pages 12\n\navailable online at www.sciencedirect.com\n\nreinforcement learning: the good, the bad and the ugly\npeter dayana and yael nivb\n\nreinforcement learning provides both qualitative and\nquantitative frameworks for understanding and modeling\nadaptive decision-making in the face of rewards and\npunishments. here we review the latest dispatches from the\nforefront of this \ufb01eld, and map out some of the territories where\nlie monsters.\n\naddresses\na ucl, united kingdom\nb psychology department and princeton neuroscience institute,\nprinceton university, united states\n\ncorresponding authors: dayan, peter (dayan@gatsby.ucl.ac.uk) and\nniv, yael (yael@princeton.edu)\n\ncurrent opinion in neurobiology 2008, 18:1\u201312\n\nthis review comes from a themed issue on\ncognitive neuroscience\nedited by read montague and john assad\n\n0959-4388/$ \u2013 see front matter\n# 2008 elsevier ltd. all rights reserved.\n\ndoi 10.1016/j.conb.2008.08.003\n\nintroduction\nreinforcement learning (rl) [1] studies the way ", "training neural networks without gradients:\n\na scalable admm approach\n\ngavin taylor1\nryan burmeister1\nzheng xu2\nbharat singh2\nankit patel3\ntom goldstein2\n1united states naval academy, annapolis, md usa\n2university of maryland, college park, md usa\n3rice university, houston, tx usa\n\nabstract\n\nwith the growing importance of large network\nmodels and enormous training datasets, gpus\nhave become increasingly necessary to train neu-\nral networks. this is largely because conven-\ntional optimization algorithms rely on stochastic\ngradient methods that don\u2019t scale well to large\nnumbers of cores in a cluster setting. further-\nmore, the convergence of all gradient methods,\nincluding batch methods, suffers from common\nproblems like saturation effects, poor condition-\ning, and saddle points. this paper explores an\nunconventional training method that uses alter-\nnating direction methods and bregman iteration\nto train networks without gradient descent steps.\nthe proposed method reduces the network tra", "3\n2\n0\n2\n\n \n\nv\no\nn\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n2\n3\n5\n4\n0\n\n.\n\n6\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nlong sequence hopfield memory\n\nhamza tahir chaudhry1,2, jacob a. zavatone-veth2,3,\n\ndmitry krotov5, cengiz pehlevan1,2,4\n\n1john a. paulson school of engineering and applied sciences,\n\n2center for brain science, 3department of physics,\n\n4kempner institute for the study of natural and artificial intelligence,\n\nharvard university\n\ncambridge, ma 02138\n\n5mit-ibm watson ai lab, ibm research,\n\ncambridge, ma 02142\n\nhchaudhry@g.harvard.edu, jzavatoneveth@g.harvard.edu,\n\nkrotov@ibm.com, cpehlevan@seas.harvard.edu\n\nabstract\n\nsequence memory is an essential attribute of natural and artificial intelligence that\nenables agents to encode, store, and retrieve complex sequences of stimuli and\nactions. computational models of sequence memory have been proposed where\nrecurrent hopfield-like neural networks are trained with temporally asymmetric\nhebbian rules. however, these networks suffer from limited sequence capacity", "distributed  hierarchical  processing\nin the  primate  cerebral  cortex\n\ndaniel j.  felleman1 and  david c. van  essen2\n\n1 department  of neurobiology and anatomy,\nuniversity of texas medical  school, houston,  texas\n77030, and  2 division of biology,  california\ninstitute  of technology,  pasadena,  california  91125\n\nin  recent  years,  many  new  cortical  areas  have  been\nidentified in the macaque monkey. the number of iden-\ntified  connections  between  areas  has  increased  even\nmore dramatically. we report here on (1) a summary of\nthe layout of cortical areas associated with vision and\nwith other modalities, (2)  a computerized database for\nstoring and representing large  amounts of  information\non connectivity patterns, and  (3) the application of these\ndata to the analysis of hierarchical organization  of the\ncerebral cortex. our analysis concentrates on the visual\nsystem,  which  includes  25  neocortical  areas  that are\npredominantly or exclusively visual in function, plu", "n\ne\nu\nr\na\nl\n \nn\ne\nt\nw\no\nr\nk\ns\n,\n \nv\no\nl\n.\n \n6\n,\n \np\np\n.\n \n8\n0\n1\n-\n8\n0\n6\n,\n \n1\n9\n9\n3\n \n0\n8\n9\n3\n-\n6\n0\n8\n0\n/\n9\n3\n \n$\n6\n.\n0\n0\n \n+\n \n.\n0\n0\n \np\nr\ni\nn\nt\ne\nd\n \ni\nn\n \nt\nh\ne\n \nu\ns\na\n.\n \na\nl\nl\n \nr\ni\ng\nh\nt\ns\n \nr\ne\ns\ne\nr\nv\ne\nd\n.\n \nc\no\np\ny\nr\ni\ng\nh\nt\n \n\u00a9\n \n1\n9\n9\n3\n \np\ne\nr\ng\na\nm\no\nn\n \np\nr\ne\ns\ns\n \nl\nt\nd\n.\n \no\nr\ni\ng\ni\nn\na\nl\n \nc\no\nn\nt\nr\ni\nb\nu\nt\ni\no\nn\n \na\np\np\nr\no\nx\ni\nm\na\nt\ni\no\nn\n \no\nf\n \nd\ny\nn\na\nm\ni\nc\na\nl\n \ns\ny\ns\nt\ne\nm\ns\n \nb\ny\n \nc\no\nn\nt\ni\nn\nu\no\nu\ns\n \nt\ni\nm\ne\n \nr\ne\nc\nu\nr\nr\ne\nn\nt\n \nn\ne\nu\nr\na\nl\n \nn\ne\nt\nw\no\nr\nk\ns\n \nk\ne\nn\n-\ni\nc\nh\ni\n \nf\nu\nn\na\nh\na\ns\nh\ni\n \na\nn\nd\n \ny\nu\ni\nc\nh\ni\n \nn\na\nk\na\nm\nu\nr\na\n \nt\no\ny\no\nh\na\ns\nh\ni\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n \no\nf\n \nt\ne\nc\nh\nn\no\nl\no\ng\ny\n \n(\nr\ne\nc\ne\ni\nv\ne\nd\n \n1\n6\n \nm\na\nr\nc\nh\n \n1\n9\n9\n2\n;\n \nr\ne\nv\ni\ns\ne\nd\n \na\nn\nd\n \na\nc\nc\ne\np\nt\ne\nd\n \n1\n0\n \nn\no\nv\ne\nm\nb\ne\nr\n \n1\n9\n9\n2\n \n)\n \na\nb\ns\nt\nr\na\nc\nt\n-\n-\ni\nn\n \nt\nh\ni\ns\n \np\na\np\ne\nr\n,\n \nw\ne\n \np\nr\no\nv\ne\n \nt\nh\na\nt\n \na\nn\ny\n \nf\ni\nn\ni\nt\ne\n \nt\ni\nm\ne\n \nt\nr\na\nj\ne\nc\nt\no\nr\ny\n \no\nf\n \na\n \ng\ni\nv\ne\nn\n \nn\n-\nd\ni\nm\ne\nn\ns\ni\no\nn\na\nl\n \nd\ny\nn\na\nm\ni\nc\na\nl\n \ns\ny\ns\nt\ne\nm\n \nc\na\nn\n \nb\n", "8\n1\n0\n2\n\n \nr\na\n\n \n\nm\n8\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n0\n6\n7\n0\n1\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nunsupervised predictive memory in a\n\ngoal-directed agent\n\ngreg wayne\u2217,1, chia-chun hung\u2217,1, david amos\u2217,1, mehdi mirza1,\n\narun ahuja1, agnieszka grabska-barwi\u00b4nska1, jack rae1, piotr mirowski1,\n\njoel z. leibo1, adam santoro1, mevlana gemici1, malcolm reynolds1,\n\ntim harley1, josh abramson1, shakir mohamed1, danilo rezende1,\n\ndavid saxton1, adam cain1, chloe hillier1, david silver1,\n\nkoray kavukcuoglu1, matt botvinick1, demis hassabis1, timothy lillicrap1.\n\n1deepmind, 5 new street square, london ec4a 3tw, uk.\n\n\u2217these authors contributed equally to this work.\n\nanimals execute goal-directed behaviours despite the limited range and scope\n\nof their sensors. to cope, they explore environments and store memories main-\n\ntaining estimates of important information that is not presently available (1).\n\nrecently, progress has been made with arti\ufb01cial intelligence (ai) agents that\n\nlearn to perform tasks from s", "neural episodic control\n\n7\n1\n0\n2\n\n \nr\na\n\nm\n6\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n8\n8\n9\n1\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nalexander pritzel\nbenigno uria\nsriram srinivasan\nadri`a puigdom`enech\noriol vinyals\ndemis hassabis\ndaan wierstra\ncharles blundell\ndeepmind, london uk\n\napritzel@google.com\nburia@google.com\nsrsrinivasan@google.com\nadriap@google.com\nvinyals@google.com\ndemishassabis@google.com\nwierstra@google.com\ncblundell@google.com\n\nabstract\n\ndeep reinforcement\nlearning methods attain\nsuper-human performance in a wide range of en-\nvironments. such methods are grossly inef\ufb01cient,\noften taking orders of magnitudes more data than\nhumans to achieve reasonable performance. we\npropose neural episodic control: a deep rein-\nforcement learning agent that is able to rapidly\nassimilate new experiences and act upon them.\nour agent uses a semi-tabular representation of\nthe value function: a buffer of past experience con-\ntaining slowly changing state representations and\nrapidly updated estimates of the valu", "probabilistic inference in general graphical models\nthrough sampling in stochastic networks of spiking\nneurons\n\ndejan pecevski*, lars buesing\u00a4, wolfgang maass\n\ninstitute for theoretical computer science, graz university of technology, graz, austria\n\nabstract\n\nan important open problem of computational neuroscience is the generic organization of computations in networks of\nneurons in the brain. we show here through rigorous theoretical analysis that inherent stochastic features of spiking\nneurons, in combination with simple nonlinear computational operations in specific network motifs and dendritic arbors,\nenable networks of spiking neurons to carry out probabilistic inference through sampling in general graphical models. in\nparticular, it enables them to carry out probabilistic inference in bayesian networks with converging arrows (\u2018\u2018explaining\naway\u2019\u2019) and with undirected loops, that occur in many real-world tasks. ubiquitous stochastic features of networks of spiking\nneurons, such as ", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/11561798\n\nsupervised and unsupervised learning with two sites of synaptic integration\n\narticle\u00a0\u00a0in\u00a0\u00a0journal of computational neuroscience \u00b7 november 2001\n\ndoi: 10.1023/a:1013776130161\u00a0\u00b7\u00a0source: pubmed\n\ncitations\n87\n\n2 authors, including:\n\npeter k\u00f6nig\nuniversit\u00e4t osnabr\u00fcck\n\n456 publications\u00a0\u00a0\u00a025,400 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n171\n\nall content following this page was uploaded by peter k\u00f6nig on 21 may 2014.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "3\n2\n0\n2\n\n \nr\na\n\nm\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n1\n3\n3\n0\n\n.\n\n0\n1\n2\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2023\n\nscaling forward gradient with local losses\n\nmengye ren1\u2217, simon kornblith2, renjie liao3, geoffrey hinton2,4\n1nyu, 2google, 3ubc, 4vector institute\n\nabstract\n\nforward gradient learning computes a noisy directional gradient and is a biologi-\ncally plausible alternative to backprop for learning deep neural networks. however,\nthe standard forward gradient algorithm, when applied naively, suffers from high\nvariance when the number of parameters to be learned is large. in this paper, we\npropose a series of architectural and algorithmic modi\ufb01cations that together make\nforward gradient learning practical for standard deep learning benchmark tasks. we\nshow that it is possible to substantially reduce the variance of the forward gradient\nestimator by applying perturbations to activations rather than weights. we further\nimprove the scalability of forward gradient by", "reliable evaluation of adversarial robustness with an ensemble of diverse \n\nparameter-free attacks \n\nfrancesco croce 1  matthias hein 1 \n\nabstract \n\nthe feld of defense strategies against adversarial \nattacks has signifcantly grown over the last years, \nbut progress is hampered as the evaluation of ad-\nversarial defenses is often insuffcient and thus \ngives a wrong impression of robustness.  many \npromising defenses could be broken later on, mak-\ning it diffcult to identify the state-of-the-art. fre-\nquent pitfalls in the evaluation are improper tun-\ning of hyperparameters of the attacks, gradient \nobfuscation or masking.  in this paper we frst \npropose two extensions of the pgd-attack over-\ncoming failures due to suboptimal step size and \nproblems of the objective function. we then com-\nbine our novel attacks with two complementary \nexisting ones to form a parameter-free, computa-\ntionally affordable and user-independent ensem-\nble of attacks to test adversarial robustness.  we \napply", "published as a conference paper at iclr 2017\n\nrevisiting classifier two-sample tests\n\ndavid lopez-paz1, maxime oquab1,2\n1facebook ai research, 2willow project team, inria / ens / cnrs\ndlp@fb.com, maxime.oquab@inria.fr\n\nabstract\n\nthe goal of two-sample tests is to assess whether two samples, sp \u223c p n and sq \u223c\nqm, are drawn from the same distribution. perhaps intriguingly, one relatively\nunexplored method to build two-sample tests is the use of binary classi\ufb01ers. in\nparticular, construct a dataset by pairing the n examples in sp with a positive label,\nand by pairing the m examples in sq with a negative label. if the null hypothesis\n\u201cp = q\u201d is true, then the classi\ufb01cation accuracy of a binary classi\ufb01er on a held-out\nsubset of this dataset should remain near chance-level. as we will show, such\nclassi\ufb01er two-sample tests (c2st) learn a suitable representation of the data on\nthe \ufb02y, return test statistics in interpretable units, have a simple null distribution,\nand their predictive uncertain", "7\n1\n0\n2\n \nc\ne\nd\n3\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n4\nv\n9\n3\n4\n8\n0\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nthinking fast and slow\n\nwith deep learning and tree search\n\nthomas anthony1, \u0000, zheng tian1, and david barber1,2\n\n1university college london\n\n2alan turing institute\n\n\u0000thomas.anthony.14@ucl.ac.uk\n\nabstract\n\nsequential decision making problems, such as structured prediction, robotic control,\nand game playing, require a combination of planning policies and generalisation of\nthose plans. in this paper, we present expert iteration (exit), a novel reinforcement\nlearning algorithm which decomposes the problem into separate planning and\ngeneralisation tasks. planning new policies is performed by tree search, while a\ndeep neural network generalises those plans. subsequently, tree search is improved\nby using the neural network policy to guide search, increasing the strength of new\nplans. in contrast, standard deep reinforcement learning algorithms rely on a\nneural network not only to generalise plans, but to dis", "letter\n\nhttps://doi.org/10.1038/s41586-018-0102-6\n\nvector-based navigation using grid-like \nrepresentations in artificial agents\n\nandrea banino1,2,3,5*, caswell barry2,5*, benigno uria1, charles blundell1, timothy lillicrap1, piotr mirowski1, alexander pritzel1, \nmartin j. chadwick1, thomas degris1, joseph modayil1, greg wayne1, hubert soyer1, fabio viola1, brian zhang1, ross goroshin1, \nneil rabinowitz1, razvan pascanu1, charlie beattie1, stig petersen1, amir sadik1, stephen gaffney1, helen king1,  \nkoray kavukcuoglu1, demis hassabis1,4, raia hadsell1 & dharshan kumaran1,3*\n\ndeep neural networks have achieved impressive successes in fields \nranging from object recognition to complex games such as go1,2. \nnavigation, however, remains a substantial challenge for artificial \nagents, with deep neural networks trained by reinforcement \nlearning3\u20135 failing to rival the proficiency of mammalian spatial \nbehaviour, which is underpinned by grid cells in the entorhinal \ncortex6. grid cells are ", "journal of machine learning research 13 (2012) 281-305\n\nsubmitted 3/11; revised 9/11; published 2/12\n\nrandom search for hyper-parameter optimization\n\njames.bergstra@umontreal.ca\nyoshua.bengio@umontreal.ca\n\njames bergstra\nyoshua bengio\nd\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle\nuniversit\u00b4e de montr\u00b4eal\nmontr\u00b4eal, qc, h3c 3j7, canada\n\neditor: leon bottou\n\nabstract\n\ngrid search and manual search are the most widely used strategies for hyper-parameter optimiza-\ntion. this paper shows empirically and theoretically that randomly chosen trials are more ef\ufb01cient\nfor hyper-parameter optimization than trials on a grid. empirical evidence comes from a compar-\nison with a large previous study that used grid search and manual search to con\ufb01gure neural net-\nworks and deep belief networks. compared with neural networks con\ufb01gured by a pure grid search,\nwe \ufb01nd that random search over the same domain is able to \ufb01nd models that are as good or better\nwithin a small fraction of the computa", "the recurrent neural tangent kernel\n\nsina alemohammad, zichao wang, randall balestriero, richard g. baraniuk\n\ndepartment of electrical and computer engineering\n\nrice university\n\n{sa86,zw16,rb42,richb}@rice.edu\n\n1\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n5\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n6\n4\n2\n0\n1\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nthe study of deep neural networks (dnns) in the in\ufb01nite-width limit, via the\nso-called neural tangent kernel (ntk) approach, has provided new insights into the\ndynamics of learning, generalization, and the impact of initialization. one key dnn\narchitecture remains to be kernelized, namely, the recurrent neural network (rnn).\nin this paper we introduce and study the recurrent neural tangent kernel (rntk),\nwhich provides new insights into the behavior of overparametrized rnns. a key\nproperty of the rntk should greatly bene\ufb01t practitioners is its ability to compare\ninputs of different length. to this end, we characterize how the rntk weights\ndifferent time steps to form its output und", "leading edge\n\nreview\n\nthe self-tuning neuron:  \nsynaptic scaling of excitatory synapses\n\ngina g. turrigiano1,*\n1department of biology, volen center for complex systems, and national center for behavioral genomics, brandeis university, waltham, \nma 02454, usa\n*correspondence: turrigiano@brandeis.edu\ndoi 10.1016/j.cell.2008.10.008\n\nhomeostatic  synaptic  scaling  is  a  form  of  synaptic  plasticity  that  adjusts  the  strength  of  all  of \na  neuron\u2019s  excitatory  synapses  up  or  down  to  stabilize  firing.  current  evidence  suggests  that \nneurons detect changes in their own firing rates through a set of calcium-dependent sensors that \nthen regulate receptor trafficking to increase or decrease the accumulation of glutamate receptors \nat synaptic sites. additional mechanisms may allow local or network-wide changes in activity to \nbe sensed through parallel pathways, generating a nested set of homeostatic mechanisms that \noperate over different temporal and spatial scales.\n\nintro", "natural actor-critic algorithms\n\nshalabh bhatnagar, richard sutton, mohammad ghavamzadeh, mark lee\n\nto cite this version:\nshalabh bhatnagar, richard sutton, mohammad ghavamzadeh, mark lee. natural actor-critic\nalgorithms. automatica, 2009, 45 (11), \uffff10.1016/j.automatica.2009.07.008\uffff. \uffffhal-00840470\uffff\n\nhal id: hal-00840470\n\nhttps://inria.hal.science/hal-00840470\n\nsubmitted on 2 jul 2013\n\nhal is a multi-disciplinary open access\narchive for the deposit and dissemination of sci-\nentific research documents, whether they are pub-\nlished or not. the documents may come from\nteaching and research institutions in france or\nabroad, or from public or private research centers.\n\nl\u2019archive ouverte pluridisciplinaire hal, est\ndestin\u00e9e au d\u00e9p\u00f4t et \u00e0 la diffusion de documents\nscientifiques de niveau recherche, publi\u00e9s ou non,\n\u00e9manant des \u00e9tablissements d\u2019enseignement et de\nrecherche fran\u00e7ais ou \u00e9trangers, des laboratoires\npublics ou priv\u00e9s.\n\n\f", "article\n\nhttps://doi.org/10.1038/s41467-019-08931-6\n\nopen\n\nhumans can decipher adversarial images\n\nzhenglong zhou\n\n1 & chaz firestone1\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\ndoes the human mind resemble the machine-learning systems that mirror its performance?\nconvolutional neural networks (cnns) have achieved human-level benchmarks in classifying\nnovel\nimages. these advances support technologies such as autonomous vehicles and\nmachine diagnosis; but beyond this, they serve as candidate models for human vision itself.\nhowever, unlike humans, cnns are \u201cfooled\u201d by adversarial examples\u2014nonsense patterns\nthat machines recognize as familiar objects, or seemingly irrelevant image perturbations that\nnevertheless alter the machine\u2019s classi\ufb01cation. such bizarre behaviors challenge the promise\nof these new advances; but do human and machine judgments fundamentally diverge? here,\nwe show that human and machine classi\ufb01cation of adversarial images are robustly related: in\n8 experiments on 5 prominent and ", "research article\nscaling properties of dimensionality\nreduction for neural populations and\nnetwork models\n\nryan c. williamson1,2,3, benjamin r. cowley1,3, ashok litwin-kumar4, brent doiron1,5,\nadam kohn6,7,8, matthew a. smith1,9,10,11\u262f, byron m. yu1,12,13\u262f*\n\n1 center for the neural basis of cognition, carnegie mellon university, pittsburgh, pennsylvania, united\nstates of america, 2 school of medicine, university of pittsburgh, pittsburgh, pennsylvania, united states of\namerica, 3 department of machine learning, carnegie mellon university, pittsburgh, pennsylvania, united\nstates of america, 4 center for theoretical neuroscience, columbia university, new york city, new york,\nunited states of america, 5 department of mathematics, university of pittsburgh, pittsburgh, pennsylvania,\nunited states of america, 6 dominick purpura department of neuroscience, albert einstein college of\nmedicine, bronx, new york, united states of america, 7 department of ophthalmology and vision sciences,\nalbert ", "9\n1\n0\n2\n\n \n\nv\no\nn\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n6\n8\n6\n1\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\non extended long short-term memory and dependent\n\nbidirectional recurrent neural network\n\nyuanhang sua,\u2217, c.-c. jay kuoa\n\nauniversity of southern california, ming hsieh department of electrical engineering, 3740\n\nmcclintock avenue, los angeles, ca, united states\n\nabstract\n\nin this work, we \ufb01rst analyze the memory behavior in three recurrent neural\n\nnetworks (rnn) cells; namely, the simple rnn (srn), the long short-term\n\nmemory (lstm) and the gated recurrent unit (gru), where the memory is\n\nde\ufb01ned as a function that maps previous elements in a sequence to the current\n\noutput. our study shows that all three of them su\ufb00er rapid memory decay.\n\nthen, to alleviate this e\ufb00ect, we introduce trainable scaling factors that act like\n\nan attention mechanism to adjust memory decay adaptively. the new design is\n\ncalled the extended lstm (elstm). finally, to design a system that is robust\n\nto previous erroneous p", "letter\ngrid cell symmetry is shaped by environmental\ngeometry\n\ndoi:10.1038/nature14153\n\njulija krupic1*, marius bauza1*, stephen burton1, caswell barry1 & john o\u2019keefe1,2\n\ngrid cells represent an animal\u2019s location by firing in multiple fields\narranged in a striking hexagonal array1. such an impressive and con-\nstant regularity prompted suggestions that grid cells represent a\nuniversal and environmental-invariant metric for navigation1,2. orig-\ninally the properties of grid patterns were believed to be independ-\nent of the shape of the environment and this notion has dominated\nalmost all theoretical grid cell models3\u20136. however, several studies\nindicate that environmental boundaries influence grid firing7\u201310,\nthough the strength, nature and longevity of this effect is unclear. here\nwe show that grid orientation, scale, symmetry and homogeneity are\nstrongly and permanently affected by environmental geometry. we\nfound that grid patterns orient to the walls of polarized enclosures\nsuch as ", "high-resolution image synthesis with latent diffusion models\n\nrobin rombach1 \u2217\n\nandreas blattmann1 \u2217\n\ndominik lorenz1\n\npatrick esser\n\nbj\u00a8orn ommer1\n\n1ludwig maximilian university of munich & iwr, heidelberg university, germany\n\nrunway ml\n\nhttps://github.com/compvis/latent-diffusion\n\nabstract\n\ninput\n\nours (f = 4)\n\ndall-e (f = 8)\n\npsnr: 27.4 r-fid: 0.58\n\npsnr: 22.8 r-fid: 32.01\n\nvqgan (f = 16)\npsnr: 19.9 r-fid: 4.98\n\nby decomposing the image formation process into a se-\nquential application of denoising autoencoders, diffusion\nmodels (dms) achieve state-of-the-art synthesis results on\nimage data and beyond. additionally, their formulation al-\nlows for a guiding mechanism to control the image gen-\neration process without retraining. however, since these\nmodels typically operate directly in pixel space, optimiza-\ntion of powerful dms often consumes hundreds of gpu\ndays and inference is expensive due to sequential evalu-\nations. to enable dm training on limited computational\nresources while", "chicco biodata mining  (2017) 10:35 \ndoi 10.1186/s13040-017-0155-3\n\nreview\n\nopen access\n\nten quick tips for machine learning in\ncomputational biology\n\ndavide chicco\n\ncorrespondence:\ndavide.chicco@davidechicco.it\nprincess margaret cancer centre,\npmcr tower 11-401, 101 college\nstreet, m5g 1l7 toronto, ontario,\ncanada\n\nabstract\nmachine learning has become a pivotal tool for many projects in computational\nbiology, bioinformatics, and health informatics. nevertheless, beginners and\nbiomedical researchers often do not have enough experience to run a data mining\nproject effectively, and therefore can follow incorrect practices, that may lead to\ncommon mistakes or over-optimistic results. with this review, we present ten quick tips\nto take advantage of machine learning in any computational biology context, by\navoiding some common errors that we observed hundreds of times in multiple\nbioinformatics projects. we believe our ten suggestions can strongly help any machine\nlearning practitioner to c", "article\n\nevolving the olfactory system with machine learning\n\nhighlights\nd neural networks trained to classify odors match the olfactory\n\nsystem connectivity\n\nauthors\n\npeter y. wang, yi sun, richard axel,\nl.f. abbott, guangyu robert yang\n\nd input units expressing the same olfactory receptor converge\n\nto form a glomerulus\n\ncorrespondence\nyanggr@mit.edu\n\nd network glomeruli exhibit sparse, unstructured connectivity\n\nonto an expansion layer\n\nd the network develops independent pathways for learned and\n\ninnate odor classi\ufb01cation\n\nin brief\nwang et al. examine whether the\nanatomic connectivity and functional\nlogic of biological olfactory systems\nwould emerge in arti\ufb01cial neural networks\ntrained with stochastic gradient descent.\nthey show that arti\ufb01cial networks trained\nto classify odor identity quantitatively\nrecapitulate the connectivity inherent in\nthe olfactory system.\n\nwang et al., 2021, neuron 109, 3879\u20133892\ndecember 1, 2021 \u00aa 2021 elsevier inc.\nhttps://doi.org/10.1016/j.neuron.2021.09.0", "an introduction to locally linear embedding\n\nlawrence k. saul\n\nat&t labs \u2013 research\n\n180 park ave, florham park, nj 07932 usa\n\nlsaul@research.att.com\n\nsam t. roweis\n\ngatsby computational neuroscience unit, ucl\n\n17 queen square, london wc1n 3ar, uk\n\nroweis@gatsby.ucl.ac.uk\n\nabstract\n\nmany problems in information processing involve some form of dimension-\nality reduction. here we describe locally linear embedding (lle), an unsu-\npervised learning algorithm that computes low dimensional, neighborhood\npreserving embeddings of high dimensional data. lle attempts to discover\nnonlinear structure in high dimensional data by exploiting the local symme-\ntries of linear reconstructions. notably, lle maps its inputs into a single\nglobal coordinate system of lower dimensionality, and its optimizations\u2014\nthough capable of generating highly nonlinear embeddings\u2014do not involve\nlocal minima. we illustrate the method on images of lips used in audiovisual\nspeech synthesis.\n\n1\n\nintroduction\n\nmany problems ", "spectrum dependent learning curves in kernel regression and wide neural\n\nnetworks\n\nblake bordelon 1 abdulkadir canatar 2 cengiz pehlevan 1 3\n\nabstract\n\nwe derive analytical expressions for the gener-\nalization performance of kernel regression as a\nfunction of the number of training samples us-\ning theoretical methods from gaussian processes\nand statistical physics. our expressions apply to\nwide neural networks due to an equivalence be-\ntween training them and kernel regression with\nthe neural tangent kernel (ntk). by computing\nthe decomposition of the total generalization error\ndue to different spectral components of the kernel,\nwe identify a new spectral principle: as the size of\nthe training set grows, kernel machines and neural\nnetworks \ufb01t successively higher spectral modes of\nthe target function. when data are sampled from\na uniform distribution on a high-dimensional hy-\npersphere, dot product kernels, including ntk,\nexhibit learning stages where different frequency\nmodes of the ta", "neuron\n\narticle\n\ninhibitory stabilization of the cortical network\nunderlies visual surround suppression\n\nhirofumi ozeki,1 ian m. finn,1 evan s. schaffer,2 kenneth d. miller,2,3,* and david ferster1,3,*\n1department of neurobiology and physiology, northwestern university, evanston, il 60208, usa\n2center for theoretical neuroscience and department of neuroscience, columbia university, college of physicians and surgeons,\nnew york, ny 10032, usa\n3these authors contributed equally to this work\n*correspondence: ken@neurotheory.columbia.edu (k.d.m.), ferster@northwestern.edu (d.f.)\ndoi 10.1016/j.neuron.2009.03.028\n\nsummary\n\nin what regime does the cortical circuit operate? our\nintracellular studies of surround suppression in cat\nprimary visual cortex (v1) provide strong evidence\non this question. although suppression has been\nthought to arise from an increase in lateral inhibition,\nwe \ufb01nd that\nthe inhibition that cells receive is\nreduced, not increased, by a surround stimulus.\ninstead, suppres", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/221618553\n\nfunctional network reorganization in motor cortex can be explained by reward-\nmodulated hebbian learning\n\nconference paper\u00a0\u00a0in\u00a0\u00a0advances in neural information processing systems \u00b7 january 2009\n\nsource: dblp\n\ncitations\n6\n\n4 authors:\n\nrobert legenstein\ngraz university of technology\n\n112 publications\u00a0\u00a0\u00a05,008 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nandrew schwartz\nuniversity of pittsburgh\n\n147 publications\u00a0\u00a0\u00a018,573 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n34\n\nsteven m chase\ncarnegie mellon university\n\n86 publications\u00a0\u00a0\u00a02,942 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nwolfgang maass\ngraz university of technology\n\n195 publications\u00a0\u00a0\u00a011,598 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nall content following this page was uploaded by wolfgang maass on 01 june 2014.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "inferring synaptic conductances from spike trains\nunder a biophysically inspired point process model\n\nkenneth w. latimer\n\nthe institute for neuroscience\n\nthe university of texas at austin\n\nlatimerk@utexas.edu\n\ne. j. chichilnisky\n\ndepartment of neurosurgery\n\nhansen experimental physics laboratory\n\nstanford university\nej@stanford.edu\n\nfred rieke\n\ndepartment of physiology and biophysics\n\nhoward hughes medical institute\n\nuniversity of washington\n\nrieke@u.washington.edu\n\njonathan w. pillow\n\nprinceton neuroscience institute\n\ndepartment of psychology\n\nprinceton university\n\npillow@princeton.edu\n\nabstract\n\na popular approach to neural characterization describes neural responses in terms\nof a cascade of linear and nonlinear stages: a linear \ufb01lter to describe stimulus\nintegration, followed by a nonlinear function to convert the \ufb01lter output to spike\nrate. however, real neurons respond to stimuli in a manner that depends on the\nnonlinear integration of excitatory and inhibitory synaptic inputs. he", "remote explainability faces the bouncer problem\n\nerwan le merrer\u200a\n\n\u200a1\u2009\u2709 and gilles tr\u00e9dan2\u2009\u2709\n\nthe concept of explainability is envisioned to satisfy society\u2019s demands for transparency about machine learning decisions. \nthe concept is simple: like humans, algorithms should explain the rationale behind their decisions so that their fairness can be \nassessed. although this approach is promising in a local context (for example, the model creator explains it during debugging at \nthe time of training), we argue that this reasoning cannot simply be transposed to a remote context, where a model trained by \na service provider is only accessible to a user through a network and its application programming interface. this is problematic, \nas it constitutes precisely the target use case requiring transparency from a societal perspective. through an analogy with a \nclub bouncer (who may provide untruthful explanations upon customer rejection), we show that providing explanations cannot \nprevent a re", "intrinsic dimension of data representations in deep\n\nneural networks\n\ninternational school for advanced studies\n\ninternational school for advanced studies\n\nalessio ansuini\n\nalessioansuini@gmail.com\n\njakob h. macke\n\nmacke@tum.de\n\nalessandro laio\n\nlaio@sissa.it\n\ndavide zoccolan\n\nzoccolan@sissa.it\n\ntechnical university of munich\n\ninternational school for advanced studies\n\nabstract\n\ndeep neural networks progressively transform their inputs across multiple pro-\ncessing layers. what are the geometrical properties of the representations learned\nby these networks? here we study the intrinsic dimensionality (id) of data-\nrepresentations, i.e. the minimal number of parameters needed to describe a repre-\nsentation. we \ufb01nd that, in a trained network, the id is orders of magnitude smaller\nthan the number of units in each layer. across layers, the id \ufb01rst increases and then\nprogressively decreases in the \ufb01nal layers. remarkably, the id of the last hidden\nlayer predicts classi\ufb01cation accuracy on the ", "rstb.royalsocietypublishing.org\n\nreview\n\ncite this article: lisman j. 2017\nglutamatergic synapses are structurally and\nbiochemically complex because of multiple\nplasticity processes: long-term potentiation,\nlong-term depression, short-term potentiation\nand scaling. phil. trans. r. soc. b 372:\n20160260.\nhttp://dx.doi.org/10.1098/rstb.2016.0260\n\naccepted: 29 june 2016\n\none contribution of 16 to a discussion meeting\nissue \u2018integrating hebbian and homeostatic\nplasticity\u2019.\n\nsubject areas:\nneuroscience, cellular biology\n\nkeywords:\nlong-term potentiation, long-term depression,\nsynaptic scaling, metaplasticity, postsynaptic\ndensity, camkii\n\nauthor for correspondence:\njohn lisman\ne-mail: lisman@brandeis.edu\n\n\u2020present address: biology deparment and\nvolen center for complex systems, brandeis\nuniversity, 415 south street, waltham,\nma 02453, usa.\n\nglutamatergic synapses are structurally\nand biochemically complex because of\nmultiple plasticity processes: long-term\npotentiation, long-term depression,", "r e v i e w s\n\nstate-dependent computations: \nspatiotemporal processing in  \ncortical networks\n\ndean v. buonomano* and wolfgang maass\u2021\n\nabstract | a conspicuous ability of the brain is to seamlessly assimilate and process spatial \nand temporal features of sensory stimuli. this ability is indispensable for the recognition of \nnatural stimuli. yet, a general computational framework for processing spatiotemporal \nstimuli remains elusive. recent theoretical and experimental work suggests that \nspatiotemporal processing emerges from the interaction between incoming stimuli and the \ninternal dynamic state of neural networks, including not only their ongoing spiking activity \nbut also their \u2018hidden\u2019 neuronal states, such as short-term synaptic plasticity.\n\nperceptron\na simple linear neuron model \nthat computes a weighted sum \nof its inputs, and outputs 1 if \nthe weighted sum is larger than \nsome threshold, and 0 \notherwise. weights and \nthresholds can be learned by \nthe perceptron learning ru", "visualizing and understanding convolutional networks\n\n3\n1\n0\n2\n\n \n\nv\no\nn\n8\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n3\nv\n1\n0\n9\n2\n\n.\n\n1\n1\n3\n1\n:\nv\ni\nx\nr\na\n\nmatthew d. zeiler\ndept. of computer science, courant institute, new york university\n\nrob fergus\ndept. of computer science, courant institute, new york university\n\nzeiler@cs.nyu.edu\n\nfergus@cs.nyu.edu\n\nabstract\n\nlarge convolutional network models have\nrecently demonstrated impressive classi\ufb01ca-\ntion performance on the imagenet bench-\nmark (krizhevsky et al., 2012). however\nthere is no clear understanding of why they\nperform so well, or how they might be im-\nproved. in this paper we address both issues.\nwe introduce a novel visualization technique\nthat gives insight into the function of inter-\nmediate feature layers and the operation of\nthe classi\ufb01er. used in a diagnostic role, these\nvisualizations allow us to \ufb01nd model architec-\ntures that outperform krizhevsky et al. on\nthe imagenet classi\ufb01cation benchmark. we\nalso perform an ablation study to dis", "this is the author version of the following paper published by nature on 27 may, 2015:\n\nghahramani, z. (2015) probabilistic machine learning and arti\ufb01cial intelligence. nature\n521:452\u2013459.\n\nprobabilistic machine learning and arti\ufb01cial intelligence\n\nzoubin ghahramani\n\nuniversity of cambridge\n\nmay 28, 2015\n\nhow can a machine learn from experience? probabilistic modelling provides a frame-\n\nwork for understanding what learning is, and has therefore emerged as one of the\n\nprincipal theoretical and practical approaches for designing machines that learn from\n\ndata acquired through experience. the probabilistic framework, which describes how\n\nto represent and manipulate uncertainty about models and predictions, plays a central\n\nrole in scienti\ufb01c data analysis, machine learning, robotics, cognitive science, and arti\ufb01-\n\ncial intelligence. this article provides an introduction to this probabilistic framework,\n\nand reviews some state-of-the-art advances in the \ufb01eld, namely, probabilistic program-", "original research article\npublished: 22 march 2013\ndoi: 10.3389/fncir.2013.00037\n\nlearning and exploration in action-perception loops\ndaniel y. little 1 and friedrich t. sommer 2*\n\n1 department of molecular and cell biology, redwood center for theoretical neuroscience, university of california, berkeley, ca, usa\n2 redwood center for theoretical neuroscience, helen wills neuroscience institute, university of california, berkeley, ca, usa\n\nedited by:\nahmed el hady, max planck\ninstitute for dynamics and self\norganization, germany\nreviewed by:\nrichard hahnloser, eth university\nzurich, switzerland\nrava azeredo da silveira, ecole\nnormale sup\u00e9rieure, france\n*correspondence:\nfriedrich t. sommer, redwood\ncenter for theoretical\nneuroscience, helen wills\nneuroscience institute, university of\ncalifornia, 575a evans hall, mc\n#3198, berkeley, ca 94720-3198,\nusa.\ne-mail: fsommer@berkeley.edu\n\ndiscovering the structure underlying observed data is a recurring problem in machine\nlearning with important ", "12368 \u2022 the journal of neuroscience, december 7, 2016 \u2022 36(49):12368 \u201312384\n\nsystems/circuits\n\norientation selectivity from very sparse lgn inputs in a\ncomprehensive model of macaque v1 cortex\n\nx logan chariker,1 robert shapley,2 and lai-sang young1,2\n1courant institute of mathematical sciences, and 2center for neural science, new york university, new york, new york 10003\n\na new computational model of the primary visual cortex (v1) of the macaque monkey was constructed to reconcile the visual functions\nof v1 with anatomical data on its lgn input, the extreme sparseness of which presented serious challenges to theoretically sound\nexplanations of cortical function. we demonstrate that, even with such sparse input, it is possible to produce robust orientation selec-\ntivity, as well as continuity in the orientation map. we went beyond that to find plausible dynamic regimes of our new model that emulate\nsimultaneously experimental data for a wide range of v1 phenomena, beginning with orient", "5\n1\n0\n2\n\n \nl\nu\nj\n \n\n2\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n4\n4\n5\n4\n0\n\n.\n\n5\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nsynapticsampling:aconnectionbetweenpspvariabilityanduncertaintyexplainsneurophysiologicalobservationslaurenceaitchisonandpetere.lathamjuly12,2015abstractwhenanactionpotentialistransmittedtoapostsynapticneuron,asmallchangeinthepostsynapticneuron\u2019smembranepotentialoccurs.thesesmallchanges,knownasapostsynapticpotentials(psps),arehighlyvariable,andcurrentmodelsassumethatthisvariabilityiscor-ruptingnoise.incontrast,weshowthatthisvariabilitycouldhaveanimportantcomputationalrole:representingasynapse\u2019suncertaintyabouttheoptimalsynapticweight(i.e.thebestpossiblesettingforthesynap-ticweight).weshowthatthislinkbetweenuncertaintyandvariability,thatwecallsynapticsampling,leadstomoreaccurateestimatesoftheuncertaintyintaskrelevantquantities,leadingtomoree\ufb00ectivedecisionmaking.synapticsamplingmakesthreepredictions,allofwhichhavesomeexperimentalsupport.firstthemorevariableasynapseis,themoreitshouldchangedurin", "o\ufb00-policy actor-critic\n\nthomas degris\nflowers team, inria, talence, ensta-paristech, paris, france\n\nthomas.degris@inria.fr\n\nmartha white\nrichard s. sutton\nrlai laboratory, department of computing science, university of alberta, edmonton, canada\n\nwhitem@cs.ualberta.ca\nsutton@cs.ualberta.ca\n\n3\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n9\n3\n8\n4\n\n.\n\n5\n0\n2\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nthis paper presents the \ufb01rst actor-critic al-\ngorithm for o\ufb00-policy reinforcement learning.\nour algorithm is online and incremental, and\nits per-time-step complexity scales linearly\nwith the number of learned weights. pre-\nvious work on actor-critic algorithms is lim-\nited to the on-policy setting and does not\ntake advantage of the recent advances in o\ufb00-\npolicy gradient temporal-di\ufb00erence learning.\no\ufb00-policy techniques, such as greedy-gq,\nenable a target policy to be learned while\nfollowing and obtaining data from another\n(behavior) policy. for many problems, how-\never, actor-critic methods are more pract", "on optimization methods for deep learning\n\nquoc v. le\njiquan ngiam\nadam coates\nabhik lahiri\nbobby prochnow\nandrew y. ng\ncomputer science department, stanford university, stanford, ca 94305, usa\n\nquocle@cs.stanford.edu\njngiam@cs.stanford.edu\nacoates@cs.stanford.edu\nalahiri@cs.stanford.edu\nprochnow@cs.stanford.edu\nang@cs.stanford.edu\n\nabstract\n\nthe predominant methodology in training\ndeep learning advocates the use of stochastic\ngradient descent methods (sgds). despite\nits ease of implementation, sgds are di\ufb03-\ncult to tune and parallelize. these problems\nmake it challenging to develop, debug and\nscale up deep learning algorithms with sgds.\nin this paper, we show that more sophisti-\ncated o\ufb00-the-shelf optimization methods such\nas limited memory bfgs (l-bfgs) and\nconjugate gradient (cg) with line search\ncan signi\ufb01cantly simplify and speed up the\nprocess of pretraining deep algorithms.\nin\nour experiments, the di\ufb00erence between l-\nbfgs/cg and sgds are more pronounced\nif we consider algorithm", "ef\ufb01cientnet: rethinking model scaling for convolutional neural networks\n\nmingxing tan 1 quoc v. le 1\n\nabstract\n\nconvolutional neural networks (convnets) are\ncommonly developed at a \ufb01xed resource budget,\nand then scaled up for better accuracy if more\nresources are available.\nin this paper, we sys-\ntematically study model scaling and identify that\ncarefully balancing network depth, width, and res-\nolution can lead to better performance. based\non this observation, we propose a new scaling\nmethod that uniformly scales all dimensions of\ndepth/width/resolution using a simple yet highly\neffective compound coef\ufb01cient. we demonstrate\nthe effectiveness of this method on scaling up\nmobilenets and resnet.\nto go even further, we use neural architec-\nture search to design a new baseline network\nand scale it up to obtain a family of models,\ncalled ef\ufb01cientnets, which achieve much\nbetter accuracy and ef\ufb01ciency than previous\nconvnets.\nin particular, our ef\ufb01cientnet-b7\nachieves state-of-the-art 84.3% to", "mathematics  of  computation,  volume  35,  number  151\njuly  1980,  pages  773-782\n\nupdating quasi-newton matrices\n\nwith limited storage\n\nby jorge nocedal\n\n  we  study  how  to  use  the  bfgs  quasi-newton  matrices \n\nabstract. \nminimization  methods \nformula  which  generates  matrices  using  information \nm  is  any  number  supplied  by  the  user.    the  quasi-newton  matrix \niteration \ntion.    it  is  shown \n\nfor  problems  where  the  storage  is  critical.    we  give  an  update\nthe  last  m  iterations,  where\nis  updated  at  every\ninforma-\n\nthe  oldest \nit  by  the  newest \nthe  matrices  generated  have  some  desirable  properties.\n\nto  precondition\n\nand  replacing \n\nby  dropping \n\ninformation \n\nfrom \n\nthat \n\nthe  resulting  algorithms \n\nare  tested  numerically \n\nand  compared  with  several  well-\n\nknown  methods.\n1.   introduction.    for  the  problem  of minimizing an unconstrained  function  /\n\nof n  variables, quasi-newton methods  are widely employed  [4].    t", "3\n1\n0\n2\n\n \n\nv\no\nn\n1\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n3\n6\n6\n6\n\n.\n\n5\n0\n3\n1\n:\nv\ni\nx\nr\na\n\ngeneralized denoising auto-encoders as generative\n\nmodels\n\nyoshua bengio, li yao, guillaume alain, and pascal vincent\n\nd\u00b4epartement d\u2019informatique et recherche op\u00b4erationnelle, universit\u00b4e de montr\u00b4eal\n\nabstract\n\nrecent work has shown how denoising and contractive autoencoders implicitly\ncapture the structure of the data-generating density, in the case where the cor-\nruption noise is gaussian, the reconstruction error is the squared error, and the\ndata is continuous-valued. this has led to various proposals for sampling from\nthis implicitly learned density function, using langevin and metropolis-hastings\nmcmc. however, it remained unclear how to connect the training procedure\nof regularized auto-encoders to the implicit estimation of the underlying data-\ngenerating distribution when the data are discrete, or using other forms of corrup-\ntion process and reconstruction errors. another issue is the mat", "6\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n8\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n1\n5\n6\n1\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nfeedforward initialization for fast inference of deep\n\ngenerative networks is biologically plausible\n\nyoshua bengio1\u2217, benjamin scellier1, olexa bilaniuk1, jo\u00e3o sacramento\u2020 and walter senn\u2020\n\n1universit\u00e9 de montr\u00e9al, montreal institute for learning algorithms\n\u2020university of bern, \u2217 canadian institute for advanced research\n\njune 29, 2016\n\nabstract\n\nwe consider deep multi-layered generative models such as boltzmann machines or hop\ufb01eld nets in\nwhich computation (which implements inference) is both recurrent and stochastic, but where the recur-\nrence is not to model sequential structure, only to perform computation. we \ufb01nd conditions under which\na simple feedforward computation is a very good initialization for inference, after the input units are\nclamped to observed values. it means that after the feedforward initialization, the recurrent network is\nvery close to a \ufb01xed point of the network dynamics, w", "\f", "neural networks 16 (2003) 1353\u20131371\n\n2003 special issue\n\nwww.elsevier.com/locate/neunet\n\nmodeling the adaptive visual system: a survey of principled approaches\n\nlars schwabe*, klaus obermayer\n\ndepartment of computer science and electrical engineering, berlin university of technology, fr2-1, franklinstrasse 28/29, berlin 10587, germany\n\nreceived 2 december 2002; revised 28 july 2003; accepted 28 july 2003\n\nabstract\n\nmodeling the visual system can be done at multiple levels of description ranging from computer simulations of detailed biophysical\nmodels to \ufb01ring rate and so-called \u2018black-box\u2019 models. re-introducing david marr\u2019s analysis levels for the visual system, we motivate the\nuse of more abstract models in order to answer the question of what the visual system is computing. the approaches we selected to review\nin this article concentrate on modeling the changes of sensory representations. the considered time-scales, range from the developmental\ntime-scale of receptive \ufb01eld formation", "chaos in neuronal networks with balanced\n\nexcitatory and inhibitory activity\n\nc. van vreeswijk and h. sompolinsky\n\nneurons in the cortex of behaving animals show temporally irregular spiking patterns.\nthe origin of this irregularity and its implications for neural processing are unknown. the\nhypothesis that the temporal variability in the firing of a neuron results from an approx-\nimate balance between its excitatory and inhibitory inputs was investigated theoretically.\nsuch a balance emerges naturally in large networks of excitatory and inhibitory neuronal\npopulations that are sparsely connected by relatively strong synapses. the resulting\nstate is characterized by strongly chaotic dynamics, even when the external inputs to the\nnetwork are constant in time. such a network exhibits a linear response, despite the\nhighly nonlinear dynamics of single neurons, and reacts to changing external stimuli on\ntime scales much smaller than the integration time constant of a single neuron.\n\na cell ", "corrected: publisher correction\n\npredictive and reactive reward signals conveyed \nby climbing fiber inputs to cerebellar purkinje cells\n\ndimitar kostadinov\u200a\n\n\u200a1*, maxime beau\u200a\n\n\u200a1, marta blanco-pozo\u200a\n\n\u200a1,2 and michael h\u00e4usser\u200a\n\n\u200a1*\n\nthere is increasing evidence for a cerebellar contribution to cognitive processing, but the specific input pathways conveying \nthis information remain unclear. we probed the role of climbing fiber inputs to purkinje cells in generating and evaluating \npredictions about associations between motor actions, sensory stimuli and reward. we trained mice to perform a visuomotor \nintegration task to receive a reward and interleaved cued and random rewards between task trials. using two-photon calcium \nimaging and neuropixels probe recordings of purkinje cell activity, we show that climbing fibers signal reward expectation, \ndelivery and omission. these signals map onto cerebellar microzones, with reward delivery activating some microzones and \nsuppressing others, a", "progress  in  neurobiology  103  (2013)  214\u2013222\n\ncontents  lists  available  at  sciverse  sciencedirect\n\nprogress  in  neurobiology\n\nj o  u  r n  a l  h o  m  e p a g  e :  w w  w . e l s  e v i e r  . c  o m  / l o  c a t  e / p n  e u  r o  b  i o\n\nfrom  \ufb01xed  points  to  chaos:  three  models  of  delayed  discrimination\n\nomri  barak a,*,  david  sussillo b,  ranulfo  romo c,g,  misha  tsodyks d,a,  l.f.  abbott a,e,f\na center  for  theoretical  neuroscience,  columbia  university,  new  york,  ny  10032,  usa\nb department  of  electrical  engineering,  neurosciences  program,  stanford  university,  stanford,  ca  94305,  usa\nc instituto  de  fisiolog\u0131\u00b4a  celular-neurociencias,  universidad  nacional  auto\u00b4noma  de  me\u00b4xico,  04510  me\u00b4xico,  d.f.,  mexico\nd department  of  neurobiology,  weizmann  institute  of  science,  rehovot  76100,  israel\ne department  of  neuroscience,  columbia  university,  new  york,  ny  10032,  usa\nf department  of  physiology  and  cellular  biophy", "letters to nature \n\n(academic,  new york,  1975). \n\n17. tolhurst, 0. j. & thompson, i. d. proc.  r.  soc.  land.  b 2 13,  183- 199 (1981 ). \n18.  de valois, r.  l.  &  de valois, k.  k.  spatial vision  (oxford  university press, oxford, 1988). \n19.  bonhoeffer,  t.  &  grinvald, a.  nat11re 353, 429~431 {1991). \n20.  movshon, j. a., thompson, i. d.  & tolhurst, d.  j.]. physiol. (lond. ) 283,  101-120 ( 1978). \n2 1.  shoham, d.  israel]. med. sci.  (aijstr.) 32, s7 ( 1996). \n22. tootell, r.  b.  h., silverman, m. s., hamilton, s. l., switkes, e. & de valois, r. l. f.  neurosci. 8  1610-\n\n1624 (1988). \n\n23. silverman, m.s., grosof, d.  h., de valois, r.  l.  & elfar. s. d. proc. nat/ acad.  sci.  usa 86, 7 11-715 \n\n( 1989). \n\n24.  born, r. t.  &  tootell, r.  b.  h.  proc.  nat! acatl.  sd.  usa 88,7066- 7070 ( 1991). \n25.  edwards, d.p., purpura, k. p.  &  kaplan, e.  visio11  res. 35,  1501-1523 (1 995). \n26.  murphy, k. m., van  sluyters, r.  c. &  jones, d.  g.  soc.  neurosci. ab", "levels4. our analysis shows that it is appropriate and necessary to\nattempt restoration on a global scale, and provides a benchmark\na\nagainst which community recovery could be assessed.\n\nmethods\ndata selection\nfor shelf communities, we compiled data from research trawl surveys from the southern\ngrand banks (43\u2013468 n, 49\u2013538 w) and saint pierre banks (45\u2013478 n, 55\u2013588 w) (ref. 28),\nthe gulf of thailand (9\u2013148 n, 100\u20131058 w) (ref. 29) and south georgia (53\u2013568 s,\n35\u2013408 w) (ref. 14). all other trawl data sets that we considered (for example, north sea,\ngeorges bank and alaska) did not capture the beginning of industrialized exploitation. we\nincluded only demersal predators; pelagic species, which were not well sampled by the\ntrawl gear, were excluded. longlining data obtained from the japanese fishery agency\nwere divided into temperate (atlantic, 40\u2013458 s; indian, 35\u2013458 s; paci\ufb01c, 30\u2013458 s),\nsubtropical (atlantic, 10\u2013408 s; indian, 10\u2013358 s; paci\ufb01c, 15\u2013308 s) and tropical\ncommunities (a", "neuroimage 11, 805\u2013821 (2000)\ndoi:10.1006/nimg.2000.0582, available online at http://www.idealibrary.com on\n\nvoxel-based morphometry\u2014the methods\n\njohn ashburner and karl j. friston\n\nthe wellcome department of cognitive neurology, institute of neurology, queen square, london wc1n 3bg, united kingdom\n\nreceived october 22, 1999\n\nat its simplest, voxel-based morphometry (vbm) in-\nvolves a voxel-wise comparison of the local concentra-\ntion of gray matter between two groups of subjects.\nthe procedure is relatively straightforward and in-\nvolves spatially normalizing high-resolution images\nfrom all the subjects in the study into the same stereo-\ntactic space. this is followed by segmenting the gray\nmatter from the spatially normalized images and\nsmoothing the gray-matter segments. voxel-wise para-\nmetric statistical tests which compare the smoothed\ngray-matter images from the two groups are per-\nformed. corrections for multiple comparisons are\nmade using the theory of gaussian random \ufb01elds. t", "research | reports\n\nshore birds (22), and interdigital webbing has\nbeen reported in theropod dinosaurs (29).\n\nreduction of the pelvic girdle and hindlimb and\nthe concomitant enhancement of axial-powered\nlocomotion are common among semiaquatic\nvertebrates. the flexibility of the tail and the\nform of the neural spines in spinosaurus suggest\ntail-assisted swimming. like extinct and extant\nsemiaquatic reptiles, spinosaurus used lateral\nundulation of the tail, in contrast to the vertical\naxial undulation adopted repeatedly by semi-\naquatic mammals (20, 21).\n\nthe dorsal \u201csail\u201d in spinosaurus, the tallest\naxial structure documented among dinosaurs,\nhas been argued to be a thermoregulatory sur-\nface, a muscle- or fat-lined hump (30), or a dis-\nplay structure. stromer (1) drew an analogy to\nthe skin-covered neural spines of the crested\nchameleon, trioceros cristatus (fig. 4e). as in\nt. cristatus, the sail of spinosaurus is centered\nover the trunk (fig. 2a). the shape and position-\ning of the sp", "nature  vol.  337  12  januar._.y_:c::l9\"'8'-9 _________  commentary----\n\n129 \n\nthe  recent  excitement  about  neural  networks \n\nfrancis  crick \n\nthe remarkable properties of some recent computer algorithms for neural networks seemed to promise \na fresh  approach  to  understanding  the  computational properties  of the  brain.  unfortunately  most  of \nthese  neural nets are  unrealistic  in  important respects. \n\nthere  has  been  a  lot  of  excitement \nrecently  about  neural  nets.  a  new \nalgorithm  has  produced quite simple nets \nthat  perform  surprisingly  well.  a  thick \ntwo-volumed  work,  parallel  distributed \nprocessing 1\n,  has  been  a  best-seller,  read \nenthusiastically  by  psychologists,  com \nputer  designers  and  physicists.  even \nundergraduates  are  now  designing  new \nnetworks.  the  interested  spectator  may \nwell  wonder what it's all  about.  what are \nneural  nets?  how  do  they  work?  and \nwhat, if anything, do they tell  us  about the \nbrain? ", "review article\n\nten simple rules for the computational\nmodeling of behavioral data\nrobert c wilson1,2\u2020*, anne ge collins3,4\u2020*\n\n1department of psychology, university of arizona, tucson, united states;\n2cognitive science program, university of arizona, tucson, united states;\n3department of psychology, university of california, berkeley, berkeley, united\nstates; 4helen wills neuroscience institute, university of california, berkeley,\nberkeley, united states\n\nabstract computational modeling of behavior has revolutionized psychology and neuroscience.\nby fitting models to experimental data we can probe the algorithms underlying behavior, find\nneural correlates of computational variables and better understand the effects of drugs, illness and\ninterventions. but with great power comes great responsibility. here, we offer ten simple rules to\nensure that computational modeling is used with care and yields meaningful insights. in particular,\nwe present a beginner-friendly, pragmatic and details-o", "8\n1\n0\n2\n\n \nt\nc\no\n9\n2\n\n \n\n \n \n]\n\n.\n\nc\no\nh\nt\na\nm\n\n[\n \n \n\n2\nv\n5\n4\n5\n9\n0\n\n.\n\n5\n0\n8\n1\n:\nv\ni\nx\nr\na\n\non the global convergence of gradient descent for\nover-parameterized models using optimal transport\n\nl\u00e9na\u00efc chizat\n\nfrancis bach\n\ninria, ens, psl research university\n\ninria, ens, psl research university\n\nparis, france\n\nlenaic.chizat@inria.fr\n\nparis, france\n\nfrancis.bach@inria.fr\n\nabstract\n\nmany tasks in machine learning and signal processing can be solved by minimizing\na convex function of a measure. this includes sparse spikes deconvolution or\ntraining a neural network with a single hidden layer. for these problems, we study\na simple minimization method: the unknown measure is discretized into a mixture\nof particles and a continuous-time gradient descent is performed on their weights\nand positions. this is an idealization of the usual way to train neural networks\nwith a large hidden layer. we show that, when initialized correctly and in the\nmany-particle limit, this gradient \ufb02ow, although non", "empirical models of spiking in neural populations\n\njakob h. macke\n\nlars b\u00a8using\n\ngatsby computational neuroscience unit\n\ngatsby computational neuroscience unit\n\nuniversity college london, uk\njakob@gatsby.ucl.ac.uk\n\nuniversity college london, uk\nlars@gatsby.ucl.ac.uk\n\njohn p. cunningham\n\ndepartment of engineering\nuniversity of cambridge, uk\n\njpc74@cam.ac.uk\n\nbyron m. yu\nece and bme\n\ncarnegie mellon university\n\nbyronyu@cmu.edu\n\nkrishna v. shenoy\n\ndepartment of electrical engineering\n\nstanford university\n\nshenoy@stanford.edu\n\nmaneesh sahani\n\ngatsby computational neuroscience unit\n\nuniversity college london, uk\n\nmaneesh@gatsby.ucl.ac.uk\n\nabstract\n\nneurons in the neocortex code and compute as part of a locally interconnected\npopulation. large-scale multi-electrode recording makes it possible to access\nthese population processes empirically by \ufb01tting statistical models to unaveraged\ndata. what statistical structure best describes the concurrent spiking of cells within\na local network? we arg", "i an update to this article is included at the end\n\nreport\n\ntiming rules for synaptic plasticity matched to\nbehavioral function\n\nhighlights\nd synaptic plasticity rules are not uniform, but tuned to speci\ufb01c\n\ncircuit function\n\nauthors\n\naparna suvrathan, hannah l. payne,\njennifer l. raymond\n\nd different rules at different cerebellar parallel \ufb01ber-to-purkinje\n\ncell synapses\n\ncorrespondence\naparnasu@stanford.edu\n\nd synaptic plasticity can precisely compensate for circuit\n\ndelays of >100 ms\n\nd provides a mechanism for solving temporal credit\n\nassignment problem\n\nin brief\nsuvrathan et al. expand the known\nrepertoire of synaptic plasticity by\nshowing that the same kind of synapses\ncan exhibit different learning rules and\nthat these rules can precisely\ncompensate for behaviorally relevant\ncircuit delays.\n\nsuvrathan et al., 2016, neuron 92, 959\u2013967\ndecember 7, 2016 \u00aa 2016 elsevier inc.\nhttp://dx.doi.org/10.1016/j.neuron.2016.10.022\n\n\f", "elifesciences.org\n\ntools and resources\n\nautomatic discovery of cell types and\nmicrocircuitry from neural connectomics\neric jonas1*, konrad kording2,3,4\n\n1department of electrical engineering and computer science, university of california,\nberkeley, berkeley, united states; 2department of physical medicine and rehabilitation,\nnorthwestern university, chicago, united states; 3department of physical medicine and\nrehabilitation, rehabilitation institute of chicago, chicago, united states; 4department\nof physiology, northwestern university, chicago, united states\n\nabstract neural connectomics has begun producing massive amounts of data, necessitating new\nanalysis methods to discover the biological and computational structure. it has long been assumed\nthat discovering neuron types and their relation to microcircuitry is crucial to understanding neural\nfunction. here we developed a non-parametric bayesian technique that identifies neuron types and\nmicrocircuitry patterns in connectomics data.", "journal of machine learning research 3 (2002) 59\u201372\n\nsubmitted 10/01; published 7/02\n\non the convergence of optimistic policy iteration\n\njohn n. tsitsiklis\nlids, room 35-209\nmassachusetts institute of technology\n77 massachusetts avenue\ncambridge, ma 02139-4307, usa\n\neditor: sridhar mahadevan\n\njnt@mit.edu\n\nabstract\n\nwe consider a \ufb01nite-state markov decision problem and establish the convergence of a spe-\ncial case of optimistic policy iteration that involves monte carlo estimation of q-values, in\nconjunction with greedy policy selection. we provide convergence results for a number of\nalgorithmic variations, including one that involves temporal di\ufb00erence learning (bootstrap-\nping) instead of monte carlo estimation. we also indicate some extensions that either fail\nor are unlikely to go through.\nkeywords: markov decision problem, dynamic programming, reinforcement learning,\nmonte carlo, stochastic approximation, temporal di\ufb00erences.\n\n1. introduction\n\nthis paper deals with simulation-based", "neuroscience and biobehavioral reviews 96 (2019) 367\u2013400\n\ncontents lists available at sciencedirect\n\nneuroscience and biobehavioral reviews\n\njournal homepage: www.elsevier.com/locate/neubiorev\n\nreview article\ndo \u2018early\u2019 brain responses reveal word form prediction during language\ncomprehension? a critical review\nmante s. nieuwlanda,b,\u204e\na max planck institute for psycholinguistics, nijmegen, the netherlands\nb donders institute for brain, cognition and behaviour, nijmegen, the netherlands\n\nt\n\na b s t r a c t\n\ncurrent theories of language comprehension posit that readers and listeners routinely try to predict the meaning but also the visual or sound form of upcoming words.\nwhereas most neuroimaging studies on word prediction focus on the n400 erp or its magnetic equivalent, various studies claim that word form prediction manifests\nitself in \u2018early\u2019, pre-n400 brain responses (e.g., elan, m100, p130, n1, p2, n200/pmn, n250). modulations of these components are often taken as evidence that\nwo", "emergence of a stable cortical map for neuroprosthetic\ncontrol\n\nkarunesh ganguly1,2,3, jose m. carmena1,2,4*\n\n1 department of electrical engineering and computer sciences, university of california berkeley, berkeley, california, united states of america, 2 helen wills neuroscience\ninstitute, university of california berkeley, berkeley, california, united states of america, 3 department of neurology, university of california san francisco, san francisco,\ncalifornia, united states of america, 4 program in cognitive science, university of california berkeley, berkeley, california, united states of america\n\nabstract\n\ncortical control of neuroprosthetic devices is known to require neuronal adaptations. it remains unclear whether a stable\ncortical representation for prosthetic function can be stored and recalled in a manner that mimics our natural recall of motor\nskills. especially in light of the mixed evidence for a stationary neuron-behavior relationship in cortical motor areas,\nunderstan", "5\n1\n0\n2\n\n \nr\np\na\n9\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n8\n6\n0\n5\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\naccepted as a workshop contribution at iclr 2015\n\ntowards deep neural network architectures\nrobust to adversarial examples\n\nshixiang gu\npanasonic silicon valley laboratory\npanasonic r&d company of america\nshane.gu@us.panasonic.com\n\nluca rigazio\npanasonic silicon valley laboratory\npanasonic r&d company of america\nluca.rigazio@us.panasonic.com\n\nabstract\n\nrecent work has shown deep neural networks (dnns) to be highly susceptible\nto well-designed, small perturbations at the input layer, or so-called adversarial\nexamples. taking images as an example, such distortions are often imperceptible,\nbut can result in 100% mis-classi\ufb01cation for a state of the art dnn. we study the\nstructure of adversarial examples and explore network topology, pre-processing\nand training strategies to improve the robustness of dnns. we perform various\nexperiments to assess the removability of adversarial examples by corrupting with\na", "neuroresource\n\nhigh-density, long-lasting, and multi-region\nelectrophysiological recordings using polymer\nelectrode arrays\n\nhighlights\nd modular polymer electrode-based system capable of\n\nrecording up to 1,024 channels\n\nd recording from 375 single units across multiple regions in\n\nfreely behaving rats\n\nd single-unit recording longevity for 160 or more days post-\n\nimplantation\n\nd system capable of tracking populations of single units\n\ncontinuously for over a week\n\nauthors\n\njason e. chung, hannah r. joo,\njiang lan fan, ..., leslie f. greengard,\nvanessa m. tolosa, loren m. frank\n\ncorrespondence\njason.chung@ucsf.edu (j.e.c.),\nloren@phy.ucsf.edu (l.m.f.)\n\nin brief\nchung et al. present a large-scale, multi-\nsite, polymer electrode-based recording\nplatform. the modular system is capable\nof data collection from up to 1,024\nchannels in freely behaving rats, enabling\nlong-term studies of distributed\nnetworks.\n\nchung et al., 2019, neuron 101, 21\u201331\njanuary 2, 2019 \u00aa 2018 elsevier inc.\nhttps://doi", "neuron\n\nreview\n\ndecision making in recurrent neuronal circuits\n\nxiao-jing wang1,*\n1department of neurobiology and kavli institute for neuroscience, yale university school of medicine, 333 cedar street,\nnew haven, ct 06510, usa\n*correspondence: xjwang@yale.edu\ndoi 10.1016/j.neuron.2008.09.034\n\ndecision making has recently emerged as a central theme in neurophysiological studies of cognition, and\nexperimental and computational work has led to the proposal of a cortical circuit mechanism of elemental\ndecision computations. this mechanism depends on slow recurrent synaptic excitation balanced by fast\nfeedback inhibition, which not only instantiates attractor states for forming categorical choices but also\nlong transients for gradually accumulating evidence in favor of or against alternative options. such a circuit\nendowed with reward-dependent synaptic plasticity is able to produce adaptive choice behavior. while de-\ncision threshold is a core concept for reaction time tasks, it can be dis", "9\n1\n0\n2\n\n \n\np\ne\ns\n5\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n4\n0\n3\n1\n1\n\n.\n\n9\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nasymptotics of wide networks\n\nfrom feynman diagrams\n\nethan dyer\u2217\n\ngoogle\n\nguy gur-ari\u2217\n\ngoogle\n\nmountain view, ca\nedyer@google.com\n\nmountain view, ca\nguyga@google.com\n\nseptember 26, 2019\n\nabstract\n\nunderstanding the asymptotic behavior of wide networks is of considerable interest. in this work,\nwe present a general method for analyzing this large width behavior. the method is an adaptation\nof feynman diagrams, a standard tool for computing multivariate gaussian integrals. we apply our\nmethod to study training dynamics, improving existing bounds and deriving new results on wide\nnetwork evolution during stochastic gradient descent. going beyond the strict large width limit, we\npresent closed-form expressions for higher-order terms governing wide network training, and test these\npredictions empirically.\n\ncontents\n\n1 introduction\n\n2 correlation function asymptotics\n\n2.1 notation . . . . . . . . . . . .", "a convergence theory for deep learning via over-parameterization\n\nzeyuan allen-zhu * 1 yuanzhi li * 2 3 zhao song * 4 5 6\n\nabstract\n\ndeep neural networks (dnns) have demon-\nstrated dominating performance in many \ufb01elds;\nsince alexnet, networks used in practice are go-\ning wider and deeper. on the theoretical side, a\nlong line of works have been focusing on why\nwe can train neural networks when there is only\none hidden layer. the theory of multi-layer net-\nworks remains unsettled. in this work, we prove\nsimple algorithms such as stochastic gradient de-\nscent (sgd) can \ufb01nd global minima on the train-\ning objective of dnns in polynomial time. we\nonly make two assumptions: the inputs do not de-\ngenerate and the network is over-parameterized.\nthe latter means the number of hidden neurons\nis suf\ufb01ciently large: polynomial in l, the num-\nber of dnn layers and in n, the number of train-\ning samples. as concrete examples, starting from\nrandomly initialized weights, we show that sgd\nattains 100% t", "1\n2\n0\n2\n\n \n\np\ne\ns\n8\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n0\n3\n0\n4\n0\n\n.\n\n2\n1\n0\n2\n:\nv\ni\nx\nr\na\n\nstatistical mechanics of deep linear neural networks:\n\nthe back-propagating kernel renormalization\n\nqianyi li1,2and haim sompolinsky2,3,4\n\n1the biophysics program, harvard university, cambridge, ma 02138, usa\n2center for brain science, harvard university, cambridge, ma 02138, usa\n\n4racah institute of physics, hebrew university, jerusalem 91904, israel\n\n5edmond and lily safra center for brain sciences, hebrew university, jerusalem 91904, israel\n\nthe groundbreaking success of deep learning in many real-world tasks has triggered an intense\ne\ufb00ort to understand theoretically the power and limitations of deep learning in the training and\ngeneralization of complex tasks, so far with limited progress. in this work we study the statistical\nmechanics of learning in deep linear neural networks (dlnns) in which the input-output func-\ntion of an individual unit is linear. despite the linearity of the units, lea", "ieee transactions on neural networks, vol. 22, no. 7, july 2011\n\n1149\n\nspectral clustering on multiple manifolds\n\nyong wang, yuan jiang, yi wu, and zhi-hua zhou, senior member, ieee\n\nabstract\u2014 spectral clustering (sc) is a large family of group-\ning methods that partition data using eigenvectors of an af\ufb01nity\nmatrix derived from the data. though sc methods have been\nsuccessfully applied to a large number of challenging clustering\nscenarios, it is noteworthy that they will fail when there are\nsigni\ufb01cant intersections among different clusters. in this paper,\nbased on the analysis that sc methods are able to work well when\nthe af\ufb01nity values of the points belonging to different clusters\nare relatively low, we propose a new method, called spectral\nmulti-manifold clustering (smmc), which is able to handle\nintersections. in our model, the data are assumed to lie on or\nclose to multiple smooth low-dimensional manifolds, where some\ndata manifolds are separated but some are intersecting. then,\n", " \n\n \n\nstructured pruning of deep convolutional \n\nneural networks \n\nsajid anwar, kyuyeon hwang and wonyong sung \n\ndepartment of electrical engineering and computer science \n\nseoul national university \n\nseoul 151-744, korea \n\n{sajid, khwang}@dsp.snu.ac.kr, wysung@snu.ac.kr \n\n \n\nabstract \n\nis  very  advantageous \n\nreal time application of deep learning algorithms is often hindered by high \ncomputational complexity and frequent memory accesses.    network pruning \nis  a  promising  technique  to  solve  this  problem.  however,  pruning  usually \nresults  in  irregular  network  connections  that  not  only  demand  extra \nrepresentation  efforts  but  also  do  not  fit  well  on  parallel  computation.  we \nintroduce  structured  sparsity  at  various  scales  for  convolutional  neural \nnetworks,  which  are  channel  wise,  kernel  wise  and  intra  kernel  strided \nsparsity.  this  structured  sparsity \nfor  direct \ncomputational resource savings on embedded computers, parallel comput", "large-scale object classi\ufb01cation using label\n\nrelation graphs\n\njia deng\u2020\u2217, nan ding\u2217, yangqing jia\u2217, andrea frome\u2217, kevin murphy\u2217,\n\nsamy bengio\u2217, yuan li\u2217, hartmut neven\u2217, hartwig adam\u2217\n\nuniversity of michigan\u2020, google inc.\u2217\n\nabstract. in this paper we study how to perform object classi\ufb01cation in\na principled way that exploits the rich structure of real world labels. we\ndevelop a new model that allows encoding of \ufb02exible relations between\nlabels. we introduce hierarchy and exclusion (hex) graphs, a new for-\nmalism that captures semantic relations between any two labels applied\nto the same object: mutual exclusion, overlap and subsumption. we then\nprovide rigorous theoretical analysis that illustrates properties of hex\ngraphs such as consistency, equivalence, and computational implications\nof the graph structure. next, we propose a probabilistic classi\ufb01cation\nmodel based on hex graphs and show that it enjoys a number of de-\nsirable properties. finally, we evaluate our method using a lar", "physiol rev 90: 1195\u20131268, 2010;\ndoi:10.1152/physrev.00035.2008.\n\nneurophysiological and computational principles of cortical\n\nrhythms in cognition\n\nxiao-jing wang\n\ndepartment of neurobiology and kavli institute of neuroscience, yale university school of medicine,\n\nnew haven, connecticut\n\ni. introduction\n\na. synchronization and stochastic neuronal activity in the cerebral cortex\nb. cortical oscillations associated with cognitive behaviors\nc. interplay between neuronal and synaptic dynamics\nd. organization of this review\n\nii. single neurons as building blocks of network oscillations\na. phase-response properties of type i and type ii neurons\nb. resonance\nc. subthreshold and mixed-mode membrane oscillations\nd. rhythmic bursting\n\niii. basic mechanisms for network synchronization\na. mutual excitation between pyramidal neurons\nb. inhibitory interneuronal network\nc. excitatory-inhibitory feedback loop\nd. synaptic filtering\ne. slow negative feedback\nf. electrical coupling\ng. correlation-induce", "t e c h n i c a l   r e p o r t s\n\nrevealed no correlation between place field location and a cell\u2019s ana-\ntomical location8. by contrast, ieg studies have provided evidence for \nmicro-clustering of cells that were active in restricted places within \na larger environment5.\n\nfunctional imaging of hippocampal place cells at \ncellular resolution during virtual navigation\ndaniel a dombeck1, christopher d harvey1, lin tian2, loren l looger2 & david w tank1\nspatial navigation is often used as a behavioral task in studies \nof the neuronal circuits that underlie cognition, learning and \nmemory in rodents. the combination of in vivo microscopy with \ngenetically encoded indicators has provided an important new tool \nfor studying neuronal circuits, but has been technically difficult \nto apply during navigation. here we describe methods for imaging \nthe activity of neurons in the ca1 region of the hippocampus with \nsubcellular resolution in behaving mice. neurons that expressed \nthe genetically enc", "review\n\ncomputational rationality: a\nconverging paradigm for intelligence\nin brains, minds, and machines\n\nsamuel j. gershman,1* eric j. horvitz,2* joshua b. tenenbaum3*\n\nafter growing up together, and mostly growing apart in the second half of the 20th century,\nthe fields of artificial intelligence (ai), cognitive science, and neuroscience are\nreconverging on a shared view of the computational foundations of intelligence that\npromotes valuable cross-disciplinary exchanges on questions, methods, and results.\nwe chart advances over the past several decades that address challenges of perception\nand action under uncertainty through the lens of computation. advances include the\ndevelopment of representations and inferential procedures for large-scale probabilistic\ninference and machinery for enabling reflection and decisions about tradeoffs in effort,\nprecision, and timeliness of computations. these tools are deployed toward the goal of\ncomputational rationality: identifying decisions with ", "trends in cognitive sciences\n\nseries: machine behavior\nreview\nembracing change: continual learning in\ndeep neural networks\n\nraia hadsell,1,*,@ dushyant rao,1,@ andrei a. rusu,1,@ and razvan pascanu1,@\n\narti\ufb01cial intelligence research has seen enormous progress over the past few de-\ncades, but it predominantly relies on \ufb01xed datasets and stationary environments.\ncontinual learning is an increasingly relevant area of study that asks how arti\ufb01cial\nsystems might learn sequentially, as biological systems do, from a continuous\nstream of correlated data. in the present review, we relate continual learning to\nthe learning dynamics of neural networks, highlighting the potential it has to con-\nsiderably improve data ef\ufb01ciency. we further consider the many new biologically\ninspired approaches that have emerged in recent years, focusing on those that\nutilize regularization, modularity, memory, and meta-learning, and highlight\nsome of the most promising and impactful directions.\n\nthe world is not s", "letter\n\nhttps://doi.org/10.1038/s41586-019-1261-9\n\nspecialized coding of sensory, motor and cognitive \nvariables in vta dopamine neurons\n\nben engelhard1,2, joel finkelstein1,3, julia cox1, weston fleming1, hee jae jang1, sharon ornelas1, sue ann koay1,  \nstephan y. thiberge1,2, nathaniel d. daw1,3, david w. tank1,2 & ilana b. witten1,2,3*\n\nthere is increased appreciation that dopamine neurons in the \nmidbrain respond not only to reward1 and reward-predicting \ncues1,2, but also to other variables such as the distance to reward3, \nmovements4\u20139 and behavioural choices10,11. an important question \nis how the responses to these diverse variables are organized across \nthe population of dopamine neurons. whether individual dopamine \nneurons multiplex several variables, or whether there are subsets \nof neurons that are specialized in encoding specific behavioural \nvariables remains unclear. this fundamental question has been \ndifficult to resolve because recordings from large populations \nof i", "j neurophysiol 92: 780 \u2013789, 2004;\n10.1152/jn.01171.2003.\n\nprecision of spike trains in primate retinal ganglion cells\n\nv. j. uzzell1,2 and e. j. chichilnisky1,2\n1the salk institute, la jolla 92037; and 2university of california, san diego, california 92037\n\nsubmitted 8 december 2003; accepted in \ufb01nal form 22 february 2004\n\nuzzell, v. j. and e. j. chichilnisky. precision of spike trains in\nprimate retinal ganglion cells. j neurophysiol 92: 780 \u2013789, 2004;\n10.1152/jn.01171.2003. recent studies have revealed striking preci-\nsion in the spike trains of retinal ganglion cells in several species and\nsuggested that this precision could be an important aspect of visual\nsignaling. however, the precision of spike trains has not yet been\ndescribed in primate retina. the spike time and count variability of\nparasol (magnocellular-projecting) retinal ganglion cells was exam-\nined in isolated macaque monkey retinas stimulated with repeated\npresentations of high contrast, spatially uniform intensity ", "bayesian statistics and modelling\n\n 5, \n\n 1\u2009\u2709, sarah\u00a0depaoli2, ruth\u00a0king \n\n 3,4, bianca\u00a0kramer \n\n 7, marina\u00a0vannucci \n\n 8, andrew\u00a0gelman9, \n\n 1, joukje\u00a0willemsen \n\n 6, mahlet\u00a0g.\u00a0tadesse \n\n 1 and christopher\u00a0yau4,10\n\nrens\u00a0van de schoot \nkaspar\u00a0m\u00e4rtens \nduco\u00a0veen \nabstract | bayesian statistics is an approach to data analysis based on bayes\u2019 theorem, where \navailable knowledge about parameters in a statistical model is updated with the information in \nobserved data. the background knowledge is expressed as a prior distribution and combined  \nwith observational data in the form of a likelihood function to determine the posterior distribution. \nthe posterior can also be used for making predictions about future events. this primer describes the \nstages involved in bayesian analysis, from specifying the prior and data models to deriving \ninference, model checking and refinement. we discuss the importance of prior and posterior \npredictive checking, selecting a proper technique for sampling f", "learning to learn with feedback and local plasticity\n\njack lindsey, ashok litwin-kumar\n\ncolumbia university, department of neuroscience\n\n{j.lindsey, a.litwin-kumar}@columbia.edu\n\nabstract\n\ninterest in biologically inspired alternatives to backpropagation is driven by the\ndesire to both advance connections between deep learning and neuroscience and\naddress backpropagation\u2019s shortcomings on tasks such as online, continual learn-\ning. however, local synaptic learning rules like those employed by the brain\nhave so far failed to match the performance of backpropagation in deep networks.\nin this study, we employ meta-learning to discover networks that learn using\nfeedback connections and local, biologically inspired learning rules. importantly,\nthe feedback connections are not tied to the feedforward weights, avoiding bio-\nlogically implausible weight transport. our experiments show that meta-trained\nnetworks effectively use feedback connections to perform online credit assignment\nin multi-l", "st01ch10-blei\n\nari\n\n4 december 2013\n\n17:0\n\nbuild, compute, critique,\nrepeat: data analysis with\nlatent variable models\ndavid m. blei\ncomputer science department, princeton university, princeton, new jersey 08540;\nemail: blei@cs.princeton.edu\n\nannu. rev. stat. appl. 2014. 1:203\u201332\n\nthe annual review of statistics and its application is\nonline at statistics.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev-statistics-022513-115657\ncopyright c(cid:2) 2014 by annual reviews.\nall rights reserved\n\nkeywords\nlatent variable models, graphical models, variational inference, predictive\nsample reuse, posterior predictive checks\n\nabstract\nwe survey latent variable models for solving data-analysis problems. a latent\nvariable model is a probabilistic model that encodes hidden patterns in the\ndata. we uncover these patterns from their conditional distribution and use\nthem to summarize data and form predictions. latent variable models are\nimportant in many \ufb01elds, including computational biology, n", "distributed representations of words and phrases\n\nand their compositionality\n\ntomas mikolov\n\ngoogle inc.\n\nmountain view\n\nilya sutskever\n\ngoogle inc.\n\nmountain view\n\nkai chen\ngoogle inc.\n\nmountain view\n\nmikolov@google.com\n\nilyasu@google.com\n\nkai@google.com\n\ngreg corrado\n\ngoogle inc.\n\nmountain view\n\njeffrey dean\ngoogle inc.\n\nmountain view\n\ngcorrado@google.com\n\njeff@google.com\n\nabstract\n\nthe recently introduced continuous skip-gram model is an ef\ufb01cient method for\nlearning high-quality distributed vector representations that capture a large num-\nber of precise syntactic and semantic word relationships. in this paper we present\nseveral extensions that improve both the quality of the vectors and the training\nspeed. by subsampling of the frequent words we obtain signi\ufb01cant speedup and\nalso learn more regular word representations. we also describe a simple alterna-\ntive to the hierarchical softmax called negative sampling.\nan inherent limitation of word representations is their indifference to", "this is the accepted manuscript made available via chorus. the article has been\n\npublished as:\n\nmachine learning conservation laws from trajectories\n\nziming liu and max tegmark\n\nphys. rev. lett. 126, 180604 \u2014 published  6 may 2021\n\ndoi: 10.1103/physrevlett.126.180604\n\n\f", "published as a conference paper at iclr 2018\n\nlearning an embedding space\nfor transferable robot skills\n\nkarol hausman\u2217\ndepartment of computer science, university of southern california\nhausman@usc.edu\n\njost tobias springenberg, ziyu wang, nicolas heess, martin riedmiller\ndeepmind\n{springenberg,ziyu,heess,riedmiller}@google.com\n\nabstract\n\nwe present a method for reinforcement learning of closely related skills that are\nparameterized via a skill embedding space. we learn such skills by taking advan-\ntage of latent variables and exploiting a connection between reinforcement learn-\ning and variational inference. the main contribution of our work is an entropy-\nregularized policy gradient formulation for hierarchical policies, and an associ-\nated, data-ef\ufb01cient and robust off-policy gradient algorithm based on stochastic\nvalue gradients. we demonstrate the effectiveness of our method on several sim-\nulated robotic manipulation tasks. we \ufb01nd that our method allows for discovery\nof multiple ", "biologically plausible error-driven learning using local activation differences:\n\nthe generalized recirculation algorithm\n\nrandall c. o\u2019reilly\n\ndepartment of psychology\ncarnegie mellon university\n\npittsburgh, pa 15213\n\noreilly+@cmu.edu\n\njuly 1, 1996\n\nneural computation, 8:5, 895-938, 1996\n\n\f", "march  1994 \n\nlids-p-2237\n\nincremental  least  squares  methodsi\n\nand  the  extended  kalman  filter\n\nby\n\ndimitri p.  bertsekas2\n\nabstract\n\nin  this  paper  we  propose  and  analyze  nonlinear  least  squares  methods,  which  process  the  data  incre-\nmentally,  one  data  block  at  a  time.  such  methods  are  well  suited  for  large  data sets  and  real time  operation,\nand  have  received  much  attention  in  the  context  of  neural  network  training  problems.  we  focus  on  the\nextended  kalman  filter,  which  may be viewed  as  an incremental  version  of the  gauss-newton  method.  we\nprovide  a  nonstochastic  analysis  of its  convergence  properties,  and  we  discuss  variants  aimed  at  accelerating\nits  convergence.\n\n1  research  supported  by  nsf  under  grant  9300494-dmi.\n2  department  of electrical  engineering  and  computer  science,  m.i.t.,  cambridge,  mass.,  02139.\n\n\f", "9\n1\n0\n2\n\n \nc\ne\nd\n8\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n4\nv\n0\n2\n7\n6\n0\n\n.\n\n2\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nwide neural networks of any depth evolve as\n\nlinear models under gradient descent\n\njaehoon lee\u2217, lechao xiao\u2217, samuel s. schoenholz, yasaman bahri\n\nroman novak, jascha sohl-dickstein, jeffrey pennington\n\ngoogle brain\n\n{jaehlee, xlc, schsam, yasamanb, romann, jaschasd, jpennin}@google.com\n\nabstract\n\na longstanding goal in deep learning research has been to precisely characterize\ntraining and generalization. however, the often complex loss landscapes of neural\nnetworks have made a theory of learning dynamics elusive. in this work, we show\nthat for wide neural networks the learning dynamics simplify considerably and\nthat, in the in\ufb01nite width limit, they are governed by a linear model obtained from\nthe \ufb01rst-order taylor expansion of the network around its initial parameters. fur-\nthermore, mirroring the correspondence between wide bayesian neural networks\nand gaussian processes, gradient-based traini", "exact learning dynamics of deep linear networks with\n\nprior knowledge\n\nlukas braun \u2020,1\n\nlukas.braun@psy.ox.ac.uk\n\ncl\u00e9mentine c. j. domin\u00e9 \u2020,2\n\nclementine.domine.20@ucl.ac.uk\n\njames e. fitzgerald 3\n\nfitzgeraldj@janelia.hhmi.org\n\nandrew m. saxe 2,4,5\na.saxe@ucl.ac.uk\n\nabstract\n\nlearning in deep neural networks is known to depend critically on the knowledge\nembedded in the initial network weights. however, few theoretical results have\nprecisely linked prior knowledge to learning dynamics. here we derive exact\nsolutions to the dynamics of learning with rich prior knowledge in deep linear\nnetworks by generalising fukumizu\u2019s matrix riccati solution [1]. we obtain\nexplicit expressions for the evolving network function, hidden representational\nsimilarity, and neural tangent kernel over training for a broad class of initialisations\nand tasks. the expressions reveal a class of task-independent initialisations that\nradically alter learning dynamics from slow non-linear dynamics to fast exponentia", "o p i n i o n\n\ndopamine reward prediction-\nerror signalling: a two-component \nresponse\n\nwolfram schultz\n\nabstract | environmental stimuli and objects, including rewards, are often \nprocessed sequentially in the brain. recent work suggests that the phasic \ndopamine reward prediction-error response follows a similar sequential pattern. \nan initial brief, unselective and highly sensitive increase in activity unspecifically \ndetects a wide range of environmental stimuli, then quickly evolves into the main \nresponse component, which reflects subjective reward value and utility. this \ntemporal evolution allows the dopamine reward prediction-error signal to \noptimally combine speed and accuracy.\n\nrewards induce behaviours that enable \nanimals to obtain necessary objects for \nsurvival. although the term \u2018reward\u2019 is \ncommonly associated with happiness, in \nscientific terms rewards have three functions. \nfirst, they can act as positive reinforcers \nto induce learning. second, rewards elicit \nmov", "amplification of trial-to-trial\nresponse variability by neurons in visual cortex\n\nopen access, freely available online plos biology\n\nmatteo carandini*\n\nsmith-kettlewell eye research institute, san francisco, california, united states of america\n\nthe visual cortex responds to repeated presentations of the same stimulus with high variability. because the firing\nmechanism is remarkably noiseless, the source of this variability is thought to lie in the membrane potential\nfluctuations that result from summated synaptic input. here this hypothesis is tested through measurements of\nmembrane potential during visual stimulation. surprisingly, trial-to-trial variability of membrane potential is found to\nbe low. the ratio of variance to mean is much lower for membrane potential than for firing rate. the high variability of\nfiring rate is explained by the threshold present in the function that converts inputs into firing rates. given an input\nwith small, constant noise, this function produces a fi", "letters  to  nature \n\nrn \n+1 \n\nthe \n\nfig.  3  percentage  virus \ninduced  mortality  in  wild \ntype  (shaded  bars)  and \nrecombinant  (black  bars) \ntreatments  at  three  virus \ndoses,  sampled  on  four \noccasions:  a,  2  days;  b,  7 \ndays; c, 11 days, and d,  16 \ndays  after  release.  error \nbars  are  the  least signifi-\ncant  difference  between \ntreatments.  each \ntime \npoint was analysed separ \nately.  for \nfirst  two \ntimepoints,  mortality  var \nied  between  doses  (day \n2;  f = 155.2;  dj. = 2,20; \np<o.ool;  anova  with \nbinomial  errors  and  scale \nparameter  2.98;  day  7; \nf =  154.8; \nd.f. =  2,20; \np<o.ool,  anova  with \nbinomial  errors  and  scale \nparameter 2.39), but mor \ntality  did  not differ signifi \ncantly \nvirus \ntreatments  (day  2;  f= \n3.04; \nn.s.; \nanova  as  above;  day  7; \nf = 3.97; \nn.s.; \nanova  as  above).  how-\never,  at  days  11  and  16, \nboth  vi rus  type  a nd  dose \nwere significantly different, \nwith the recombinant virus \ncausing signi", " \n\nreview\n\nfeature\nwhat  learning  systems  do\nintelligent  agents  need?\ncomplementary  learning\nsystems  theory  updated\ndharshan  kumaran,1,2,*  demis  hassabis,1,3,*  and\njames  l.  mcclelland4,*\n\nwe  update  complementary  learning  systems  (cls)  theory,  which  holds  that\nintelligent  agents  must  possess  two  learning  systems,  instantiated  in  mamma-\nlians  in  neocortex  and  hippocampus.  the \n\ufb01rst  gradually  acquires  structured\nknowledge  representations  while  the  second  quickly  learns  the  speci\ufb01cs  of\nindividual  experiences.  we  broaden  the  role  of  replay  of  hippocampal  memories\nin  the  theory,  noting  that  replay  allows  goal-dependent  weighting  of  experience\nstatistics.  we  also  address  recent  challenges  to  the  theory  and  extend  it  by\nshowing  that  recurrent  activation  of  hippocampal  traces  can  support  some\nforms  of  generalization  and  that  neocortical  learning  can  be  rapid  for  informa-\ntion  that  is  consisten", "predictive reward signal of dopamine neurons\n\nwolfram schultz\ninstitute of physiology and program in neuroscience, university of fribourg, ch-1700 fribourg, switzerland\n\ninvited review\n\nis called rewards, which elicit and reinforce approach behav-\nior. the functions of rewards were developed further during\nthe evolution of higher mammals to support more sophisti-\ncated forms of individual and social behavior. thus biologi-\ncal and cognitive needs de\ufb01ne the nature of rewards, and\nthe availability of rewards determines some of the basic\nparameters of the subject\u2019s life conditions.\n\nschultz, wolfram. predictive reward signal of dopamine neurons.\nj. neurophysiol. 80: 1\u201327, 1998. the effects of lesions, receptor\nblocking, electrical self-stimulation, and drugs of abuse suggest\nthat midbrain dopamine systems are involved in processing reward\ninformation and learning approach behavior. most dopamine neu-\nrons show phasic activations after primary liquid and food rewards\nand conditioned, rewar", "the intentional unintentional agent:\n\nlearning to solve many continuous control tasks\n\nsimultaneously\n\nserkan cabi\n\nsergio g\u00f3mez colmenarejo matthew w. hoffman\n\nmisha denil ziyu wang nando de freitas\n\ndeepmind\n\n{cabi,sergomez,mwhoffman,mdenil,ziyu,nandodefreitas}@google.com\n\n7\n1\n0\n2\n\n \nl\nu\nj\n \n\n1\n1\n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\nv\n0\n0\n3\n3\n0\n\n.\n\n7\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract: this paper introduces the intentional unintentional (iu) agent. this\nagent endows the deep deterministic policy gradients (ddpg) agent for contin-\nuous control with the ability to solve several tasks simultaneously. learning to\nsolve many tasks simultaneously has been a long-standing, core goal of arti\ufb01cial\nintelligence, inspired by infant development and motivated by the desire to build\n\ufb02exible robot manipulators capable of many diverse behaviours. we show that the\niu agent not only learns to solve many tasks simultaneously but it also learns faster\nthan agents that target a single task at-a-time. in some cases, where", "letter\n\ncommunicated by gal chechik\n\nreinforcement learning through modulation of\nspike-timing-dependent synaptic plasticity\n\nr\u02d8azvan v. florian\n\ufb02orian@coneural.org\ncenter for cognitive and neural studies (coneural), 400504 cluj-napoca, romania,\nand babes\u00b8-bolyai university, institute for interdisciplinary experimental research,\n400271 cluj-napoca, romania\n\nthe persistent modi\ufb01cation of synaptic ef\ufb01cacy as a function of the rela-\ntive timing of pre- and postsynaptic spikes is a phenomenon known as\nspike-timing-dependent plasticity (stdp). here we show that the modu-\nlation of stdp by a global reward signal leads to reinforcement learning.\nwe \ufb01rst derive analytically learning rules involving reward-modulated\nspike-timing-dependent synaptic and intrinsic plasticity, by applying a\nreinforcement learning algorithm to the stochastic spike response model\nof spiking neurons. these rules have several features common to plastic-\nity mechanisms experimentally found in the brain. we then demonstr", "neural ordinary differential equations\n\nricky t. q. chen*, yulia rubanova*, jesse bettencourt*, david duvenaud\n\n{rtqichen, rubanova, jessebett, duvenaud}@cs.toronto.edu\n\nuniversity of toronto, vector institute\n\n9\n1\n0\n2\n\n \nc\ne\nd\n4\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n6\n6\n3\n7\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe introduce a new family of deep neural network models. instead of specifying a\ndiscrete sequence of hidden layers, we parameterize the derivative of the hidden\nstate using a neural network. the output of the network is computed using a black-\nbox differential equation solver. these continuous-depth models have constant\nmemory cost, adapt their evaluation strategy to each input, and can explicitly trade\nnumerical precision for speed. we demonstrate these properties in continuous-depth\nresidual networks and continuous-time latent variable models. we also construct\ncontinuous normalizing \ufb02ows, a generative model that can train by maximum\nlikelihood, without partitioning or ordering t", "a r t i c l e s\n\nhistory-dependent variability in population dynamics \nduring evidence accumulation in cortex\nari s morcos & christopher d harvey\nwe studied how the posterior parietal cortex combines new information with ongoing activity dynamics as mice accumulate \nevidence during a virtual navigation task. using new methods to analyze population activity on single trials, we found that activity \ntransitioned rapidly between different sets of active neurons. each event in a trial, whether an evidence cue or a behavioral \nchoice, caused seconds-long modifications to the probabilities that govern how one activity pattern transitions to the next, \nforming a short-term memory. a sequence of evidence cues triggered a chain of these modifications resulting in a signal for \naccumulated evidence. multiple distinguishable activity patterns were possible for the same accumulated evidence because \nrepresentations of ongoing events were influenced by previous within- and across-trial events. ther", "the forward-forward algorithm: some preliminary\n\ninvestigations\n\ngeoffrey hinton\n\ngoogle brain\n\ngeoffhinton@google.com\n\nabstract\n\nthe aim of this paper is to introduce a new learning procedure for neural networks\nand to demonstrate that it works well enough on a few small problems to be worth\nserious investigation. the forward-forward algorithm replaces the forward and\nbackward passes of backpropagation by two forward passes, one with positive\n(i.e. real) data and the other with negative data which could be generated by the\nnetwork itself. each layer has its own objective function which is simply to have\nhigh goodness for positive data and low goodness for negative data. the sum of the\nsquared activities in a layer can be used as the goodness but there are many other\npossibilities, including minus the sum of the squared activities. if the positive and\nnegative passes can be separated in time, the negative passes can be done of\ufb02ine,\nwhich makes the learning much simpler in the positive ", "published as a conference paper at iclr 2018\n\nspherical cnns\n\ntaco s. cohen\u2217\nuniversity of amsterdam\n\nmario geiger\u2217\nepfl\n\njonas k\u00f6hler\u2217\nuniversity of amsterdam\n\nmax welling\nuniversity of amsterdam & cifar\n\nabstract\n\nconvolutional neural networks (cnns) have become the method of choice for\nlearning problems involving 2d planar images. however, a number of problems of\nrecent interest have created a demand for models that can analyze spherical images.\nexamples include omnidirectional vision for drones, robots, and autonomous cars,\nmolecular regression problems, and global weather and climate modelling. a\nnaive application of convolutional networks to a planar projection of the spherical\nsignal is destined to fail, because the space-varying distortions introduced by such\na projection will make translational weight sharing ineffective.\nin this paper we introduce the building blocks for constructing spherical cnns.\nwe propose a de\ufb01nition for the spherical cross-correlation that is both expre", "ieee transactions on neural networks and learning systems, vol. 32, no. 10, october 2021\n\n4323\n\ndistributed min\u2013max learning scheme for neural\nnetworks with applications to high-dimensional\n\nclassi\ufb01cation\n\nkrishnan raghavan , member, ieee, shweta garg , sarangapani jagannathan , fellow, ieee,\n\nand v. a. samaranayake\n\nabstract\u2014 in this article, a novel\n\nlearning methodology is\nintroduced for the problem of classi\ufb01cation in the context of\nhigh-dimensional data. in particular, the challenges introduced\nby high-dimensional data sets are addressed by formulating\na l1 regularized zero-sum game where optimal sparsity is\nestimated through a two-player game between the penalty coef\ufb01-\ncients/sparsity parameters and the deep neural network weights.\nin order to solve this game, a distributed learning methodology\nis proposed where additional variables are utilized to derive\nlayerwise cost functions. finally, an alternating minimization\napproach developed to solve the problem where the nash solution", "a theoretical analysis of contrastive unsupervised representation learning\n\nsanjeev arora 1 2 hrishikesh khandeparkar 1 mikhail khodak 3 orestis plevrakis 1 nikunj saunshi 1\n\n{arora, hrk, orestisp, nsaunshi}@cs.princeton.edu\n\nkhodak@cmu.edu\n\n9\n1\n0\n2\n\n \n\nb\ne\nf\n5\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n9\n2\n2\n9\n0\n\n.\n\n2\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nrecent empirical works have successfully used\nunlabeled data to learn feature representations\nthat are broadly useful in downstream classi\ufb01ca-\ntion tasks. several of these methods are remi-\nniscent of the well-known word2vec embedding\nalgorithm: leveraging availability of pairs of se-\nmantically \u201csimilar\u201d data points and \u201cnegative\nsamples,\u201d the learner forces the inner product of\nrepresentations of similar pairs with each other to\nbe higher on average than with negative samples.\nthe current paper uses the term contrastive learn-\ning for such algorithms and presents a theoretical\nframework for analyzing them by introducing la-\ntent classes and hypothes", "under review as a conference paper at iclr 2016\n\nunsupervised representation learning\nwith deep convolutional\ngenerative adversarial networks\n\nalec radford & luke metz\nindico research\nboston, ma\n{alec,luke}@indico.io\n\nsoumith chintala\nfacebook ai research\nnew york, ny\nsoumith@fb.com\n\nabstract\n\nin recent years, supervised learning with convolutional networks (cnns) has\nseen huge adoption in computer vision applications. comparatively, unsupervised\nlearning with cnns has received less attention.\nin this work we hope to help\nbridge the gap between the success of cnns for supervised learning and unsuper-\nvised learning. we introduce a class of cnns called deep convolutional generative\nadversarial networks (dcgans), that have certain architectural constraints, and\ndemonstrate that they are a strong candidate for unsupervised learning. training\non various image datasets, we show convincing evidence that our deep convolu-\ntional adversarial pair learns a hierarchy of representations from obje", "manifold-tiling localized receptive fields are\n\noptimal in similarity-preserving neural networks\n\nanirvan m. sengupta\u2020\u2021\n\nmariano tepper\u2021\u21e4\n\ncengiz pehlevan\u2021\u21e4\n\nalexander genkin\u00a7\n\ndmitri b. chklovskii\u2021\u00a7\n\n\u2020rutgers university\n\n\u2021flatiron institute\n\n\u00a7nyu langone medical center\n\nanirvans@physics.rutgers.edu, alexander.genkin@gmail.com\n{mtepper,cpehlevan,dchklovskii}@flatironinstitute.org\n\nabstract\n\nmany neurons in the brain, such as place cells in the rodent hippocampus, have lo-\ncalized receptive \ufb01elds, i.e., they respond to a small neighborhood of stimulus space.\nwhat is the functional signi\ufb01cance of such representations and how can they arise?\nhere, we propose that localized receptive \ufb01elds emerge in similarity-preserving\nnetworks of rectifying neurons that learn low-dimensional manifolds populated by\nsensory inputs. numerical simulations of such networks on standard datasets yield\nmanifold-tiling localized receptive \ufb01elds. more generally, we show analytically\nthat, for data lying on symmet", "flywire: online community for whole-brain \nconnectomics\n\n\u200a1,2,6, nico kemnitz1,6, kisuk lee1,3,6, \n\n\u200a1,4, shang mu1, dodam ih1, manuel castro\u200a\n\nsven dorkenwald1,2,6, claire e. mckellar1,6, thomas macrina\u200a\nran lu1,6, jingpeng wu1,6, sergiy popovych1,2, eric mitchell\u200a\nj. alexander bae\u200a\nakhilesh halageri1, kai kuehner1, amy r. sterling1, zoe ashwood1,2, jonathan zung1,2, \nderrick brittain5, forrest collman5, casey schneider-mizell5, chris jordan1, william silversmith1, \nchrista baker1, david deutsch\u200a\naustin burke1, doug bland1, jay gager1, james hebditch1, selden koolman1, merlin moore1, \nsarah morejohn1, ben silverman1, kyle willie1, ryan willie1, szi-chieh yu1, mala murthy\u200a\nh. sebastian seung\u200a\n\n\u200a1, lucas encarnacion-rivera1, sandeep kumar\u200a\n\n\u200a1, oluwaseun ogedengbe1, \n\n\u200a1, barak nehoran\u200a\n\n\u200a1,2, zhen jia1,2, \n\n\u200a1\u2009\u2709 and \n\n\u200a1,2\u2009\u2709\n\n\u200a1, \n\ndue to advances in automated image acquisition and analysis, whole-brain connectomes with 100,000 or more neurons are \non the horizon. proofreading of whole", "article\n\ndynamic control of response criterion in premotor\ncortex during perceptual detection under temporal\nuncertainty\n\nhighlights\nd a template-matching algorithm detects neural correlates of\n\nfalse alarm events\n\nd the subject\u2019s response criterion modulates over the course\n\nof a trial\n\nd the response criterion is represented by the dynamics of a\n\nneural population\n\nd a trained recurrent network unveils a mechanism for \ufb02exible\n\nresponse criterion\n\nauthors\n\nfederico carnevale,\nvictor de lafuente, ranulfo romo,\nomri barak, ne\u00b4 stor parga\n\ncorrespondence\nrromo@ifc.unam.mx\n\nin brief\ncarnevale et al. explore how monkeys\nexploit previous knowledge to cope with\ntemporal uncertainty in a perceptual\ndetection task. the study demonstrates a\nneural mechanism by which prior\ninformation is intrinsically encoded in the\ndynamics of a neural population.\n\ncarnevale et al., 2015, neuron 86, 1067\u20131077\nmay 20, 2015 \u00aa2015 elsevier inc.\nhttp://dx.doi.org/10.1016/j.neuron.2015.04.014\n\n\f", "hypothesis and theory\npublished: 24 march 2016\ndoi: 10.3389/fnins.2016.00106\n\nbeliever-skeptic meets actor-critic:\nrethinking the role of basal ganglia\npathways during decision-making\nand reinforcement learning\n\nkyle dunovan 1, 2 and timothy verstynen 2, 3*\n\n1 department of psychology, university of pittsburgh, pittsburgh, pa, usa, 2 center for the neural basis of cognition,\nuniversity of pittsburgh and carnegie mellon university, pittsburgh, pa, usa, 3 department of psychology, carnegie mellon\n\nuniversity, pittsburgh, pa, usa\n\nthe \ufb02exibility of behavioral control is a testament to the brain\u2019s capacity for dynamically\nresolving uncertainty during goal-directed actions. this ability to select actions and learn\nfrom immediate feedback is driven by the dynamics of basal ganglia (bg) pathways.\na growing body of empirical evidence con\ufb02icts with the traditional view that these\npathways act as independent levers for facilitating (i.e., direct pathway) or suppressing\n(i.e., indirect pathway) m", "a further discussions\n\non primal (parameter space) vs. dual (feature space) dynamics: although the cross-entropy\nloss is convex, it does not admit an analytical solution, even in a simple logistic regression [103].\nimportantly, it also does not have a \ufb01nite solution when the data is linearly separable [6] (which is\nthe case in high dimensions [27]). as such, our study is concerned with characterizing the solutions\nthat the training algorithm converges to. a dual optimization approach enables us to describe these\nsolutions in terms of contributions of the training examples [43]. while primal and dual dynamics\nare not guaranteed to match, the solution they converge to is guaranteed to match [21], and that is\nwhat our theory builds upon.\nfor further intuition, we provide a simple experiment in app c, directly visualizing the primal vs. the\ndual dynamics as well as the effect of the proposed spectral decoupling method.\nthe intuition behind spectral decoupling (sd): consider a training data", "fenchel lifted networks:\n\na lagrange relaxation of neural network training\n\nfangda gu*\n\narmin askari*\n\nlaurent el ghaoui\n\ndepartment of electrical engineering and computer sciences\n\nuniversity of california at berkeley\n\n{gfd18, aaskari, elghaoui}@berkeley.edu\n\nabstract\n\ndespite the recent successes of deep neural\nnetworks, the corresponding training prob-\nlem remains highly non-convex and di\ufb03cult\nto optimize. classes of models have been\nproposed that introduce greater structure to\nthe objective function at the cost of lift-\ning the dimension of the problem. how-\never, these lifted methods sometimes perform\npoorly compared to traditional neural net-\nworks.\nin this paper, we introduce a new\nclass of lifted models, fenchel lifted networks,\nthat enjoy the same bene\ufb01ts as previous\nlifted models, without su\ufb00ering a degrada-\ntion in performance over classical networks.\nour model represents activation functions as\nequivalent biconvex constraints and uses la-\ngrange multipliers to arrive at a r", "research article\n\nreward-based training of recurrent neural\nnetworks for cognitive and value-based\ntasks\nh francis song1, guangyu r yang1, xiao-jing wang1,2*\n\n1center for neural science, new york university, new york, united states; 2nyu-\necnu institute of brain and cognitive science, nyu shanghai, shanghai, china\n\nabstract trained neural network models, which exhibit features of neural activity recorded from\nbehaving animals, may provide insights into the circuit mechanisms of cognitive functions through\nsystematic analysis of network activity and connectivity. however, in contrast to the graded error\nsignals commonly used to train networks through supervised learning, animals learn from reward\nfeedback on definite actions through reinforcement learning. reward maximization is particularly\nrelevant when optimal behavior depends on an animal\u2019s internal judgment of confidence or\nsubjective preferences. here, we implement reward-based training of recurrent neural networks in\nwhich a valu", ".\n\nd\ne\nv\nr\ne\ns\ne\nr\n \n\ns\nt\nh\ng\ni\nr\n \nl\nl\n\na\n\n \n.\n\nc\nn\n\ni\n \n,\n\na\nc\n\n \n\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n6\n1\n0\n2\n\u00a9\n\n \n\n \n\ng\np\nn\n\np e r s p e c t i v e  \n\nf o c u s   o n   n e u r a l   c o m p u t a t i o n   a n d   t h e o r y\n\nusing goal-driven deep learning models to understand \nsensory cortex\n\ndaniel l k yamins1,2 & james j dicarlo1,2\n\nfueled by innovation in the computer vision and artificial \nintelligence communities, recent developments in \ncomputational neuroscience have used goal-driven hierarchical \nconvolutional neural networks (hcnns) to make strides in \nmodeling neural single-unit and population responses in higher \nvisual cortical areas. in this perspective, we review the recent \nprogress in a broader modeling context and describe some of \nthe key technical innovations that have supported it. we then \noutline how the goal-driven hcnn approach can be used to \ndelve even more deeply into understanding the development \nand organization of sensory cortical processing. \n\nwhat should one e", "deepmind control suite\n\nyuval tassa, yotam doron, alistair muldal, tom erez,\n\nyazhe li, diego de las casas, david budden, abbas abdolmaleki, josh merel,\n\nandrew lefrancq, timothy lillicrap, martin riedmiller\n\n8\n1\n0\n2\n\n \n\nn\na\nj\n \n\n2\n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\nv\n0\n9\n6\n0\n0\n\n.\n\n1\n0\n8\n1\n:\nv\ni\nx\nr\na\n\njanuary 3, 2018\n\nfigure 1: benchmarking domains. top: acrobot, ball-in-cup, cart-pole, cheetah, finger, fish, hopper.\nbottom: humanoid, manipulator, pendulum, point-mass, reacher, swimmer (6 and 15 links), walker.\n\nabstract\n\nthe deepmind control suite is a set of continuous control tasks with a stan-\ndardised structure and interpretable rewards, intended to serve as performance\nbenchmarks for reinforcement learning agents. the tasks are written in python\nand powered by the mujoco physics engine, making them easy to use and mod-\nify. we include benchmarks for several learning algorithms. the control suite\nis publicly available at github.com/deepmind/dm_control. a video summary\nof all tasks is ava", "b r i e f c o m m u n i c at i o n s\n\nreinforcement learning in\npopulations of spiking neurons\n\nrobert urbanczik & walter senn\n\npopulation coding is widely regarded as an important\nmechanism for achieving reliable behavioral responses despite\nneuronal variability. however, standard reinforcement learning\nslows down with increasing population size, as the global\nreward signal becomes less and less related to the performance\nof any single neuron. we found that learning speeds up with\nincreasing population size if, in addition to global reward,\nfeedback about the population response modulates\nsynaptic plasticity.\n\nthe role of neuronal populations in encoding sensory stimuli has been\nintensively studied1,2. however, most models of reinforcement learning\nwith spiking neurons have focused on just single neurons or small\nneuronal assemblies3\u20136. furthermore, the following result indicates that\nsuch models do not scale well with population size. using spike\ntiming\u2013dependent plasticity modulated", "characteristics of sequential activity in networks with\ntemporally asymmetric hebbian learning\n\nmaxwell gilletta,b\n\n, ulises pereirac,1, and nicolas brunela,b,c,d,2\n\nadepartment of neurobiology, the university of chicago, chicago, il 60637; bdepartment of neurobiology, duke university, durham, nc 27708;\ncdepartment of statistics, the university of chicago, chicago, il 60637; and ddepartment of physics, duke university, durham, nc 27708\n\nedited by terrence j. sejnowski, salk institute for biological studies, la jolla, ca, and approved october 16, 2020 (received for review october 24, 2019)\n\nsequential activity has been observed in multiple neuronal cir-\ncuits across species, neural structures, and behaviors.\nit has\nbeen hypothesized that sequences could arise from learning\nprocesses. however, it is still unclear whether biologically plau-\nsible synaptic plasticity rules can organize neuronal activity to\nform sequences whose statistics match experimental observa-\ntions. here, we investig", "gqa: training generalized multi-query transformer models from\n\nmulti-head checkpoints\n\njoshua ainslie\u2217, james lee-thorp\u2217, michiel de jong\u2217 \u2020\u2020\nyury zemlyanskiy, federico lebr\u00f3n, sumit sanghai\n\ngoogle research\n\n3\n2\n0\n2\n\n \nt\nc\no\n4\n2\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n2\nv\n5\n4\n2\n3\n1\n\n.\n\n5\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nmulti-query attention (mqa), which only uses\na single key-value head, drastically speeds up\ndecoder inference. however, mqa can lead to\nquality degradation, and moreover it may not\nbe desirable to train a separate model just for\nfaster inference. we (1) propose a recipe for\nuptraining existing multi-head language model\ncheckpoints into models with mqa using 5%\nof original pre-training compute, and (2) intro-\nduce grouped-query attention (gqa), a gener-\nalization of multi-query attention which uses\nan intermediate (more than one, less than num-\nber of query heads) number of key-value heads.\nwe show that uptrained gqa achieves quality\nclose to multi-head attention with comparable\nspeed t", "the tolman-eichenbaum machine: unifying space\nand relational memory through generalization in the\nhippocampal formation\n\narticle\n\ngraphical abstract\n\nauthors\njames c.r. whittington,\ntimothy h. muller, shirley mark,\nguifen chen, caswell barry, neil burgess,\ntimothy e.j. behrens\n\ncorrespondence\njcrwhittington@gmail.com\n\nin brief\nthe tolman-eichenbaum machine,\nnamed in honor of edward chace tolman\nand howard eichenbaum for their\ncontributions to cognitive theory,\nprovides a unifying framework for the\nhippocampal role in spatial and\nnonspatial generalization and unifying\nprinciples underlying many entorhinal\nand hippocampal cell types.\n\nhighlights\nd common principles for space and relational memory in the\n\nhippocampal formation\n\nd explains hippocampal generalization in both spatial and\n\nnon-spatial problems\n\nd accounts for many reported hippocampal and entorhinal cell\n\ntypes from such tasks\n\nd predicts how hippocampus remaps in both spatial and non-\n\nspatial tasks\n\nwhittington et al., 2020", "on the importance of normalisation layers in deep learning with piecewise\n\nlinear activation units\n\narc centre of excellence for robotic vision, university of adelaide, australia \u2217\n\nzhibin liao and gustavo carneiro\n\n{zhibin.liao,gustavo.carneiro}@adelaide.edu.au\n\n5\n1\n0\n2\n\n \n\nv\no\nn\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n0\n3\n3\n0\n0\n\n.\n\n8\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\ndeep feedforward neural networks with piecewise lin-\near activations are currently producing the state-of-the-art\nresults in several public datasets (e.g., cifar-10, cifar-\n100, mnist, and svhn). the combination of deep learn-\ning models and piecewise linear activation functions allows\nfor the estimation of exponentially complex functions with\nthe use of a large number of subnetworks specialized in the\nclassi\ufb01cation of similar input examples. during the train-\ning process, these subnetworks avoid over\ufb01tting with an\nimplicit regularization scheme based on the fact that they\nmust share their parameters with other subnetworks. us-\ning", "megatron-lm: training multi-billion parameter language models using\n\nmodel parallelism\n\nmohammad shoeybi 1 2 mostofa patwary 1 2 raul puri 1 2 patrick legresley 2 jared casper 2\n\nbryan catanzaro 2\n\n0\n2\n0\n2\n\n \nr\na\n\n \n\nm\n3\n1\n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n4\nv\n3\n5\n0\n8\n0\n\n.\n\n9\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nrecent work in language modeling demonstrates\nthat training large transformer models advances\nthe state of the art in natural language processing\napplications. however, very large models can be\nquite dif\ufb01cult to train due to memory constraints.\nin this work, we present our techniques for train-\ning very large transformer models and implement\na simple, ef\ufb01cient intra-layer model parallel ap-\nproach that enables training transformer models\nwith billions of parameters. our approach does\nnot require a new compiler or library changes, is\northogonal and complimentary to pipeline model\nparallelism, and can be fully implemented with\nthe insertion of a few communication operations\nin native pytorch. we i", "cortical microcircuitry of performance monitoring\n\namirsaman\u00a0sajad, david\u00a0c.\u00a0godlove and jeffrey\u00a0d.\u00a0schall\u200a\n\n\u200a*\n\nthe medial frontal cortex enables performance monitoring, indexed by the error-related negativity (ern) and manifested by \nperformance adaptations. we recorded electroencephalogram over and neural spiking across all layers of the supplementary \neye field, an agranular cortical area, in monkeys performing a saccade-countermanding (stop signal) task. neurons signaling \nerror production, feedback predicting reward gain or loss, and delivery of fluid reward had different spike widths and were \nconcentrated differently across layers. neurons signaling error or loss of reward were more common in layers 2 and 3 (l2/3), \nwhereas neurons signaling gain of reward were more common in layers 5 and 6 (l5/6). variation of error\u2013 and reinforcement-\nrelated spike rates in l2/3 but not l5/6 predicted response time adaptation. variation in error-related spike rate in l2/3 but \nnot l5/6 predic", "getting aligned on representational alignment\n\nilia sucholutsky\u2217\nprinceton university\n\nis2961@princeton.edu\n\nlukas muttenthaler\u2217\n\ngoogle deepmind; technische universit\u00e4t berlin\n\nlukas.muttenthaler@tu-berlin.de\n\n3\n2\n0\n2\n\n \nt\nc\no\n \n8\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n8\n1\n0\n3\n1\n\n.\n\n0\n1\n3\n2\n:\nv\ni\nx\nr\na\n\nadrian weller\n\nuniversity of cambridge\n\nandi peng\n\nmit\n\nandreea bobu\nuc berkeley\n\nbeen kim\n\ngoogle deepmind\n\nbradley c. love\n\nucl\n\nerin grant\n\nucl\n\njascha achterberg\n\nuniversity of cambridge\n\njoshua b. tenenbaum\n\nmit\n\nkatherine m . collins\nuniversity of cambridge\u2020\n\nkatherine l. hermann\n\ngoogle deepmind\n\nkerem oktar\n\nprinceton university\n\nklaus greff\n\ngoogle deepmind\n\nmartin n. hebart\n\nmpi for human cognitive and brain sciences\n\nnori jacoby\n\nmpi for empirical aesthetics\n\nqiuyi (richard) zhang\n\ngoogle deepmind\n\nraja marjieh\n\nprinceton university\n\nrobert geirhos\ngoogle deepmind\n\nsherol chen\n\ngoogle research\n\nsimon kornblith\ngoogle deepmind\u2021\n\nsunayana rane\n\nprinceton university\n\ntalia konkl", "matconvnet\n\nconvolutional neural networks for matlab\n\nandrea vedaldi\n\nkarel lenc\n\n6\n1\n0\n2\n\n \n\ny\na\nm\n5\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n3\nv\n4\n6\n5\n4\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\ni\n\n\f", "learning a latent manifold of odor representations\n\nfrom neural responses in piriform cortex\n\nanqi wu1\n\nstan l. pashkovski2\n\nsandeep robert datta2\n\njonathan w. pillow1\n\n1 princeton neuroscience institute, princeton university,\n\n{anqiw, pillow}@princeton.edu\n\n2 department of neurobiology, harvard medical school,\n\n{pashkovs, srdatta}@hms.harvard.edu\n\nabstract\n\na major dif\ufb01culty in studying the neural mechanisms underlying olfactory percep-\ntion is the lack of obvious structure in the relationship between odorants and the\nneural activity patterns they elicit. here we use odor-evoked responses in piriform\ncortex to identify a latent manifold specifying latent distance relationships between\nolfactory stimuli. our approach is based on the gaussian process latent variable\nmodel, and seeks to map odorants to points in a low-dimensional embedding space,\nwhere distances between points in the embedding space relate to the similarity of\npopulation responses they elicit. the model is speci\ufb01ed by an", "journal of mathematical psychology 53 (2009) 139\u2013154\n\ncontents lists available at sciencedirect\n\njournal of mathematical psychology\n\njournal homepage: www.elsevier.com/locate/jmp\n\nreinforcement learning in the brain\nyael niv\u2217\n\npsychology department & princeton neuroscience institute, princeton university, united states\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 9 june 2008\nreceived in revised form\n3 december 2008\navailable online 10 february 2009\n\na wealth of research focuses on the decision-making processes that animals and humans employ when\nselecting actions in the face of reward and punishment. initially such work stemmed from psychological\ninvestigations of conditioned behavior, and explanations of these in terms of computational models.\nincreasingly, analysis at the computational level has drawn on ideas from reinforcement learning, which\nprovide a normative framework within which decision-making can be analyzed. more recently, the fruits\nof these extensi", "[geoffrey hinton, li deng, dong yu, george e. dahl, abdel-rahman mohamed, navdeep jaitly, \nandrew senior, vincent vanhoucke, patrick nguyen, tara n. sainath, and brian kingsbury]\n\n[the shared views of four research groups]\n\ni\n\n \n\nt\na\np\nd\na\nt\nr\ne\nl\na\no\nh\nc\nu\ns\nm\no\nc\no\nt\no\nh\np\nk\nc\no\nt\ns\n\n.\n\n/\n\ni\n \n\n\u00a9\n\nfundamental technologies \nin modern speech recognition\n\nmost  current  speech  recognition  systems  use \n\nhidden  markov  models  (hmms)  to  deal  with \nthe  temporal  variability  of  speech  and \ngaussian  mixture  models  (gmms)  to  deter-\nmine how well each state of each hmm fits a \nframe or a short window of frames of coefficients that repre-\nsents the acoustic input. an alternative way to evaluate the fit \nis  to  use  a  feed-forward  neural  network  that  takes  several \nframes of coefficients as input and produces posterior proba-\n\n digital object identifier 10.1109/msp.2012.2205597\n date of publication: 15 october 2012\n\nbilities  over  hmm  states  as  output.  deep  neural  n", "the journal of neuroscience, november 15, 2000, 20(22):8443\u20138951\n\ndopamine and camp-regulated phosphoprotein 32 kda controls\nboth striatal long-term depression and long-term potentiation,\nopposing forms of synaptic plasticity\n\npaolo calabresi,1,2 paolo gubellini,1 diego centonze,1,2 barbara picconi,1 giorgio bernardi,1,2\nkarima chergui,3 per svenningsson,3 allen a. fienberg,3 and paul greengard3\n1clinica neurologica, dipartimento di neuroscienze, universita` di tor vergata, rome, italy, 2istituto di ricovero e cura a\ncarattere scienti\ufb01co ospedale santa lucia, rome, italy, and 3laboratory of molecular and cellular neuroscience, the\nrockefeller university, new york, new york 10021\n\na complex chain of intracellular signaling events, critically impor-\ntant in motor control, is activated by the stimulation of d1-like\ndopamine (da) receptors in striatal neurons. at corticostriatal\nsynapses on medium spiny neurons, we provide evidence that\nthe d1-like receptor-dependent activation of da and c", "journal of mathematical psychology 56 (2012) 1\u201312\n\ncontents lists available at sciverse sciencedirect\n\njournal of mathematical psychology\n\njournal homepage: www.elsevier.com/locate/jmp\n\ntutorial\na tutorial on bayesian nonparametric models\nsamuel j. gershman a,\u2217, david m. blei b\n\na department of psychology and princeton neuroscience institute, princeton university, princeton nj 08540, usa\nb department of computer science, princeton university, princeton nj 08540, usa\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 25 january 2011\nreceived in revised form\n4 august 2011\navailable online 1 september 2011\n\nkeywords:\nbayesian methods\nchinese restaurant process\nindian buffet process\ncontents\n\na key problem in statistical modeling is model selection, that is, how to choose a model at an appropriate\nlevel of complexity. this problem appears in many settings, most prominently in choosing the number of\nclusters in mixture models or the number of factors in factor analysis. in ", "a r t i c l e s\n\ninformation-limiting correlations\nrub\u00e9n moreno-bote1,2, jeffrey beck3, ingmar kanitscheider4, xaq pitkow3,5,6, peter latham7 &  \nalexandre pouget3,4,7\ncomputational strategies used by the brain strongly depend on the amount of information that can be stored in population \nactivity, which in turn strongly depends on the pattern of noise correlations. in vivo, noise correlations tend to be positive and \nproportional to the similarity in tuning properties. such correlations are thought to limit information, which has led to the \nsuggestion that decorrelation increases information. in contrast, we found, analytically and numerically, that decorrelation does \nnot imply an increase in information. instead, the only information-limiting correlations are what we refer to as differential \ncorrelations: correlations proportional to the product of the derivatives of the tuning curves. unfortunately, differential correlations \nare likely to be very small and buried under correlati", "5\n1\n0\n2\n\n \nl\nu\nj\n \n7\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n0\n8\n5\n7\n0\n\n.\n\n7\n0\n5\n1\n:\nv\ni\nx\nr\na\n\ncomputational principles of biological memory\n\nmarcus k. benna1, stefano fusi1\u2217\n\n1center for theoretical neuroscience, columbia university,\n\ncollege of physicians and surgeons, new york, ny 10032, usa\n\n\u2217to whom correspondence should be addressed; e-mail: sf2237@columbia.edu.\n\njuly 29, 2015\n\nmemories are stored, retained, and recollected through complex, coupled processes\noperating on multiple timescales. to understand the computational principles behind\nthese intricate networks of interactions we construct a broad class of synaptic mod-\nels that ef\ufb01ciently harnesses biological complexity to preserve numerous memories.\nthe memory capacity scales almost linearly with the number of synapses, which is a\nsubstantial improvement over the square root scaling of previous models. this was\nachieved by combining multiple dynamical processes that initially store memories in\nfast variables and then progres", "0\n2\n0\n2\n\n \n\ny\na\nm\n0\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n4\nv\n6\n6\n7\n0\n1\n\n.\n\n1\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nsynaptic plasticity dynamics for deep continuous local learning\n\n(decolle)\n\nfzi research center for information technology\n\njacques kaiser,\n\nkarlsruhe, germany\n\nhesham mostafa,\n\ndepartment of bioengineering\n\nuniversity of california san diego\n\nla jolla, usa\n\nemre neftci,\n\ndepartment of cognitive sciences\ndepartment of computer science\n\nuniversity of california irvine, irvine, usa\n\nmay 22, 2020\n\nabstract\n\na growing body of work underlines striking similarities between biological neural networks and recurrent, binary\nneural networks. a relatively smaller body of work, however, addresses the similarities between learning dynamics\nemployed in deep arti\ufb01cial neural networks and synaptic plasticity in spiking neural networks. the challenge preventing\nthis is largely caused by the discrepancy between the dynamical properties of synaptic plasticity and the requirements\nfor gradient backpropagation. learning", "research article\n\na general decoding strategy explains \nthe relationship between behavior and \ncorrelated\u00a0variability\namy m ni1,2*, chengcheng huang1,2,3, brent doiron2,3, marlene r cohen1,2\n\n1department of neuroscience,university of pittsburgh, pittsburgh, united states; \n2center for the neural basis of cognition, pittsburgh, united states; 3department of \nmathematics, university of pittsburgh, pittsburgh, united states\n\nabstract improvements in perception are frequently accompanied by decreases in correlated \nvariability in sensory cortex. this relationship is puzzling because overall changes in correlated \nvariability should minimally affect optimal information coding. we hypothesize that this relation-\nship arises because instead of using optimal strategies for decoding the specific stimuli at hand, \nobservers prioritize generality: a single set of neuronal weights to decode any stimuli. we tested this \nusing a combination of multineuron recordings in the visual cortex of behaving ", "the code for facial identity in the primate brain\n\narticle\n\ngraphical abstract\n\nauthors\nle chang, doris y. tsao\n\ncorrespondence\nlechang@caltech.edu (l.c.),\ndortsao@caltech.edu (d.y.t.)\n\nin brief\nfacial identity is encoded via a\nremarkably simple neural code that relies\non the ability of neurons to distinguish\nfacial features along speci\ufb01c axes in face\nspace, disavowing the long-standing\nassumption that single face cells encode\nindividual faces.\n\nhighlights\nd facial images can be linearly reconstructed using responses\n\nof \u0018200 face cells\n\nd face cells display \ufb02at tuning along dimensions orthogonal to\n\nthe axis being coded\n\nd the axis model is more ef\ufb01cient, robust, and \ufb02exible than the\n\nexemplar model\n\nd face patches ml/mf and am carry complementary\n\ninformation about faces\n\nchang & tsao, 2017, cell 169, 1013\u20131028\njune 1, 2017 \u00aa 2017 elsevier inc.\nhttp://dx.doi.org/10.1016/j.cell.2017.05.011\n\n\f", "article\n\nuncovering spatial representations from\nspatiotemporal patterns of rodent hippocampal \ufb01eld\npotentials\n\ngraphical abstract\n\nauthors\n\na lfp feature extraction methods\n\nb spatiotemporal patterns estimation\n\nraw lfp \ntraces\n\n4-12 hz\nbandpass \n\n300 hz\nhighpass \n\ndemodulation\n\noptical flow\n\nhilbert tranform\n\noptical flow\n\nra w tra ce s\n\nc position dependent\nspatiotemporal patterns\n\nattern s\n\nw  p\n\nptic al flo\n\no\n\nliang cao, viktor varga, zhe s. chen\n\ncorrespondence\nzhe.chen@nyulangone.org\n\nin brief\ncao et al. develop supervised and\nunsupervised methods to extract\namplitude and phase information from\nhigh-density rodent hippocampal\nelectrophysiological recordings and\ndemonstrate their use in position\ndecoding, replay analysis, and decision\nprediction.\n\nhighlights\nd rodent hippocampal \ufb01eld potentials provide spatial readouts\n\nin run and ripples\n\nd unsupervised learning reveals latent structures from\n\nspatiotemporal patterns\n\nd optical \ufb02ow method identi\ufb01es spatiotemporal patterns from\n", "journal of machine learning research 8 (2007) 2169-2231\n\nsubmitted 6/06; revised 3/07; published 10/07\n\nproto-value functions: a laplacian framework for learning\n\nrepresentation and control in markov decision processes\n\nsridhar mahadevan\ndepartment of computer science\nuniversity of massachusetts\namherst, ma 01003, usa\n\nmauro maggioni\ndepartment of mathematics and computer science\nduke university\ndurham, nc 27708\n\neditor: carlos guestrin\n\nmahadeva@cs.umass.edu\n\nmauro.maggioni@duke.edu\n\nabstract\n\nthis paper introduces a novel spectral framework for solving markov decision processes (mdps)\nby jointly learning representations and optimal policies. the major components of the framework\ndescribed in this paper include: (i) a general scheme for constructing representations or basis func-\ntions by diagonalizing symmetric diffusion operators (ii) a speci\ufb01c instantiation of this approach\nwhere global basis functions called proto-value functions (pvfs) are formed using the eigenvectors\nof the gra", "volume 65, number 13\n\nphysical review letters\n\n24 september 1990\n\nlearning from examples in large neural networks\n\nh. sompolinsky\n\n' and n. tishby\n\nat& t bell laboratories, murray hill,\n\n/ver jersey 07974\n\ndepartment of physics, harvard university, cambridge, massachusetts 02t38\n\n(received 29 may 1990)\n\nh. s. seung\n\na statistical mechanical\n\ntheory of learning from examples\n\nin layered networks\n\nat finite temperature\n\nis\n\nstudied. when the training error is a smooth function of continuously\nerror falls off asymptotically\nsingle-layer\na discontinuous\nof perfect generalization\n\nto perfect generalization.\ncoexists with a metastable\n\nfor intermediate\n\ntransition\n\nspin-glass\n\nstate.\n\nas the inverse number of examples. by analytical\n\nperceptrons we show that when the weights are discrete the generalization\n\nand numerical\n\nstudies of\nerror can exhibit\nsizes of the example set, the state\n\nvarying weights\n\nthe generalization\n\npacs numbers:\n\n87. 10.+e, 02.50.+s, 05.20.-y\n\nimportance.\n\nhow system", "workshop track - iclr 2017\n\nexplaining the learning dynamics of direct\nfeedback alignment\n\njustin gilmer\u2217, colin raffel\u2217, samuel s. schoenholz\u2217, maithra raghu & jascha sohl-dickstein\ngoogle brain\n{gilmer,craffel,schsam,maithra,jaschasd}@google.com\n\nabstract\n\ntwo recently developed methods, feedback alignment (fa) and direct feedback\nalignment (dfa), have been shown to obtain surprising performance on vision\ntasks by replacing the traditional backpropagation update with a random feedback\nupdate. however, it is still not clear what mechanisms allow learning to happen\nwith these random updates. in this work we argue that dfa can be viewed as a\nnoisy variant of a layer-wise training method we call linear aligned feedback\nsystems (lafs). we support this connection theoretically by comparing the up-\ndate rules for the two methods. we additionally empirically verify that the random\nupdate matrices used in dfa work effectively as readout matrices, and that strong\ncorrelations exist between the", "r e v i e w s\n\noptimal feedback control and\nthe neural basis of volitional\nmotor control\n\nstephen h. scott\n\nskilled motor behaviour, from the graceful leap of a ballerina to a precise pitch by a baseball\nplayer, appears effortless but reflects an intimate interaction between the complex mechanical\nproperties of the body and control by a highly distributed circuit in the cns. an important\nchallenge for understanding motor function is to connect these three levels of the motor system\n\u2014 motor behaviour, limb mechanics and neural control. optimal feedback control theory might\nprovide the important link across these levels of the motor system and help to unravel how the\nprimary motor cortex and other regions of the brain plan and control movement.\n\nperhaps the most surprising feature of the motor system\nis the ease with which humans and other animals can\nmove. it is only when we observe the clumsy movements\nof a child, or the motor challenges faced by individuals\nwith neurological disorders", "the journal of neuroscience, july 8, 2015 \u2022 35(27):10005\u201310014 \u2022 10005\n\nbehavioral/cognitive\n\ndeep neural networks reveal a gradient in the complexity\nof neural representations across the ventral stream\n\numut gu\u00a8c\u00b8lu\u00a8 and marcel a. j. van gerven\nradboud university, donders institute for brain, cognition and behaviour, nijmegen, the netherlands\n\nconverging evidence suggests that the primate ventral visual pathway encodes increasingly complex stimulus features in downstream\nareas. we quantitatively show that there indeed exists an explicit gradient for feature complexity in the ventral pathway of the human\nbrain. this was achieved by mapping thousands of stimulus features of increasing complexity across the cortical sheet using a deep neural\nnetwork. our approach also revealed a fine-grained functional specialization of downstream areas of the ventral stream. furthermore, it\nallowed decoding of representations from human brain activity at an unsurpassed degree of accuracy, confirming the", "0\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n9\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n5\n9\n9\n3\n\n.\n\n2\n1\n9\n0\n:\nv\ni\nx\nr\na\n\ngaussian process optimization in the bandit setting:\n\nno regret and experimental design\n\nniranjan srinivas\n\nandreas krause\n\ncalifornia institute of technology\n\ncalifornia institute of technology\n\nniranjan@caltech.edu\n\nkrausea@caltech.edu\n\nsham m. kakade\n\nuniversity of pennsylvania\n\nmatthias seeger\n\nsaarland university\n\nskakade@wharton.upenn.edu\n\nmseeger@mmci.uni-saarland.de\n\nabstract\n\nmany applications require optimizing an un-\nknown, noisy function that is expensive to\nevaluate. we formalize this task as a multi-\narmed bandit problem, where the payo\ufb00 function\nis either sampled from a gaussian process (gp)\nor has low rkhs norm. we resolve the impor-\ntant open problem of deriving regret bounds for\nthis setting, which imply novel convergence rates\nfor gp optimization. we analyze gp-ucb, an\nintuitive upper-con\ufb01dence based algorithm, and\nbound its cumulative regret in terms of maximal\ninformation gain, est", "context,learning,andextinctionsamuelj.gershman,davidm.blei,andyaelnivprincetonuniversitya.redishetal.(2007)proposedareinforcementlearningmodelofcontext-dependentlearningandextinctioninconditioningexperiments,usingtheideaof\u201cstateclassification\u201dtocategorizenewobservationsintostates.inthecurrentarticle,theauthorsproposeaninterpretationofthisideaintermsofnormativestatisticalinference.theyfocusonrenewalandlatentinhibition,2conditioningparadigmsinwhichcontextualmanipulationshavebeenstudiedextensively,andshowthatonlinebayesianinferencewithinamodelthatassumesanunboundednumberoflatentcausescancharacterizeadiversesetofbehavioralresultsfromsuchmanipulations,someofwhichposeproblemsforthemodelofredishetal.moreover,inbothparadigms,contextdependenceisabsentinyoungeranimals,orifhippocampallesionsaremadepriortotraining.theauthorssuggestanexplanationintermsofarestrictedcapacitytoinfernewcauses.keywords:classicalconditioning,renewal,latentinhibition,bayesian,hippocampusanenduringprobleminthestudyofclassi", "towards biologically plausible deep learning\n\nyoshua bengio1, dong-hyun lee, jorg bornschein, thomas mesnard and zhouhan lin\nmontreal institute for learning algorithms, university of montreal, montreal, qc, h3c 3j7\n1cifar senior fellow\n\n6\n1\n0\n2\n\n \n\ng\nu\na\n9\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n6\n5\n1\n4\n0\n\n.\n\n2\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nneuroscientists have long criticised deep learn-\ning algorithms as incompatible with current\nknowledge of neurobiology. we explore more bi-\nologically plausible versions of deep representa-\ntion learning, focusing here mostly on unsuper-\nvised learning but developing a learning mecha-\nnism that could account for supervised, unsuper-\nvised and reinforcement learning. the starting\npoint is that the basic learning rule believed to\ngovern synaptic weight updates (spike-timing-\ndependent plasticity) arises out of a simple up-\ndate rule that makes a lot of sense from a machine\nlearning point of view and can be interpreted\nas gradient descent on some objective func", "under review as a conference paper at iclr 2022\n\ndetecting modularity in deep neural net-\nworks\n\nanonymous authors\npaper under double-blind review\n\nabstract\n\na neural network is modular to the extent that parts of its computational graph (i.e.\nstructure) can be represented as performing some comprehensible subtask relevant\nto the overall task (i.e. functionality). are modern deep neural networks modular?\nhow can this be quanti\ufb01ed? in this paper, we consider the problem of assessing\nthe modularity exhibited by a partitioning of a network\u2019s neurons. we propose\ntwo proxies for this: importance, which re\ufb02ects how crucial sets of neurons are to\nnetwork performance; and coherence, which re\ufb02ects how consistently their neurons\nassociate with features of the inputs. to measure these proxies, we develop a set of\nstatistical methods based on techniques conventionally used to interpret individual\nneurons. we apply the proxies to partitionings generated by spectrally clustering\na graph representati", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nwith  or  without  you:  predictive  coding  and  bayesian\ninference  in  the  brain\nlaurence  aitchison1 and  ma\u00b4 te\u00b4 lengyel1,2\n\ntwo  theoretical  ideas  have  emerged  recently  with  the  ambition\nto  provide  a  unifying  functional  explanation  of  neural  population\ncoding  and  dynamics:  predictive  coding  and  bayesian\ninference.  here,  we  describe  the  two  theories  and  their\ncombination  into  a  single  framework:  bayesian  predictive\ncoding.  we  clarify  how  the  two  theories  can  be  distinguished,\ndespite  sharing  core  computational  concepts  and  addressing\nan  overlapping  set  of  empirical  phenomena.  we  argue  that\npredictive  coding  is  an  algorithmic/representational  motif  that\ncan  serve  several  different  computational  goals  of  which\nbayesian  inference  is  but  one.  conversely,  while  bayesian\ninference  can  utilize  predictive  coding,  it  can  also  be  realiz", "inception loops discover what excites neurons \nmost using deep predictive models\n\nedgar y. walker\u200a\nemmanouil froudarakis\u200a\nxaq pitkow\u200a\n\n\u200a1,2,7 and andreas s. tolias\u200a\n\n\u200a1,2,7*\n\n\u200a1,2,8*, fabian h. sinz\u200a\n\n\u200a1,2,3,4,8*, erick cobos1,2, taliah muhammad1,2, \n\n\u200a1,2, paul g. fahey1,2, alexander s. ecker\u200a\n\n\u200a1,3,5,6, jacob reimer1,2, \n\nfinding sensory stimuli that drive neurons optimally is central to understanding information processing in the brain. however, \noptimizing sensory input is difficult due to the predominantly nonlinear nature of sensory processing and high dimensionality \nof the input. we developed \u2018inception loops\u2019, a closed-loop experimental paradigm combining in\u00a0vivo recordings from thousands \nof neurons with in silico nonlinear response modeling. our end-to-end trained, deep-learning-based model predicted thousands \nof neuronal responses to arbitrary, new natural input with high accuracy and was used to synthesize optimal stimuli\u2014most \nexciting inputs (meis). for mouse primary vi", "original research\npublished: 28 february 2020\ndoi: 10.3389/fnins.2020.00119\n\nenabling spike-based\nbackpropagation for training deep\nneural network architectures\n\nchankyu lee*\u2020, syed shakib sarwar*\u2020, priyadarshini panda, gopalakrishnan srinivasan\nand kaushik roy\n\nnanoelectronics research laboratory, school of electrical and computer engineering, purdue university, west lafayette, in,\n\nunited states\n\nspiking neural networks (snns) have recently emerged as a prominent neural computing\nparadigm. however, the typical shallow snn architectures have limited capacity for\nexpressing complex representations while training deep snns using input spikes has\nnot been successful so far. diverse methods have been proposed to get around this\nissue such as converting off-the-shelf trained deep arti\ufb01cial neural networks (anns)\nto snns. however, the ann-snn conversion scheme fails to capture the temporal\ndynamics of a spiking system. on the other hand, it is still a dif\ufb01cult problem to directly\ntrain deep", "gradient-based learning drives robust \nrepresentations in recurrent neural networks by \nbalancing compression and expansion\n\nmatthew farrell\u200a\neric shea-brown1,2,5\n\n\u200a1,2,6\u2009\u2709, stefano recanatesi2, timothy moore2, guillaume lajoie3,4 and \n\nneural networks need the right representations of input data to learn. here we ask how gradient-based learning shapes a fun-\ndamental property of representations in recurrent neural networks (rnns)\u2014their dimensionality. through simulations and \nmathematical analysis, we show how gradient descent can lead rnns to compress the dimensionality of their representations \nin a way that matches task demands during training while supporting generalization to unseen examples. this can require \nan expansion of dimensionality in early timesteps and compression in later ones, and strongly chaotic rnns appear particu-\nlarly adept at learning this balance. beyond helping to elucidate the power of appropriately initialized artificial rnns, this fact \nhas implications f", "ieee transactions on automatic control, vol. 46, no. 4, april 2001\n\n613\n\n[12]\n\n, \u201cfrequency response of sampled-data systems ii: closed-loop\n\nconsiderations,\u201d in proc. ifac world congr., 1993, pp. 263\u2013266.\n\n[13] y. yamamoto and p. p. khargonekar, \u201cfrequency response of sam-\npled-data systems,\u201d ieee trans. automat. contr., vol. 41, pp. 166\u2013176,\nfeb. 1996.\n\n[14] p. gahinet, a. nemirovski, a. j. laub, and m. chilali, \u201cthe lmi control\ntoolbox,\u201d in proc. third european control conf., 1995, pp. 3206\u2013321.\n[15] g. j. balas, j. c. doyle, k. glover, a. packard, and r. smith, \u201cthe\n\u0016-analysis and synthesis toolbox (\u0016-tools),\u201d automatica, vol. 30, no. 4,\npp. 733\u2013735, 1994.\n\n[16] h. bercovici, c. foias, and a. tannenbaum, \u201cstructured interpolation\ntheory,\u201d in extension and interpolation of linear operators and matrix\nfunctions. boston, ma: birkh\u00e4user, 1990, pp. 195\u2013220.\n\nadaptive control of markov chains with average cost\n\nzhiyuan ren and bruce h. krogh\n\nabstract\u2014we present an adaptive control schem", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/299416173\n\ndiversity in neural \ufb01ring dynamics supports both rigid and learned\nhippocampal sequences\n\narticle\u00a0\u00a0in\u00a0\u00a0science \u00b7 march 2016\n\ndoi: 10.1126/science.aad1935\n\ncitations\n243\n\n2 authors:\n\nandres grosmark\nuconn health center\n\n29 publications\u00a0\u00a0\u00a01,786 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n195\n\ngyorgy buzs\u00e1ki\nnyu langone medical center\n\n510 publications\u00a0\u00a0\u00a0106,444 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nsome of the authors of this publication are also working on these related projects:\n\noptoelectrode view project\n\ncomputational approaches to uncover neural representation of population codes in rodent hippocampal-cortical circuits view project\n\nall content following this page was uploaded by andres grosmark on 01 november 2021.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "article\n\ndoi: 10.1038/s41467-017-02717-4\n\nopen\n\ngeneralized leaky integrate-and-\ufb01re models\nclassify multiple neuron types\n\ncorinne teeter\naaron szafer1, nicholas cain\n\n1, ramakrishnan iyer\n\n1, vilas menon1,2, nathan gouwens1, david feng\n\n1, jim berg1,\n\n1, hongkui zeng1, michael hawrylycz1, christof koch\n\n1 & stefan mihalas\n\n1\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\nthere is a high diversity of neuronal types in the mammalian neocortex. to facilitate con-\nstruction of system models with multiple cell types, we generate a database of point models\nassociated with the allen cell types database. we construct a set of generalized leaky\nintegrate-and-\ufb01re (glif) models of increasing complexity to reproduce the spiking behaviors\nof 645 recorded neurons from 16 transgenic lines. the more complex models have an\nincreased capacity to predict spiking behavior of hold-out stimuli. we use unsupervised\nmethods to classify cell types, and \ufb01nd that high level glif model parameters are able to\ndifferentiate tran", "2\n2\n0\n2\n\n \nt\nc\no\n1\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n4\n6\n3\n0\n0\n\n.\n\n6\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nelucidating the design space of diffusion-based\n\ngenerative models\n\ntero karras\n\nnvidia\n\nmiika aittala\n\nnvidia\n\ntimo aila\nnvidia\n\nsamuli laine\n\nnvidia\n\nabstract\n\nwe argue that the theory and practice of diffusion-based generative models are\ncurrently unnecessarily convoluted and seek to remedy the situation by presenting\na design space that clearly separates the concrete design choices. this lets us\nidentify several changes to both the sampling and training processes, as well as\npreconditioning of the score networks. together, our improvements yield new\nstate-of-the-art fid of 1.79 for cifar-10 in a class-conditional setting and 1.97 in\nan unconditional setting, with much faster sampling (35 network evaluations per\nimage) than prior designs. to further demonstrate their modular nature, we show\nthat our design changes dramatically improve both the ef\ufb01ciency and quality ob-\ntainable with pre-trained s", "neuron\n\narticle\n\nhippocampal representation of related\nand opposing memories develop within distinct,\nhierarchically organized neural schemas\n\nsam mckenzie,1 andrea j. frank,1 nathaniel r. kinsky,1 blake porter,1 pamela d. rivie` re,1 and howard eichenbaum1,*\n1center for memory and brain, boston university, boston, ma 02215, usa\n*correspondence: hbe@bu.edu\nhttp://dx.doi.org/10.1016/j.neuron.2014.05.019\n\nsummary\n\nthat\n\nrecent evidence suggests that the hippocampus\nmay integrate overlapping memories into relational\nrepresentations, or schemas,\nlink indirectly\nrelated events and support \ufb02exible memory expres-\nsion. here we explored the nature of hippocampal\nneural population representations for multiple fea-\ntures of events and the locations and contexts in\nwhich they occurred. hippocampal networks devel-\noped hierarchical organizations of associated ele-\nments of related but separately acquired memories\nwithin a context, and distinct organizations for\nmemories where the contexts differen", "6\n1\n0\n2\n\n \nc\ne\nd\n7\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n0\n1\n1\n7\n0\n\n.\n\n5\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ndeep learning without poor local minima\n\nkenji kawaguchi\n\nmassachusetts institute of technology\n\nkawaguch@mit.edu\n\nabstract\n\nin this paper, we prove a conjecture published in 1989 and also partially address\nan open problem announced at the conference on learning theory (colt) 2015.\nwith no unrealistic assumption, we \ufb01rst prove the following statements for the\nsquared loss function of deep linear neural networks with any depth and any\nwidths: 1) the function is non-convex and non-concave, 2) every local minimum is\na global minimum, 3) every critical point that is not a global minimum is a saddle\npoint, and 4) there exist \u201cbad\u201d saddle points (where the hessian has no negative\neigenvalue) for the deeper networks (with more than three layers), whereas there\nis no bad saddle point for the shallow networks (with three layers). moreover, for\ndeep nonlinear neural networks, we prove the same four statem", "open source tools and methods\n\nnovel tools and methods\n\npsychrnn: an accessible and flexible python\npackage for training recurrent neural network\nmodels on cognitive tasks\n\ndaniel b. ehrlich,1,p jasmine t. stone,2,p david brandfonbrener,2,3 alexander atanasov,4,5 and\njohn d. murray1,4,6\n\nhttps://doi.org/10.1523/eneuro.0427-20.2020\n\n1interdepartmental neuroscience program, yale university, new haven, ct 06520-8074, 2department of computer\nscience, yale university, new haven, ct 06520-8285, 3department of computer science, new york university, new york,\nny 10012, 4department of physics, yale university, new haven, ct 06511-8499, 5department of physics, harvard\nuniversity, cambridge, ma 02138, and 6department of psychiatry, yale school of medicine, new haven, ct 06511\n\nvisual abstract\n\ndefine task\n\ncurriculum learning / task shaping\n\nn inputs\n\nm targets\n\nmodularity supports task flexibility\n\ntrain network\n\nrnn\n\noutput\n\ninput\n\nprobe network output, \n\nstructure, dynamics\n\ne.g. perturbation\n", "8\n1\n0\n2\n\n \nr\na\n\n \n\nm\n4\n2\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n2\n8\n0\n9\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nworkshop track - iclr 2018\n\na proximal block coordinate descent algo-\nrithm for deep neural network training\n\ntim tsz-kit lau\u2217, jinshan zeng\u2020\u2217, baoyuan wu\u2021, yuan yao\u2217\n\u2217department of mathematics, the hong kong university of science and technology\n\u2020school of computer and information engineering, jiangxi normal university\n\u2021tencent ai lab\n\nabstract\n\ntraining deep neural networks (dnns) ef\ufb01ciently is a challenge due to the associ-\nated highly nonconvex optimization. the backpropagation (backprop) algorithm\nhas long been the most widely used algorithm for gradient computation of pa-\nrameters of dnns and is used along with gradient descent-type algorithms for\nthis optimization task. recent work have shown the ef\ufb01ciency of block coordi-\nnate descent (bcd) type methods empirically for training dnns. in view of this,\nwe propose a novel algorithm based on the bcd method for training dnns and\nprovide its g", "j.  phys. a:  math. gen. 22  (1989) 2181-2190.  printed in the u k  \n\nthe space of  interactions in neural networks: gardner\u2019s \ncomputation with the cavity method \n\nmarc mczard \nlaboratoire  de physique  theorique de  i\u2019ecole  normale  superieuret, 24  rue  lhomond, \n75231 paris cedex 05, france \n\nreceived  16 january  1989 \n\nabstract.  gardner\u2019s computation of the number of  n-bit patterns  which can be stored in \nan optimal neural network used as an associative memory is derived without replicas, using \nthe cavity method.  this allows for a unified  presentation  whatever the basic measure  in \nthe  space of  coupling constants,  but  above all  it  gives  the  clear  physical  content  of  the \nassumption  of  replica  symmetry.  tap equations are also derived. \n\nforeword \n\none of  the most  exciting  recent  developments  in  the theory  of  neural  networks  is  a \ncontribution  of  elizabeth  gardner.  she showed how one can analyse the space of all \nthe networks that  are able t", "packnet: adding multiple tasks to a single network by iterative pruning\n\narun mallya and svetlana lazebnik\n\nuniversity of illinois at urbana-champaign\n\n{amallya2,slazebni}@illinois.edu\n\n8\n1\n0\n2\n\n \n\ny\na\nm\n3\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n9\n6\n7\n5\n0\n\n.\n\n1\n1\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nthis paper presents a method for adding multiple tasks\nto a single deep neural network while avoiding catastrophic\nforgetting. inspired by network pruning techniques, we ex-\nploit redundancies in large deep networks to free up pa-\nrameters that can then be employed to learn new tasks. by\nperforming iterative pruning and network re-training, we\nare able to sequentially \u201cpack\u201d multiple tasks into a sin-\ngle network while ensuring minimal drop in performance\nand minimal storage overhead. unlike prior work that uses\nproxy losses to maintain accuracy on older tasks, we al-\nways optimize for the task at hand. we perform extensive\nexperiments on a variety of network architectures and large-\nscale datasets, and ob", "7\n1\n0\n2\n\n \n\ng\nu\na\n8\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n4\n4\n1\n5\n0\n\n.\n\n8\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nscalabletrust-regionmethodfordeepreinforcementlearningusingkronecker-factoredapproximationyuhuaiwu\u2217universityoftorontovectorinstituteywu@cs.toronto.eduelmanmansimov\u2217newyorkuniversitymansimov@cs.nyu.edushunliaouniversityoftorontovectorinstitutesliao3@cs.toronto.edurogergrosseuniversityoftorontovectorinstitutergrosse@cs.toronto.edujimmybauniversityoftorontovectorinstitutejimmy@psi.utoronto.caabstractinthiswork,weproposetoapplytrustregionoptimizationtodeepreinforce-mentlearningusingarecentlyproposedkronecker-factoredapproximationtothecurvature.weextendtheframeworkofnaturalpolicygradientandproposetooptimizeboththeactorandthecriticusingkronecker-factoredapproximatecurvature(k-fac)withtrustregion;hencewecallourmethodactorcriticusingkronecker-factoredtrustregion(acktr).tothebestofourknowledge,thisisthe\ufb01rstscalabletrustregionnaturalgradientmethodforactor-criticmethods.itisalsoamethodthatlearnsnon-trivialta", "the journal of neuroscience, march 13, 2013 \u2022 33(11):4693\u2013 4709 \u2022 4693\n\nsystems/circuits\n\ndiversity and homogeneity in responses of midbrain\ndopamine neurons\n\nchristopher d. fiorillo,1,2 sora r. yun,1 and minryung r. song1\n1department of bio and brain engineering, kaist (korea advanced institute of science and technology), daejeon 305-701, republic of korea, and\n2department of neurobiology, stanford university, stanford, california 94305\n\ndopamine neurons of the ventral midbrain have been found to signal a reward prediction error that can mediate positive reinforcement.\ndespite the demonstration of modest diversity at the cellular and molecular levels, there has been little analysis of response diversity in\nbehaving animals. here we examine response diversity in rhesus macaques to appetitive, aversive, and neutral stimuli having relative\nmotivational values that were measured and controlled through a choice task. first, consistent with previous studies, we observed a\ncontinuum of respo", "supplementary information for \u201cseparability and geometry\n\nof object manifolds in deep neural networks\u201d\n\ndecember 22, 2019\n\ncontents\n\n1 figures\n\n1.1 object manifolds in deep convolutional networks (alexnet, vgg-16, resnet-50) . . . . . . . . . . .\n1.1.1 capacity for point-cloud manifolds in alexnet, vgg-16, resnet-50 . . . . . . . . . . . . . .\n1.1.2 capacity for smooth manifolds in alexnet, vgg-16, resnet-50\n. . . . . . . . . . . . . . . .\n1.1.3 geometry of point-cloud and smooth manifolds in alexnet, vgg-16, resnet-50 . . . . . . .\n1.1.4 geometry of 1-d vs 2-d smooth manifolds in alexnet, vgg-16, resnet-50 . . . . . . . . . . .\n1.1.5 manifold correlations and their e\ufb00ect on capacity in alexnet, vgg-16, resnet-50 . . . . . .\n1.1.6 deep network building-blocks have a consistent e\ufb00ect on manifold geometry and correlations\n1.2 predictions of manifold separability theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2.1 comparison between theory and numerically measur", "and ti2ni incoherently precipitate in the b2\nmatrix upon cooling from 700\u00b0c to 87\u00b0c. the\nb2 matrix transforms completely to b19 upon\nfurther cooling to 22\u00b0c, aided by the simulta-\nneous ti2cu/b19 epitaxies (figs. 3 and 4, table 1,\nand table s3). the b19/ti2cu epitaxy provides an\ninternal stress pattern, which stabilizes the b19\nphase at low temperatures. during stress cycling,\nthe equivalent epitaxy alternatingly stabilizes the\nb2 phase. at each temperature and stress, the\ntransforming phases attain equilibrium by form-\ning a compatible morphology directed by the\ninternal epitaxy-generated stress distribution.\ncomplete transformation is attained at each cycle\nbecause the epitaxial stresses are reversible. hence,\nwe propose that the epitaxially promoted comple-\ntion of the b2\u2194b19 phase transformation creates\nthe low-fatigue state of the ti54ni34cu12 films.\nthe ti2cu precipitates act like sentinels, assuring\nthat the b2\u2194b19 transformation proceeds toward\ncompletion at each cycle. the tra", "a deep learning framework for neuroscience\n\n\u200a5,6,42, philippe beaudoin7, yoshua bengio1,4,8,  \n\nblake a. richards1,2,3,4,42*, timothy p. lillicrap\u200a\nrafal bogacz9, amelia christensen10, claudia clopath\u200a\nsurya ganguli14,15, colleen j. gillon\u200a\nnikolaus kriegeskorte21,22, peter latham\u200a\nrichard naud\u200a\njo\u00e3o sacramento30, andrew saxe31, benjamin scellier1,8, anna c. schapiro\u200a\n\u200a32, walter senn13, \ngreg wayne5, daniel yamins33,34,35, friedemann zenke36,37, joel zylberberg4,38,39, denis therien\u200a\nand konrad p. kording\u200a\n\n\u200a23, grace w. lindsay22,24, kenneth d. miller\u200a\n\u200a29, \n\n\u200a26,27, christopher c. pack3, panayiota poirazi\u200a\n\n\u200a11, rui ponte costa12,13, archy de berker7,  \n\n\u200a15,18,19, adam kepecs20, \n\n\u200a28, pieter roelfsema\u200a\n\n\u200a16,17, danijar hafner\u200a\n\n\u200a22,24,25, \n\n\u200a4,40,41,42\n\n\u200a7,42 \n\nsystems neuroscience seeks explanations for how the brain implements a wide variety of perceptual, cognitive and motor tasks. \nconversely, artificial intelligence attempts to design computational systems based on the tasks ", "2005 american control conference \njune 8-10, 2005. portland, or, usa \n\nwea10.2 \n\na  generalized iterative lqg method for locally-optimal feedback \n\ncontrol of constrained nonlinear stochastic systems \n\nemanuel todorov and weiwei li \n\nabstract-we \n\npresent  an \n\niterative  linear-quadratic- \ngaussian  method  for  locally-optimal  feedback  control  of \nnonlinear  stochastic  systems  subject  to  control  constraints. \npreviously, similar methods have been restricted to determin- \nistic  unconstrained  problems  with  quadratic  costs.  the  new \nmethod constructs an affine feedback control law, obtained by \nminimizing  a  novel  quadratic  approximation to  the  optimal \ncost-to-go function. global convergence is guaranteed through \na  levenberg-marquardt method; convergence in  the  vicinity \nof  a  local  minimum  is  quadratic.  performance  is  illustrated \non a limited-torque inverted pendulum problem,  as well  as  a \ncomplex biomechanical control problem involving a stochastic ", "successful reconstruction of a physiological circuit with\nknown connectivity from spiking activity alone\n\nfelipe gerhard1*, tilman kispersky2, gabrielle j. gutierrez2,3, eve marder2, mark kramer4, uri eden4\n\n1 brain mind institute, ecole polytechnique fe\u00b4de\u00b4rale de lausanne (epfl), lausanne, switzerland, 2 biology department and volen center, brandeis university, waltham,\nmassachusetts, united states of america, 3 group for neural theory, e\u00b4cole normale supe\u00b4rieure, paris, france, 4 department of mathematics and statistics, boston\nuniversity, boston, massachusetts, united states of america\n\nabstract\n\nidentifying the structure and dynamics of synaptic interactions between neurons is the first step to understanding neural\nnetwork dynamics. the presence of synaptic connections is traditionally inferred through the use of targeted stimulation\nand paired recordings or by post-hoc histology. more recently, causal network inference algorithms have been proposed to\ndeduce connectivity directly", "3\n2\n0\n2\n\n \n\nb\ne\nf\n \n4\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n4\nv\n8\n8\n0\n8\n0\n\n.\n\n1\n1\n0\n2\n:\nv\ni\nx\nr\na\n\noriginal article\ncommentary\nstrongandweakprinciplesofneuraldimension\nreduction\nmarkd.humphries1\n1schoolofpsychology,universityof\nnottingham,uk\ncorrespondence\nschoolofpsychology,universityof\nnottingham,uk\nemail: mark.humphries@nottingham.ac.uk\nfundinginformation\nthetimeneededtothinkthesethoughts\nwasmadeavailablethankstothegenerous\nsupportofthemedicalresearchcouncil\ngrantsmr/j008648/1,mr/p005659/1,\nandmr/s025944/1.\n\nif spikes are the medium, what is the message? answer-\ningthatquestionisdrivingthedevelopmentoflarge-scale,\nsingleneuronresolutionrecordingsfrombehavinganimals,\non the scale of thousands of neurons. but these data are\ninherently high-dimensional, with as many dimensions as\nneurons - so how do we make sense of them? for many\nthe answer is to reduce the number of dimensions. here\ni argue we can distinguish weak and strong principles of\nneural dimension reduction. the weak principle is", "48. h. s. mayberg et al., ann. neurol. 28, 57 (1990).\n49. r. m. cohen et al., neuropsychopharmacology 2,\n\n241 (1989).\n\n50. j. e. ledoux, sci. am. 6, 50 (june 1994); m. davis,\n\nannu. rev. neurosci. 15, 353 (1992).\n\n51. j. e. ledoux, curr. opin. neurobiol. 2, 191 (1992); l.\nm. romanski and j. e. ledoux, j. neurosci. 12, 4501\n(1992); j. l. armony, j. d. cohen, d. servan-schre-\niber, j. e. ledoux, behav. neurosci. 109, 246 (1995).\n52. k. p. corodimas and j. e. ledoux, behav. neurosci.\n\n109, 613 (1995).\n\n53. m. j. d. miserendino, c. b. sananes, k. r. melia,\nm. davis, nature 345, 716 (1990); c. farb et al.,\n\nbrain res. 593, 145 (1992); m. davis, d. rainne, m.\ncassell, trends neurosci. 17, 208 (1994).\n\n54. m. e. p. seligman, j. abnorm. psychol. 74, 1 (1976);\nf. schneider et al., am. j. psychiatry 153, 206\n(1996).\n\n55. w. c. drevets et al., j. neuroscience 12, 3628\n\n(1992).\n\n56. ssupported in part by national institiute of mental\nhealth grants mh31593, mh40856, and mh-\ncrc43271; by a research ", "capturing the dynamical repertoire of single neurons\nwith generalized linear models\nalison i. weber1 & jonathan w. pillow2,3\n1graduate program in neuroscience, university of washington, seattle, wa, usa.\n2princeton neuroscience institute, princeton university, princeton, nj, usa.\n3dept. of psychology, princeton university, princeton, nj, usa.\n\nkeywords: point process; generalized linear model (glm); izhikevich model; spike timing;\nvariability\n\nabstract\n\na key problem in computational neuroscience is to \ufb01nd simple, tractable models that are never-\ntheless \ufb02exible enough to capture the response properties of real neurons. here we examine\nthe capabilities of recurrent point process models known as poisson generalized linear models\n(glms). these models are de\ufb01ned by a set of linear \ufb01lters, a point nonlinearity, and condi-\ntionally poisson spiking. they have desirable statistical properties for \ufb01tting and have been\nwidely used to analyze spike trains from electrophysiological recordings. ho", "true online td(\u03bb)\n\nharm van seijen\nrichard s. sutton\ndepartment of computing science, university of alberta, edmonton, alberta, t6g 2e8, canada\n\nharm.vanseijen@ualberta.ca\nsutton@cs.ualberta.ca\n\nabstract\n\ntd(\u03bb) is a core algorithm of modern reinforce-\nment learning. its appeal comes from its equiv-\nalence to a clear and conceptually simple for-\nward view, and the fact that it can be imple-\nmented online in an inexpensive manner. how-\never, the equivalence between td(\u03bb) and the for-\nward view is exact only for the off-line version of\nthe algorithm (in which updates are made only at\nthe end of each episode). in the online version of\ntd(\u03bb) (in which updates are made at each step,\nwhich generally performs better and is always\nused in applications) the match to the forward\nview is only approximate. in a sense this is un-\navoidable for the conventional forward view, as it\nitself presumes that the estimates are unchanging\nduring an episode. in this paper we introduce a\nnew forward view that t", "ella: an e\ufb03cient lifelong learning algorithm\n\npaul ruvolo\neric eaton\nbryn mawr college, computer science department, 101 north merion avenue, bryn mawr, pa 19010 usa\n\npruvolo@cs.brynmawr.edu\neeaton@cs.brynmawr.edu\n\nabstract\n\nthe problem of learning multiple consecu-\ntive tasks, known as lifelong learning, is of\ngreat importance to the creation of intelli-\ngent, general-purpose, and \ufb02exible machines.\nin this paper, we develop a method for on-\nline multi-task learning in the lifelong learn-\ning setting. the proposed e\ufb03cient life-\nlong learning algorithm (ella) maintains\na sparsely shared basis for all task models,\ntransfers knowledge from the basis to learn\neach new task, and re\ufb01nes the basis over time\nto maximize performance across all tasks.\nwe show that ella has strong connections\nto both online dictionary learning for sparse\ncoding and state-of-the-art batch multi-task\nlearning methods, and provide robust the-\noretical performance guarantees. we show\nempirically that ella yields near", "jmlr: workshop and conference proceedings 27:17\u201337, 2012 workshop on unsupervised and transfer learning\n\ndeep learning of representations for unsupervised and\n\ntransfer learning\n\nyoshua bengio\ndept. iro, universit\u00b4e de montr\u00b4eal. montr\u00b4eal (qc), h2c 3j7, canada\n\nyoshua.bengio@umontreal.ca\n\neditor: i. guyon, g. dror, v. lemaire, g. taylor, and d. silver\n\nabstract\n\ndeep learning algorithms seek to exploit the unknown structure in the input distribution\nin order to discover good representations, often at multiple levels, with higher-level learned\nfeatures de\ufb01ned in terms of lower-level features. the objective is to make these higher-\nlevel representations more abstract, with their individual features more invariant to most\nof the variations that are typically present in the training distribution, while collectively\npreserving as much as possible of the information in the input. ideally, we would like these\nrepresentations to disentangle the unknown factors of variation that underlie the t", "letter\n\ncommunicated by mark ungless\n\nrepresentation and timing in theories of the dopamine\nsystem\n\nnathaniel d. daw\ndaw@gatsby.ucl.ac.uk\nucl, gatsby computational neuroscience unit, london, wc1n3ar, u.k.\n\naaron c. courville\naaronc@cs.cmu.edu\ncarnegie mellon university, robotics institute and center for the neural basis of\ncognition, pittsburgh, pa 15213, u.s.a.\n\ndavid s. tourtezky\ndst@cs.cmu.edu\ncarnegie mellon university, computer science department and center for the\nneural basis of cognition, pittsburgh, pa 15213, u.s.a.\n\nalthough the responses of dopamine neurons in the primate midbrain\nare well characterized as carrying a temporal difference (td) error signal\nfor reward prediction, existing theories do not offer a credible account\nof how the brain keeps track of past sensory events that may be relevant\nto predicting future reward. empirically, these shortcomings of previous\ntheories are particularly evident in their account of experiments in which\nanimals were exposed to variatio", "mach learn (2012) 87:159\u2013182\ndoi 10.1007/s10994-012-5278-7\n\noptimal control as a graphical model inference problem\n\nhilbert j. kappen \u00b7 vicen\u00e7 g\u00f3mez \u00b7 manfred opper\n\nreceived: 3 december 2010 / accepted: 11 january 2012 / published online: 1 february 2012\n\u00a9 the author(s) 2012. this article is published with open access at springerlink.com\n\nabstract we reformulate a class of non-linear stochastic optimal control problems in-\ntroduced by todorov (in advances in neural information processing systems, vol. 19,\npp. 1369\u20131376, 2007) as a kullback-leibler (kl) minimization problem. as a result, the\noptimal control computation reduces to an inference computation and approximate infer-\nence methods can be applied to ef\ufb01ciently compute approximate optimal controls. we show\nhow this kl control theory contains the path integral control method as a special case. we\nprovide an example of a block stacking task and a multi-agent cooperative game where we\ndemonstrate how approximate inference can be su", "\f", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/2023.07.09.548255\n; \n\nthis version posted july 9, 2023. \n\nthe copyright holder for this preprint (which\n\nwas not certified by peer review) is the author/funder. all rights reserved. no reuse allowed without permission. \n\na biochemical description of postsynaptic\nplasticity \u2013 with timescales ranging from\nmilliseconds to seconds\n\nguanchun lia, david w. mclaughlina,b,c,2, and charles s. peskina\n\nacourant institute of mathematical sciences and center for neural science, new york university, 251 mercer street, ny, ny 10012; binstitute of mathematical sciences, new\nyork university shanghai, shanghai 200122, china; cneuroscience institute of new york university langone health, new york university, new york, ny 10016\n\nthis manuscript was compiled on july 9, 2023\n\nsynaptic plasticity (long term potentiation/depression (ltp/d)), is a cellular mechanism underlying learning. two distinct types of early ltp/d\n(e-ltp/d), acting on very different time", "1\n2\n0\n2\n\n \nr\np\na\n8\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n7\n1\n2\n2\n0\n\n.\n\n8\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nhopfield networks is all you need\n\nhubert ramsauer\u2217 bernhard sch\u00e4\ufb02\u2217 johannes lehner\u2217 philipp seidl\u2217\nmichael widrich\u2217 thomas adler\u2217 lukas gruber\u2217 markus holzleitner\u2217\nmilena pavlovi\u00b4c\u2021 ,\u00a7 geir kjetil sandve\u00a7 victor greiff\u2021 david kreil\u2020\nmichael kopp\u2020 g\u00fcnter klambauer\u2217 johannes brandstetter\u2217 sepp hochreiter\u2217 ,\u2020\n\u2217ellis unit linz, lit ai lab, institute for machine learning,\njohannes kepler university linz, austria\n\u2020 institute of advanced research in arti\ufb01cial intelligence (iarai)\n\u2021 department of immunology, university of oslo, norway\n\u00a7 department of informatics, university of oslo, norway\n\nabstract\n\nwe introduce a modern hop\ufb01eld network with continuous states and a correspond-\ning update rule. the new hop\ufb01eld network can store exponentially (with the\ndimension of the associative space) many patterns, retrieves the pattern with one\nupdate, and has exponentially small retrieval errors. it has three types of ", "journal of machine learning research 10 (2009) 2413-2444\n\nsubmitted 11/06; revised 12/08; published 11/09\n\nreinforcement learning in finite mdps: pac analysis\n\nalexander l. strehl\u2217\nfacebook\n1601 s california ave.\npalo alto, ca 94304\nlihong li\u2020\nyahoo! research\n4401 great america parkway\nsanta clara, ca 95054\n\nmichael l. littman\ndepartment of computer science\nrutgers university\npiscataway, nj 08854\n\neditor: sridhar mahadevan\n\nastrehl@facebook.com\n\nlihong@yahoo-inc.com\n\nmlittman@cs.rutgers.edu\n\nabstract\n\nwe study the problem of learning near-optimal behavior in \ufb01nite markov decision processes\n(mdps) with a polynomial number of samples. these \u201cpac-mdp\u201d algorithms include the well-\nknown e3 and r-max algorithms as well as the more recent delayed q-learning algorithm. we\nsummarize the current state-of-the-art by presenting bounds for the problem in a uni\ufb01ed theoretical\nframework. a more re\ufb01ned analysis for upper and lower bounds is presented to yield insight into\nthe differences between the ", "statistical mechanics of generalization in kernel \nregression and wide neural networks\n\ncitation\ncanatar, abdulkadir. 2022. statistical mechanics of generalization in kernel regression and \nwide neural networks. doctoral dissertation, harvard university graduate school of arts and \nsciences.\n\npermanent link\nhttps://nrs.harvard.edu/urn-3:hul.instrepos:37373673\n\nterms of use\nthis article was downloaded from harvard university\u2019s dash repository, and is made available \nunder the terms and conditions applicable to other posted material, as set forth at http://\nnrs.harvard.edu/urn-3:hul.instrepos:dash.current.terms-of-use#laa\nshare your story\nthe harvard community has made this article openly available.\nplease share how this access benefits you.  submit a story .\n\naccessibility\n\n\f", "7\n1\n0\n2\n\n \n\ny\na\nm\n3\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n3\n4\n0\n5\n0\n\n.\n\n2\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nunbiased online recurrent optimization\n\ncorentin tallec, yann ollivier\n\nabstract\n\nthe novel unbiased online recurrent optimization (uoro) algorithm allows\nfor online learning of general recurrent computational graphs such as recurrent\nnetwork models. it works in a streaming fashion and avoids backtracking through\npast activations and inputs. uoro is computationally as costly as truncated\nbackpropagation through time (truncated bptt), a widespread algorithm for online\nlearning of recurrent networks [jae02]. uoro is a modi\ufb01cation of nobacktrack\n[otc15] that bypasses the need for model sparsity and makes implementation easy\nin current deep learning frameworks, even for complex models.\n\nlike nobacktrack, uoro provides unbiased gradient estimates; unbiasedness is\nthe core hypothesis in stochastic gradient descent theory, without which convergence\nto a local optimum is not guaranteed. on the contrary, tru", "8\n1\n0\n2\n\n \n\ng\nu\na\n2\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n3\nv\n1\n1\n6\n2\n0\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nencoder-decoder with atrous separable\n\nconvolution for semantic image segmentation\n\nliang-chieh chen, yukun zhu, george papandreou, florian schro\ufb00, and\n\nhartwig adam\n\n{lcchen, yukun, gpapan, fschroff, hadam}@google.com\n\ngoogle inc.\n\nabstract. spatial pyramid pooling module or encode-decoder structure\nare used in deep neural networks for semantic segmentation task. the\nformer networks are able to encode multi-scale contextual information by\nprobing the incoming features with \ufb01lters or pooling operations at mul-\ntiple rates and multiple e\ufb00ective \ufb01elds-of-view, while the latter networks\ncan capture sharper object boundaries by gradually recovering the spatial\ninformation. in this work, we propose to combine the advantages from\nboth methods. speci\ufb01cally, our proposed model, deeplabv3+, extends\ndeeplabv3 by adding a simple yet e\ufb00ective decoder module to re\ufb01ne the\nsegmentation results especially along ob", "mation in the liver (6). thus, we investigated the\neffect of manipulating mir-33 levels in vivo in\nmice using lentiviruses encoding pre-mir-33,\nanti-mir-33, or control. efficient lentiviral deliv-\nery was confirmed by measuring green fluores-\ncent protein in the liver (fig. s9d). consistent\nwith our in vitro results, mir-33 significantly\nreduced hepatic abca1 expression (fig. 4d;\nquantification in fig. s9e). it also modestly de-\ncreased abcg1 and npc1 protein levels, al-\nthough the effect was not statistically significant.\nno changes in sr-b1, a cognate receptor for\nhdl in the liver (7), or other cholesterol-related\ngenes were observed (fig. 4d). moreover, an\nunbiased assessment of hepatic gene expres-\nsion revealed few significant differences in the\nexpression of other cholesterol metabolism\u2013\nrelated genes in mice treated with mir-33 or\nanti-mir-33 lentiviruses (fig. s10). in vivo over-\nexpression of mir-33 resulted in a progressive\ndecline of plasma hdl, as expected from the\nrequirem", "available online at www.sciencedirect.com\n\nsciencedirect\n\nstatistical methods for dissecting interactions between\nbrain areas\njoa\u02dc o d semedo1,*, evren gokcen1,*[\nadam kohn3,4,5 and byron m yu1,6\n\n], christian k machens2,\n\nthe brain is composed of many functionally distinct areas. this\norganization supports distributed processing[\n], and requires the\ncoordination of signals across areas. our understanding of how\npopulations of neurons in different areas interact with each other\nis still in its infancy. as the availability of recordings from large\npopulations of neurons across multiple brain areas increases, so\ndoes the need for statistical methods that are well suited for\ndissecting and interrogating these recordings. here we review\nmultivariate statistical methods that have been, or could be,\napplied to this class of recordings. by leveraging population\nresponses, these methods can provide a rich description of inter-\nareal interactions. at the same time, these methods can\nintroduce i", "spike-based learning rules and stabilization of \n\npersistent neural activity \n\nxiaohui xie and h. sebastian seung \n\ndept.  of brain &  cog.  sci., mit, cambridge, ma 02139 \n\n{xhxie, seung}@mit.edu \n\nabstract \n\nwe  analyze  the  conditions  under  which  synaptic  learning  rules  based \non  action  potential timing can  be approximated by  learning rules  based \non  firing  rates.  in  particular, we  consider a form  of plasticity in  which \nsynapses depress when a presynaptic spike is followed by a postsynaptic \nspike, and potentiate with the opposite temporal ordering.  such differen \ntial anti-hebbian plasticity can be approximated under certain conditions \nby a learning rule that depends on the time derivative of the postsynaptic \nfiring rate.  such a learning rule acts to stabilize persistent neural activity \npatterns in recurrent neural networks. \n\n1 \n\nintroduction \n\npre \n\n1000 \n\nt \npost \n\no \n- t \n\n\u00b0 \n\nexperiments \n\na  o~i =~=::====: \nb  0l - . i  _:3/_-----' \n\n~re 11111111111  ", "communicated by fernando pineda \n\na learning algorithm for  continually running fully \nrecurrent neural networks \n\nronald j.  williams \ncollege of  computer science, northeastern  university, \nboston, ma  02115,  usa \n\ndavid zipser \ninstitute for  cognitive science, university of california, \nla  jolla, ca  92093, usa \n\nthe  exact  form  of  a gradient-following  learning  algorithm for com- \npletely recurrent networks running in continually sampled time is de- \nrived and used as the basis for practical algorithms for temporal super- \nvised learning tasks. these algorithms have (1) the advantage that they \ndo  not  require  a precisely defined training  interval, operating while \nthe network runs; and (2) the disadvantage that they require nonlocal \ncommunication in the network being trained and are computationally \nexpensive. these algorithms allow networks having recurrent connec- \ntions to learn complex tasks that require the retention  of  information \nover time periods having eith", "institute of mathematical statistics is collaborating with jstor to digitize, preserve, and extend access to\nthe annals of statistics.\n\nwww.jstor.org\n\n\u00ae\n\n\f", "evolving deep neural networks\n\nristo miikkulainen1,2, jason liang1,2, elliot meyerson1,2, aditya rawal1,2, dan fink1, olivier\nfrancon1, bala raju1, hormoz shahrzad1, arshak navruzyan1, nigel du(cid:130)y1, babak hodjat1\n\n1sentient technologies, inc.\n\n2(cid:140)e university of texas at austin\n\n7\n1\n0\n2\n\n \nr\na\n\nm\n4\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n8\n4\n5\n0\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n(cid:140)e success of deep learning depends on (cid:128)nding an architecture\nto (cid:128)t the task. as deep learning has scaled up to more challenging\ntasks, the architectures have become di(cid:129)cult to design by hand.\n(cid:140)is paper proposes an automated method, codeepneat, for opti-\nmizing deep learning architectures through evolution. by extending\nexisting neuroevolution methods to topology, components, and hy-\nperparameters, this method achieves results comparable to best\nhuman designs in standard benchmarks in object recognition and\nlanguage modeling. it also supports building a real-world ", "5\n1\n0\n2\n\n \nr\na\n\n \n\nm\n3\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n1\n7\n5\n5\n0\n\n.\n\n3\n0\n5\n1\n:\nv\ni\nx\nr\na\n\ngsns: generative stochastic networks\n\nguillaume alain\u2217+, yoshua bengio\u2217+, li yao\u2217, jason yosinski\u2020, \u00b4eric thibodeau-\nlaufer\u2217, saizheng zhang\u2217 and pascal vincent\u2217\n\u2217 department of computer science and operations research\nuniversity of montreal\nmontreal, h3c 3j7, quebec, canada\n\u2020 department of computer science, cornell university\n\neditor:\n\nabstract\n\nwe introduce a novel training principle for generative probabilistic models that is an al-\nternative to maximum likelihood. the proposed generative stochastic networks (gsn)\nframework generalizes denoising auto-encoders (dae) and is based on learning the tran-\nsition operator of a markov chain whose stationary distribution estimates the data distri-\nbution. the transition distribution is a conditional distribution that generally involves a\nsmall move, so it has fewer dominant modes and is unimodal in the limit of small moves.\nthis simpli\ufb01es the learning pr", "article\n\nserotonin neurons modulate learning rate through\nuncertainty\n\nhighlights\nd mice demonstrate variable behavioral \ufb02exibility during\n\ndecision making\n\nauthors\n\ncooper d. grossman, bilal a. bari,\njeremiah y. cohen\n\nd flexible behavior can be characterized as meta-learning\n\nguided by uncertainty\n\ncorrespondence\njeremiah.cohen@jhmi.edu\n\nd serotonin neuron activity correlates with expected and\n\nunexpected uncertainty\n\nd reversible inhibition of serotonin neuron activity impairs\n\nmeta-learning\n\nin brief\nlearning about actions and their\noutcomes is not a static process and\nshould be adapted to complement the\nenvironment. grossman et al. show\nevidence of variable learning rates in mice\nthat can be characterized by uncertainty-\ndriven meta-learning and demonstrate a\nrole for serotonin neuron activity in\ntracking uncertainty to modulate learning.\n\ngrossman et al., 2022, current biology 32, 586\u2013599\nfebruary 7, 2022 \u00aa 2021 elsevier inc.\nhttps://doi.org/10.1016/j.cub.2021.12.006\n\nll\n\n\f", "d\ny\nn\na\n,\n \na\nn\n \ni\nn\nt\ne\ng\nr\na\nt\ne\nd\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n \nf\no\nr\n \nl\ne\na\nr\nn\ni\nn\ng\n,\n \np\nl\na\nn\nn\ni\nn\ng\n,\n \na\nn\nd\n \nr\ne\na\nc\nt\ni\nn\ng\n \nr\ni\nc\nh\na\nr\nd\n \ns\n.\n \ns\nu\nt\nt\no\nn\n \ng\nt\ne\n \nl\na\nb\no\nr\na\nt\no\nr\ni\ne\ns\n \ni\nn\nc\no\nr\np\no\nr\na\nt\ne\nd\n \nw\na\nl\nt\nh\na\nm\n,\n \nm\na\n \n0\n2\n2\n5\n4\n \ng\nu\nt\nt\no\nn\n~\ng\nt\ne\n.\nc\no\nm\n \na\nb\ns\nt\nr\na\nc\nt\n \nd\ny\nn\na\n \ni\ns\n \na\nn\n \na\ni\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n \nt\nh\na\nt\n \ni\nn\nt\ne\ng\nr\na\nt\ne\ns\n \nl\ne\na\nr\nn\ni\nn\ng\n,\n \np\nl\na\nn\nn\ni\nn\ng\n,\n \na\nn\nd\n \nr\ne\na\nc\nt\ni\nv\ne\n \ne\nx\ne\nc\nu\nt\ni\no\nn\n.\n \nl\ne\na\nr\nn\ni\nn\ng\n \nm\ne\nt\nh\n-\n \no\nd\ns\n \na\nr\ne\n \nu\ns\ne\nd\n \ni\nn\n \nd\ny\nn\na\n \nb\no\nt\nh\n \nf\no\nr\n \nc\no\nm\np\ni\nl\ni\nn\ng\n \np\nl\na\nn\nn\ni\nn\ng\n \nr\ne\ns\nu\nl\nt\ns\n \na\nn\nd\n \nf\no\nr\n \nu\np\nd\na\nt\ni\nn\ng\n \na\n \nm\no\nd\ne\nl\n \no\nf\n \nt\nh\ne\n \ne\nf\nf\ne\nc\nt\ns\n \no\nf\n \nt\nh\ne\n \na\ng\ne\nn\nt\n'\ns\n \na\nc\nt\ni\no\nn\ns\n \no\nn\n \nt\nh\ne\n \nw\no\nr\nl\nd\n.\n \np\nl\na\nn\nn\ni\nn\ng\n \ni\ns\n \ni\nn\nc\nr\ne\n-\n \nm\ne\nn\nt\na\nl\n \na\nn\nd\n \nc\na\nn\n \nu\ns\ne\n \nt\nh\ne\n \np\nr\no\nb\na\nb\ni\nl\ni\ns\nt\ni\nc\n \na\nn\nd\n \no\nf\nt\nt\ni\nm\ne\ns\n \ni\nn\nc\no\nr\nr\ne\nc\nt\n \nw\no\nr\nl\nd\n \nm\no\nd\ne\nl\ns\n \ng\ne\nn\ne\nr\na\nt\ne\nd\n \nb\ny\n", "leee transactions  on  neural  networks,  vol.  5, no.  2,  march  1994 \n\n213 \n\nback propagation through adjoints for \nthe identification of  nonlinear dynamic \nsystems using recurrent neural models \n\nb.  srinivasan,  u.  r.  prasad, member, ieee,  and  n.  j.  rao \n\nabstract-in \n\nthis paper, back propagation  is reinvestigated for \nan efficient  evaluation of  the  gradient in  arbitrary interconnec- \ntions of  recurrent subsystems. it is shown that the error has to \nbe  back-propagated through  the  adjoint  model  of  the  system \nand  that  the  gradient  can  only  be  obtained  after  a  delay.  a \nfaster version,  accelerated back propagation, that eliminates this \ndelay, is also developed. various schemes including the sensitivity \nmethod  are studied to update the  weights of  the  network  using \nthese  gradients. motivated  by  the  lyapunov  approach and  the \nadjoint model, the  predictive back  propagation and its  variant, \ntargeted back  propagation, are proposed. a  fu", "research article\n\ndissociable components of\nrule-guided behavior depend on\ndistinct medial and prefrontal regions\nmark j. buckley,1*\u2020 farshad a. mansouri,2*\u2020 hassan hoda,2 majid mahboubi,2\nphilip g. f. browning,1 sze c. kwok,1 adam phillips,2 keiji tanaka2\n\nmuch of our behavior is guided by rules. although human prefrontal cortex (pfc) and anterior\ncingulate cortex (acc) are implicated in implementing rule-guided behavior, the crucial\ncontributions made by different regions within these areas are not yet specified. in an attempt\nto bridge human neuropsychology and nonhuman primate neurophysiology, we report the effects\nof circumscribed lesions to macaque orbitofrontal cortex (ofc), principal sulcus (ps), superior\ndorsolateral pfc, ventrolateral pfc, or acc sulcus, on separable cognitive components of a\nwisconsin card sorting test (wcst) analog. only the ps lesions impaired maintenance of abstract\nrules in working memory; only the ofc lesions impaired rapid reward-based updating of\nrepr", "1342\n\nieee transactions on pattern analysis and machine intelligence,  vol.  20,  no.  12,  december  1998\n\nbayesian classification\n\nwith gaussian processes\n\nchristopher k.i. williams, member, ieee computer society, and david barber\n\nabstract\u2014we consider the problem of assigning an input vector to one of m classes by predicting p(c|x) for c = 1, \u2026, m. for a two-\nclass problem, the probability of class one given x is estimated by s(y(x)), where s(y) = 1/(1 + e-y\n). a gaussian process prior is\nplaced on y(x), and is combined with the training data to obtain predictions for new x points. we provide a bayesian treatment,\nintegrating over uncertainty in y and in the parameters that control the gaussian process prior; the necessary integration over y is\ncarried out using laplace\u2019s approximation. the method is generalized to multiclass problems (m > 2) using the softmax function. we\ndemonstrate the effectiveness of the method on a number of datasets.\n\nindex terms\u2014gaussian processes, classific", "rainbow: combining improvements in deep reinforcement learning\n\nmatteo hessel\n\ndeepmind\n\njoseph modayil\n\ndeepmind\n\nhado van hasselt\n\ndeepmind\n\ntom schaul\n\ndeepmind\n\ngeorg ostrovski\n\ndeepmind\n\nwill dabney\n\ndeepmind\n\ndan horgan\n\ndeepmind\n\nbilal piot\ndeepmind\n\nmohammad azar\n\ndeepmind\n\ndavid silver\n\ndeepmind\n\n7\n1\n0\n2\n\n \nt\nc\no\n6\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\nv\n8\n9\n2\n2\n0\n\n.\n\n0\n1\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nthe deep reinforcement learning community has made sev-\neral independent improvements to the dqn algorithm. how-\never, it is unclear which of these extensions are complemen-\ntary and can be fruitfully combined. this paper examines\nsix extensions to the dqn algorithm and empirically studies\ntheir combination. our experiments show that the combina-\ntion provides state-of-the-art performance on the atari 2600\nbenchmark, both in terms of data ef\ufb01ciency and \ufb01nal perfor-\nmance. we also provide results from a detailed ablation study\nthat shows the contribution of each component to overall per-\nfor", "vol 451 | 3 january 2008 | doi:10.1038/nature06445\n\nletters\n\nsparse optical microstimulation in barrel cortex\ndrives learned behaviour in freely moving mice\ndaniel huber1,2, leopoldo petreanu1,2, nima ghitani1, sachin ranade2, toma\u00b4s\u02c7 hroma\u00b4dka2, zach mainen2\n& karel svoboda1,2\n\nelectrical microstimulation can establish causal links between the\nactivity of groups of neurons and perceptual and cognitive\nfunctions1\u20136. however, the number and identities of neurons\nmicrostimulated, as well as the number of action potentials\nevoked, are difficult to ascertain7,8. to address these issues we\nintroduced the light-gated algal channel channelrhodopsin-2\n(chr2)9 specifically into a small fraction of layer 2/3 neurons of\nthe mouse primary somatosensory cortex. chr2 photostimulation\nin vivo reliably generated stimulus-locked action potentials10\u201313 at\nfrequencies up to 50 hz. here we show that naive mice readily\nlearned to detect brief trains of action potentials (five light pulses,\n1 ms, 20 hz). af", "0\n2\n0\n2\n\n \n\ng\nu\na\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n9\n3\n1\n5\n1\n\n.\n\n7\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nderiving di\ufb00erential target propagation from\n\niterating approximate inverses\n\nyoshua bengio\u2217\n\naugust 19, 2020\n\nabstract\n\nwe show that a particular form of target propagation, i.e., relying on\nlearned inverses of each layer, which is di\ufb00erential, i.e., where the target is\na small perturbation of the forward propagation, gives rise to an update\nrule which corresponds to an approximate gauss-newton gradient-based\noptimization, without requiring the manipulation or inversion of large\nmatrices. what is interesting is that this is more biologically plausible\nthan back-propagation yet may turn out to implicitly provide a stronger\noptimization procedure. extending di\ufb00erence target propagation, we con-\nsider several iterative calculations based on local auto-encoders at each\nlayer in order to achieve more precise inversions for more accurate target\npropagation and we show that these iterative procedures con", "350 \u2022 the journal of neuroscience, january 6, 2010 \u2022 30(1):350 \u2013360\n\nbehavioral/systems/cognitive\n\nfunctional, but not anatomical, separation of \u201cwhat\u201d\nand \u201cwhen\u201d in prefrontal cortex\n\nchristian k. machens,1 ranulfo romo,2 and carlos d. brody3,4,5\n1group for neural theory, inserm u960, de\u00b4partement d\u2019e\u00b4tudes cognitives, e\u00b4cole normale supe\u00b4rieure, 75005 paris, france, 2instituto de fisiolog\u00eda celular\u2013\nneurociencias, universidad nacional auto\u00b4noma de me\u00b4xico, 04510 me\u00b4xico d.f., me\u00b4xico, and 3howard hughes medical institute, 4princeton neuroscience\ninstitute, and 5department of molecular biology, princeton university, princeton, new jersey 08544\n\nhow does the brain store information over a short period of time? typically, the short-term memory of items or values is thought to be\nstored in the persistent activity of neurons in higher cortical areas. however, the activity of these neurons often varies strongly in time,\neven if time is unimportant for whether or not rewards are received. t", "neuron, vol. 29, 33\u201344, january, 2001, copyright \u00aa 2001 by cell press\n\ntraveling electrical waves in cortex:\ninsights from phase dynamics\nand speculation on a computational role\n\nviewpoint\n\ng. bard ermentrout*\u2020k and david kleinfeld\u2021\u00a7k\n* department of mathematics\n\u2020 department of neurobiology\nuniversity of pittsburgh\npittsburgh, pennsylvania 15260\n\u2021 department of physics\n\u00a7 neurosciences graduate program\nuniversity of california, san diego\nla jolla, california 92093\n\nsummary\n\nthe theory of coupled phase oscillators provides a\nframework to understand the emergent properties of\nnetworks of neuronal oscillators. when the architec-\nture of the network is dominated by short-range con-\nnections, the pattern of electrical output is predicted\nto correspond to traveling plane and rotating waves,\nin addition to synchronized output. we argue that this\ntheory provides the foundation for understanding the\ntraveling electrical waves that are observed across\nolfactory, visual, and visuomotor areas of co", "available online at www.sciencedirect.com\n\nthe case for and against muscle synergies\nmatthew c tresch1,2,3 and anthony jarc1\n\na long standing goal in motor control is to determine the\nfundamental output controlled by the cns: does the cns\ncontrol the activation of individual motor units, individual\nmuscles, groups of muscles, kinematic or dynamic features of\nmovement, or does it simply care about accomplishing a task?\nof course, the output controlled by the cns might not be\nexclusive but instead multiple outputs might be controlled in\nparallel or hierarchically. in this review we examine one\nparticular hypothesized level of control: that the cns produces\nmovement through the \ufb02exible combination of groups of\nmuscles, or muscle synergies. several recent studies have\nexamined this hypothesis, providing evidence both in support\nand in opposition to it. we discuss these results and the current\nstate of the muscle synergy hypothesis.\n\naddresses\n1 northwestern university, department of biomed", "neighborhood preserving embedding\n\nxiaofei he1\n\ndeng cai2\n\nshuicheng yan3\n\nhong-jiang zhang4\n\n1 department of computer science, university of chicago, chicago, il 60637\n\n2 department of computer science, university of illinois at urbana-champaign, urbana, il 61081\n\n3 department of information engineering, chinese university of hong kong, hong kong\n\n4 microsoft research asia, beijing, p.r. china\ncontact: xiaofei@cs.uchicago.edu\n\nabstract\n\ninterest\n\nrecently there has been a lot of\nin geometri-\ncally motivated approaches to data analysis in high di-\nmensional spaces. we consider the case where data is\ndrawn from sampling a probability distribution that has\nsupport on or near a submanifold of euclidean space. in\nthis paper, we propose a novel subspace learning algorithm\ncalled neighborhood preserving embedding (npe). differ-\nent from principal component analysis (pca) which aims\nat preserving the global euclidean structure, npe aims at\npreserving the local neighborhood structure on the da", "emergence of memory manifolds\n\ntankut can1,\u2217\n\nand kamesh krishnamurthy2,\u2217\n\n1\n2\n0\n2\n\n \nc\ne\nd\n5\n\n \n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n9\n7\n8\n3\n0\n\n.\n\n9\n0\n1\n2\n:\nv\ni\nx\nr\na\n\n1school of natural sciences, institute for advanced study, princeton nj\n\n2joseph henry laboratories of physics and pni, princeton university, princeton nj\n\n(dated: december 7, 2021)\n\nthe ability to store continuous variables in the state of a biological system (e.g. a neural network)\nis critical for many behaviours. most models for implementing such a memory manifold require hand-\ncrafted symmetries in the interactions or precise \ufb01ne-tuning of parameters. we present a general\nprinciple that we refer to as frozen stabilisation (fs), which allows a family of neural networks to\nself-organise to a critical state exhibiting multiple memory manifolds without parameter \ufb01ne-tuning\nor symmetries. memory manifolds arising from fs exhibit a wide range of emergent relaxational\ntimescales and can be used as general purpose integrator", "5\n1\n0\n2\n\n \n\nv\no\nn\n8\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n3\n4\n5\n2\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nsandwiching the marginal likelihood using\n\nbidirectional monte carlo\n\nroger b. grosse\nuniversity of toronto\n\nzoubin ghahramani\nuniversity of cambridge\n\nryan p. adams\n\ntwitter and harvard university\n\nrgrosse@cs.toronto.edu\n\nzoubin@eng.cam.ac.uk\n\nrpa@seas.harvard.edu\n\nabstract\n\ncomputing the marginal likelihood (ml) of a model requires marginalizing out all of the\nparameters and latent variables, a di\ufb03cult high-dimensional summation or integration problem.\nto make matters worse, it is often hard to measure the accuracy of one\u2019s ml estimates. we\npresent bidirectional monte carlo, a technique for obtaining accurate log-ml estimates on data\nsimulated from a model. this method obtains stochastic lower bounds on the log-ml using\nannealed importance sampling or sequential monte carlo, and obtains stochastic upper bounds\nby running these same algorithms in reverse starting from an exact posterior sample. ", ".\n\nd\ne\nv\nr\ne\ns\ne\nr\n \n\ns\nt\nh\ng\ni\nr\n \nl\nl\n\na\n\n \n \n.\n\nc\nn\n\ni\n \n,\n\na\nc\n\n \n\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n9\n0\n0\n2\n\u00a9\n\n \n\nb r i e f   c o m m u n i c at i o n s\n\nthe timing of external input \ncontrols the sign of plasticity  \nat local synapses\n\njeehyun kwag & ole paulsen\n\nthe method by which local networks in the brain store \ninformation from extrinsic afferent inputs is not well \nunderstood. we found that the timing of afferent input can \nbidirectionally control the sign of spike timing\u2013dependent \nplasticity at local synapses in rat hippocampus. this \nmechanism provides a means by which temporal information  \nin external input can be encoded in the local matrix of  \nsynaptic weights.\n\nspike timing\u2013dependent plasticity (stdp) is a ubiquitous hebbian \nlearning rule1 in which synaptic modification depends on the order \nof pre- and postsynaptic spiking in time windows of a few tens of \nmilliseconds. if the presynaptic input is active before the postsynaptic \nspike, then potentiation occurs, as was origi", "siam j. imaging sciences\nvol. 1, no. 1, pp. 143\u2013168\n\nc(cid:2) 2008 society for industrial and applied mathematics\n\nbregman iterative algorithms for (cid:2)1-minimization with applications to\n\n\u2217\ncompressed sensing\n\n\u00a7\n\u2021\n\u2020\n, donald goldfarb\n, stanley osher\nwotao yin\n, and jerome darbon\n\n\u2021\n\nabstract. we propose simple and extremely e\ufb03cient methods for solving the basis pursuit problem min{(cid:2)u(cid:2)1 :\nau = f, u \u2208 rn}, which is used in compressed sensing. our methods are based on bregman iterative\nregularization, and they give a very accurate solution after solving only a very small number of\ninstances of the unconstrained problem minu\u2208rn \u03bc(cid:2)u(cid:2)1 + 1\n2 for given matrix a and vector\nf k. we show analytically that this iterative approach yields exact solutions in a \ufb01nite number of\nsteps and present numerical results that demonstrate that as few as two to six iterations are su\ufb03cient\nin most cases. our approach is especially useful for many compressed sensing applications where\n", "a simple framework for contrastive learning of visual representations\n\nting chen 1 simon kornblith 1 mohammad norouzi 1 geoffrey hinton 1\n\n0\n2\n0\n2\n\n \nl\nu\nj\n \n\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n9\n0\n7\n5\n0\n\n.\n\n2\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nthis paper presents simclr: a simple framework\nfor contrastive learning of visual representations.\nwe simplify recently proposed contrastive self-\nsupervised learning algorithms without requiring\nspecialized architectures or a memory bank. in\norder to understand what enables the contrastive\nprediction tasks to learn useful representations,\nwe systematically study the major components of\nour framework. we show that (1) composition of\ndata augmentations plays a critical role in de\ufb01ning\neffective predictive tasks, (2) introducing a learn-\nable nonlinear transformation between the repre-\nsentation and the contrastive loss substantially im-\nproves the quality of the learned representations,\nand (3) contrastive learning bene\ufb01ts from larger\nbatch sizes and more ", "neuron\n\narticle\n\nspatial attention decorrelates intrinsic\nactivity fluctuations in macaque area v4\n\njude f. mitchell,1 kristy a. sundberg,1 and john h. reynolds1,*\n1systems neurobiology lab, the salk institute, la jolla, ca 92037-1099, usa\n*correspondence: reynolds@salk.edu\ndoi 10.1016/j.neuron.2009.09.013\n\nsummary\n\nattention typically ampli\ufb01es neuronal\nresponses\nevoked by task-relevant stimuli while attenuating\nresponses to task-irrelevant distracters. in this con-\ntext, visual distracters constitute an external source\nof noise that is diminished to improve attended sig-\nnal quality. activity that is internal to the cortex\nitself, stimulus-independent ongoing correlated \ufb02uc-\ntuations in \ufb01ring, might also act as task-irrelevant\nnoise. to examine this, we recorded from area v4\nof macaques performing an attention-demanding\ntask. the \ufb01ring of neurons to identically repeated\nstimuli was highly variable. much of this variability\noriginates from ongoing low-frequency (<5 hz) \ufb02uc-\ntuations in", "research article\n\nnonlinear hebbian learning as a unifying\nprinciple in receptive field formation\ncarlos s. n. brito1,2*, wulfram gerstner1\n\n1 school of computer and communication sciences and school of life science, brain mind institute, ecole\npolytechnique federale de lausanne, lausanne epfl, switzerland, 2 gatsby computational neuroscience\nunit, university college london, london, united kingdom\n\na11111\n\n* c.brito@ucl.ac.uk\n\nabstract\n\nthe development of sensory receptive fields has been modeled in the past by a variety of\nmodels including normative models such as sparse coding or independent component\nanalysis and bottom-up models such as spike-timing dependent plasticity or the bienen-\nstock-cooper-munro model of synaptic plasticity. here we show that the above variety of\napproaches can all be unified into a single common principle, namely nonlinear hebbian\nlearning. when nonlinear hebbian learning is applied to natural images, receptive field\nshapes were strongly constrained by the", "journal of machine learning research 12 (2011) 2121-2159\n\nsubmitted 3/10; revised 3/11; published 7/11\n\nadaptive subgradient methods for\n\nonline learning and stochastic optimization\u2217\n\njohn duchi\ncomputer science division\nuniversity of california, berkeley\nberkeley, ca 94720 usa\nelad hazan\ntechnion - israel institute of technology\ntechnion city\nhaifa, 32000, israel\n\nyoram singer\ngoogle\n1600 amphitheatre parkway\nmountain view, ca 94043 usa\n\neditor: tong zhang\n\njduchi@cs.berkeley.edu\n\nehazan@ie.technion.ac.il\n\nsinger@google.com\n\nabstract\n\nwe present a new family of subgradient methods that dynamically incorporate knowledge of the\ngeometry of the data observed in earlier iterations to perform more informative gradient-based\nlearning. metaphorically, the adaptation allows us to \ufb01nd needles in haystacks in the form of very\npredictive but rarely seen features. our paradigm stems from recent advances in stochastic op-\ntimization and online learning which employ proximal functions to control th", "a r t i c l e s\n\nattention improves performance primarily by reducing \ninterneuronal correlations\n\nmarlene r cohen & john h r maunsell\n\nvisual attention can improve behavioral performance by allowing observers to focus on the important information in a complex \nscene. attention also typically increases the firing rates of cortical sensory neurons. rate increases improve the signal-to-noise \nratio of individual neurons, and this improvement has been assumed to underlie attention-related improvements in behavior. \nwe recorded dozens of neurons simultaneously in visual area v4 and found that changes in single neurons accounted for only a \nsmall fraction of the improvement in the sensitivity of the population. instead, over 80% of the attentional improvement in the \npopulation signal was caused by decreases in the correlations between the trial-to-trial fluctuations in the responses of pairs of \nneurons. these results suggest that the representation of sensory information in populations of", "credit assignment through\n\nbroadcasting a global error vector\n\ndavid g. clark, l.f. abbott, sueyeon chung\n\ncenter for theoretical neuroscience\n\ncolumbia university\n\nnew york, ny\n\n{david.clark, lfabbott, sueyeon.chung}@columbia.edu\n\nabstract\n\nbackpropagation (bp) uses detailed, unit-speci\ufb01c feedback to train deep neural\nnetworks (dnns) with remarkable success. that biological neural circuits appear\nto perform credit assignment, but cannot implement bp, implies the existence of\nother powerful learning algorithms. here, we explore the extent to which a globally\nbroadcast learning signal, coupled with local weight updates, enables training of\ndnns. we present both a learning rule, called global error-vector broadcasting\n(gevb), and a class of dnns, called vectorized nonnegative networks (vnns), in\nwhich this learning rule operates. vnns have vector-valued units and nonnegative\nweights past the \ufb01rst layer. the gevb learning rule generalizes three-factor\nhebbian learning, updating each weigh", "biorxiv preprint \nthe copyright holder for this preprint (which\nwas not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\nthis version posted november 11, 2016. \n\nhttps://doi.org/10.1101/085886\n; \n\ndoi: \n\navailable under a\n\ncc-by-nc-nd 4.0 international license\n.\n\ncause for pause before leaping to conclusions about stepping \n\nariel zylberberg*, michael n. shadlen* \n\nhoward hughes medical institute, kavli institute and department of neuroscience, zuckerman \nmind brain behavior institute, columbia university, new york, ny 10032, usa  \n*correspondence: ariel.zylberberg@gmail.com, shadlen@columbia.edu \n \nabstract \nmany neurons in parietal and prefrontal association cortex undergo gradual changes in firing \nrate during the formation of some perceptual decisions. these dynamics are often ramp-like \nincreases or decreases depending on the sign and strength of the sensory evidence and are \nthus hypothesized to ", "learning hierarchical category structure in deep neural networks\n\nandrew m. saxe (asaxe@stanford.edu)\n\ndepartment of electrical engineering\n\njames l. mcclelland (mcclelland@stanford.edu)\n\ndepartment of psychology\n\nsurya ganguli (sganguli@stanford.edu)\n\ndepartment of applied physics\n\nstanford university, stanford, ca 94305 usa\n\nabstract\n\npsychological experiments have revealed remarkable regulari-\nties in the developmental time course of cognition. infants gen-\nerally acquire broad categorical distinctions (i.e., plant/animal)\nbefore \ufb01ner ones (i.e., bird/\ufb01sh), and periods of little change\nare often punctuated by stage-like transitions. this pattern of\nprogressive differentiation has also been seen in neural net-\nwork models as they learn from exposure to training data. our\nwork explains why the networks exhibit these phenomena. we\n\ufb01nd solutions to the dynamics of error-correcting learning in\nlinear three layer neural networks. these solutions link the\nstatistics of the training set and", "6\n1\n0\n2\n\n \n\ny\na\nm\n9\n1\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n7\nv\n3\n7\n4\n0\n\n.\n\n9\n0\n4\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2015\n\nneural machine translation\nby jointly learning to align and translate\n\ndzmitry bahdanau\njacobs university bremen, germany\n\nkyunghyun cho\nuniversit\u00b4e de montr\u00b4eal\n\nyoshua bengio\u2217\n\nabstract\n\nneural machine translation is a recently proposed approach to machine transla-\ntion. unlike the traditional statistical machine translation, the neural machine\ntranslation aims at building a single neural network that can be jointly tuned to\nmaximize the translation performance. the models proposed recently for neu-\nral machine translation often belong to a family of encoder\u2013decoders and encode\na source sentence into a \ufb01xed-length vector from which a decoder generates a\ntranslation. in this paper, we conjecture that the use of a \ufb01xed-length vector is a\nbottleneck in improving the performance of this basic encoder\u2013decoder architec-\nture, and propose to extend this by allo", "r. e. kalman \nresearch institute for advanced study,2 \nbaltimore, md. \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n\n \n\na new approach to linear filtering  \nand prediction problems1\n \nthe  classical  filtering  and  prediction  problem  is  re-examined  using  the  bode-\nshannon  representation  of  random  processes  and  the  \u201cstate  transition\u201d  method  of \nanalysis of dynamic systems.  new results are: \n(1) the formulation and methods of solution of the problem apply without modifica- \ntion  to  stationary  and  nonstationary  statistics  and  to  growing-memory  and  infinite- \nmemory filters.  \n(2)  a  nonlinear  difference  (or  differential)  equation  is  derived  for  the  covariance \nmatrix  of  the  optimal  estimation  error.    from  the  solution  of  this  equation  the  co- \nefficients of the difference (or differential) equation of the optimal linear filter are ob- \ntained without further calculations. \n(3) the filtering problem is shown to be the dual of the noise-free regulator pr", "a r t i c l e s\n\nlearning enhances the relative impact of top-down \nprocessing in the visual cortex\nhiroshi makino1 & takaki komiyama1,2\ntheories have proposed that, in sensory cortices, learning can enhance top-down modulation by higher brain areas while reducing \nbottom-up sensory drives. to address circuit mechanisms underlying this process, we examined the activity of layer 2/3 (l2/3) \nexcitatory neurons in the mouse primary visual cortex (v1) as well as l4 excitatory neurons, the main bottom-up source,  \nand long-range top-down projections from the retrosplenial cortex (rsc) during associative learning over days using chronic  \ntwo-photon calcium imaging. during learning, l4 responses gradually weakened, whereas rsc inputs became stronger. \nfurthermore, l2/3 acquired a ramp-up response temporal profile, potentially encoding the timing of the associated event, which \ncoincided with a similar change in rsc inputs. learning also reduced the activity of somatostatin-expressing inhibit", "the journal of comparative neurology 285:54-72  (1989) \n\nterminal arbors of individual \u201cfeedback\u201d \naxons projecting from area v2 to v l  in the \n\nmacaque monkey: a study using \n\nimmunohistochemistry of anterogradely \n\ntransported phaseoh \nvulgaris-leucoagglutinin \n\neye research institute, boston, massachusetts 02115; enrm v.a. hospital, bedford, \n\nkathleen s. rockland and agnes virga \n\nmassachusetts 01730 \n\nabstract \n\nin the present study, the anterograde tracer phaseolus vulgaris-leucoag- \nglutinin (pha-l) was injected into area v2 in order to demonstrate the pre- \ncise morphology  of  individual axons from area v2 to v1.  on the basis of  28 \ncomplete axon reconstructions, several characteristic features have been iden- \ntified.  i) individual  axons arborize  in  multiple layers:  1, 2,  5, and  (incon- \nstantly) 3. a single axon may have numerous terminal clusters in layers 1 and \n2, but at most one in layer 3. 2) axons typically ascend to layer  1, turn asym- \nmetrically in one di", "unsupervised learning by competing hidden units\n\ndmitry krotova,b,1,2 and john j. hop\ufb01eldc,1,2\n\namassachusetts institute of technology\u2013international business machines (ibm) watson arti\ufb01cial intelligence laboratory, ibm research, cambridge, ma\n02142; binstitute for advanced study, princeton, nj 08540; and cprinceton neuroscience institute, princeton university, princeton, nj 08544\n\ncontributed by john j. hop\ufb01eld, february 11, 2019 (sent for review november 30, 2018; reviewed by dmitri b. chklovskii, david j. heeger, and\ndaniel d. lee)\n\nit is widely believed that end-to-end training with the backprop-\nagation algorithm is essential for learning good feature detectors\nin early layers of arti\ufb01cial neural networks, so that these detec-\ntors are useful for the task performed by the higher layers of that\nneural network. at the same time, the traditional form of back-\npropagation is biologically implausible. in the present paper we\npropose an unusual learning rule, which has a degree of biolog", "a map of object space in primate \ninferotemporal cortex\n\nhttps://doi.org/10.1038/s41586-020-2350-5\nreceived: 21 january 2019\naccepted: 17 march 2020\npublished online: 3 june 2020\n\n check for updates\n\npinglei bao1,2\u2009\u2709, liang she1, mason mcgill3 & doris y. tsao1,2,3\u2009\u2709\n\nthe inferotemporal (it) cortex is responsible for object recognition, but it is unclear \nhow the representation of visual objects is organized in this part of the brain. areas \nthat are selective for categories such as faces, bodies, and scenes have been found1\u20135, \nbut large parts of it cortex lack any known specialization, raising the question of  \nwhat general principle governs it organization. here we used functional mri, \nmicrostimulation, electrophysiology, and deep networks to investigate the \norganization of macaque it cortex. we built a low-dimensional object space to \ndescribe general objects using a feedforward deep neural network trained on object \nclassification6. responses of it cells to a large set of objects", "www.elsevier.com/locate/ynimg\nneuroimage 26 (2005) 839 \u2013 851\n\nunified segmentation\njohn ashburnert and karl j. friston\n\nwellcome department of imaging neuroscience, functional imaging labolatory 12 queen square, london, wc1n 3bg, uk\n\nreceived 10 november 2004; revised 2 february 2005; accepted 10 february 2005\navailable online 1 april 2005\n\na probabilistic framework is presented that enables image registration,\ntissue classification, and bias correction to be combined within the\nsame generative model. a derivation of a log-likelihood objective\nfunction for the unified model is provided. the model is based on a\nmixture of gaussians and is extended to incorporate a smooth intensity\nvariation and nonlinear registration with tissue probability maps. a\nstrategy for optimising the model parameters is described, along with\nthe requisite partial derivatives of the objective function.\nd 2005 elsevier inc. all rights reserved.\n\nkeywords:\nmixture of gaussians; tissue probability maps\n\nimage regis", "modular networks:\n\nlearning to decompose neural computation\n\nlouis kirsch\u2217\n\ndepartment of computer science\n\nuniversity college london\nmail@louiskirsch.com\n\njulius kunze\n\ndepartment of computer science\n\nuniversity college london\njuliuskunze@gmail.com\n\ndavid barber\n\ndepartment of computer science\n\nuniversity college london\ndavid.barber@ucl.ac.uk\n\nabstract\n\nscaling model capacity has been vital in the success of deep learning. for a\ntypical network, necessary compute resources and training time grow dramatically\nwith model size. conditional computation is a promising way to increase the\nnumber of parameters with a relatively small increase in resources. we propose\na training algorithm that \ufb02exibly chooses neural modules based on the data to\nbe processed. both the decomposition and modules are learned end-to-end. in\ncontrast to existing approaches, training does not rely on regularization to enforce\ndiversity in module use. we apply modular networks both to image recognition\nand language m", "978-1-4799-9988-0/16/$31.00 \u00a92016 ieee\n\n4960\n\nicassp 2016\n\nauthorized licensed use limited to: university of washington libraries. downloaded on november 16,2023 at 08:21:20 utc from ieee xplore.  restrictions apply. \n\nlisten,attendandspell:aneuralnetworkforlargevocabularyconversationalspeechrecognitionwilliamchancarnegiemellonuniversitynavdeepjaitly,quocle,oriolvinyalsgooglebrainabstractwepresentlisten,attendandspell(las),aneuralspeechrecog-nizerthattranscribesspeechutterancesdirectlytocharacterswith-outpronunciationmodels,hmmsorothercomponentsoftraditionalspeechrecognizers.inlas,theneuralnetworkarchitecturesub-sumestheacoustic,pronunciationandlanguagemodelsmakingitnotonlyanend-to-endtrainedsystembutanend-to-endmodel.incontrasttodnn-hmm,ctcandmostothermodels,lasmakesnoindependenceassumptionsabouttheprobabilitydistributionoftheoutputcharactersequencesgiventheacousticsequence.oursystemhastwocomponents:alistenerandaspeller.thelistenerisapyra-midalrecurrentnetworkencoderthataccepts\ufb01lterba", "on the expressive power of deep neural networks\n\nmaithra raghu 1 2 ben poole 3 jon kleinberg 1 surya ganguli 3 jascha sohl dickstein 2\n\n7\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n8\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n6\nv\n6\n3\n3\n5\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe propose a new approach to the problem of\nneural network expressivity, which seeks to char-\nacterize how structural properties of a neural net-\nwork family affect the functions it is able to com-\npute. our approach is based on an interrelated\nset of measures of expressivity, uni\ufb01ed by the\nnovel notion of trajectory length, which mea-\nsures how the output of a network changes as the\ninput sweeps along a one-dimensional path. our\n\ufb01ndings can be summarized as follows:\n(1) the complexity of the computed function\ngrows exponentially with depth. we de-\nsign measures of expressivity that capture\nthe non-linearity of the computed func-\ntion. due to how the network transforms\nits input, these measures grow exponentially\nwith depth.\n\n(2) all weights are not e", "a multimodal cell census and atlas of the \nmammalian primary motor cortex\n\n       \nhttps://doi.org/10.1038/s41586-021-03950-0\nreceived: 4 october 2020\naccepted: 25 august 2021\npublished online: 6 october 2021\nopen access\n\n check for updates\n\nbrain initiative cell census network (biccn)1*\n\nhere we report the generation of a multimodal cell census and atlas of the mammalian \nprimary motor cortex as the initial product of the brain initiative cell census \nnetwork (biccn). this was achieved by coordinated large-scale analyses of single-cell \ntranscriptomes, chromatin accessibility, dna methylomes, spatially resolved \nsingle-cell transcriptomes, morphological and electrophysiological properties and \ncellular resolution input\u2013output mapping, integrated through cross-modal \ncomputational analysis. our results advance the collective knowledge and \nunderstanding of brain cell-type organization1\u20135. first, our study reveals a unified \nmolecular genetic landscape of cortical cell types that integr", "gaussian processes in machine learning\n\ncarl edward rasmussen\n\nmax planck institute for biological cybernetics, 72076 t\u00a8ubingen, germany\n\nwww home page: http://www.tuebingen.mpg.de/\u223ccarl\n\ncarl@tuebingen.mpg.de\n\nabstract. we give a basic introduction to gaussian process regression\nmodels. we focus on understanding the role of the stochastic process\nand how it is used to de\ufb01ne a distribution over functions. we present\nthe simple equations for incorporating training data and examine how\nto learn the hyperparameters using the marginal likelihood. we explain\nthe practical advantages of gaussian process and end with conclusions\nand a look at the current trends in gp work.\n\nsupervised learning in the form of regression (for continuous outputs) and\nclassi\ufb01cation (for discrete outputs) is an important constituent of statistics and\nmachine learning, either for analysis of data sets, or as a subgoal of a more\ncomplex problem.\n\ntraditionally parametric1 models have been used for this purpose. thes", "reports\n\nthe high affinity of carbon for certain metals (11),\nmolten metal nanoparticles are able to act as\ncatalytic sites for the uptake of carbon with sub-\nsequent carbon nanotube outgrowth (12).\n\nunder continuing heating, we observed nano-\nwire growth over the course of 10 min (fig. 3).\nmovie s2, taken at a higher resolution, better shows\nthe initial stages of growth at high temperature (10).\nthe faceted end of the emerging wire indicates\nthat the free end of the wire is solid during the\ngrowth process.\n\nbecause the walls of the microcrucible con-\ntain the ions that are consumed in the formation\nof the nanowire, as the nanowire grows it would\nbe expected that the dimensions of the micro-\ncrucible itself would change, leading to alteration\nof the nanowire morphology. the continual evo-\nlution of the liquid-solid interface of the micro-\ncrucible is a dynamic process that leads to creep\nof the interface and the concomitant morphogen-\nesis of nonclassical crystal structures. changes in", "mujoco: a physics engine for model-based control\n\nemanuel todorov, tom erez and yuval tassa\n\nuniversity of washington\n\nabstract\u2014 we describe a new physics engine tailored to\nmodel-based control. multi-joint dynamics are represented in\ngeneralized coordinates and computed via recursive algorithms.\ncontact responses are computed via ef\ufb01cient new algorithms\nwe have developed, based on the modern velocity-stepping\napproach which avoids the dif\ufb01culties with spring-dampers.\nmodels are speci\ufb01ed using either a high-level c++ api or an\nintuitive xml \ufb01le format. a built-in compiler transforms the\nuser model into an optimized data structure used for runtime\ncomputation. the engine can compute both forward and inverse\ndynamics. the latter are well-de\ufb01ned even in the presence of\ncontacts and equality constraints. the model can include tendon\nwrapping as well as actuator activation states (e.g. pneumatic\ncylinders or muscles). to facilitate optimal control applications\nand in particular sampling and", "a r t i c l e s\n\nsubtype-specific plasticity of inhibitory circuits  \nin motor cortex during motor learning\nsimon x chen1\u20133, an na kim1\u20133, andrew j peters1\u20133 & takaki komiyama1\u20134\nmotor skill learning induces long-lasting reorganization of dendritic spines, principal sites of excitatory synapses, in the motor \ncortex. however, mechanisms that regulate these excitatory synaptic changes remain poorly understood. here, using in vivo  \ntwo-photon imaging in awake mice, we found that learning-induced spine reorganization of layer (l) 2/3 excitatory neurons occurs \nin the distal branches of their apical dendrites in l1 but not in the perisomatic dendrites. this compartment-specific spine \nreorganization coincided with subtype-specific plasticity of local inhibitory circuits. somatostatin-expressing inhibitory neurons \n(som-ins), which mainly inhibit distal dendrites of excitatory neurons, showed a decrease in axonal boutons immediately after \nthe training began, whereas parvalbumin-expressing", "single-trial neural dynamics are dominated by \nrichly varied movements\n\nsimon musall1,5, matthew t. kaufman1,2,3,5, ashley l. juavinett1,4, steven gluf1 and \nanne k. churchland\u200a\n\n\u200a1*\n\nwhen experts are immersed in a task, do their brains prioritize task-related activity? most efforts to understand neural activ-\nity during well-learned tasks focus on cognitive computations and task-related movements. we wondered whether task-per-\nforming animals explore a broader movement landscape and how this impacts neural activity. we characterized movements \nusing video and other sensors and measured neural activity using widefield and two-photon imaging. cortex-wide activity was \ndominated by movements, especially uninstructed movements not required for the task. some uninstructed movements were \naligned to trial events. accounting for them revealed that neurons with similar trial-averaged activity often reflected utterly \ndifferent combinations of cognitive and movement variables. other movements ", "math. program., ser. a 108, 177\u2013205 (2006)\n\ndigital object identi\ufb01er (doi) 10.1007/s10107-006-0706-8\n\nyurii nesterov \u00b7 b.t. polyak\ncubic regularization of newton method and its global\nperformance(cid:1)\n\nreceived: august 31, 2005 / accepted: january 27, 2006\npublished online: april 25, 2006 \u2013 \u00a9 springer-verlag 2006\n\nabstract. in this paper, we provide theoretical analysis for a cubic regularization of newton method as\napplied to unconstrained minimization problem. for this scheme, we prove general local convergence results.\nhowever, the main contribution of the paper is related to global worst-case complexity bounds for different\nproblem classes including some nonconvex cases. it is shown that the search direction can be computed by\nstandard linear algebra technique.\n\nkey words. general nonlinear optimization \u2013 unconstrained optimization \u2013 newton method \u2013 trust-region\nmethods \u2013 global complexity bounds \u2013 global rate of convergence\n\n1. introduction\n\nmotivation. starting from seminal pap", "available online at www.sciencedirect.com\n\nsystems & control letters 53 (2004) 65 \u2013 78\n\nwww.elsevier.com/locate/sysconle\n\nfast linear iterations for distributed averaging\n\nlin xiao\u2217, stephen boyd\n\ninformation systems laboratory, stanford university, stanford, ca 94305-9510, usa\n\nreceived 21 march 2003; received in revised form 24 february 2004; accepted 25 february 2004\n\nabstract\n\nwe consider the problem of 0nding a linear iteration that yields distributed averaging consensus over a network, i.e., that\nasymptotically computes the average of some initial values given at the nodes. when the iteration is assumed symmetric, the\nproblem of 0nding the fastest converging linear iteration can be cast as a semide0nite program, and therefore e2ciently and\nglobally solved. these optimal linear iterations are often substantially faster than several common heuristics that are based\non the laplacian of the associated graph.\n\nwe show how problem structure can be exploited to speed up interior-point m", "concepts in a probabilistic language of thought\n\nnoah d. goodman\u2217, stanford university\n\njoshua b. tenenbaum, mit\ntobias gerstenberg, mit\n\nfebruary 15, 2014\n\nto appear in concepts: new directions, eds. margolis and laurence, mit press.\n\n1 introduction\n\nknowledge organizes our understanding of the world, determining what we expect\ngiven what we have already seen. our predictive representations have two key prop-\nerties: they are productive, and they are graded. productive generalization is possible\nbecause our knowledge decomposes into concepts\u2014elements of knowledge that are\ncombined and recombined to describe particular situations. gradedness is the observ-\nable effect of accounting for uncertainty\u2014our knowledge encodes degrees of belief\nthat lead to graded probabilistic predictions. to put this a different way, concepts form\na combinatorial system that enables description of many different situations; each such\nsituation speci\ufb01es a distribution over what we expect to see in the world, ", "signsgd: compressed optimisation for non-convex problems\n\njeremy bernstein 1 2 yu-xiang wang 2 3 kamyar azizzadenesheli 4 anima anandkumar 1 2\n\nabstract\n\ntraining large neural networks requires distribut-\ning learning across multiple workers, where the\ncost of communicating gradients can be a signif-\nicant bottleneck. signsgd alleviates this prob-\nlem by transmitting just the sign of each minibatch\nstochastic gradient. we prove that it can get the\nbest of both worlds: compressed gradients and\nsgd-level convergence rate. the relative `1/`2\ngeometry of gradients, noise and curvature in-\nforms whether signsgd or sgd is theoretically\nbetter suited to a particular problem. on the prac-\ntical side we \ufb01nd that the momentum counterpart\nof signsgd is able to match the accuracy and\nconvergence speed of adam on deep imagenet\nmodels. we extend our theory to the distributed\nsetting, where the parameter server uses majority\nvote to aggregate gradient signs from each worker\nenabling 1-bit compression", "article\n\nthe stabilized supralinear network: a unifying\ncircuit motif underlying multi-input integration in\nsensory cortex\n\nhighlights\nd a simple, uni\ufb01ed circuit model of contextual modulation and\n\nnormalization\n\nauthors\n\ndaniel b. rubin, stephen d. van hooser,\nkenneth d. miller\n\nd explains transition from facilitation to suppression w/\n\nincreasing stimulus strength\n\ncorrespondence\nken@neurotheory.columbia.edu\n\nd both excitatory and inhibitory neurons show normalization or\n\nsuppression\n\nd new experiments in v1 con\ufb01rm multiple model predictions\n\nin brief\nresponses to multiple stimuli are\ntypically suppressed relative to summed\nresponses to the individual stimuli, but\nmay facilitate for weak stimuli. rubin et al.\ndemonstrate a \u2018\u2018canonical\u2019\u2019 circuit\nmechanism explaining many aspects of\nmulti-input integration and test key model\npredictions.\n\nrubin et al., 2015, neuron 85, 402\u2013417\njanuary 21, 2015 \u00aa2015 elsevier inc.\nhttp://dx.doi.org/10.1016/j.neuron.2014.12.026\n\n\f", "report\n\na representation of effort in decision-making and\nmotor control\n\nhighlights\nd in reaching, like walking, there is a movement speed that\n\nminimizes energetic cost\n\nauthors\n\nreza shadmehr, helen j. huang, alaa\na. ahmed\n\nd reward makes it worthwhile to be energetically inef\ufb01cient\n\nd effort may be represented objectively via energetic cost and\n\ndiscounted in time\n\nd neural control of decisions and movements may share a\n\ncommon utility\n\ncorrespondence\nalaa.ahmed@colorado.edu\n\nin brief\nboth decisions and movements are\nin\ufb02uenced by reward and effort,\nsuggesting that they may share a\ncommon utility. shadmehr et al.\ndemonstrate that a utility in which effort is\nobjectively represented as energetic cost\nand discounted in time can account for\nboth the choices animals make and the\nvigor of their movements.\n\nshadmehr et al., 2016, current biology 26, 1929\u20131934\njuly 25, 2016 \u00aa 2016 elsevier ltd.\nhttp://dx.doi.org/10.1016/j.cub.2016.05.065\n\n\f", "multi-step reinforcement learning: a unifying algorithm\n\nkristopher de asis,1 j. fernando hernandez-garcia,1 g. zacharias holland,1 richard s. sutton\n\nreinforcement learning and arti\ufb01cial intelligence laboratory, university of alberta\n\n{kldeasis,jfhernan,gholland,rsutton}@ualberta.ca\n\n8\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n1\n1\n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n2\nv\n7\n2\n3\n1\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nunifying seemingly disparate algorithmic ideas to produce bet-\nter performing algorithms has been a longstanding goal in re-\ninforcement learning. as a primary example, td(\u03bb) elegantly\nuni\ufb01es one-step td prediction with monte carlo methods\nthrough the use of eligibility traces and the trace-decay param-\neter \u03bb. currently, there are a multitude of algorithms that can\nbe used to perform td control, including sarsa, q-learning,\nand expected sarsa. these methods are often studied in the\none-step case, but they can be extended across multiple time\nsteps to achieve better performance. each of these algorithms\nis s", "understanding image representations\n\nby measuring their equivariance and equivalence\n\nkarel lenc\n\nandrea vedaldi\n\ndepartment of engineering science, university of oxford\n\n5\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n2\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n8\n0\n9\n5\n\n.\n\n1\n1\n4\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\ndespite the importance of image representations such as\nhistograms of oriented gradients and deep convolutional\nneural networks (cnn), our theoretical understanding of\nthem remains limited. aiming at \ufb01lling this gap, we inves-\ntigate three key mathematical properties of representations:\nequivariance, invariance, and equivalence. equivariance\nstudies how transformations of the input image are encoded\nby the representation, invariance being a special case where\na transformation has no effect. equivalence studies whether\ntwo representations, for example two different parametrisa-\ntions of a cnn, capture the same visual information or not.\na number of methods to establish these properties empir-\nically are proposed, including ", "original research\npublished: 27 december 2016\ndoi: 10.3389/fncom.2016.00131\n\nrepresentational distance learning\nfor deep neural networks\n\npatrick mcclure * and nikolaus kriegeskorte\n\nmrc cognition and brain sciences unit, cambridge, uk\n\ndeep neural networks (dnns) provide useful models of visual\nrepresentational\ntransformations. we present a method that enables a dnn (student) to learn from\nthe internal representational spaces of a reference model (teacher), which could be\nanother dnn or,\nin the future, a biological brain. representational spaces of the\nstudent and the teacher are characterized by representational distance matrices (rdms).\nwe propose representational distance learning (rdl), a stochastic gradient descent\nmethod that drives the rdms of the student to approximate the rdms of the teacher.\nwe demonstrate that rdl is competitive with other transfer learning techniques for\ntwo publicly available benchmark computer vision datasets (mnist and cifar-100),\nwhile allowing for arc", "ne40ch25-foster\n\nari\n\n8 july 2017\n\n8:23\n\nreplay comes of age\ndavid j. foster\ndepartment of psychology and helen wills neuroscience institute, university of california,\nberkeley, california 94720; email: davidfoster@berkeley.edu\n\nannual \n\n reviews further\n\nclick here to view this article's \nonline features:\n\u2022 download \ufb01gures as ppt slides\n\u2022 navigate linked references\n\u2022 download citations\n\u2022 explore related articles\n\u2022 search keywords\n\nannu. rev. neurosci. 2017. 40:581\u2013602\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nhttps://doi.org/10.1146/annurev-neuro-072116-\n031538\ncopyright c(cid:2) 2017 by annual reviews.\nall rights reserved\n\nkeywords\nhippocampus, place cell, replay, memory\n\nabstract\nhippocampal place cells take part in sequenced patterns of reactivation af-\nter behavioral experience, known as replay. since replay was \ufb01rst reported,\nnearly 20 years ago, many new results have been found, necessitating revi-\nsion of the original interpretations. we review so", "66\n\nieeeproc. conf. oncomp. vis. patt. recog. (cvpr'97)\n\ngradient vector flow: a new external force for snakes\n\nchenyang xu and jerry l. prince\n\ndepartment of electrical and computer engineering\nthe johns hopkins university, baltimore, md 21218\n\nabstract\n\nsnakes, or active contours, are used extensively in com-\nputer vision and image processing applications, particu-\nlarly to locate object boundaries. problems associated with\ninitialization and poor convergence to concave boundaries,\nhowever, have limited their utility. this paper develops a\nnew external force for active contours, largely solving both\nproblems. this external force, which we call gradient vec-\ntor \ufb02ow (gvf), is computed as a diffusion of the gradient\nvectors of a gray-level or binary edge map derived from the\nimage. the resultant \ufb01eld has a large capture range and\nforces active contours into concave regions. examples on\nsimulated images and one real image are presented.\n\n1 introduction\n\nsnakes [10], or active contours, ", "identifying and attacking the saddle point\n\nproblem in high-dimensional non-convex optimization\n\nyann n. dauphin razvan pascanu caglar gulcehre kyunghyun cho\n\nuniversit\u00b4e de montr\u00b4eal\n\ndauphiya@iro.umontreal.ca, r.pascanu@gmail.com,\n\ngulcehrc@iro.umontreal.ca, kyunghyun.cho@umontreal.ca\n\nsurya ganguli\n\nstanford university\n\nsganguli@standford.edu\n\nyoshua bengio\n\nuniversit\u00b4e de montr\u00b4eal, cifar fellow\nyoshua.bengio@umontreal.ca\n\nabstract\n\na central challenge to many fields of science and engineering involves minimizing\nnon-convex error functions over continuous, high dimensional spaces. gradient descent\nor quasi-newton methods are almost ubiquitously used to perform such minimizations,\nand it is often thought that a main source of difficulty for these local methods to find\nthe global minimum is the proliferation of local minima with much higher error than\nthe global minimum. here we argue, based on results from statistical physics, random\nmatrix theory, neural network theory, and empiric", "taking the human out of the loop:\na review of bayesian optimization\n\nbobak shahriari, kevin swersky, ziyu wang, ryan p. adams and nando de freitas\n\n1\n\nabstract\u2014big data applications are typically associated with\nsystems involving large numbers of users, massive complex\nsoftware systems, and large-scale heterogeneous computing and\nstorage architectures. the construction of such systems involves\nmany distributed design choices. the end products (e.g., rec-\nommendation systems, medical analysis tools, real-time game\nengines, speech recognizers) thus involves many tunable con\ufb01g-\nuration parameters. these parameters are often speci\ufb01ed and\nhard-coded into the software by various developers or teams.\nif optimized jointly, these parameters can result in signi\ufb01cant\nimprovements. bayesian optimization is a powerful tool for\nthe joint optimization of design choices that is gaining great\npopularity in recent years. it promises greater automation so as\nto increase both product quality and human pro", "bipartite expander hop\ufb01eld networks as\n\nself-decoding high-capacity error correcting codes\n\nrishidev chaudhuri\n\ncenter for neuroscience,\n\ndepartments of mathematics and\n\nneurobiology, physiology and behavior,\n\nuniversity of california, davis,\n\ndavis, ca 95616\n\nrchaudhuri@ucdavis.edu\n\nila fiete\n\nbrain and cognitive sciences,\n\nmassachusetts institute of technology,\n\ncambridge, ma 02139\n\nfiete@mit.edu\n\nabstract\n\nneural network models of memory and error correction famously include the hop-\n\ufb01eld network, which can directly store\u2014and error-correct through its dynamics\u2014\narbitrary n-bit patterns, but only for \u223c n such patterns. on the other end of\nthe spectrum, shannon\u2019s coding theory established that it is possible to represent\nexponentially many states (\u223c en ) using n symbols in such a way that an optimal\ndecoder could correct all noise upto a threshold. we prove that it is possible to\nconstruct an associative content-addressable network that combines the properties\nof strong error correcti", "the journal of neuroscience, december 1, 1998, 18(23):9870\u20139895\n\ncorrelation-based development of ocularly matched orientation\nand ocular dominance maps: determination of required\ninput activities\n\ned erwin1,4 and kenneth d. miller1,2,3,4,5\ndepartments of 1physiology and 2otolaryngology, 3neuroscience graduate program, 4w. m. keck center for integrative\nneuroscience, and 5sloan center for theoretical neurobiology, university of california, san francisco,\ncalifornia 94143-0444\n\nwe extend previous models for separate development of ocular\ndominance and orientation selectivity in cortical\nlayer 4 by\nexploring conditions permitting combined organization of both\nproperties. these conditions are expressed in terms of func-\ntions describing the degree of correlation in the \ufb01ring of two\ninputs from the lateral geniculate nucleus (lgn), as a function\nof their retinotopic separation and their \u201ctype\u201d (on center or\noff center and left eye or right eye).\n\nthe development of ocular dominance require", "6\n1\n0\n2\n\n \nr\na\n\n \n\nm\n9\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n6\n4\n9\n5\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2016\n\nacdc: a structured efficient linear layer\n\nmarcin moczulski1 misha denil1\n\njeremy appleyard2 nando de freitas1,3\n\n1university of oxford\n2nvidia 3cifar\nmarcin.moczulski@stcatz.ox.ac.uk\nmisha.denil@gmail.com\njappleyard@nvidia.com\nnando.de.freitas@cs.ox.ac.uk\n\nabstract\n\nthe linear layer is one of the most pervasive modules in deep learning representa-\ntions. however, it requires o(n 2) parameters and o(n 2) operations. these costs\ncan be prohibitive in mobile applications or prevent scaling in many domains.\nhere, we introduce a deep, differentiable, fully-connected neural network module\ncomposed of diagonal matrices of parameters, a and d, and the discrete cosine\ntransform c. the core module, structured as acdc\u22121, has o(n ) parameters\nand incurs o(n log n ) operations. we present theoretical results showing how\ndeep cascades of acdc layers approximate line", "corrected: author correction\n\nperceptual straightening of natural videos\n\nolivier j. h\u00e9naff\u200a\n\n\u200a1,6*, robbe l. t. goris2 and eero p. simoncelli1,3,4,5\n\nmany behaviors rely on predictions derived from recent visual input, but the temporal evolution of those inputs is generally \ncomplex and difficult to extrapolate. we propose that the visual system transforms these inputs to follow straighter temporal \ntrajectories. to test this \u2018temporal straightening\u2019 hypothesis, we develop a methodology for estimating the curvature of an \ninternal trajectory from human perceptual judgments. we use this to test three distinct predictions: natural sequences that are \nhighly curved in the space of pixel intensities should be substantially straighter perceptually; in contrast, artificial sequences \nthat are straight in the intensity domain should be more curved perceptually; finally, naturalistic sequences that are straight  \nin the intensity domain should be relatively less curved. perceptual data valida", "6\n1\n0\n2\n\n \n\nb\ne\nf\n5\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n2\n5\n9\n5\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2016\n\nprioritized experience replay\n\ntom schaul, john quan, ioannis antonoglou and david silver\ngoogle deepmind\n{schaul,johnquan,ioannisa,davidsilver}@google.com\n\nabstract\n\nexperience replay lets online reinforcement learning agents remember and reuse\nexperiences from the past. in prior work, experience transitions were uniformly\nsampled from a replay memory. however, this approach simply replays transitions\nat the same frequency that they were originally experienced, regardless of their\nsigni\ufb01cance. in this paper we develop a framework for prioritizing experience,\nso as to replay important transitions more frequently, and therefore learn more\nef\ufb01ciently. we use prioritized experience replay in deep q-networks (dqn), a\nreinforcement learning algorithm that achieved human-level performance across\nmany atari games. dqn with prioritized experience replay achieve", "available online at www.sciencedirect.com\n\nsciencedirect\n\ncurrent opinion in\n\nneurobiology\n\ngrounding neuroscience in behavioral changes using\nartificial neural networks\ngrace w. lindsay\n\nabstract\nconnecting neural activity to function is a common aim in\nneuroscience. how to define and conceptualize function,\nhowever, can vary. here i focus on grounding this goal in the\nspecific question of how a given change in behavior is pro-\nduced by a change in neural circuits or activity. artificial neural\nnetwork models offer a particularly fruitful format for tackling\nsuch questions because they use neural mechanisms to\nperform complex transformations and produce appropriate\nbehavior. therefore, they can be a means of causally testing\nthe extent to which a neural change can be responsible for an\nexperimentally observed behavioral change. furthermore,\nbecause the field of interpretability in artificial intelligence has\nsimilar aims, neuroscientists can look to interpretability\nmethods for new wa", "3\n2\n0\n2\n\n \nt\nc\no\n9\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n7\n1\n2\n5\n0\n\n.\n\n1\n0\n3\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2023\n\nprogress measures for grokking via\nmechanistic interpretability\n\nneel nanda\u2217, \u2020\n\nlawrence chan\u2021\n\ntom lieberum\u2020\n\njess smith\u2020\n\njacob steinhardt\u2021\n\nabstract\n\nneural networks often exhibit emergent behavior, where qualitatively new capa-\nbilities arise from scaling up the amount of parameters, training data, or training\nsteps. one approach to understanding emergence is to find continuous progress\nmeasures that underlie the seemingly discontinuous qualitative changes. we ar-\ngue that progress measures can be found via mechanistic interpretability: reverse-\nengineering learned behaviors into their individual components. as a case study,\nwe investigate the recently-discovered phenomenon of \u201cgrokking\u201d exhibited by\nsmall transformers trained on modular addition tasks. we fully reverse engineer\nthe algorithm learned by these networks, which uses discrete fourier tr", "research article\n\ncan sleep protect memories from\ncatastrophic forgetting?\noscar c gonza\u00b4 lez1\u2020, yury sokolov1\u2020, giri p krishnan1, jean erik delanois1,2,\nmaxim bazhenov1*\n\n1department of medicine, university of california, san diego, la jolla, united\nstates; 2department of computer science and engineering, university of california,\nsan diego, la jolla, united states\n\nabstract continual learning remains an unsolved problem in artificial neural networks. the brain\nhas evolved mechanisms to prevent catastrophic forgetting of old knowledge during new training.\nbuilding upon data suggesting the importance of sleep in learning and memory, we tested a\nhypothesis that sleep protects old memories from being forgotten after new learning. in the\nthalamocortical model, training a new memory interfered with previously learned old memories\nleading to degradation and forgetting of the old memory traces. simulating sleep after new\nlearning reversed the damage and enhanced old and new memories. we foun", "proc. natl. acad. sci. usa\nvol. 81, pp. 3088-3092, may 1984\nbiophysics\n\nneurons with graded response have collective computational\nproperties like those of two-state neurons\n\n(associative memory/neural network/stability/action potentials)\n\nj. j. hopfield\ndivisions of chemistry and biology, california institute of technology, pasadena, ca 91125; and bell laboratories, murray hill, nj 07974\n\ncontributed by j. j. hopfield, february 13, 1984\n\na model for a large network of \"neurons\"\nabstract\nwith a graded response (or sigmoid input-output relation) is\nstudied. this deterministic system has collective properties in\nvery close correspondence with the earlier stochastic model\nbased on mcculloch-pitts neurons. the content-addressable\nmemory and other emergent collective properties of the origi-\nnal model also are present in the graded response model. the\nidea that such collective properties are used in biological sys-\ntems is given added credence by the continued presence of such\nproperties fo", "available online at www.sciencedirect.com\n\nsciencedirect\n\ncurrent opinion in\n\nneurobiology\n\nneural population geometry: an approach for\nunderstanding biological and artificial neural networks\nsueyeon chung and l. f. abbott\n\nabstract\nadvances in experimental neuroscience have transformed our\nability to explore the structure and function of neural circuits. at\nthe same time, advances in machine learning have unleashed\nthe remarkable computational power of artificial neural net-\nworks (anns). while these two fields have different tools and\napplications, they present a similar challenge: namely, under-\nstanding how information is embedded and processed through\nhigh-dimensional representations to solve complex tasks. one\napproach to addressing this challenge is to utilize mathemat-\nical and computational tools to analyze the geometry of these\nhigh-dimensional representations, i.e., neural population ge-\nometry. we review examples of geometrical approaches\nproviding insight into the function", "measuring invariances in deep networks\n\nian j. goodfellow, quoc v. le, andrew m. saxe, honglak lee, andrew y. ng\n\ncomputer science department\n\nstanford university\nstanford, ca 94305\n\n{ia3n,quocle,asaxe,hllee,ang}@cs.stanford.edu\n\nabstract\n\nfor many pattern recognition tasks, the ideal input feature would be invariant to\nmultiple confounding properties (such as illumination and viewing angle, in com-\nputer vision applications). recently, deep architectures trained in an unsupervised\nmanner have been proposed as an automatic method for extracting useful features.\nhowever, it is dif\ufb01cult to evaluate the learned features by any means other than\nusing them in a classi\ufb01er. in this paper, we propose a number of empirical tests\nthat directly measure the degree to which these learned features are invariant to\ndifferent input transformations. we \ufb01nd that stacked autoencoders learn modestly\nincreasingly invariant features with depth when trained on natural images. we \ufb01nd\nthat convolutional deep b", "further\nannual\nreviews\nclick here for quick links to \nannual reviews content online, \nincluding:\n\u2022 other articles in this volume\n\u2022 top cited articles\n\u2022 top downloaded articles\n\u2022 our comprehensive search\n\nplace cells, grid cells,\nand the brain\u2019s spatial\nrepresentation system\n\nedvard i. moser,1 emilio kropff,1,2\nand may-britt moser1\n1kavli institute for systems neuroscience and centre for the biology of memory,\nnorwegian university of science and technology, 7489 trondheim, norway\n2cognitive neuroscience sector, international school for advanced studies, trieste, italy;\nemail: edvard.moser@cbm.ntnu.no\n\nannu. rev. neurosci. 2008. 31:69\u201389\n\nfirst published online as a review in advance on\nfebruary 19, 2008\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev.neuro.31.061307.090723\ncopyright c(cid:2) 2008 by annual reviews.\nall rights reserved\n\n0147-006x/08/0721-0069$20.00\n\nkey words\nhippocampus, entorhinal cortex, path integration, at", "ef\ufb01cient neural architecture search via parameter sharing\n\n8\n1\n0\n2\n\n \n\nb\ne\nf\n2\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n8\n6\n2\n3\n0\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nhieu pham * 1 2 melody y. guan * 3 barret zoph 1 quoc v. le 1 jeff dean 1\n\nabstract\n\nwe propose ef\ufb01cient neural architecture search\n(enas), a fast and inexpensive approach for au-\ntomatic model design. in enas, a controller dis-\ncovers neural network architectures by searching\nfor an optimal subgraph within a large computa-\ntional graph. the controller is trained with pol-\nicy gradient to select a subgraph that maximizes\nthe expected reward on a validation set. mean-\nwhile the model corresponding to the selected\nsubgraph is trained to minimize a canonical cross\nentropy loss. sharing parameters among child\nmodels allows enas to deliver strong empiri-\ncal performances, while using much fewer gpu-\nhours than existing automatic model design ap-\nproaches, and notably, 1000x less expensive than\nstandard neural architecture search. on the\npenn tre", "neuroscience 282 (2014) 248\u2013257\n\nreview\n\nthe place of dopamine in the cortico-basal ganglia circuit\n\ns. n. haber *\n\nsignificance of\n\ndepartment of pharmacology and physiology, university of\nrochester school of medicine, 601 elmwood avenue, rochester,\nny 14642, united states\n\nfunctions:\n\nabstract\u2014the midbrain dopamine (da) neurons play a cen-\ntral role in developing appropriate goal-directed behaviors,\nincluding the motivation and cognition to develop appropri-\nate actions to obtain a speci\ufb01c outcome. indeed, subpopula-\ntions of da neurons have been associated with these\ndi\ufb00erent\nthe mesolimbic, mesocortical, and\nnigrostriatal pathways. the mesolimbic and nigrostriatal\npathways are an integral part of the basal ganglia through\nits reciprocal connections to the ventral and dorsal striatum\nrespectively. this chapter reviews the connections of the\nmidbrain da cells and their role in integrating information\nacross limbic, cognitive and motor functions. emphasis is\nplaced on the interface be", "j neurophysiol 102: 1315\u20131330, 2009.\nfirst published march 18, 2009; doi:10.1152/jn.00097.2009.\n\ninnovative methodolgy\n\nfactor-analysis methods for higher-performance neural prostheses\n\ngopal santhanam,1 byron m. yu,1,2,6 vikash gilja,3 stephen i. ryu,1,4 afsheen afshar,1,5 maneesh sahani,6\nand krishna v. shenoy1,2\n1department of electrical engineering, 2neurosciences program, 3department of computer science, 4department of neurosurgery,\n5medical scientist training program, stanford university, stanford, california; and 6gatsby computational neuroscience unit, university\ncollege london, london, united kingdom\n\nsubmitted 2 february 2009; accepted in \ufb01nal form 19 march 2009\n\nsanthanam g, yu bm, gilja v, ryu si, afshar a, sahani m,\nshenoy kv. factor-analysis methods for higher-performance neural\nprostheses. j neurophysiol 102: 1315\u20131330, 2009. first published\nmarch 18, 2009; doi:10.1152/jn.00097.2009. neural prostheses aim to\nprovide treatment options for individuals with nervous-system d", "ne33ch05-shadmehr\n\nari\n\n11 march 2010\n\n14:32\n\ne\n\nr\n\nv\n\ni e\n\nw\n\ns\n\ni\n\nn\n\na d v\n\ne\nc\n\nn\n\na\n\nannu. rev. neurosci. 2010. 33:89\u2013108\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev-neuro-060909-153135\ncopyright c(cid:2) 2010 by annual reviews.\nall rights reserved\n\n0147-006x/10/0721-0089$20.00\n\nerror correction, sensory\nprediction, and adaptation\nin motor control\nreza shadmehr,1 maurice a. smith,2\nand john w. krakauer3\n1department of biomedical engineering, johns hopkins school of medicine, baltimore,\nmaryland 21205; email: shadmehr@jhu.edu\n2school of engineering and applied science, harvard university, cambridge,\nmassachusetts 02138\n3the neurological institute, columbia university school college of physicians and\nsurgeons, new york, ny 10032\n\nkey words\nforward models, reaching, saccades, motor adaptation, error feedback,\nsensorimotor integration\n\nabstract\nmotor control is the study of how organisms make accurate goal-\ndirected movem", "8\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n8\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n2\n2\n8\n1\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nrelational recurrent neural networks\n\nadam santoro*\u03b1, ryan faulkner*\u03b1, david raposo*\u03b1, jack rae\u03b1\u03b2, mike chrzanowski\u03b1,\n\nth\u00e9ophane weber\u03b1, daan wierstra\u03b1, oriol vinyals\u03b1, razvan pascanu\u03b1, timothy lillicrap\u03b1\u03b2\n\n*equal contribution\n\n\u03b1deepmind\n\nlondon, united kingdom\n\n\u03b2complex, computer science, university college london\n\nlondon, united kingdom\n\n{adamsantoro; rfaulk; draposo; jwrae; chrzanowskim;\n\ntheophane; weirstra; vinyals; razp; countzero}@google.com\n\nabstract\n\nmemory-based neural networks model temporal data by leveraging an ability to\nremember information for long periods. it is unclear, however, whether they also\nhave an ability to perform complex relational reasoning with the information they\nremember. here, we \ufb01rst con\ufb01rm our intuitions that standard memory architectures\nmay struggle at tasks that heavily involve an understanding of the ways in which\nentities are connected \u2013 i.e., tasks involvin", "letters\n\nvol 466 | 29 july 2010 | doi:10.1038/nature09159\n\nregulation of parkinsonian motor behaviours by\noptogenetic control of basal ganglia circuitry\nalexxai v. kravitz1, benjamin s. freeze1,4,5, philip r. l. parker1,3, kenneth kay1,5, myo t. thwin1, karl deisseroth6\n& anatol c. kreitzer1,2,3,4,5\n\nneural circuits of the basal ganglia are critical for motor planning\nand action selection1\u20133. two parallel basal ganglia pathways have\nbeen described4, and have been proposed to exert opposing influ-\nences on motor function5\u20137. according to this classical model,\nactivation of the \u2018direct\u2019 pathway facilitates movement and activa-\ntion of the \u2018indirect\u2019 pathway inhibits movement. however, more\nrecent anatomical and functional evidence has called into question\nthe validity of this hypothesis8\u201310. because this model has never\nbeen empirically tested, the specific function of these circuits in\nbehaving animals remains unknown. here we report direct activa-\ntion of basal ganglia circuitry in viv", "research article\n\nsingle-cell transcriptomic evidence for\ndense intracortical neuropeptide\nnetworks\nstephen j smith1*, uygar su\u00a8 mbu\u00a8 l1, lucas t graybuck1, forrest collman1,\nsharmishtaa seshamani1, rohan gala1, olga gliko1, leila elabbady1,\njeremy a miller1, trygve e bakken1, jean rossier2, zizhen yao1, ed lein1,\nhongkui zeng1, bosiljka tasic1, michael hawrylycz1*\n\n1allen institute for brain science, seattle, united states; 2neuroscience paris seine,\nsorbonne universite\u00b4 , paris, france\n\nabstract seeking new insights into the homeostasis, modulation and plasticity of cortical\nsynaptic networks, we have analyzed results from a single-cell rna-seq study of 22,439 mouse\nneocortical neurons. our analysis exposes transcriptomic evidence for dozens of molecularly\ndistinct neuropeptidergic modulatory networks that directly interconnect all cortical neurons. this\nevidence begins with a discovery that transcripts of one or more neuropeptide precursor (npp) and\none or more neuropeptide-selectiv", "e\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\ne\nr\nu\nt\na\nn\nm\no\nc\n.\ne\nr\nu\nt\na\nn\nw\nw\nw\n\n/\n\n.\n\n/\n/\n:\np\nt\nt\nh\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n5\n0\n0\n2\n\u00a9\n\n \n\na r t i c l e s\n\ncomputation and systems\n\nuncertainty-based competition between prefrontal and\ndorsolateral striatal systems for behavioral control\n\nnathaniel d daw1, yael niv1,2 & peter dayan1\n\na broad range of neural and behavioral data suggests that the brain contains multiple systems for behavioral choice, including\none associated with prefrontal cortex and another with dorsolateral striatum. however, such a surfeit of control raises an additional\nchoice problem: how to arbitrate between the systems when they disagree. here, we consider dual-action choice systems from a\nnormative perspective, using the computational theory of reinforcement learning. we identify a key trade-off pitting computational\nsimplicity against the \ufb02exible and statistically ef\ufb01cient use of experience. the trade-off is realized in a competition between the\ndor", "research\n\nresearch articles \u25e5\n\ncognitive science\n\nhuman-level concept learning\nthrough probabilistic\nprogram induction\n\nbrenden m. lake,1* ruslan salakhutdinov,2 joshua b. tenenbaum3\n\nnew concept, and even children can make mean-\ningful generalizations via \u201cone-shot learning\u201d\n(1\u20133). in contrast, many of the leading approaches\nin machine learning are also the most data-hungry,\nespecially \u201cdeep learning\u201d models that have\nachieved new levels of performance on object\nand speech recognition benchmarks (4\u20139). sec-\nond, people learn richer representations than\nmachines do, even for simple concepts (fig. 1b),\nusing them for a wider range of functions, in-\ncluding (fig. 1, ii) creating new exemplars (10),\n(fig. 1, iii) parsing objects into parts and rela-\ntions (11), and (fig. 1, iv) creating new abstract\ncategories of objects based on existing categories\n(12, 13). in contrast, the best machine classifiers\ndo not perform these additional functions, which\nare rarely studied and usually require s", "5\n1\n0\n2\n\n \nr\np\na\n5\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n6\n5\n8\n6\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2015\n\nobject detectors emerge in deep scene cnns\n\nbolei zhou, aditya khosla, agata lapedriza, aude oliva, antonio torralba\ncomputer science and arti\ufb01cial intelligence laboratory, mit\n{bolei,khosla,agata,oliva,torralba}@mit.edu\n\nabstract\n\nwith the success of new computational architectures for visual processing, such as\nconvolutional neural networks (cnn) and access to image databases with millions\nof labeled examples (e.g., imagenet, places), the state of the art in computer vision\nis advancing rapidly. one important factor for continued progress is to understand\nthe representations that are learned by the inner layers of these deep architectures.\nhere we show that object detectors emerge from training cnns to perform scene\nclassi\ufb01cation. as scenes are composed of objects, the cnn for scene classi\ufb01ca-\ntion automatically discovers meaningful objects detectors, re", "can the brain do backpropagation?\n\n\u2014 exact implementation of backpropagation in\n\npredictive coding networks\n\nyuhang song1, thomas lukasiewicz1, zhenghua xu2,\u2217, rafal bogacz3\n\n1department of computer science, university of oxford, uk\n\n2state key laboratory of reliability and intelligence of electrical equipment,\n\nhebei university of technology, tianjin, china\n\n3mrc brain network dynamics unit, university of oxford, uk\n\nyuhang.song@some.ox.ac.uk, thomas.lukasiewicz@cs.ox.ac.uk,\n\nzhenghua.xu@hebut.edu.cn, rafal.bogacz@ndcn.ox.ac.uk\n\nabstract\n\nbackpropagation (bp) has been the most successful algorithm used to train arti\ufb01cial\nneural networks. however, there are several gaps between bp and learning in\nbiologically plausible neuronal networks of the brain (learning in the brain, or\nsimply bl, for short), in particular, (1) it has been unclear to date, if bp can\nbe implemented exactly via bl, (2) there is a lack of local plasticity in bp, i.e.,\nweight updates require information that is not l", "information sciences 328 (2016) 26\u201341\n\ncontents lists available at sciencedirect\n\ninformation sciences\n\njournal homepage: www.elsevier.com/locate/ins\n\nintrinsic dimension estimation: advances and open problems\n\n\u2217\nfrancesco camastra\n, antonino staiano\n\ndepartment of science and technology, university of naples parthenope, centro direzionale isola c4 - 80143 napoli, italy\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\ndimensionality reduction methods are preprocessing techniques used for coping with high\ndimensionality. they have the aim of projecting the original data set of dimensionality n,\nwithout information loss, onto a lower m-dimensional submanifold. since the value of m is\nunknown, techniques that allow knowing in advance the value of m, called intrinsic dimen-\nsion (id), are quite useful. the aim of the paper is to review state-of-the-art of the methods of\nid estimation, underlining the recent advances and the open problems.\n\n\u00a9 2015 elsevier inc. all rights reserved.\n\narticle histor", "a framework for studying synaptic plasticity\n\nwith neural spike train data\n\nscott w. linderman\nharvard university\n\ncambridge, ma 02138\n\nchristopher h. stock\n\nharvard college\n\ncambridge, ma 02138\n\nryan p. adams\nharvard university\n\ncambridge, ma 02138\n\nswl@seas.harvard.edu\n\ncstock@post.harvard.edu\n\nrpa@seas.harvard.edu\n\nabstract\n\nlearning and memory in the brain are implemented by complex, time-varying\nchanges in neural circuitry. the computational rules according to which synaptic\nweights change over time are the subject of much research, and are not precisely\nunderstood. until recently, limitations in experimental methods have made it chal-\nlenging to test hypotheses about synaptic plasticity on a large scale. however, as\nsuch data become available and these barriers are lifted, it becomes necessary\nto develop analysis techniques to validate plasticity models. here, we present\na highly extensible framework for modeling arbitrary synaptic plasticity rules\non spike train data in populati", "published as a conference paper at iclr 2017\n\nmultilayer recurrent network models of pri-\nmate retinal ganglion cell responses\n\neleanor batty, josh merel \u2217\ndoctoral program in neurobiology & behavior, columbia university\nerb2180@columbia.edu,jsmerel@gmail.com\n\nnora brackbill *\ndepartment of physics, stanford university\nnbrack@stanford.edu\n\nalexander heitman\nneurosciences graduate program, university of california, san diego\nalexkenheitman@gmail.com\n\nalexander sher & alan litke\nsanta cruz institute for particle physics, university of california, santa cruz\nsashake3@ucsc.edu, alan.litke@cern.ch\n\ne.j. chichilnisky\ndepartment of neurosurgery and hansen experimental physics laboratory, stanford university\nej@stanford.edu\n\nliam paninski\ndepartments of statistics and neuroscience, columbia university\nliam@stat.columbia.edu\n\nabstract\n\ndeveloping accurate predictive models of sensory neurons is vital to understanding\nsensory processing and brain computations. the current standard approach to\nmo", "published as a conference paper at iclr 2020\n\nlearning to solve the credit assignment\nproblem\n\nbenjamin james lansdell\ndepartment of bioengineering\nuniversity of pennsylvania\npennsylvania, pa 19104\nlansdell@seas.upenn.edu\n\nkonrad paul kording\ndepartment of bioengineering\nuniversity of pennsylvania\npennsylvania, pa 19104\n\nprashanth ravi prakash\ndepartment of bioengineering\nuniversity of pennsylvania\npennsylvania, pa 19104\n\nabstract\n\nbackpropagation is driving today\u2019s arti\ufb01cial neural networks (anns). however,\ndespite extensive research, it remains unclear if the brain implements this algo-\nrithm. among neuroscientists, reinforcement learning (rl) algorithms are often\nseen as a realistic alternative: neurons can randomly introduce change, and use un-\nspeci\ufb01c feedback signals to observe their effect on the cost and thus approximate\ntheir gradient. however, the convergence rate of such learning scales poorly with\nthe number of involved neurons. here we propose a hybrid learning approach.\ne", "7\n1\n0\n2\n\n \nr\np\na\n7\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n4\nv\n3\n4\n7\n6\n0\n\n.\n\n5\n0\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\ninductive bias of deep convolutional\nnetworks through pooling geometry\n\nnadav cohen & amnon shashua\n{cohennadav,shashua}@cs.huji.ac.il\n\nabstract\n\nour formal understanding of the inductive bias that drives the success of convo-\nlutional networks on computer vision tasks is limited. in particular, it is unclear\nwhat makes hypotheses spaces born from convolution and pooling operations so\nsuitable for natural images. in this paper we study the ability of convolutional\nnetworks to model correlations among regions of their input. we theoretically\nanalyze convolutional arithmetic circuits, and empirically validate our \ufb01ndings\non other types of convolutional networks as well. correlations are formalized\nthrough the notion of separation rank, which for a given partition of the input,\nmeasures how far a function is from being separable. we show that a polynomi-\nall", "neural path features and neural path kernel :\nunderstanding the role of gates in deep learning\n\nchandrashekar lakshminarayanan\u21e4 and amit vikram singh\u21e4,\n\nindian institute of technology palakkad\n\nchandru@iitpkd.ac.in, amitkvikram@gmail.com\n\nabstract\n\nrecti\ufb01ed linear unit (relu) activations can also be thought of as gates, which,\neither pass or stop their pre-activation input when they are on (when the pre-\nactivation input is positive) or off (when the pre-activation input is negative)\nrespectively. a deep neural network (dnn) with relu activations has many gates,\nand the on/off status of each gate changes across input examples as well as network\nweights. for a given input example, only a subset of gates are active, i.e., on, and the\nsub-network of weights connected to these active gates is responsible for producing\nthe output. at randomised initialisation, the active sub-network corresponding to\na given input example is random. during training, as the weights are learnt, the\nactive sub-", "ly charged air. several clouds growing over\nthe negative charge source developed exten-\nsive regions containing positive charge in\ntheir lower portions. an example is shown\nin fig. 2. the measurements reported for\n1985 were made with ground-based equip-\nment because the instrumented airplane and\nballoon systems used in 1984 were not\navailable.\n\nit is conceivable that the abnormal polari-\nexhibited by these\nty of electrification\nstorms may have been an unusual natural\noccurrence that was not related to the artifi-\ncial release of negative space charge during\nthe growing period of the thundercloud.\nthis is unlikely, however. observations in\nthis region of new mexico over the past 20\nyears show that this polarity of electrifica-\ntion very rarely, if ever, occurs naturally in\nisolated clouds. of the approximately 1000\nfields have\nthunderstorms whose electric\nbeen observed, no isolated cloud above us\nhas exhibited abnormal polarity. that three\nabnormal storms would have occurred by\nchance d", "the role of population structure in computations \nthrough neural dynamics\n\nalexis dubreuil\u200a\nand srdjan ostojic\u200a\n\n\u200a1\u2009\u2709\n\n\u200a1,2,6\u2009\u2709, adrian valente\u200a\n\n\u200a1,6\u2009\u2709, manuel beiran1,3, francesca mastrogiuseppe\u200a\n\n\u200a4,5  \n\nneural computations are currently investigated using two separate approaches: sorting neurons into functional subpopula-\ntions or examining the low-dimensional dynamics of collective activity. whether and how these two aspects interact to shape \ncomputations is currently unclear. using a novel approach to extract computational mechanisms from networks trained on neu-\nroscience tasks, here we show that the dimensionality of the dynamics and subpopulation structure play fundamentally com-\nplementary roles. although various tasks can be implemented by increasing the dimensionality in networks with fully random \npopulation structure, flexible input\u2013output mappings instead require a non-random population structure that can be described \nin terms of multiple subpopulations. our analyses r", "\u0000\u0002\u0001\u0004\u0003\u0006\u0005\u0007\u0001\t\b\u000b\n\r\f\u000f\u000e\u0011\u0010\u0012\u0003\u0006\u0013\u0014\u0001\u0016\u0015\u0017\u0013\u0014\u0005\u0007\u0018\u001a\u0019\u001b\u0018\u001d\u001c\t\u001e \u001f!\u0001\u0004\"\u0014\f\u000f\u001f!\f\u000f\u0018#\u0015\u0011\u0019$\u0015\u0017\u0013\u0014\u0005\u0007\u0018%\n\r\f&\u0015\u0011\u0019'\u0013\u0014\"\u0014\u000e\n\n]^r\u0012_`nopsnqa=n#bdc\n\n(\u001d)\u0011*,+\u0007-/.0*\u00111\u000f2\u000434)6587\u000f9;:=<?>@1\u000f-bac1\u000f9;d!e\u0011fgfgh\na\u0011lfe\u000fg,hipsjlk\u0012mngixyo\u0011z[vqp6p[n;c\n\u0086\u001bt\u0016\u0087\n\n\u00abd\u00aa^\u00b1q\u00b6\u0014\u00aau\u00ae\u00bd\u00ab\n\nj;n/a=r\u0012wsr\u0016vtw6a\u0011j\u0017nqpsgir\u0012_q_\u0014u6novqxyv\n\nikjmlonqpsr4tupsv;w\u0017xypsz[z\\v l\nv\u0016r\u0011p[w\u0006vqlx_\u0014psn\u0014uygmb/mzj;lqz{_`lqa=|\u0011v\n}\u007f~`\u0080t\u00816\u0082 \u0083,\u0084\u0085m\u001djmlqz{_`loa\u0011|\u0011v\nlqp[v;w6xyz\u0089\u00888psloj6\u008a\u008ba\u0011jij6\u008a\u008cw\u0017v\n\u00ed\u0017\u0003s\u00b2$\u00ae\u00bd\u00e6;\u00b68\u00b1q\u00b6\u0014\u00aa\u00bd\u00b7\u008c\u00e5d\u00afx\u00ae\u0089\u00b7\u008c\u00e5\u00ac\u00b6c\u00aa\u00bd\u00b6\u0014\u00ae\u009a\u00af\u00e2\u00b7\u00b9\u00b0m\u00adc\u00b7\u008c\u00ae\u0089\u00ad/\u00ad\u0089\u00b7\u008c\u00e1d\u00b0\u0017\u00e86\u00ae\u0089\u00e6m\u00b6u\u00e9;\u00a8\u00df\u00b1m\u00afx\u00ae\u0089\u00b60\u00e4\n\u00b2{\u00af\u00ac\u00bb^\u00ae\u0089\u00ab\u00ac\u00aa\n\u00e5d\u00afx\u00b8\u008c\u00e9;\u00b6u\u00b7\u008c\u00ad#\u00ad\u0089\u00b8\u008c\u00b7\u008c\u00e1d\u00e6q\u00ae\u0089\u00b8\u008c\u00ec\u00fa\u00b7\u008c\u00b0i\u00bb^\u00aa\u0089\u00b6q\u00afx\u00ad\u0089\u00b6q\u00b1y\u00b7\u008c\u00b0\n\u00af\u00ac\u00bb\u0014\u00bb^\u00b6\u0014\u00b8\u008c\u00b6\u0014\u00aa^\u00afx\u00ae\u0089\u00b6\u001d\u00bb^\u00ab\u00ac\u00b0q\u00e5d\u00b6\u0014\u00aa\u00e4\u00e4\n\u00aa\u0089\u00b6\u0014\u00e1\u00ac\u00b7\u008c\u00abd\u00b0;\u00adq\u00ed\u0011\u00f8'\u00b1;\u00b1q\u00b7\u008c\u00ae\u00bd\u00b7\u00b9\u00ab\u00ac\u00b0i\u00afx\u00b8\u008c\u00b8\u008c\u00ecq\u00e8\u0014\u00b7\u008c\u00b08\u00bb\u0014\u00afx\u00ad\u00bd\u00b6g\u00abx\u00b2=\u00af\u001b\u00bb\u00bd\u00e6i\u00afx\u00b0m\u00e1d\u00b6\n\u00e1d\u00b6`\u00b0i\u00bb^\u00b6$\u00b7\u008c\u00b0#\u00ad\u0089\u00e6i\u00af\u00e2\u00b8\u008c\u00b8\u00b9\u00abx\u00d7\n\u00b7\u008c\u00b0\u001d\u00ad\u0089\u00b7\u008c\u00e1\u00ac\u00b0\u0017\u00e8\u0006\u00ae\u0089\u00e6m\u00b6\u0014\u00aa\u0089\u00b6\u0016\u00ad\u0089\u00e6;\u00ab\u00ac\u00e9;\u00b8\u00eb\u00b1\u0004\u00bai\u00b6u\u00b0m\u00abn\u00af\u00ac\u00b1;\u00afx\u00a8m\u00ae\u009a\u00afx\u00ae\u00bd\u00b7\u008c\u00abd\u00b0\u001d\u00b7\u008c\u00b0\u00fa\u00ae\u0089\u00e6;\u00b6#\u00ad\u0089\u00e9i\u00bb`\u00bb^\u00b6\u0014\u00b6o\u00b1q\u00b7\u008c\u00b0;\u00e1\n\u00ba \u00ecu\u00ad\u00bd\u00b6\u0014\u00ae\u0089\u00ae\u00bd\u00b7\u008c\u00b0;\u00e1\n\u00b8\u008c\u00b6o\u00afx\u00aa\u00bd\u00b0;\u00b7\u008c\u00b0;\u00e1\u0007\u00ad\u00bd\u00ae\u0089\u00b6\u0014\u00a8\u0017\u00ed\u0018\u0003\u00a5\u00b08\u00a8;\u00aa^\u00afd\u00bb^\u00ae\u00bd\u00b7\u00eb\u00bb^\u00b6d\u00e8;\u00ae\u00bd\u00e6;\u00b7\u008c\u00ad$\u00bb\u0014\u00afx\u00b0\u0016\u00bai\u00b6\u0007\u00af\u00ac\u00bb\u0089\u00e6m\u00b7\u00b9\u00b6`\u00e5d\u00b6o\u00b1\n\u00e6\u008c\u00e7\u0019\u000e\u001a\u0010\u0089\u00e8\n\u00b7\u008c\u00b0\u001d\u00ae\u0089\u00e6;\u00b6u\u00afx\u00ba,\u00abx\u00e5d\u00b6u\u00afd\u00b1m\u00afx\u00a8;\u00ae^\u00afx\u00ae\u0089\u00b7\u008c\u00ab\u00ac\u00b0f\u00aa\u00bd\u00e9;\u00b8\u008c\u00b6\n\u00ad\u0089\u00b6\u0014\u00b68\u00af\u00e2\u00b8\u008c\u00ad\u0089\u00ab8\u00ae\u00bd\u00e6;\u00b6\n\u00f5q\u00f7\n\u00eb\u00b5\u00ed\n\u00f5q\u00f8\n\u00b1q\u00b6\u0014\u00ad^\u00bb^\u00aa\u0089\u00b7\u008c\u00a8;\u00ae\u00bd\u00b7\u008c\u00abd\u00b0\u0004\u00abx\u00b2\u0012\u00ae\u0089\u00e6m\u00b6\u009e\u00af\u00e2\u00b8\u008c\u00e1", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\ndendritic  solutions  to  the  credit  assignment  problem\nblake  a  richards1,2,3 and  timothy  p  lillicrap4\n\nguaranteeing  that  synaptic  plasticity  leads  to  effective  learning\nrequires  a  means  for  assigning  credit  to  each  neuron  for  its\ncontribution  to  behavior.  the  \u2018credit  assignment  problem\u2019\nrefers  to  the  fact  that  credit  assignment  is  non-trivial  in\nhierarchical  networks  with  multiple  stages  of  processing.  one\ndif\ufb01culty  is  that  if  credit  signals  are  integrated  with  other  inputs,\nthen  it  is  hard  for  synaptic  plasticity  rules  to  distinguish  credit-\nrelated  activity  from  non-credit-related  activity.  a  potential\nsolution  is  to  use  the  spatial  layout  and  non-linear  properties  of\ndendrites  to  distinguish  credit  signals  from  other  inputs.  in\ncortical  pyramidal  neurons,  evidence  hints  that  top-down\nfeedback  signals  are  integrated ", "de\u00a0novo design of protein structure and \nfunction with rfdiffusion\n\nhttps://doi.org/10.1038/s41586-023-06415-8\nreceived: 14 december 2022\naccepted: 7 july 2023\npublished online: 11 july 2023\nopen access\n\njoseph l. watson1,2,15, david juergens1,2,3,15, nathaniel r. bennett1,2,3,15, brian l. trippe2,4,5,15, \njason yim2,6,15, helen e. eisenach1,2,15, woody ahern1,2,7,15, andrew j. borst1,2, robert j. ragotte1,2, \nlukas f. milles1,2, basile i. m. wicky1,2, nikita hanikel1,2, samuel j. pellock1,2, alexis courbet1,2,8, \nwilliam sheffler1,2, jue wang1,2, preetham venkatesh1,2,9, isaac sappington1,2,9, \nsusana v\u00e1zquez torres1,2,9, anna lauko1,2,9, valentin de bortoli8, emile mathieu10, \nsergey ovchinnikov11,12, regina barzilay6, tommi s. jaakkola6, frank dimaio1,2, minkyung baek13 \n& david baker1,2,14\u2009\u2709\n\n check for updates\n\nthere has been considerable recent progress in designing new proteins using deep- \nlearning methods1\u20139. despite this progress, a general deep-learning framework for \nprotei", "rapid neural coding in the retina with relative\nspike latencies\ntim gollisch,\n 319\nscience\ndoi: 10.1126/science.1149639 \n\n et al.\n, 1108 (2008);\n\n \n\nthe following resources related to this article are available online at\nwww.sciencemag.org (this information is current as of february 22, 2008 ):\n\n \n\nupdated information and services,\nversion of this article at: \nhttp://www.sciencemag.org/cgi/content/full/319/5866/1108\n \n\n including high-resolution figures, can be found in the online\n\nsupporting online material\nhttp://www.sciencemag.org/cgi/content/full/319/5866/1108/dc1\n \n\n can be found at: \n\nthis article \nhttp://www.sciencemag.org/cgi/content/full/319/5866/1108#otherarticles\n \n\n, 12 of which can be accessed for free: \n\ncites 30 articles\n\ninformation about obtaining \nthis article\n in whole or in part can be found at: \nhttp://www.sciencemag.org/about/permissions.dtl\n \n\nreprints\n\n of this article or about obtaining \n\npermission to reproduce\n\n \n\n8\n0\n0\n2\n\n \n,\n\n2\n2\n\n \ny\nr\na\nu\nr\nb\ne\nf\nn\no\n\n \n\n", "1\n2\n0\n2\n\n \n\nb\ne\nf\n0\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n5\n4\n3\n1\n\n.\n\n1\n1\n0\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2021\n\nscore-based generative modeling through\nstochastic differential equations\n\nyang song\u02da\nstanford university\nyangsong@cs.stanford.edu\n\njascha sohl-dickstein\ngoogle brain\njaschasd@google.com\n\ndiederik p. kingma\ngoogle brain\ndurk@google.com\n\nabhishek kumar\ngoogle brain\nabhishk@google.com\n\nstefano ermon\nstanford university\nermon@cs.stanford.edu\n\nben poole\ngoogle brain\npooleb@google.com\n\nabstract\n\ncreating noise from data is easy; creating data from noise is generative modeling.\nwe present a stochastic differential equation (sde) that smoothly transforms a com-\nplex data distribution to a known prior distribution by slowly injecting noise, and a\ncorresponding reverse-time sde that transforms the prior distribution back into the\ndata distribution by slowly removing the noise. crucially, the reverse-time sde\ndepends only on the time-dependent gradient \ufb01eld (a.k.", "neuron\n\nperspective\n\nthe brain as an ef\ufb01cient\nand robust adaptive learner\n\nsophie dene` ve,1,* alireza alemi,1 and ralph bourdoukan1\n1group for neural theory, de\u00b4 partement d\u2019etudes cognitives, ecole normale supe\u00b4 rieure, 75005 paris, france\n*correspondence: sophie.deneve@ens.fr\nhttp://dx.doi.org/10.1016/j.neuron.2017.05.016\n\nunderstanding how the brain learns to compute functions reliably, ef\ufb01ciently, and robustly with noisy spiking\nactivity is a fundamental challenge in neuroscience. most sensory and motor tasks can be described as\ndynamical systems and could presumably be learned by adjusting connection weights in a recurrent biolog-\nical neural network. however, this is greatly complicated by the credit assignment problem for learning in\nrecurrent networks, e.g., the contribution of each connection to the global output error cannot be determined\nbased only on locally accessible quantities to the synapse. combining tools from adaptive control theory and\nef\ufb01cient coding theories, we ", "5\n1\n0\n2\n \nc\ne\nd\n1\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n3\nv\n7\n6\n5\n0\n0\n\n.\n\n2\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nrethinking the inception architecture for computer vision\n\nchristian szegedy\n\ngoogle inc.\n\nszegedy@google.com\n\nvincent vanhoucke\n\nsergey ioffe\n\nvanhoucke@google.com\n\nsioffe@google.com\n\njonathon shlens\nshlens@google.com\n\nzbigniew wojna\n\nuniversity college london\nzbigniewwojna@gmail.com\n\nabstract\n\nconvolutional networks are at the core of most state-\nof-the-art computer vision solutions for a wide variety of\ntasks. since 2014 very deep convolutional networks started\nto become mainstream, yielding substantial gains in vari-\nous benchmarks. although increased model size and com-\nputational cost tend to translate to immediate quality gains\nfor most tasks (as long as enough labeled data is provided\nfor training), computational ef\ufb01ciency and low parameter\ncount are still enabling factors for various use cases such as\nmobile vision and big-data scenarios. here we are explor-\ning ways to scale up networks in ", "adversarial robustness through local linearization\n\nchongli qin\ndeepmind\n\njames martens\n\ndeepmind\n\nsven gowal\ndeepmind\n\ndilip krishnan\n\ngoogle\n\nkrishnamurthy (dj) dvijotham\n\ndeepmind\n\nalhussein fawzi\n\ndeepmind\n\nsoham de\ndeepmind\n\nrobert stanforth\n\ndeepmind\n\npushmeet kohli\n\ndeepmind\n\nchongliqin@google.com\n\nabstract\n\nadversarial training is an effective methodology to train deep neural networks\nwhich are robust against adversarial, norm-bounded perturbations. however, the\ncomputational cost of adversarial training grows prohibitively as the size of the\nmodel and number of input dimensions increase. further, training against less\nexpensive and therefore weaker adversaries produces models that are robust against\nweak attacks but break down under attacks that are stronger. this is often attributed\nto the phenomenon of gradient obfuscation; such models have a highly non-linear\nloss surface in the vicinity of training examples, making it hard for gradient-based\nattacks to succeed even though ", "lecture notes in computer science\ncommenced publication in 1973\nfounding and former series editors:\ngerhard goos, juris hartmanis, and jan van leeuwen\n\n7700\n\neditorial board\n\ndavid hutchison\n\nlancaster university, uk\n\ntakeo kanade\n\ncarnegie mellon university, pittsburgh, pa, usa\n\njosef kittler\n\nuniversity of surrey, guildford, uk\n\njon m. kleinberg\n\ncornell university, ithaca, ny, usa\n\nalfred kobsa\n\nuniversity of california, irvine, ca, usa\n\nfriedemann mattern\n\neth zurich, switzerland\n\njohn c. mitchell\n\nstanford university, ca, usa\n\nmoni naor\n\nweizmann institute of science, rehovot, israel\n\noscar nierstrasz\n\nuniversity of bern, switzerland\n\nc. pandu rangan\n\nindian institute of technology, madras, india\n\nbernhard steffen\n\ntu dortmund university, germany\n\nmadhu sudan\n\nmicrosoft research, cambridge, ma, usa\n\ndemetri terzopoulos\n\nuniversity of california, los angeles, ca, usa\n\ndoug tygar\n\nuniversity of california, berkeley, ca, usa\n\ngerhard weikum\n\nmax planck institute for informatics, saar", "7\n1\n0\n2\n\n \n\nb\ne\nf\n7\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n0\n6\n3\n8\n0\n\n.\n\n2\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nneural map: structured memory for deep re-\ninforcement learning\n\nemilio parisotto & ruslan salakhutdinov\ndepartment of machine learning\ncarnegie mellon university\npittsburgh, pa 15213, usa\n{eparisot,rsalakhu}@cs.cmu.edu\n\nabstract\n\na critical component to enabling intelligent reasoning in partially observable en-\nvironments is memory. despite this importance, deep reinforcement learning\n(drl) agents have so far used relatively simple memory architectures, with the\nmain methods to overcome partial observability being either a temporal convo-\nlution over the past k frames or an lstm layer. more recent work (oh et al.,\n2016) has went beyond these architectures by using memory networks which can\nallow more sophisticated addressing schemes over the past k frames. but even\nthese architectures are unsatisfactory due to the reason that they are limited to\nonly remembering information from the last k frames.", "a biologically plausible 3-factor learning rule for expectation\nmaximization in reinforcement learning and decision making\n\nmohammad javad faraji\n\nschool of life sciences, brain mind institute\n\nschool of computer and communication sciences\n\u00b4ecole polytechnique f\u00b4ed\u00b4eral de lausanne (epfl)\n\nlausanne, ch-1015\n\nmohammadjavad.faraji@epfl.ch\n\nkerstin preuschoff\n\ngeneva \ufb01nance research institute\n\nschool of economics and management\n\nuniversity of geneva\n\ngeneva, ch-1211\n\nkerstin.preuschoff@unige.ch\n\nwulfram gerstner\n\nschool of life sciences, brain mind institute\n\nschool of computer and communication sciences\n\u00b4ecole polytechnique f\u00b4ed\u00b4eral de lausanne (epfl)\n\nlausanne, ch-1015\n\nwulfram.gerstner@epfl.ch\n\nabstract\n\none of the most frequent problems in both decision making and reinforcement learning (rl) is expectation maximization\ninvolving functionals such as reward or utility. generally, these problems consist of computing the optimal solution of a\ndensity function. instead of trying to \ufb01nd th", "lyapunov spectra of chaotic recurrent neural networks\n\nrainer engelken\n\ndepartment of neuroscience, zuckerman institute,\n\ncolumbia university, new york, ny, united states of america\n\nfred wolf\n\nmax planck institute for dynamics and self-organization, g\u00f6ttingen, germany\n\nbernstein center for computational neuroscience, g\u00f6ttingen, germany\n\nbernstein focus for neurotechnology, g\u00f6ttingen, germany and\nfaculty of physics, university of g\u00f6ttingen, g\u00f6ttingen, germany\n\n0\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n3\n\n \n \n]\n\n.\n\nd\nc\nn\ni\nl\nn\n[\n \n \n\n1\nv\n7\n2\n4\n2\n0\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nl. f. abbott\n\ndepartment of neuroscience, zuckerman institute,\n\ncolumbia university, new york, ny, united states of america and\n\ndepartment of physiology and cellular biophysics,\n\ncolumbia university, new york, ny, united states of america\n\nbrains process information through the collective dynamics of large neural networks. collective\nchaos was suggested to underlie the complex ongoing dynamics observed in cerebral cortical circuits\nand de", "https://doi.org/10.1038/s41583-022-00642-0\n\ncheck for updates\n\nattractor and integrator \nnetworks in the brain\n\nmikail khona1,2,3,4 & ila r. fiete\u2009\nabstract\n\n \u20091,2,3 \n\nin this review, we describe the singular success of attractor neural \nnetwork models in describing how the brain maintains persistent \nactivity states for working memory, corrects errors and integrates noisy \ncues. we consider the mechanisms by which simple and forgetful units \ncan organize to collectively generate dynamics on the long timescales \nrequired for such computations. we discuss the myriad potential uses \nof attractor dynamics for computation in the brain, and showcase \nnotable examples of brain systems in which inherently low-dimensional \ncontinuous-attractor dynamics have been concretely and rigorously \nidentified. thus, it is now possible to conclusively state that the brain \nconstructs and uses such systems for computation. finally, we highlight \nrecent theoretical advances in understanding how the fundame", "original research article\npublished: 19 november 2014\ndoi: 10.3389/fnhum.2014.00825\n\nuncertainty in perception and the hierarchical gaussian\nfilter\nchristoph d. mathys 1,2,3,4*, ekaterina i. lomakina 3,4,5, jean daunizeau 6, sandra iglesias 3,4,\nkay h. brodersen 3,4, karl j. friston 1 and klaas e. stephan 1,3,4\n\n1 wellcome trust centre for neuroimaging, institute of neurology, university college london, london, uk\n2 max planck ucl centre for computational psychiatry and ageing research, london, uk\n3 translational neuromodeling unit, institute for biomedical engineering, university of zurich and eth zurich, zurich, switzerland\n4 laboratory for social and neural systems research (sns lab), department of economics, university of zurich, zurich, switzerland\n5 department of computer science, eth zurich, zurich, switzerland\n6 institut du cerveau et de la moelle \u00e9pini\u00e8re, h\u00f4pital piti\u00e9 salp\u00eatri\u00e8re, paris, france\n\nedited by:\nhauke r. heekeren, freie\nuniversit\u00e4t berlin, germany\nreviewed by:\ndir", "article\n\nneural trajectories in the supplementary motor area\nand motor cortex exhibit distinct geometries,\ncompatible with different classes of computation\n\nhighlights\nd guiding action across time necessitates population activity\n\nwith \u2018\u2018low divergence\u2019\u2019\n\nd the supplementary motor area, but not motor cortex, exhibits\n\nlow divergence\n\nd low divergence explains diverse single-neuron and\n\npopulation-level features\n\nauthors\n\nabigail a. russo, ramin khajeh,\nsean r. bittner, sean m. perkins,\njohn p. cunningham, l.f. abbott,\nmark m. churchland\n\ncorrespondence\nmc3502@columbia.edu\n\nin brief\nthe supplementary motor area is\nbelieved to guide action by \u2018\u2018looking\nahead\u2019\u2019 in time. russo et al. formalize this\nidea and predict a basic property that\nneural activity must have to serve that\npurpose. that property is present,\nexplains diverse features of activity, and\ndistinguishes higher- from lower-level\nmotor areas.\n\nrusso et al., 2020, neuron 107, 745\u2013758\naugust 19, 2020 \u00aa 2020 elsevier inc.\nhttps://d", "neuron\n\narticle\n\nstates versus rewards: dissociable neural\nprediction error signals underlying model-based\nand model-free reinforcement learning\n\njan gla\u00a8 scher,1,3,* nathaniel daw,4 peter dayan,5 and john p. o\u2019doherty1,2,6\n1division of humanities and social sciences\n2computation and neural systems program\ncalifornia institute of technology, pasadena, ca 91101, usa\n3neuroimage nord, department of systems neuroscience, university medical center hamburg-eppendorf, 20246 hamburg, germany\n4center for neural science and department of psychology, new york university, ny 10003, usa\n5gatsby computational neuroscience unit, university college london, london wc1n 3ar, uk\n6trinity college institute of neuroscience and school of psychology, trinity college dublin 2, ireland\n*correspondence: glascher@hss.caltech.edu\ndoi 10.1016/j.neuron.2010.04.016\n\nsummary\n\n(rpe), model-based rl uses it\n\nreinforcement learning (rl) uses sequential experi-\nence with situations (\u2018\u2018states\u2019\u2019) and outcomes to\nassess ac", "neuroscience needs network science\n\nd\u00e1niel l barab\u00e1si1,2,\ud83d\udce7, ginestra bianconi3,4, ed bullmore5, mark burgess6, sueyeon chung7,8, tina\neliassi-rad9,10,11, dileep george12, istv\u00e1n a. kov\u00e1cs13,14, hern\u00e1n makse15, christos papadimitriou16,\nthomas e. nichols17,18, olaf sporns19, kim stachenfeld12,16, zolt\u00e1n toroczkai20, emma k. towlson21,\n\nanthony m zador22, hongkui zeng23, albert-l\u00e1szl\u00f3 barab\u00e1si9,24,25, amy bernard26, gy\u00f6rgy\n\nbuzs\u00e1ki27,28,\ud83d\udce7\n\n1 biophysics program, harvard university, cambridge, ma, usa\n2 department of molecular and cellular biology and center for brain science, harvard university, cambridge,\nmassachusetts, usa\n3 school of mathematical sciences, queen mary university of london, london, e1 4ns, uk\n4 the alan turing institute, the british library, london, nw1 2db, uk\n5 department of psychiatry and wolfson brain imaging centre, university of cambridge, cambridge, united\nkingdom\n6 chitek-i as, oslo, norway\n7 center for neural science, new york university, new york, ny, usa.\n8 ce", "real-time recurrent reinforcement learning\n\njulian lemmel, radu grosu\n\ntechnical university vienna\nkarlsplatz 13, 1040 wien at\njulian.lemmel@tuwien.ac.at\n\n3\n2\n0\n2\n\n \n\nv\no\nn\n8\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n0\n3\n8\n4\n0\n\n.\n\n1\n1\n3\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nrecent advances in reinforcement learning, for partially-ob-\nservable markov decision processes (pomdps), rely on the\nbiologically implausible backpropagation through time algo-\nrithm (bptt) to perform gradient-descent optimisation. in\nthis paper we propose a novel reinforcement learning algo-\nrithm that makes use of random feedback local online learn-\ning (rflo), a biologically plausible approximation of real-\ntime recurrent learning (rtrl) to compute the gradients of\nthe parameters of a recurrent neural network in an online\nmanner. by combining it with td(\u03bb), a variant of temporal-\ndifference reinforcement learning with eligibility traces, we\ncreate a biologically plausible, recurrent actor-critic algo-\nrithm, capable of solving discret", "volume 61, number 3\n\nphysical review letters\n\n18 july 1988\n\nchaos in random neural networks\n\nh. sompolinsky\n\n' and a. crisanti\n\natd'c t bell laboratories, murray hill, new jersey 07974, and\n\nracah institute of physics, the hebrew university, 91904 jerusalem,\n\nisrael\n\nand\n\nh. j. sommers\n\n'~\n\nfachbereich physik, universitat gesa-mthochschule\n\nessen, d-4300 essen, federal republic of germany\n\n(received 30 march 1988)\n\na continuous-time\n\nmetric couplings\ntransition\nthe autocorrelations\n\nis studied. a self-consistent mean-field\n\ndynamic model of a network of n nonlinear\n\nvia random asym-\n~ limit, predicts\na\nphase to a chaotic phase occurring at a critical value of the gain parameter.\n\ntheory, exact\n\nin the n\n\ninteracting\n\nelements\n\nof the chaotic flow as well as the maximal lyapunov exponent are calculated.\n\nfrom a stationary\n\npacs numbers:\n\n05.45.+b, 05.20.\u2014y, 47.20.tg, 87. 10.+e\n\nin\n\nthese\n\nthrough\n\nin large\n\nnonlinear\n\nscenarios\n\nproperties\n\ndynamical\n\ntheoretical\n\ninvestigations\n\nare re", "ieee signal processing letters, vol. 9, no. 6, june 2002\n\n177\n\nconditions for nonnegative independent\n\ncomponent analysis\n\nmark plumbley, member, ieee\n\nabstract\u2014 we consider the noiseless linear independent\ncomponent analysis problem,\nin the case where the hid-\nden sources s are non-negative. we assume that the ran-\ndom variables sis are well-grounded in that they have a non-\nvanishing pdf in the (positive) neighbourhood of zero. for\nan orthonormal rotation y = wx of pre-whitened observa-\ntions x = qas, under certain reasonable conditions we show\nthat y is a permutation of the s (apart from a scaling factor)\nif and only if y is non-negative with probability 1. we sug-\ngest that this may enable the construction of practical learn-\ning algorithms, particularly for sparse non-negative sources.\n\nkeywords\u2014 independent\n\ncomponent\n\nanalysis, non-\n\nnegative matrix factorization, sparse coding, ica.\n\ni. introduction\n\nseveral authors have suggested that decomposition\n\nof a observation into non-n", "vol 436|7 july 2005|doi:10.1038/nature03689\n\narticles\n\ndynamic predictive coding by the retina\n\ntoshihiko hosoya1\u2020, stephen a. baccus1\u2020 & markus meister1\n\nretinal ganglion cells convey the visual image from the eye to the brain. they generally encode local differences in space\nand changes in time rather than the raw image intensity. this can be seen as a strategy of predictive coding, adapted\nthrough evolution to the average image statistics of the natural environment. yet animals encounter many environments\nwith visual statistics different from the average scene. here we show that when this happens, the retina adjusts its\nprocessing dynamically. the spatio-temporal receptive \ufb01elds of retinal ganglion cells change after a few seconds in a\nnew environment. the changes are adaptive, in that the new receptive \ufb01eld improves predictive coding under the new\nimage statistics. we show that a network model with plastic synapses can account for the large variety of observed\nadaptations.\n\nbecause", "1\n2\n0\n2\n\n \nr\na\n\n \n\nm\n2\n1\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n3\n9\n5\n2\n1\n\n.\n\n3\n0\n1\n2\n:\nv\ni\nx\nr\na\n\naccurate and ef\ufb01cient time-domain classi\ufb01cation\nwith adaptive spiking recurrent neural networks\nbojian yin1,*, federico corradi2, and sander m. boht\u00b4e1,3,4\n\n1cwi, machine learning group, amsterdam, the netherlands\n2stichting imec netherlands, holst centre, eindhoven, the netherlands\n3univ of amsterdam, faculty of science, amsterdam, the netherlands\n4rijksuniversiteit groningen, faculty of science and engineering, groningen, the netherlands\n*byin@cwi.nl (corresponding author)\n\nabstract\n\ninspired by more detailed modeling of biological neurons, spiking neural networks (snns) have been investigated both as\nmore biologically plausible and potentially more powerful models of neural computation, and also with the aim of extracting\nbiological neurons\u2019 energy ef\ufb01ciency; the performance of such networks however has remained lacking compared to classical\narti\ufb01cial neural networks (anns). here, we demonstrate ", "article\n\ncommunicated by ilya sutskever\n\nopening the black box: low-dimensional dynamics\nin high-dimensional recurrent neural networks\n\ndavid sussillo\nsussillo@stanford.edu\ndepartment of electrical engineering, neurosciences program,\nstanford university, stanford, ca 94305-9505, u.s.a.\n\nomri barak\nomri.barak@gmail.com\ndepartment of neuroscience, columbia university college of\nphysicians and surgeons, new york, ny 10032-2695, u.s.a.\n\nrecurrent neural networks (rnns) are useful tools for learning nonlin-\near relationships between time-varying inputs and outputs with complex\ntemporal dependencies. recently developed algorithms have been suc-\ncessful at training rnns to perform a wide variety of tasks, but the\nresulting networks have been treated as black boxes: their mechanism of\noperation remains unknown. here we explore the hypothesis that \ufb01xed\npoints, both stable and unstable, and the linearized dynamics around\nthem, can reveal crucial aspects of how rnns implement their com-\nputations", "a r t i c l e s\n\nthe reorganization and reactivation of hippocampal \nmaps predict spatial memory performance\ndavid dupret, joseph o\u2019neill, barty pleydell-bouverie & jozsef csicsvari\nthe hippocampus is an important brain circuit for spatial memory and the spatially selective spiking of hippocampal neuronal \nassemblies is thought to provide a mnemonic representation of space. we found that remembering newly learnt goal locations \nrequired nmda receptor\u2013dependent stabilization and enhanced reactivation of goal-related hippocampal assemblies. during \nspatial learning, place-related firing patterns in the ca1, but not ca3, region of the rat hippocampus were reorganized to \nrepresent new goal locations. such reorganization did not occur when goals were marked by visual cues. the stabilization and \nsuccessful retrieval of these newly acquired ca1 representations of behaviorally relevant places was nmdar dependent and \nnecessary for subsequent memory retention performance. goal-related assembl", "unsupervised learning of invariant feature hierarchies\n\nwith applications to object recognition\n\nmarc\u2019aurelio ranzato, fu-jie huang, y-lan boureau, yann lecun\n\ncourant institute of mathematical sciences, new york university, new york, ny, usa\n\n{ranzato,jhuangfu,ylan,yann}@cs.nyu.edu, http://www.cs.nyu.edu/\u02dcyann\n\nabstract\n\nwe present an unsupervised method for learning a hier-\narchy of sparse feature detectors that are invariant to small\nshifts and distortions. the resulting feature extractor con-\nsists of multiple convolution \ufb01lters, followed by a point-\nwise sigmoid non-linearity, and a feature-pooling layer\nthat computes the max of each \ufb01lter output within adja-\ncent windows. a second level of larger and more invari-\nant features is obtained by training the same algorithm\non patches of features from the \ufb01rst level. training a su-\npervised classi\ufb01er on these features yields 0.64% error on\nmnist, and 54% average recognition rate on caltech 101\nwith 30 training samples per category. whi", "research\n\nneuroscience\n\nbehavioral time scale synaptic\nplasticity underlies ca1 place fields\n\nkatie c. bittner,1* aaron d. milstein,1,2* christine grienberger,1\nsandro romani,1 jeffrey c. magee1\u2020\n\nlearning is primarily mediated by activity-dependent modifications of synaptic strength within\nneuronal circuits. we discovered that place fields in hippocampal area ca1 are produced by a\nsynaptic potentiation notably different from hebbian plasticity. place fields could be produced\nin vivo in a single trial by potentiation of input that arrived seconds before and after complex\nspiking. the potentiated synaptic input was not initially coincident with action potentials or\ndepolarization.this rule, named behavioral time scale synaptic plasticity, abruptly modifies inputs\nthat were neither causal nor close in time to postsynaptic activation. in slices, five pairings of\nsubthreshold presynaptic activity and calcium (ca2+) plateau potentials produced a large\npotentiation with an asymmetric seconds", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\non  simplicity  and  complexity  in  the  brave  new  world\nof  large-scale  neuroscience\npeiran  gao1 and  surya  ganguli2\n\ntechnological  advances  have  dramatically  expanded  our\nability  to  probe  multi-neuronal  dynamics  and  connectivity  in  the\nbrain.  however,  our  ability  to  extract  a  simple  conceptual\nunderstanding  from  complex  data  is  increasingly  hampered  by\nthe  lack  of  theoretically  principled  data  analytic  procedures,  as\nwell  as  theoretical  frameworks  for  how  circuit  connectivity  and\ndynamics  can  conspire  to  generate  emergent  behavioral  and\ncognitive  functions.  we  review  and  outline  potential  avenues\nfor  progress,  including  new  theories  of  high  dimensional  data\nanalysis,  the  need  to  analyze  complex  arti\ufb01cial  networks,  and\nmethods  for  analyzing  entire  spaces  of  circuit  models,  rather\nthan  one  model  at  a  time.  such  interplay  between  e", "8\n1\n0\n2\n\n \nt\nc\no\n9\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n6\nv\n0\n7\n0\n6\n0\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ndiversity is all you need:\nlearning skills without a reward function\n\nbenjamin eysenbach\u2217\ncarnegie mellon university\nbeysenba@cs.cmu.edu\n\nabhishek gupta\nuc berkeley\n\njulian ibarz\ngoogle brain\n\nsergey levine\nuc berkeley\ngoogle brain\n\nabstract\n\nintelligent creatures can explore their environments and learn useful skills without\nsupervision. in this paper, we propose \u201cdiversity is all you need\u201d(diayn), a\nmethod for learning useful skills without a reward function. our proposed method\nlearns skills by maximizing an information theoretic objective using a maximum\nentropy policy. on a variety of simulated robotic tasks, we show that this simple\nobjective results in the unsupervised emergence of diverse skills, such as walking\nand jumping. in a number of reinforcement learning benchmark environments, our\nmethod is able to learn a skill that solves the benchmark task despite never receiving\nthe true task rewa", "topic models\n\ndavid m. blei\n\nprinceton university\n\njohn d. lafferty\n\ncarnegie mellon university\n\n1. introduction\n\nscientists need new tools to explore and browse large collections of schol-\narly literature. thanks to organizations such as jstor, which scan and\nindex the original bound archives of many journals, modern scientists can\nsearch digital libraries spanning hundreds of years. a scientist, suddenly\nfaced with access to millions of articles in her \ufb01eld, is not satis\ufb01ed with\nsimple search. effectively using such collections requires interacting with\nthem in a more structured way: \ufb01nding articles similar to those of interest,\nand exploring the collection through the underlying topics that run through\nit.\n\nthe central problem is that this structure\u2014the index of ideas contained\nin the articles and which other articles are about the same kinds of ideas\u2014is\nnot readily available in most modern collections, and the size and growth\nrate of these collections preclude us from building it b", "open-ended learning leads to generally\ncapable agents\n\nopen-ended learning team*, adam stooke, anuj mahajan, catarina barros, charlie deck, jakob bauer, jakub sygnowski, maja\ntrebacz, max jaderberg, michael mathieu, nat mcaleese, nathalie bradley-schmieg, nathaniel wong, nicolas porcel, roberta\nraileanu, steph hughes-fitt, valentin dalibard and wojciech marian czarnecki\ndeepmind, london, uk\n\nfigure 1 | example zero-shot behaviour of an agent playing a capture the flag task at test time. the agent has trained on 700k games, but\nhas never experienced any capture the flag games before in training. the red player\u2019s goal is to put both the purple cube (the opponent\u2019s\ncube) and the black cube (its own cube) onto its base (the grey \ufb02oor), while the blue player tries to put them on the blue \ufb02oor \u2013 the cubes\nare used as \ufb02ags. the red player \ufb01nds the opponent\u2019s cube, brings it back to its cube at its base, at which point reward is given to the\nagent. shortly after, the opponent, played by anothe", "learning enhances sensory and multiple non-\nsensory representations in primary visual cortex\n\narticle\n\nhighlights\nd v1 neurons increasingly discriminate task-relevant stimuli\n\nwith learning\n\nd chronic imaging reveals single cell changes underlying this\n\npopulation effect\n\nd learning-related changes are reduced when animals ignore\n\ntask-relevant stimuli\n\nd anticipatory and behavioral choice-related signals emerge in\n\nreward-predicting cells\n\nauthors\n\njasper poort, adil g. khan,\nmarius pachitariu, ..., georg b. keller,\nthomas d. mrsic-flogel, sonja b. hofer\n\ncorrespondence\nsonja.hofer@unibas.ch\n\nin brief\nby tracking the same visual cortex\nneurons across days, poort et al.\ndemonstrate how learning a visual task\nleads to increasingly distinguishable\nrepresentations of relevant stimuli. these\nchanges parallel the emergence of\ndiverse non-sensory signals in speci\ufb01c\nneuronal subsets.\n\npoort et al., 2015, neuron 86, 1478\u20131490\njune 17, 2015 \u00aa2015 the authors\nhttp://dx.doi.org/10.1016/j.neuron.2", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nneuromodulation  of  neurons  and  synapses\nfarzan  nadim  and  dirk  bucher\n\nneuromodulation  underlies  the  \ufb02exibility  of  neural  circuit\noperation  and  behavior.  individual  neuromodulators  can  have\ndivergent  actions  in  a  neuron  by  targeting  multiple\nphysiological  mechanisms.  conversely,  multiple\nneuromodulators  may  have  convergent  actions  through\noverlapping  targets.  the  divergent  and  convergent\nneuromodulator  actions  can  be  unambiguously  synergistic  or\nantagonistic,  but  neuromodulation  often  entails  balanced\nadjustment  of  nonlinear  membrane  and  synaptic  properties  by\ntargeting  ion  channel  and  synaptic  dynamics  rather  than  just\nexcitability  or  synaptic  strength.  in  addition,  neuromodulators\ncan  exert  effects  at  multiple  timescales,  from  short-term\nadjustments  of  neuron  and  synapse  function  to  persistent  long-\nterm  regulation.  this  short  review  ", "published as a conference paper at iclr 2022\n\nneural collapse under mse loss: proximity\nto and dynamics on the central path\n\nx.y. han\u2217\ncornell university\nxh332@cornell.edu\n\nvardan papyan\u2217\nuniversity of toronto\nvardan.papyan@utoronto.ca\n\ndavid l. donoho\nstanford university\ndonoho@stanford.edu\n\nabstract\n\nthe recently discovered neural collapse (nc) phenomenon occurs pervasively\nin today\u2019s deep net training paradigm of driving cross-entropy (ce) loss towards\nzero. during nc, last-layer features collapse to their class-means, both classi\ufb01ers\nand class-means collapse to the same simplex equiangular tight frame, and clas-\nsi\ufb01er behavior collapses to the nearest-class-mean decision rule. recent works\ndemonstrated that deep nets trained with mean squared error (mse) loss perform\ncomparably to those trained with ce. as a preliminary, we empirically establish\nthat nc emerges in such mse-trained deep nets as well through experiments on\nthree canonical networks and \ufb01ve benchmark datasets. we provi", "article\n\ndoi:10.1038/nature14446\n\nneural dynamics for landmark\norientation and angular path integration\n\njohannes d. seelig1 & vivek jayaraman1\n\nmany animals navigate using a combination of visual landmarks and path integration. in mammalian brains, head\ndirection cells integrate these two streams of information by representing an animal\u2019s heading relative to landmarks,\nyet maintaining their directional tuning in darkness based on self-motion cues. here we use two-photon calcium\nimaging in head-fixed drosophila melanogaster walking on a ball in a virtual reality arena to demonstrate that\nlandmark-based orientation and angular path integration are combined in the population responses of neurons\nwhose dendrites tile the ellipsoid body, a toroidal structure in the centre of the fly brain. the neural population\nencodes the fly\u2019s azimuth relative to its environment, tracking visual landmarks when available and relying on\nself-motion cues in darkness. when both visual and self-motion cues ar", "8\n1\n0\n2\n\n \nr\na\n\nm\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n3\n3\n9\n0\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2018\n\ndistributed prioritized experience replay\n\ndan horgan\ndeepmind\nhorgan@google.com\n\njohn quan\ndeepmind\njohnquan@google.com\n\ndavid budden\ndeepmind\nbudden@google.com\n\ngabriel barth-maron\ndeepmind\ngabrielbm@google.com\n\nmatteo hessel\ndeepmind\nmtthss@google.com\n\nhado van hasselt\ndeepmind\nhado@google.com\n\ndavid silver\ndeepmind\ndavidsilver@google.com\n\nabstract\n\nwe propose a distributed architecture for deep reinforcement learning at scale,\nthat enables agents to learn effectively from orders of magnitude more data than\npreviously possible. the algorithm decouples acting from learning: the actors\ninteract with their own instances of the environment by selecting actions according\nto a shared neural network, and accumulate the resulting experience in a shared\nexperience replay memory; the learner replays samples of experience and updates\nthe neural network. the archi", "3\n2\n0\n2\n\n \n\nn\na\nj\n \n\n1\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n9\n7\n0\n1\n0\n\n.\n\n6\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nwhen does return-conditioned supervised learning\n\nwork for of\ufb02ine reinforcement learning?\n\ndavid brandfonbrener\nnew york university\n\nalberto bietti\n\nnew york university\n\njacob buckman\n\nmila\n\ndavid.brandfonbrener@nyu.edu\n\nromain laroche\nmicrosoft research\n\njoan bruna\n\nnew york university\n\nabstract\n\nseveral recent works have proposed a class of algorithms for the of\ufb02ine reinforce-\nment learning (rl) problem that we will refer to as return-conditioned supervised\nlearning (rcsl). rcsl algorithms learn the distribution of actions conditioned\non both the state and the return of the trajectory. then they de\ufb01ne a policy by\nconditioning on achieving high return. in this paper, we provide a rigorous study\nof the capabilities and limitations of rcsl, something which is crucially miss-\ning in previous work. we \ufb01nd that rcsl returns the optimal policy under a set\nof assumptions that are stronger than those need", "4\n1\n0\n2\n\n \n\nb\ne\nf\n9\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n4\nv\n9\n9\n1\n6\n\n.\n\n2\n1\n3\n1\n:\nv\ni\nx\nr\na\n\nintriguing properties of neural networks\n\nchristian szegedy\n\ngoogle inc.\n\nwojciech zaremba\nnew york university\n\nilya sutskever\n\njoan bruna\n\ngoogle inc.\n\nnew york university\n\ndumitru erhan\n\ngoogle inc.\n\nian goodfellow\n\nrob fergus\n\nuniversity of montreal\n\nnew york university\n\nfacebook inc.\n\nabstract\n\ndeep neural networks are highly expressive models that have recently achieved\nstate of the art performance on speech and visual recognition tasks. while their\nexpressiveness is the reason they succeed, it also causes them to learn uninter-\npretable solutions that could have counter-intuitive properties. in this paper we\nreport two such properties.\nfirst, we \ufb01nd that there is no distinction between individual high level units and\nrandom linear combinations of high level units, according to various methods of\nunit analysis. it suggests that it is the space, rather than the individual units, that\ncontains the", "0\n2\n0\n2\n\n \n\ny\na\nm\n \n7\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n9\n8\n1\n0\n1\n\n.\n\n2\n1\n9\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2020\n\nemergence of functional and structural\nproperties of the head direction system by op-\ntimization of recurrent neural networks\n\nchristopher j. cueva\u2217\u2020, peter y. wang\u2217, matthew chin\u2217, xue-xin wei\u2020\ncolumbia university\nnew york, ny 10027, usa\n\nabstract\n\nrecent work suggests goal-driven training of neural networks can be used to model\nneural activity in the brain. while response properties of neurons in arti\ufb01cial neural\nnetworks bear similarities to those in the brain, the network architectures are often\nconstrained to be different. here we ask if a neural network can recover both\nneural representations and, if the architecture is unconstrained and optimized, the\nanatomical properties of neural circuits. we demonstrate this in a system where\nthe connectivity and the functional organization have been characterized, namely,\nthe head direction circuits of", "\f", "journal of computational neuroscience 11, 207\u2013215, 2001\nc(cid:2) 2002 kluwer academic publishers. manufactured in the netherlands.\n\nsupervised and unsupervised learning with two sites\n\nof synaptic integration\n\ninstitute of neuroinformatics, eth/uni z\u00a8urich, winterthurerstr. 190, 8057 z\u00a8urich, switzerland\n\nkonrad p. k \u00a8ording and peter k \u00a8onig\n\nkoerding@ini.phys.ethz.ch\n\nreceived june 1, 2000; revised august 31, 2001; accepted august 31, 2001\n\naction editor: terrence sejnowski\n\nabstract. many learning rules for neural networks derive from abstract objective functions. the weights in those\nnetworks are typically optimized utilizing gradient ascent on the objective function. in those networks each neuron\nneeds to store two variables. one variable, called activity, contains the bottom-up sensory-fugal information involved\nin the core signal processing. the other variable typically describes the derivative of the objective function with\nrespect to the cell\u2019s activity and is exclusively used", "a r t i c l e s\n\nexpectancy-related changes in firing of dopamine \nneurons depend on orbitofrontal cortex\nyuji k takahashi1, matthew r roesch2,3, robert c wilson4,5, kathy toreson1, patricio o\u2019donnell1,6, yael niv4,5,8 & \ngeoffrey schoenbaum1,6\u20138\n\nthe orbitofrontal cortex has been hypothesized to carry information regarding the value of expected rewards. such information \nis essential for associative learning, which relies on comparisons between expected and obtained reward for generating \ninstructive error signals. these error signals are thought to be conveyed by dopamine neurons. to test whether orbitofrontal cortex \ncontributes to these error signals, we recorded from dopamine neurons in orbitofrontal-lesioned rats performing a reward learning \ntask. lesions caused marked changes in dopaminergic error signaling. however, the effect of lesions was not consistent with a \nsimple loss of information regarding expected value. instead, without orbitofrontal input, dopaminergic error sign", "biorxiv preprint \ncertified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available under \n\nthe copyright holder for this preprint (which was not\n\nthis version posted march 1, 2019. \n\nhttps://doi.org/10.1101/564476\n; \n\ndoi: \n\na\n\ncc-by-nc-nd 4.0 international license\n.\n\ndynamic compression and expansion in a classifying recurrent\n\nnetwork\n\nmatthew farrell1-2, stefano recanatesi1, guillaume lajoie3-4, and eric\n\nshea-brown1-2\n\n1computational neuroscience center, university of washington\n2department of applied mathematics, university of washington\n\n4dept. of mathematics and statistics, universit\u00b4e de montr\u00b4eal\n\n3mila\u2014qu\u00b4ebec ai institute\n\nabstract\n\nrecordings of neural circuits in the brain reveal extraordinary dynamical richness and high\nvariability. at the same time, dimensionality reduction techniques generally uncover low-\ndimensional structures underlying these dynamics when tasks are performed.\nin general, it", "sequential decision-making with big data: papers from the aaai-14 workshop\n\nsurprise and curiosity for big data robotics\n\nadam white, joseph modayil, richard s. sutton\nreinforcement learning and arti\ufb01cial intelligence laboratory\n\nuniversity of alberta, edmonton, alberta, canada\n\nabstract\n\nthis paper introduces a new perspective on curiosity and in-\ntrinsic motivation, viewed as the problem of generating be-\nhavior data for parallel off-policy learning. we provide 1) the\n\ufb01rst measure of surprise based on off-policy general value\nfunction learning progress, 2) the \ufb01rst investigation of reac-\ntive behavior control with parallel gradient temporal differ-\nence learning and function approximation, and 3) the \ufb01rst\ndemonstration of using curiosity driven control to react to a\nnon-stationary learning task\u2014all on a mobile robot. our ap-\nproach improves scalability over previous off-policy, robot\nlearning systems, essential for making progress on the ul-\ntimate big-data decision making problem\u2014li", "kickback cuts backprop\u2019s red-tape:\n\nbiologically plausible credit assignment in neural networks\n\ndavid balduzzi\n\ndavid.balduzzi@vuw.ac.nz\nvictoria university of wellington\n\nhastagiri vanchinathan\nhastagiri@inf.ethz.ch\n\neth zurich\n\njoachim buhmann\n\njbuhmann@inf.ethz.ch\n\neth zurich\n\n4\n1\n0\n2\n\n \n\nv\no\nn\n3\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n1\n9\n1\n6\n\n.\n\n1\n1\n4\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nerror backpropagation is an extremely effective algorithm\nfor assigning credit in arti\ufb01cial neural networks. however,\nweight updates under backprop depend on lengthy recursive\ncomputations and require separate output and error messages\n\u2013 features not shared by biological neurons, that are perhaps\nunnecessary. in this paper, we revisit backprop and the credit\nassignment problem.\nwe \ufb01rst decompose backprop into a collection of interact-\ning learning algorithms; provide regret bounds on the perfor-\nmance of these sub-algorithms; and factorize backprop\u2019s er-\nror signals. using these results, we derive a new credit ass", "taylorized training: towards better approximation of\n\nneural network training at finite width\n\nyu bai 1 ben krause 1 huan wang 1 caiming xiong 1 richard socher 1\n\n0\n2\n0\n2\n\n \n\nb\ne\nf\n4\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n0\n1\n0\n4\n0\n\n.\n\n2\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe propose taylorized training as an initiative to-\nwards better understanding neural network train-\ning at \ufb01nite width. taylorized training involves\ntraining the k-th order taylor expansion of the\nneural network at initialization, and is a princi-\npled extension of linearized training\u2014a recently\nproposed theory for understanding the success of\ndeep learning.\nwe experiment with taylorized training on mod-\nern neural network architectures, and show that\ntaylorized training (1) agrees with full neural net-\nwork training increasingly better as we increase\nk, and (2) can signi\ufb01cantly close the performance\ngap between linearized and full training. com-\npared with linearized training, higher-order train-\ning works in more realistic se", "www.neuroscience-ibro.com\n\npii: s 0 3 0 6 - 4 5 2 2 ( 0 2 ) 0 0 0 2 6 - x\n\nneuroscience vol. 111, no. 4, pp. 815^835, 2002\n\u00df 2002 ibro. published by elsevier science ltd\nall rights reserved. printed in great britain\n0306-4522 / 02 $22.00+0.00\n\nneuromodulatory transmitter systems in the cortex and\n\ntheir role in cortical plasticity\n\nbrain research center, and department of ophthalmology, university of british columbia, and vancouver hospital and health\n\nsciences center, 2550 willow street, vancouver, bc, canada v5z 3n9\n\nq. gu\u0003\n\nabstract\u00f6cortical neuromodulatory transmitter systems refer to those classical neurotransmitters such as acetylcholine\nand monoamines, which share a number of common features. for instance, their centers are located in subcortical regions\nand send long projection axons to innervate the cortex. the same transmitter can either excite or inhibit cortical neurons\ndepending on the composition of postsynaptic transmitter receptor subtypes. the overall functions of thes", "1684 \u2022 the journal of neuroscience, january 23, 2013 \u2022 33(4):1684 \u20131695\n\nsystems/circuits\n\ngating of sensory input by spontaneous cortical activity\n\nartur luczak,1,2 peter bartho,1,3 and kenneth d. harris1,4\n1center for molecular and behavioral neuroscience, rutgers university, newark, new jersey 07102, 2canadian centre for behavioural neuroscience,\nuniversity of lethbridge, lethbridge, alberta, canada t1k 3m4, 3institute of experimental medicine, hungarian academy of sciences, budapest 1083,\nhungary, and 4departments of bioengineering, electrical and electronic engineering, imperial college london, london sw7 2az, united kingdom\n\nthe activity of neural populations is determined not only by sensory inputs but also by internally generated patterns. during quiet wakefulness,\nthe brain produces spontaneous firing events that can spread over large areas of cortex and have been suggested to underlie processes such as\nmemory recall and consolidation. here we demonstrate a different role for ", "single neurons in the human brain encode numbers\n\narticle\n\nhighlights\nd single neurons in the human medial temporal lobe (mtl)\n\nencode numerical information\n\nd numerosity and abstract numerals are encoded by distinct\n\nneuronal populations\n\nd numerosity representation shows a distance effect; numerals\n\nare encoded categorically\n\nd representation of symbolic numerals may evolve from\n\nnumerosity representations\n\nauthors\n\nesther f. kutter, jan bostroem,\nchristian e. elger, florian mormann,\nandreas nieder\n\ncorrespondence\n\ufb02orian.mormann@ukbonn.de (f.m.),\nandreas.nieder@uni-tuebingen.de (a.n.)\n\nin brief\nkutter et al. show how nonsymbolic and\nsymbolic numerical quantity is encoded\nin the human brain by neurons of the\nmedial temporal lobe. the data support\nthe hypothesis that high-level human\nnumerical abilities are rooted in\nbiologically determined mechanisms.\n\nkutter et al., 2018, neuron 100, 753\u2013761\nnovember 7, 2018 \u00aa 2018 elsevier inc.\nhttps://doi.org/10.1016/j.neuron.2018.08.036\n\n\f", "contrastive learning inverts the data generating process\n\nroland s. zimmermann * 1 2 yash sharma * 1 2 steffen schneider * 1 2 3 matthias bethge \u2020 1 wieland brendel \u2020 1\n\nabstract\n\ncontrastive learning has recently seen tremendous\nsuccess in self-supervised learning. so far, how-\never, it is largely unclear why the learned represen-\ntations generalize so effectively to a large variety\nof downstream tasks. we here prove that feed-\nforward models trained with objectives belonging\nto the commonly used infonce family learn to\nimplicitly invert the underlying generative model\nof the observed data. while the proofs make cer-\ntain statistical assumptions about the generative\nmodel, we observe empirically that our \ufb01ndings\nhold even if these assumptions are severely vio-\nlated. our theory highlights a fundamental con-\nnection between contrastive learning, generative\nmodeling, and nonlinear independent component\nanalysis, thereby furthering our understanding of\nthe learned representations as well", "if deep learning is the answer, what \nis the question?\n\nandrew\u00a0saxe \n\n , stephanie\u00a0nelli \n\n  and christopher\u00a0summerfield \n\n \n\nabstract | neuroscience research is undergoing a minor revolution. recent \nadvances in machine learning and artificial intelligence research have opened up \nnew ways of thinking about neural computation. many researchers are excited by \nthe possibility that deep neural networks may offer theories of perception, \ncognition and action for biological brains. this approach has the potential to \nradically reshape our approach to understanding neural systems, because the \ncomputations performed by deep networks are learned from experience, and not \nendowed by the researcher. if so, how can neuroscientists use deep networks to \nmodel and understand biological brains? what is the outlook for neuroscientists \nwho seek to characterize computations or neural codes, or who wish to understand \nperception, attention, memory and executive functions? in this perspective, our \ng", "letter\nneural constraints on learning\n\ndoi:10.1038/nature13665\n\npatrick t. sadtler1,2,3, kristin m. quick1,2,3, matthew d. golub2,4, steven m. chase2,5, stephen i. ryu6,7,\nelizabeth c. tyler-kabara1,8,9, byron m. yu2,4,5* & aaron p. batista1,2,3*\n\nlearning, whether motor, sensory or cognitive, requires networks of\nneurons to generate new activity patterns. as some behaviours are\neasier to learn than others1,2, we asked if some neural activity patterns\nare easier to generate than others. here we investigate whether an\nexisting network constrains the patterns that a subset of its neurons\nis capable of exhibiting, and if so, what principles define this con-\nstraint. we employed a closed-loop intracortical brain\u2013computer inter-\nface learning paradigm in which rhesus macaques (macacamulatta)\ncontrolled a computer cursor by modulating neural activity patterns\nin the primary motor cortex. using the brain\u2013computer interface par-\nadigm, we could specify and alter how neural activity mapped to c", "catalyzing next-generation artificial intelligence through neuroai\n\nby anthony zador*, sean escola*, blake richards, bence \u00f6lveczky, yoshua bengio, kwabena boahen,\n\nmatthew botvinick,  dmitri chklovskii, anne churchland, claudia clopath, james dicarlo, surya\nganguli, jeff hawkins, konrad k\u00f6rding, alexei koulakov, yann lecun, timothy lillicrap, adam\n\nmarblestone, bruno olshausen, alexandre pouget, cristina savin, terrence sejnowski, eero simoncelli,\n\nsara solla, david sussillo, andreas s. tolias, doris tsao\n\n* equal contributions\n\nabstract: neuroscience has long been an essential driver of progress in artificial intelligence (ai). we\npropose that to accelerate progress in ai, we must invest in fundamental research in neuroai. a core\ncomponent of this is the embodied turing test, which challenges ai animal models to interact with the\nsensorimotor world at skill levels akin to their living counterparts. the embodied turing test shifts the\nfocus from those capabilities like game playing an", "3\n2\n0\n2\n\n \nt\nc\no\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n7\n0\n2\n2\n0\n\n.\n\n0\n1\n3\n2\n:\nv\ni\nx\nr\na\n\npreprint\n\nlanguage models represent space and time\n\nwes gurnee & max tegmark\nmassachusetts institute of technology\n{wesg, tegmark}@mit.edu\n\nabstract\n\nthe capabilities of large language models (llms) have sparked debate over\nwhether such systems just learn an enormous collection of superficial statistics\nor a coherent model of the data generating process\u2014a world model. we find\nevidence for the latter by analyzing the learned representations of three spatial\ndatasets (world, us, nyc places) and three temporal datasets (historical figures,\nartworks, news headlines) in the llama-2 family of models. we discover that\nllms learn linear representations of space and time across multiple scales. these\nrepresentations are robust to prompting variations and unified across different en-\ntity types (e.g. cities and landmarks). in addition, we identify individual \u201cspace\nneurons\u201d and \u201ctime neurons\u201d that reliably enco", "1798\n\nieee transactions on pattern analysis and machine intelligence, vol. 35, no. 8, august 2013\n\nrepresentation learning:\n\na review and new perspectives\n\nyoshua bengio, aaron courville, and pascal vincent\n\nabstract\u2014the success of machine learning algorithms generally depends on data representation, and we hypothesize that this is\nbecause different representations can entangle and hide more or less the different explanatory factors of variation behind the data.\nalthough specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the\nquest for ai is motivating the design of more powerful representation-learning algorithms implementing such priors. this paper reviews\nrecent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders,\nmanifold learning, and deep networks. this motivates longer term unanswered questions about the appropriate objectives for learning", "article\n\nencoding of discriminative fear memory by input-\nspeci\ufb01c ltp in the amygdala\n\nhighlights\nd ltp is not induced globally in acx/mgn-la pathways in\n\nauthors\n\nwoong bin kim, jun-hyeong cho\n\ndiscriminative fear learning\n\nd ltp is induced in cs+, but not cs\u2013, pathways to la in\n\ndiscriminative fear learning\n\nd synapses in cs+ pathways to la remain potentiated after\n\nfear extinction\n\nd depotentiation of cs+, but not cs\u2013, pathways to la prevents\n\nthe recall of fear memory\n\ncorrespondence\njuncho@ucr.edu\n\nin brief\nkim and cho demonstrate that the\nformation of fear memory associated with\na speci\ufb01c auditory cue requires selective\nsynaptic strengthening in functionally\nde\ufb01ned neural pathways that convey the\nauditory signals to the amygdala.\n\nkim & cho, 2017, neuron 95, 1129\u20131146\naugust 30, 2017 \u00aa 2017 elsevier inc.\nhttp://dx.doi.org/10.1016/j.neuron.2017.08.004\n\n\f", "7\n1\n0\n2\n\n \n\nb\ne\nf\n7\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n3\n0\n8\n8\n0\n\n.\n\n5\n0\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\ndensity estimation using real nvp\n\nlaurent dinh\u2217\nmontreal institute for learning algorithms\nuniversity of montreal\nmontreal, qc h3t1j4\n\njascha sohl-dickstein\ngoogle brain\n\nsamy bengio\ngoogle brain\n\nabstract\n\nunsupervised learning of probabilistic models is a central yet challenging problem\nin machine learning. speci\ufb01cally, designing models with tractable learning, sam-\npling, inference and evaluation is crucial in solving this task. we extend the space\nof such models using real-valued non-volume preserving (real nvp) transforma-\ntions, a set of powerful, stably invertible, and learnable transformations, resulting\nin an unsupervised learning algorithm with exact log-likelihood computation, exact\nand ef\ufb01cient sampling, exact and ef\ufb01cient inference of latent variables, and an\ninterpretable latent space. we demonstrate its ability to model natural images\non", "caffe: convolutional architecture\n\nfor fast feature embedding\u2217\n\nyangqing jia\u2217, evan shelhamer\u2217, jeff donahue, sergey karayev,\njonathan long, ross girshick, sergio guadarrama, trevor darrell\n\nuc berkeley eecs, berkeley, ca 94702\n\n{jiayq,shelhamer,jdonahue,sergeyk,jonlong,rbg,sguada,trevor}@eecs.berkeley.edu\n\n4\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n2\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n3\n9\n0\n5\n\n.\n\n8\n0\n4\n1\n:\nv\ni\nx\nr\na\n\nabstract\nca\ufb00e provides multimedia scientists and practitioners with\na clean and modi\ufb01able framework for state-of-the-art deep\nlearning algorithms and a collection of reference models.\nthe framework is a bsd-licensed c++ library with python\nand matlab bindings for training and deploying general-\npurpose convolutional neural networks and other deep mod-\nels e\ufb03ciently on commodity architectures. ca\ufb00e \ufb01ts indus-\ntry and internet-scale media needs by cuda gpu computa-\ntion, processing over 40 million images a day on a single k40\nor titan gpu (\u2248 2.5 ms per image). by separating model\nrepresentation from", "1\n2\n0\n2\n\n \n\ny\na\nm\n1\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n4\n0\n7\n4\n0\n\n.\n\n1\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nsubmitted to ieee xxxx\n\n1\n\nboundary-aware segmentation network for\n\nmobile and web applications\n\nxuebin qin, deng-ping fan, chenyang huang, cyril diagne, zichen zhang,\n\nadri`a cabeza sant\u2019anna, albert su`arez, martin jagersand, and ling shao, fellow, ieee\n\nabstract\u2014although deep models have greatly improved the accuracy and robustness of image segmentation, obtaining segmentation\nresults with highly accurate boundaries and \ufb01ne structures is still a challenging problem. in this paper, we propose a simple yet\npowerful boundary-aware segmentation network (basnet), which comprises a predict-re\ufb01ne architecture and a hybrid loss, for\nhighly accurate image segmentation. the predict-re\ufb01ne architecture consists of a densely supervised encoder-decoder network and\na residual re\ufb01nement module, which are respectively used to predict and re\ufb01ne a segmentation probability map. the hybrid loss is\na combination of t", "original research\npublished: 04 may 2017\ndoi: 10.3389/fncom.2017.00024\n\nequilibrium propagation: bridging\nthe gap between energy-based\nmodels and backpropagation\n\nbenjamin scellier * and yoshua bengio \u2020\n\nd\u00e9partement d\u2019informatique et de recherche op\u00e9rationnelle, montreal institute for learning algorithms, universit\u00e9 de\n\nmontr\u00e9al, montreal, qc, canada\n\nwe introduce equilibrium propagation, a learning framework for energy-based models.\nit involves only one kind of neural computation, performed in both the \ufb01rst phase (when\nthe prediction is made) and the second phase of training (after the target or prediction\nerror is revealed). although this algorithm computes the gradient of an objective function\njust like backpropagation, it does not need a special computation or circuit for the\nsecond phase, where errors are implicitly propagated. equilibrium propagation shares\nsimilarities with contrastive hebbian learning and contrastive divergence while solving\nthe theoretical\nissues of both algor", "real-time reinforcement learning\n\nsimon ramstedt\nmila, element ai,\n\nuniversit\u00e9 de montr\u00e9al\n\nsimonramstedt@gmail.com\n\nchristopher pal\nmila, element ai,\n\npolytechnique montr\u00e9al\n\nchristopher.pal@polymtl.ca\n\n9\n1\n0\n2\n\n \nc\ne\nd\n2\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n8\n4\n4\n4\n0\n\n.\n\n1\n1\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nmarkov decision processes (mdps), the mathematical framework underlying\nmost algorithms in reinforcement learning (rl), are often used in a way that\nwrongfully assumes that the state of an agent\u2019s environment does not change during\naction selection. as rl systems based on mdps begin to \ufb01nd application in real-\nworld, safety-critical situations, this mismatch between the assumptions underlying\nclassical mdps and the reality of real-time computation may lead to undesirable\noutcomes. in this paper, we introduce a new framework, in which states and actions\nevolve simultaneously and show how it is related to the classical mdp formulation.\nwe analyze existing algorithms under the new real-time f", "article\n\ndoi:10.1038/nature12742\n\ncontext-dependent computation by\nrecurrent dynamics in prefrontal cortex\n\nvalerio mante1{*, david sussillo2*, krishna v. shenoy2,3 & william t. newsome1\n\nprefrontal cortex is thought to have a fundamental role in flexible, context-dependent behaviour, but the exact nature of\nthe computations underlying this role remains largely unknown. in particular, individual prefrontal neurons often\ngenerate remarkably complex responses that defy deep understanding of their contribution to behaviour. here we\nstudy prefrontal cortex activity in macaque monkeys trained to flexibly select and integrate noisy sensory inputs\ntowards a choice. we find that the observed complexity and functional roles of single neurons are readily understood\nin the framework of a dynamical process unfolding at the level of the population. the population dynamics can be\nreproduced by a trained recurrent neural network, which suggests a previously unknown mechanism for selection\nand integra", "a r t i c l e s\n\nmembrane potential correlates of sensory perception \nin mouse barrel cortex\nshankar sachidhanandam, varun sreenivasan, alexandros kyriakatos, yves kremer & carl c h petersen\nneocortical activity can evoke sensory percepts, but the cellular mechanisms remain poorly understood. we trained mice to \ndetect single brief whisker stimuli and report perceived stimuli by licking to obtain a reward. pharmacological inactivation and \noptogenetic stimulation demonstrated a causal role for the primary somatosensory barrel cortex. whole-cell recordings from \nbarrel cortex neurons revealed membrane potential correlates of sensory perception. sensory responses depended strongly \non prestimulus cortical state, but both slow-wave and desynchronized cortical states were compatible with task performance. \nwhisker deflection evoked an early (<50 ms) reliable sensory response that was encoded through cell-specific reversal potentials. \na secondary late (50\u2013400 ms) depolarization was enhance", "6\n1\n0\n2\n\n \n\nn\na\nj\n \n\n7\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n9\n2\n6\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nunder review as a conference paper at iclr 2016\n\npolicy distillation\n\nandrei a. rusu, sergio g\u00b4omez colmenarejo, c\u00b8 a\u02d8glar g\u00a8ulc\u00b8ehre\u2217, guillaume desjardins,\njames kirkpatrick, razvan pascanu, volodymyr mnih, koray kavukcuoglu & raia hadsell\ngoogle deepmind\nlondon, uk\n{andreirusu, sergomez, gdesjardins, kirkpatrick, razp, vmnih,\nkorayk, raia}@google.com, gulcehrc@iro.umontreal.ca\n\nabstract\n\npolicies for complex visual tasks have been successfully learned with deep rein-\nforcement learning, using an approach called deep q-networks (dqn), but rela-\ntively large (task-speci\ufb01c) networks and extensive training are needed to achieve\ngood performance. in this work, we present a novel method called policy dis-\ntillation that can be used to extract the policy of a reinforcement learning agent\nand train a new network that performs at the expert level while being dramati-\ncally smaller and more ef\ufb01cient. furthe", "l\n\ny\ng\no\no\ni\ns\ny\nh\np\n\nf\no\n\nl\n\na\nn\nr\nu\no\nj\n\ne\nh\nt\n\nj physiol 601.15 (2023) pp 3141\u20133149\n\nt o p i c a l r e v i e w\n\n3141\n\nthe study of plasticity has always been about gradients\n\nand konrad paul kording5,6,7\n\nblake aaron richards1,2,3,4,5\n1mila, montreal, quebec, canada\n2school of computer science, mcgill university, montreal, quebec, canada\n3department of neurology & neurosurgery, mcgill university, montreal, quebec, canada\n4montreal neurological institute, montreal, quebec, canada\n5learning in machines and brains program, cifar, toronto, ontario, canada\n6department of bioengineering, university of pennsylvania, philadelphia, pennsylvania, usa\n7department of neuroscience, university of pennsylvania, philadelphia, pennsylvania, usa\n\nhandling editors: katalin toth & jean-claude b\u00e9\u00efque\nthe peer review history is available in the supporting information section of this article\n(https://doi.org/10.1113/jp282747#support-information-section).\n\nabstract the experimental study of learning and pl", "the discovery of structural form\n\ncharles kemp*\u2020 and joshua b. tenenbaum\u2021\n\n*department of psychology, carnegie mellon university, 5000 forbes avenue, pittsburgh, pa 15213; and \u2021department of brain and cognitive sciences,\nmassachusetts institute of technology, 77 massachusetts avenue, cambridge, ma 02139\n\nedited by richard m. shiffrin, indiana university, bloomington, in, and approved may 30, 2008 (received for review march 17, 2008)\n\nalgorithms for \ufb01nding structure in data have become increasingly\nimportant both as tools for scienti\ufb01c data analysis and as models\nof human learning, yet they suffer from a critical\nlimitation.\nscientists discover qualitatively new forms of structure in observed\ndata: for instance, linnaeus recognized the hierarchical organiza-\ntion of biological species, and mendeleev recognized the periodic\nstructure of the chemical elements. analogous insights play a\npivotal role in cognitive development: children discover that object\ncategory labels can be organized in", "journal of computational neuroscience 11, 63\u201385, 2001\nc(cid:2) 2001 kluwer academic publishers. manufactured in the netherlands.\n\neffects of neuromodulation in a cortical network model of object working\n\nmemory dominated by recurrent inhibition\n\nlps, ecole normale sup\u00b4erieure, 24 rue lhomond, 75231 paris cedex 05, france\n\nnicolas brunel\n\nbrunel@lps.ens.fr\n\nxiao-jing wang\n\nvolen center for complex systems, brandeis university, 415 south st., waltham, ma 02254-9110, usa\n\nxjwang@volen.brandeis.edu\n\nreceived november 22, 2000; revised april 13, 2001; accepted april 13, 2001\n\naction editor: prof. m. tsodyks\n\nabstract. experimental evidence suggests that the maintenance of an item in working memory is achieved through\npersistent activity in selective neural assemblies of the cortex. to understand the mechanisms underlying this\nphenomenon, it is essential to investigate how persistent activity is affected by external inputs or neuromodulation.\nwe have addressed these questions using a recurre", "physical review research 3, 013176 (2021)\n\nquality of internal representation shapes learning performance in feedback neural networks\n\nlee susman ,1,2,*,\u2020 francesca mastrogiuseppe,3,*,\u2021 naama brenner\nand omri barak 2,5,\u00a7,\u00b6\n1interdisciplinary program in applied mathematics, technion israel institute of technology, haifa 32000, israel\n\n,2,4,\u00a7,(cid:2)\n\n2network biology research laboratories, technion israel institute of technology, haifa 32000, israel\n\n3gatsby computational neuroscience unit, university college london, london w1t 4jg, united kingdom\n\n4department of chemical engineering, technion israel institute of technology, haifa 32000, israel\n\n5rappaport faculty of medicine, technion israel institute of technology, haifa 32000, israel\n\n(received 10 november 2020; accepted 29 january 2021; published 23 february 2021)\n\na fundamental feature of complex biological systems is the ability to form feedback interactions with their\nenvironment. a prominent model for studying such interactions ", "i an update to this article is included at the end\n\nwriting memories with light-addressable\nreinforcement circuitry\n\nadam claridge-chang,1,3 robert d. roorda,1 eleftheria vrontou,1 lucas sjulson,1,4 haiyan li,2,5 jay hirsh,2\nand gero miesenbo\u00a8 ck1,*\n1department of physiology, anatomy and genetics, university of oxford, parks road, oxford ox1 3pt, uk\n2department of biology, university of virginia, gilmer hall, charlottesville, va 22903, usa\n3present address: wellcome trust centre for human genetics, university of oxford, roosevelt drive, oxford ox3 7bn, uk\n4present address: department of psychiatry, new york university school of medicine, 550 first avenue, new york, ny 10016, usa\n5present address: department of psychiatry, university of california, 401 parnassus avenue, san francisco, ca 94143, usa\n*correspondence: gero.miesenboeck@dpag.ox.ac.uk\ndoi 10.1016/j.cell.2009.08.034\n\nsummary\n\ndopaminergic neurons are thought to drive learning\nby signaling changes in the expectations of salient", "research article\n\ndynamic representation of partially\noccluded objects in primate prefrontal\nand visual cortex\namber m fyall1\u2020, yasmine el-shamayleh2\u2020, hannah choi3, eric shea-brown4,\nanitha pasupathy5*\n\n1department of biological structure, washington national primate research center,\nuniversity of washington, seattle, united states; 2physiology and biophysics,\nwashington national primate research center, university of washington, seattle,\nunited states; 3applied mathematics, university of washington institute for\nneuroengineering, university of washington, seattle, united states; 4department\nof applied mathematics, university of washington, seattle, united states;\n5department of biological structure, university of washington, seattle, united\nstates\n\nabstract successful recognition of partially occluded objects is presumed to involve dynamic\ninteractions between brain areas responsible for vision and cognition, but neurophysiological\nevidence for the involvement of feedback signals is ", "deep neural networks rival the representation of\nprimate it cortex for core visual object recognition\n\ncharles f. cadieu1*, ha hong1,2, daniel l. k. yamins1, nicolas pinto1, diego ardila1, ethan a. solomon1,\nnajib j. majaj1, james j. dicarlo1\n\n1 department of brain and cognitive sciences and mcgovern institute for brain research, massachusetts institute of technology, cambridge, massachusetts, united states\nof america, 2 harvard\u2013mit division of health sciences and technology, institute for medical engineering and science, massachusetts institute of technology, cambridge,\nmassachusetts, united states of america\n\nabstract\n\nthe primate visual system achieves remarkable visual object recognition performance even in brief presentations, and\nunder changes to object exemplar, geometric transformations, and background variation (a.k.a. core visual object\nrecognition). this remarkable performance is mediated by the representation formed in inferior temporal (it) cortex. in\nparallel, recent adva", "journal of machine learning research 9 (2008) 2579-2605\n\nsubmitted 5/08; revised 9/08; published 11/08\n\nvisualizing data using t-sne\n\nlaurens van der maaten\nticc\ntilburg university\np.o. box 90153, 5000 le tilburg, the netherlands\n\ngeoffrey hinton\ndepartment of computer science\nuniversity of toronto\n6 king\u2019s college road, m5s 3g4 toronto, on, canada\n\neditor: yoshua bengio\n\nlvdmaaten@gmail.com\n\nhinton@cs.toronto.edu\n\nabstract\n\nwe present a new technique called \u201ct-sne\u201d that visualizes high-dimensional data by giving each\ndatapoint a location in a two or three-dimensional map. the technique is a variation of stochastic\nneighbor embedding (hinton and roweis, 2002) that is much easier to optimize, and produces\nsigni\ufb01cantly better visualizations by reducing the tendency to crowd points together in the center\nof the map. t-sne is better than existing techniques at creating a single map that reveals structure\nat many different scales. this is particularly important for high-dimensional data tha", "nerf in the wild: neural radiance fields for unconstrained photo collections\n\nricardo martin-brualla\u2217, noha radwan\u2217, mehdi s. m. sajjadi\u2217,\njonathan t. barron, alexey dosovitskiy, and daniel duckworth\n\n{rmbrualla, noharadwan, msajjadi, barron, adosovitskiy, duckworthd}@google.com\n\ngoogle research\n\nabstract\n\nwe present a learning-based method for synthesizing\nnovel views of complex scenes using only unstructured col-\nlections of in-the-wild photographs. we build on neural\nradiance fields (nerf), which uses the weights of a multi-\nlayer perceptron to model the density and color of a scene\nas a function of 3d coordinates. while nerf works well on\nimages of static subjects captured under controlled settings,\nit is incapable of modeling many ubiquitous, real-world\nphenomena in uncontrolled images, such as variable illu-\nmination or transient occluders. we introduce a series of\nextensions to nerf to address these issues, thereby enabling\naccurate reconstructions from unstructured image collec", "align, then memorise:\n\nthe dynamics of learning with feedback alignment\n\nmaria re\ufb01netti * 1 2 st\u00b4ephane d\u2019ascoli * 1 3 ruben ohana 1 4 sebastian goldt 5\n\n1\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n8\n2\n4\n2\n1\n\n.\n\n1\n1\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\ndirect feedback alignment (dfa) is emerging\nas an ef\ufb01cient and biologically plausible alterna-\ntive to backpropagation for training deep neural\nnetworks. despite relying on random feedback\nweights for the backward pass, dfa successfully\ntrains state-of-the-art models such as transform-\ners. on the other hand, it notoriously fails to train\nconvolutional networks. an understanding of the\ninner workings of dfa to explain these diverg-\ning results remains elusive. here, we propose\na theory of feedback alignment algorithms. we\n\ufb01rst show that learning in shallow networks pro-\nceeds in two steps: an alignment phase, where the\nmodel adapts its weights to align the approximate\ngradient with the true gradient of the loss function,\nis followed ", "r e v i e w\n\nthe neocortical circuit: themes and \nvariations\n\nkenneth d harris1 & gordon m g shepherd2\n\nsimilarities in neocortical circuit organization across areas and species suggest a common strategy to process diverse types of \ninformation, including sensation from diverse modalities, motor control and higher cognitive processes. cortical neurons belong to \na small number of main classes. the properties of these classes, including their local and long-range connectivity, developmental \nhistory, gene expression, intrinsic physiology and in vivo activity patterns, are remarkably similar across areas. each class contains \nsubclasses; for a rapidly growing number of these, conserved patterns of input and output connections are also becoming evident. \nthe ensemble of circuit connections constitutes a basic circuit pattern that appears to be repeated across neocortical areas, with \narea- and species-specific modifications. such \u2018serially homologous\u2019 organization may adapt individual neo", "8\n1\n0\n2\n\n \nt\nc\no\n1\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n0\n0\n9\n0\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nalgorithmic regularization in learning deep homogeneous\n\nmodels: layers are automatically balanced\n\nsimon s. du\u2217\n\nwei hu\u2020\n\njason d. lee\u2021\n\nabstract\n\nwe study the implicit regularization imposed by gradient descent for learning multi-layer homogeneous\nfunctions including feed-forward fully connected and convolutional deep neural networks with linear,\nrelu or leaky relu activation. we rigorously prove that gradient \ufb02ow (i.e. gradient descent with\nin\ufb01nitesimal step size) e\ufb00ectively enforces the di\ufb00erences between squared norms across di\ufb00erent layers\nto remain invariant without any explicit regularization. this result implies that if the weights are\ninitially small, gradient \ufb02ow automatically balances the magnitudes of all layers. using a discretization\nargument, we analyze gradient descent with positive step size for the non-convex low-rank asymmetric\ninspired by our \ufb01ndings for gradient \ufb02ow,\nmatrix fac", "published as a conference paper at iclr 2019\n\nkernel rnn learning (kernl)\n\nchristopher roth[1],[2], ingmar kanitscheider[2],[3], and ila fiete[2],[4]\n[1] department of physics, university of texas at austin, austin, tx, 78712\n[2] department of neuroscience, university of texas at austin, austin, tx, 78712\n[3] openai, san francisco ca, 94110\n[4] department of brain and cognitive sciences, massachusetts institute of technology, cambridge, ma, 02139\n\nabstract\n\nwe describe kernel rnn learning (kernl), a reduced-rank, temporal eligibility\ntrace-based approximation to backpropagation through time (bptt) for training\nrecurrent neural networks (rnns) that gives competitive performance to bptt\non long time-dependence tasks. the approximation replaces a rank-4 gradient\nlearning tensor, which describes how past hidden unit activations affect the cur-\nrent state, by a simple reduced-rank product of a sensitivity weight and a temporal\neligibility trace. in this structured approximation motivated by", "\u00a0\u00a0\u00a0\u00a0a\u00a0tutorial\u00a0on\n\nenergy\u00adbased\u00a0learning\n\n\u00a0yann\u00a0lecun,\u00a0sumit\u00a0chopra,\u00a0raia\u00a0hadsell,\u00a0\n\nmarc'aurelio\u00a0ranzato,\u00a0fu\u00adjie\u00a0huang\n\n\u00a0\u00a0\u00a0\u00a0the\u00a0courant\u00a0institute\u00a0of\u00a0mathematical\u00a0sciences\n\nnew\u00a0york\u00a0university\n\nhttp://yann.lecun.com\nhttp://www.cs.nyu.edu/~yann\n\nyann\u00a0lecun\n\n\f", "human neuroscience\na bayesian foundation for individual learning under \nuncertainty\n\noriginal research article\npublished: 02 may 2011\ndoi: 10.3389/fnhum.2011.00039\n\nchristoph mathys1,2*, jean daunizeau1,3, karl j. friston3 and klaas e. stephan1,3\n\n1  laboratory for social and neural systems research, department of economics, university of zurich, zurich, switzerland\n2  institute for biomedical engineering, eidgen\u00f6ssische technische hochschule zurich, zurich, switzerland\n3  wellcome trust centre for neuroimaging, institute of neurology, university college london, london, uk\n\nedited by:\nsven bestmann, university college \nlondon, uk\nreviewed by:\nquentin huys, university college \nlondon, uk\nrobert c. wilson, princeton university, \nusa\n*correspondence:\nchristoph mathys, laboratory for \nsocial and neural systems research, \ndepartment of economics, university \nof zurich, bluemlisalpstrasse 10, 8006 \nzurich, switzerland. \ne-mail: chmathys@ethz.ch\n\ncomputational learning models are critical for", "2\n2\n0\n2\n\n \n\nb\ne\nf\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n4\n8\n3\n8\n0\n\n.\n\n2\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nlimitations of neural collapse\n\nfor understanding generalization in deep learning\n\nlike hui\n\nlhui@ucsd.edu\n\nmikhail belkin\n\nmbelkin@ucsd.edu\n\npreetum nakkiran\npreetum@ucsd.edu\n\nhal\u0131c\u0131o\u02d8glu data science institute\nuniversity of california san diego\n\nabstract\n\n(cid:145)e recent work of papyan, han, and donoho (2020) presented an intriguing \u201cneural collapse\u201d phenomenon,\nshowing a structural property of interpolating classi(cid:128)ers in the late stage of training. (cid:145)is opened a rich area of\nexploration studying this phenomenon. our motivation is to study the upper limits of this research program: how\nfar will understanding neural collapse take us in understanding deep learning? first, we investigate its role in\ngeneralization. we re(cid:128)ne the neural collapse conjecture into two separate conjectures: collapse on the train set\n(an optimization property) and collapse on the test distribution ", "neuron\n\nreview\n\ndopamine in motivational control:\nrewarding, aversive, and alerting\n\nethan s. bromberg-martin,1 masayuki matsumoto,1,2 and okihide hikosaka1,*\n1laboratory of sensorimotor research, national eye institute, national institutes of health, bethesda, maryland 20892, usa\n2primate research institute, kyoto university, inuyama, aichi 484-8506, japan\n*correspondence: oh@lsr.nei.nih.gov\ndoi 10.1016/j.neuron.2010.11.022\n\nmidbrain dopamine neurons are well known for their strong responses to rewards and their critical role\nin positive motivation. it has become increasingly clear, however, that dopamine neurons also transmit\nsignals related to salient but nonrewarding experiences such as aversive and alerting events. here we review\nrecent advances in understanding the reward and nonreward functions of dopamine. based on this data, we\npropose that dopamine neurons come in multiple types that are connected with distinct brain networks and\nhave distinct roles in motivational control. s", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nmeta-learning  in  natural  and  arti\ufb01cial  intelligence\njane  x  wang\n\nmeta-learning,  or  learning  to  learn,  has  gained  renewed  interest\nin  recent  years  within  the  arti\ufb01cial  intelligence  community.\nhowever,  meta-learning  is  incredibly  prevalent  within  nature,\nhas  deep  roots  in  cognitive  science  and  psychology,  and  is\ncurrently  studied  in  various  forms  within  neuroscience.  the  aim\nof  this  review  is  to  recast  previous  lines  of  research  in  the  study\nof  biological  intelligence  within  the  lens  of  meta-learning,\nplacing  these  works  into  a  common  framework.  more  recent\npoints  of  interaction  between  ai  and  neuroscience  will  be\ndiscussed,  as  well  as  interesting  new  directions  that  arise  under\nthis  perspective.\n\naddress\ndeepmind  technologies  ltd,  united  kingdom\n\ncorresponding  author:  wang,  jane  x  (wangjane@google.com)\n\ncurrent  opinion  ", "cognitive, affective, & behavioral neuroscience\n2004, 4 (4), 483-500\n\nvisual attention as a multilevel selection process\n\nsabine kastner and mark a. pinsk\n\nprinceton university, princeton, new jersey\n\nnatural visual scenes are cluttered and contain many different objects that cannot all be processed\nsimultaneously. therefore, attentional mechanisms are needed to select relevant and to filter out ir-\nrelevant information. evidence from functional brain imaging reveals that attention operates at vari-\nous processing levels within the visual system and beyond. first, the lateral geniculate nucleus appears\nto be the first stage in the processing of visual information that is modulated by attention, consistent\nwith the idea that it may play an important role as an early gatekeeper in controlling neural gain. sec-\nond, areas at intermediate cortical-processing levels, such as v4 and teo, appear to be important sites\nat which attention filters out unwanted information by means of receptive fi", "university of massachusetts amherst\nscholarworks@umass amherst\ncomputer science department faculty publication\nseries\n\ncomputer science\n\n2000\n\neligibility traces for off-policy policy evaluation\n\ndoina precup\nuniversity of massachusetts amherst\n\nrichard s. sutton\nat&t\n\nsatinder singh\nat&t\n\nfollow this and additional works at: https://scholarworks.umass.edu/cs_faculty_pubs\n\npart of the computer sciences commons\n\nrecommended citation\nprecup, doina; sutton, richard s.; and singh, satinder, \"eligibility traces for off-policy policy evaluation\" (2000). icml '00\nproceedings of the seventeenth international conference on machine learning. 80.\nretrieved from https://scholarworks.umass.edu/cs_faculty_pubs/80\n\nthis article is brought to you for free and open access by the computer science at scholarworks@umass amherst. it has been accepted for inclusion\nin computer science department faculty publication series by an authorized administrator of scholarworks@umass amherst. for more information,\npl", "9\n1\n0\n2\n\n \n\ny\na\nm\n8\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n4\n0\n8\n3\n0\n\n.\n\n1\n1\n8\n1\n:\nv\ni\nx\nr\na\n\ngradient descent finds global minima of deep neural networks\n\nsimon s. du * 1 jason d. lee * 2 haochuan li * 3 4 liwei wang * 5 4 xiyu zhai * 6\n\nabstract\n\ngradient descent \ufb01nds a global minimum in train-\ning deep neural networks despite the objective\nfunction being non-convex. the current pa-\nper proves gradient descent achieves zero train-\ning loss in polynomial time for a deep over-\nparameterized neural network with residual con-\nnections (resnet). our analysis relies on the\nparticular structure of the gram matrix induced\nby the neural network architecture. this struc-\nture allows us to show the gram matrix is stable\nthroughout the training process and this stability\nimplies the global optimality of the gradient de-\nscent algorithm. we further extend our analysis\nto deep residual convolutional neural networks\nand obtain a similar convergence result.\n\nthe second mysterious phenomenon in training d", "5314 \u2022 the journal of neuroscience, may 11, 2016 \u2022 36(19):5314 \u20135327\n\nsystems/circuits\n\nselectivity of neuromodulatory projections from the basal\nforebrain and locus ceruleus to primary sensory cortices\n\njae-hyun kim,1 a-hyun jung,1 daun jeong,1 ilsong choi,1 kwansoo kim,1 soowon shin,2 sung june kim,2\nand x seung-hee lee1\n1department of biological sciences, korea advanced institute of science and technology, daejeon 34141, korea, and 2department of electrical and\ncomputer engineering, seoul national university, seoul 08826, korea\n\nacetylcholine and noradrenaline are major neuromodulators that affect sensory processing in the cortex. modality-specific sensory\ninformation is processed in defined areas of the cortex, but it is unclear whether cholinergic neurons in the basal forebrain (bf) and\nnoradrenergic neurons in the locus ceruleus (lc) project to and modulate these areas in a sensory modality-selective manner. here, we\nmapped bf and lc projections to different sensory cortices of t", "a r t i c l e s\n\nconjunctive input processing drives feature selectivity \nin hippocampal ca1 neurons\nkatie c bittner1, christine grienberger1, sachin p vaidya1, aaron d milstein1, john j macklin1, junghyup suh2,3, \nsusumu tonegawa2,3 & jeffrey c magee1\n\nfeature-selective firing allows networks to produce representations of the external and internal environments. despite its \nimportance, the mechanisms generating neuronal feature selectivity are incompletely understood. in many cortical microcircuits \nthe integration of two functionally distinct inputs occurs nonlinearly through generation of active dendritic signals that drive \nburst firing and robust plasticity. to examine the role of this processing in feature selectivity, we recorded ca1 pyramidal neuron \nmembrane potential and local field potential in mice running on a linear treadmill. we found that dendritic plateau potentials \nwere produced by an interaction between properly timed input from entorhinal cortex and hippocampal ca3", "understanding the dif\ufb01culty of training deep feedforward neural networks\n\nxavier glorot\n\nyoshua bengio\n\ndiro, universit\u00b4e de montr\u00b4eal, montr\u00b4eal, qu\u00b4ebec, canada\n\nabstract\n\nwhereas before 2006 it appears that deep multi-\nlayer neural networks were not successfully\ntrained, since then several algorithms have been\nshown to successfully train them, with experi-\nmental results showing the superiority of deeper\nvs less deep architectures. all these experimen-\ntal results were obtained with new initialization\nor training mechanisms. our objective here is to\nunderstand better why standard gradient descent\nfrom random initialization is doing so poorly\nwith deep neural networks, to better understand\nthese recent relative successes and help design\nbetter algorithms in the future. we \ufb01rst observe\nthe in\ufb02uence of the non-linear activations func-\ntions. we \ufb01nd that the logistic sigmoid activation\nis unsuited for deep networks with random ini-\ntialization because of its mean value, which can\ndrive ", "convolutional networks for fast, energy-efficient\nneuromorphic computing\n\nsteven k. essera,1, paul a. merollaa, john v. arthura, andrew s. cassidya, rathinakumar appuswamya,\nalexander andreopoulosa, david j. berga, jeffrey l. mckinstrya, timothy melanoa, davis r. barcha, carmelo di nolfoa,\npallab dattaa, arnon amira, brian tabaa, myron d. flicknera, and dharmendra s. modhaa\n\nabrain-inspired computing, ibm research\u2013almaden, san jose, ca 95120\n\nedited by karlheinz meier, university of heidelberg, heidelberg, germany, and accepted by editorial board member j. a. movshon august 9, 2016 (received\nfor review march 24, 2016)\n\ny\nr\na\nt\nn\ne\nm\nm\no\nc\n\ne\ne\ns\n\ndeep networks are now able to achieve human-level performance\non a broad spectrum of recognition tasks. independently, neuromorphic\ncomputing has now demonstrated unprecedented energy-efficiency\nthrough a new chip architecture based on spiking neurons, low\nprecision synapses, and a scalable communication network. here, we\ndemonstrate that neur", "the journal of neuroscience, october 29, 2003 \u2022 23(30):9833\u20139841 \u2022 9833\n\nbehavioral/systems/cognitive\n\nlesions of nucleus accumbens disrupt learning about\naversive outcomes\n\ngeoffrey schoenbaum and barry setlow\ndepartment of psychological and brain sciences, johns hopkins university, baltimore, maryland 21218\n\nnucleus accumbens (nacc) is critical for encoding and using information regarding the learned significance of cues predictive of reward.\nhowever, its role in processing information about cues predictive of aversive outcomes is less well studied. here, we examined the effects\nof nacc lesions in an odor-guided discrimination task in which rats use odor cues predictive of either appetitive or aversive outcomes to\nguide responding. rats with sham or neurotoxic lesions of nacc were trained on a series of two-odor discrimination problems. perfor-\nmance on each problem was assessed by monitoring accuracy of choice behavior and by measuring latency to respond for fluid reinforce-\nment af", "a r t i c l e s\n\nproperties of basal dendrites of layer 5 pyramidal\nneurons: a direct patch-clamp recording study\n\nthomas nevian1,2,4, matthew e larkum1,4, alon polsky3 & jackie schiller3\n\nbasal dendrites receive the majority of synapses that contact neocortical pyramidal neurons, yet our knowledge of synaptic\nprocessing in these dendrites has been hampered by their inaccessibility for electrical recordings. a new approach to patch-clamp\nrecordings enabled us to characterize the integrative properties of these cells. despite the short physical length of rat basal\ndendrites, synaptic inputs were electrotonically remote from the soma ( > 30-fold excitatory postsynaptic potential (epsp)\nattenuation) and back-propagating action potentials were signi\ufb01cantly attenuated. unitary epsps were location dependent,\nreaching large amplitudes distally ( > 8 mv), yet their somatic contribution was relatively location independent. basal dendrites\nsupport sodium and nmda spikes, but not calcium spikes, ", "9\n1\n0\n2\n\n \n\np\ne\ns\n8\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n6\n5\n1\n8\n0\n\n.\n\n9\n0\n9\n1\n:\nv\ni\nx\nr\na\n\ndynamics of deep neural networks and neural tangent hierarchy\n\njiaoyang huang\n\nias\n\ne-mail: jiaoyang@ias.edu\n\nhorng-tzer yau\n\nharvard university\n\ne-mail: htyau@math.harvard.edu\n\nabstract\n\nthe evolution of a deep neural network trained by the gradient descent can be described by its neural\ntangent kernel (ntk) as introduced in [20], where it was proven that in the in\ufb01nite width limit the\nntk converges to an explicit limiting kernel and it stays constant during training. the ntk was also\nimplicit in some other recent papers [6, 13, 14]. in the overparametrization regime, a fully-trained deep\nneural network is indeed equivalent to the kernel regression predictor using the limiting ntk. and the\ngradient descent achieves zero training loss for a deep overparameterized neural network. however, it\nwas observed in [5] that there is a performance gap between the kernel regression using the limiting ntk\nand ", "9\n1\n0\n2\n\n \n\nb\ne\nf\n2\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n5\nv\n5\n8\n8\n0\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nessentially no barriers in neural network energy landscape\n\nfelix draxler 1 2 kambis veschgini 2 manfred salmhofer 2 fred a. hamprecht 1\n\nabstract\n\ntraining neural networks involves \ufb01nding min-\nima of a high-dimensional non-convex loss func-\ntion. relaxing from linear interpolations, we con-\nstruct continuous paths between minima of re-\ncent neural network architectures on cifar10\nand cifar100. surprisingly, the paths are essen-\ntially \ufb02at in both the training and test landscapes.\nthis implies that minima are perhaps best seen as\npoints on a single connected manifold of low loss,\nrather than as the bottoms of distinct valleys.\n\nform a connected manifold. more precisely, we argue that\nthe part of the parameter space where the loss remains be-\nlow a certain low threshold forms one single connected\ncomponent.\nwe support the above claim by studying the energy land-\nscape of several resnets and dens", "letter\n\ncommunicated by jo\u00e3o sacramento\n\ncontrastive similarity matching for supervised learning\n\nshanshan qin\nssqin@g.harvard.edu\njohn a. paulson school of engineering and applied sciences, harvard university,\ncambridge, ma 02138, u.s.a.\n\nnayantara mudur\nnmudur@g.harvard.edu\ndepartment of physics, harvard university, cambridge, ma 02138, u.s.a.\n\ncengiz pehlevan\ncpehlevan@seas.harvard.edu\njohn a. paulson school of engineering and applied sciences, harvard university,\ncambridge, ma 02138, u.s.a.\n\nwe propose a novel biologically plausible solution to the credit assign-\nment problem motivated by observations in the ventral visual pathway\nand trained deep neural networks. in both, representations of objects\nin the same category become progressively more similar, while objects\nbelonging to different categories become less similar. we use this ob-\nservation to motivate a layer-specific learning goal in a deep network:\neach layer aims to learn a representational similarity matrix that inter-\n", "a fast pairwise heuristic for planning under uncertainty\n\nkoosha khalvati and alan k. mackworth\n\n{kooshakh, mack}@cs.ubc.ca\ndepartment of computer science\nuniversity of british columbia\n\nvancouver, b.c. v6t 1z4 canada\n\nabstract\n\npomdp (partially observable markov decision process) is\na mathematical framework that models planning under un-\ncertainty. solving a pomdp is an intractable problem and\neven the state of the art pomdp solvers are too computation-\nally expensive for large domains. this is a major bottleneck.\nin this paper, we propose a new heuristic, called the pairwise\nheuristic, that can be used in a one-step greedy strategy to \ufb01nd\na near optimal solution for pomdp problems very quickly.\nthis approach is a good candidate for large problems where\nreal-time solution is a necessity but exact optimality of the\nsolution is not vital. the pairwise heuristic uses the optimal\nsolutions for pairs of states. for each pair of states in the\npomdp, we \ufb01nd the optimal sequence of actions to", "multi-task deep reinforcement learning with popart\n\nmatteo hessel\n\nhubert soyer\n\nlasse espeholt\n\nwojciech czarnecki\n\nsimon schmitt\n\nhado van hasselt\n\n8\n1\n0\n2\n\n \n\np\ne\ns\n2\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n4\n7\n4\n4\n0\n\n.\n\n9\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nthe reinforcement\nlearning community has made great\nstrides in designing algorithms capable of exceeding human\nperformance on speci\ufb01c tasks. these algorithms are mostly\ntrained one task at the time, each new task requiring to train\na brand new agent instance. this means the learning algo-\nrithm is general, but each solution is not; each agent can only\nsolve the one task it was trained on. in this work, we study the\nproblem of learning to master not one but multiple sequential-\ndecision tasks at once. a general issue in multi-task learning\nis that a balance must be found between the needs of multiple\ntasks competing for the limited resources of a single learn-\ning system. many learning algorithms can get distracted by\ncertain tasks in the set", "ll\n\nopen access\n\nperspective\nplanning in the brain\n\nmarcelo g. mattar1,2,* and ma\u00b4 te\u00b4 lengyel2,3\n1department of cognitive science, university of california, san diego, san diego, ca, usa\n2computational and biological learning lab, department of engineering, university of cambridge, cambridge, uk\n3center for cognitive computation, department of cognitive science, central european university, budapest, hungary\n*correspondence: mmattar@ucsd.edu\nhttps://doi.org/10.1016/j.neuron.2021.12.018\n\nsummary\n\nrecent breakthroughs in arti\ufb01cial intelligence (ai) have enabled machines to plan in tasks previously thought\nto be uniquely human. meanwhile, the planning algorithms implemented by the brain itself remain largely un-\nknown. here, we review neural and behavioral data in sequential decision-making tasks that elucidate the\nways in which the brain does\u2014and does not\u2014plan. to systematically review available biological data, we\ncreate a taxonomy of planning algorithms by summarizing the relevant des", "independent component\nanalysis: recent advances\naapo hyv\u00e4rinen\n\nrsta.royalsocietypublishing.org\n\ndepartment of computer science, department of mathematics and\nstatistics, and hiit, university of helsinki, helsinki, finland\n\nreview\ncite this article: hyv\u00e4rinen a. 2013\nindependent component analysis: recent\nadvances. phil trans r soc a 371: 20110534.\nhttp://dx.doi.org/10.1098/rsta.2011.0534\n\none contribution of 17 to a discussion meeting\nissue \u2018signal processing and inference for the\nphysical sciences\u2019.\n\nsubject areas:\nstatistics, electrical engineering,\npattern recognition\n\nkeywords:\nindependent component analysis, blind source\nseparation, non-gaussianity, causal analysis\n\nauthor for correspondence:\naapo hyv\u00e4rinen\ne-mail: aapo.hyvarinen@helsinki.fi\n\nindependent component analysis is a probabilistic\nmethod for learning a linear transform of a random\nvector. the goal\nis to \ufb01nd components that are\nmaximally independent and non-gaussian (non-\nnormal). its fundamental difference to classical", "stochastic backpropagation and approximate inference\n\nin deep generative models\n\ndanilo j. rezende, shakir mohamed, daan wierstra\n\n{danilor, shakir, daanw}@google.com\n\ngoogle deepmind, london\n\n4\n1\n0\n2\n\n \n\ny\na\nm\n0\n3\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n2\n8\n0\n4\n\n.\n\n1\n0\n4\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe marry ideas from deep neural networks\nand approximate bayesian inference to derive\na generalised class of deep, directed genera-\ntive models, endowed with a new algorithm\nfor scalable inference and learning. our algo-\nrithm introduces a recognition model to rep-\nresent an approximate posterior distribution\nand uses this for optimisation of a variational\nlower bound. we develop stochastic back-\npropagation \u2013 rules for gradient backpropa-\ngation through stochastic variables \u2013 and de-\nrive an algorithm that allows for joint optimi-\nsation of the parameters of both the genera-\ntive and recognition models. we demonstrate\non several real-world data sets that by using\nstochastic backpropagation and va", "continual lifelong learning with neural networks:\n\na review\n\ngerman i. parisi1, ronald kemker2, jose l. part3, christopher kanan2, stefan wermter1\n\n1knowledge technology, department of informatics, universit\u00a8at hamburg, germany\n\n2chester f. carlson center for imaging science, rochester institute of technology, ny, usa\n\n3department of computer science, heriot-watt university, edinburgh centre for robotics, scotland, uk\n\n9\n1\n0\n2\n\n \n\nb\ne\nf\n1\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n9\n6\n5\n7\n0\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract: humans and animals have the ability to continually acquire, \ufb01ne-tune,\nand transfer knowledge and skills throughout their lifespan. this ability, referred\nto as lifelong learning, is mediated by a rich set of neurocognitive mechanisms\nthat together contribute to the development and specialization of our sensori-\nmotor skills as well as to long-term memory consolidation and retrieval. con-\nsequently, lifelong learning capabilities are crucial for computational systems\nand aut", "2\n2\n0\n2\n\n \nr\np\na\n9\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n5\nv\n6\n8\n6\n2\n0\n\n.\n\n7\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nmeta-learning through hebbian plasticity in\n\nrandom networks\n\nelias najarro and sebastian risi\n\nit university of copenhagen\n2300 copenhagen, denmark\nenaj@itu.dk, sebr@itu.dk\n\nabstract\n\nlifelong learning and adaptability are two de\ufb01ning aspects of biological agents.\nmodern reinforcement learning (rl) approaches have shown signi\ufb01cant progress\nin solving complex tasks, however once training is concluded, the found solutions\nare typically static and incapable of adapting to new information or perturbations.\nwhile it is still not completely understood how biological brains learn and adapt so\nef\ufb01ciently from experience, it is believed that synaptic plasticity plays a prominent\nrole in this process. inspired by this biological mechanism, we propose a search\nmethod that, instead of optimizing the weight parameters of neural networks\ndirectly, only searches for synapse-speci\ufb01c hebbian learning rules that allo", "a biophysical model of bidirectional synaptic\nplasticity: dependence on ampa\nand nmda receptors\n\ngastone c. castellani*, elizabeth m. quinlan\u2020, leon n cooper\u2021\u00a7\u00b6, and harel z. shouval\u2021i\n\n*physics department, cig and dimor\ufb01pa bologna university, bologna 40121, italy; \u2020department of biology, university of maryland, college park, md 20742;\nand \u2021institute for brain and neural systems, \u00a7department of neuroscience, and \u00b6department of physics, brown university, providence, ri 02912\n\ncontributed by leon n cooper, august 1, 2001\n\nin many regions of the brain, including the mammalian cortex, the\nmagnitude and direction of activity-dependent changes in synaptic\nstrength depend on the frequency of presynaptic stimulation (syn-\naptic plasticity), as well as the history of activity at those synapses\n(metaplasticity). we present a model of a molecular mechanism of\nbidirectional synaptic plasticity based on the observation that long-\nterm synaptic potentiation (ltp) and long-term synaptic depression\n(l", "2\n2\n0\n2\n\n \nr\na\n\n \n\nm\n1\n1\n\n \n \n]\ne\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n6\n5\n0\n6\n0\n\n.\n\n3\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nidentifying causal e\ufb00ects using instrumental time series:\n\nnuisance iv and correcting for the past\n\nnikolaj thams\n\nrikke s\u00f8ndergaard\n\nsebastian weichwald\n\njonas peters\n\ndepartment of mathematical sciences\nuniversity of copenhagen\ndenmark\n\nthams@math.ku.dk\n\nrsn@math.ku.dk\n\nsweichwald@math.ku.dk\n\njonas.peters@math.ku.dk\n\nabstract\n\ninstrumental variable (iv) regression relies on instruments to infer causal e\ufb00ects from observational\ndata with unobserved confounding. we consider iv regression in time series models, such as vector\nauto-regressive (var) processes. direct applications of i.i.d. techniques are generally inconsistent\nas they do not correctly adjust for dependencies in the past. in this paper, we propose methodology\nfor constructing identifying equations that can be used for consistently estimating causal e\ufb00ects.\nto do so, we develop nuisance iv, which can be of interest even in the i.i.d", "neuronal codes for arithmetic rule processing in the\nhuman brain\n\narticle\n\ngraphical abstract\n\nauthors\n\nesther f. kutter, jan bostro\u00a8 m,\nchristian e. elger, andreas nieder,\nflorian mormann\n\ncorrespondence\nandreas.nieder@uni-tuebingen.de (a.n.),\n\ufb02orian.mormann@ukbonn.de (f.m.)\n\nin brief\nkutter et al. demonstrate abstract and\nnotation-independent codes for addition\nand subtraction in neuronal populations\nin the human medial temporal lobe (mtl).\na dynamic code in the parahippocampal\ncortex contrasts with a static code in the\nhippocampus, suggesting different\ncognitive functions for these mtl regions\nin arithmetic.\n\nhighlights\nd single neurons in the human mtl show abstract codes for\n\naddition and subtraction\n\nd time-resolved decoding analysis shows a dynamic code in\n\nthe parahippocampal cortex\n\nd the hippocampus shows a static code based on persistently\n\nrule-selective neurons\n\nd different codes suggest different cognitive functions of mtl\n\nregions in arithmetic\n\nkutter et al., 2022, curr", "b r i e f c o m m u n i c at i o n s\n\nneural repetition suppression\nre\ufb02ects ful\ufb01lled perceptual\nexpectations\nchristopher summer\ufb01eld1,2, emily h trittschuh3,\njim m monti3, m-marsel mesulam3 & tobias egner3\n\nstimulus-evoked neural activity is attenuated on stimulus\nrepetition (repetition suppression), a phenomenon that is\nattributed to largely automatic processes in sensory neurons.\nby manipulating the likelihood of stimulus repetition, we found\nthat repetition suppression in the human brain was reduced\nwhen stimulus repetitions were improbable (and thus,\nunexpected). our data suggest that repetition suppression\nre\ufb02ects a relative reduction in top-down perceptual \u2018prediction\nerror\u2019 when processing an expected, compared with an\nunexpected, stimulus.\n\nstimulus-speci\ufb01c repetition suppression, the relative attenuation in\nneural signal evoked by the repeated occurrence of a stimulus, is one of\nthe most well-known neural phenomena1\u20134 and has been widely\nemployed in functional magnetic resonanc", "interpretable compositional convolutional neural networks\n\nwen shen1,\u2217 , zhihua wei1,\u2217 , shikun huang1 , binbin zhang1 , jiaqi fan1 , ping zhao1 ,\n\nquanshi zhang2,\u2020\n\n1tongji university, shanghai, china\n\n2shanghai jiao tong university, shanghai, china\n\n{wen shen,zhihua wei,hsk,0206zbb,1930795,zhaoping}@tongji.edu.cn,zqs1022@sjtu.edu.cn\n\n1\n2\n0\n2\n\n \nl\nu\nj\n \n\n9\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n4\n7\n4\n4\n0\n\n.\n\n7\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nthe reasonable de\ufb01nition of semantic interpretabil-\nity presents the core challenge in explainable ai.\nthis paper proposes a method to modify a tradi-\ntional convolutional neural network (cnn) into an\ninterpretable compositional cnn, in order to learn\n\ufb01lters that encode meaningful visual patterns in in-\ntermediate convolutional layers. in a compositional\ncnn, each \ufb01lter is supposed to consistently rep-\nresent a speci\ufb01c compositional object part or im-\nage region with a clear meaning. the composi-\ntional cnn learns from image labels for classi\ufb01ca-\ntion without ", "article\n\ndoi:10.1038/nature11649\n\nthe entorhinal grid map is discretized\n\nhanne stensola1*, tor stensola1*, trygve solstad1, kristian fr\u00f8land1, may-britt moser1 & edvard i. moser1\n\nthe medial entorhinal cortex (mec) is part of the brain\u2019s circuit for dynamic representation of self-location. the metric\nof this representation is provided by grid cells, cells with spatial firing fields that tile environments in a periodic\nhexagonal pattern. limited anatomical sampling has obscured whether the grid system operates as a unified system\nor a conglomerate of independent modules. here we show with recordings from up to 186 grid cells in individual rats that\ngrid cells cluster into a small number of layer-spanning anatomically overlapping modules with distinct scale,\norientation, asymmetry and theta-frequency modulation. these modules can respond independently to changes in\nthe geometry of the environment. the discrete topography of the grid-map, and the apparent autonomy of the\nmodules, differ ", "biorxiv preprint \n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\nhttps://doi.org/10.1101/2023.01.16.523429\n; \n\nthis version posted january 19, 2023. \n\nthe copyright holder for this preprint\n\ndoi: \n\navailable under a\n\ncc-by 4.0 international license\n.\n\na recurrent network model of planning explains hippocampal replay\n\nand human behavior\n\nkristopher t. jensen@1, guillaume hennequin\u22171, and marcelo g. mattar\u22172,3\n\n1 computational and biological learning lab, department of engineering, university of cambridge, cambridge, uk\n\n2 department of cognitive science, university of california, san diego, usa\n\n3 department of psychology, new york university, new york, usa\n\n@ corresponding author (ktj21@cam.ac.uk)\n\n\u2217 these authors jointly supervised the work\n\nabstract\n\nwhen interacting with complex environments, humans can rapidly adapt their behavior to changes in task\nor context. to facilitate this a", "material on science online.\n\ns. j. morrison, cell 121, 1109 (2005).\n\n8. m. j. kiel, o. h. yilmaz, t. iwashita, c. terhorst,\n9. d. a. sipkins et al., nature 435, 969 (2005).\n10. materials and methods are available as supporting\n11. a. peled et al., science 283, 845 (1999).\n12. t. lapidot, o. kollet, leukemia 16, 1992 (2002).\n13. h. e. broxmeyer, curr. opin. hematol. 15, 49 (2008).\n14. d. j. ceradini et al., nat. med. 10, 858 (2004).\n15. c. hitchon et al., arthritis rheum. 46, 2587 (2002).\n16. b. nervi, d. c. link, j. f. dipersio, j. cell. biochem. 99,\n17. g. calandra et al., bone marrow transplant. 41, 331 (2008).\n18. w. bensinger et al., j. clin. oncol. 13, 2547 (1995).\n19. n. okumura et al., blood 87, 4100 (1996).\n\n690 (2006).\n\n31, 1284 (2003).\n\n20. r. l. driessen, h. m. johnston, s. k. nilsson, exp. hematol.\n21. l. k. ashman, int. j. biochem. cell biol. 31, 1037 (1999).\n22. s. sharma et al., stem cells dev. 15, 755 (2006).\n23. n. th\u00e9ou-anton et al., br. j. cancer 94, 1180 (2006).\n24.", "the thirty-second aaai conference\non artificial intelligence (aaai-18)\n\ndistributional reinforcement\n\nlearning with quantile regression\n\nwill dabney\n\ndeepmind\n\nmark rowland\n\nuniversity of cambridge\u2217\n\nmarc g. bellemare\n\ngoogle brain\n\nr\u00b4emi munos\n\ndeepmind\n\nabstract\n\nin reinforcement learning (rl), an agent interacts with the\nenvironment by taking actions and observing the next state\nand reward. when sampled probabilistically, these state tran-\nsitions, rewards, and actions can all induce randomness in\nthe observed long-term return. traditionally, reinforcement\nlearning algorithms average over this randomness to estimate\nthe value function. in this paper, we build on recent work ad-\nvocating a distributional approach to reinforcement learning\nin which the distribution over returns is modeled explicitly\ninstead of only estimating the mean. that is, we examine\nmethods of learning the value distribution instead of the value\nfunction. we give results that close a number of gaps between\nthe t", "communicated by david haussler \n\na practical bayesian framework for backpropagation \nnetworks \n\ndavid j.  c. mackay\u2019 \ncomputation and neural systems, california lnstitute of technology 139-74, \npasadena, c a  91125 usa \n\na quantitative and practical bayesian framework is described for learn- \ning  of  mappings  in  feedforward  networks.  the  framework  makes \npossible (1) objective comparisons between solutions using alternative \nnetwork architectures, (2) objective stopping rules for network  prun- \ning or growing procedures, (3) objective choice of  magnitude and type \nof  weight  decay terms  or  additive  regularizers  (for penalizing  large \nweights, etc.), (4) a measure of  the effective number of  well-determined \nparameters in a model, (5) quantified estimates of  the error bars on net- \nwork parameters and on network output, and (6) objective comparisons \nwith alternative learning and interpolation models such as splines and \nradial basis functions. the bayesian \u201cevidence\u201d a", "letter\n\ncommunicated by alessandro treves\n\nmemory states and transitions between them\nin attractor neural networks\n\nstefano recanatesi\nstefano.recanatesi@gmail.com\nmikhail katkov\nmikhail.katkov@gmail.com\nmisha tsodyks\nmtsodyks@gmail.com\nneurobiology department, weizmann institute of science, rehovot 76100, israel\n\nhuman memory is capable of retrieving similar memories to a just re-\ntrieved one. this associative ability is at the base of our everyday pro-\ncessing of information. current models of memory have not been able\nto underpin the mechanism that the brain could use in order to actively\nexploit similarities between memories. the current idea is that to induce\ntransitions in attractor neural networks, it is necessary to extinguish the\ncurrent memory. we introduce a novel mechanism capable of inducing\ntransitions between memories where similarities between memories are\nactively exploited by the neural dynamics to retrieve a new memory. pop-\nulations of neurons that are selective for", "p\ns\ny\nc\nh\no\nm\ne\nt\nr\ni\nk\na\n-\n-\nv\no\nl\n.\n \n3\n1\n~\n \nn\no\n,\n \n1\n \nm\na\na\nc\nh\n,\n \n1\n9\n6\n6\n \na\n \ng\ne\nn\ne\nr\na\nl\ni\nz\ne\nd\n \ns\no\nl\nu\nt\ni\no\nn\n \no\nf\n \nt\nh\ne\n \no\nr\nt\nh\no\ng\no\nn\na\nl\n \np\nr\no\nc\nr\nu\ns\nt\ne\ns\n \np\nr\no\nb\nl\ne\nm\n*\n \np\ne\nt\ne\nr\n \nh\n.\n \ns\nc\ni\n-\ni\n6\nn\ne\nm\na\nn\nn\n \np\ns\ny\nc\nh\no\nm\ne\nt\nr\ni\nc\n \nl\na\nb\no\nr\na\nt\no\nr\ny\n \nn\no\nr\nt\nh\n \nc\na\nr\no\nl\ni\nn\na\n \nt\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n \no\nf\n \na\n \ns\no\nl\nu\nt\ni\no\nn\n \nt\n \no\nf\n \nt\nh\ne\n \nl\ne\na\ns\nt\n-\ns\nq\nu\na\nr\ne\ns\n \np\nr\no\nb\nl\ne\nm\n \na\n \nt\n \n=\n \nb\n \n+\n \ne\n,\n \ng\ni\nv\ne\nn\n \na\n \na\nn\nd\n \nb\n \ns\no\n \nt\nh\na\nt\n \nt\nr\na\nc\ne\n \n(\ne\n'\ne\n)\n \n-\n~\n \nm\ni\nn\ni\nm\nu\nm\n \na\nn\nd\n \nt\n'\n~\n'\n \n=\n \ni\n \ni\ns\n \np\nr\ne\ns\ne\nn\nt\ne\nd\n.\n \ni\nt\n \ni\ns\n \nc\no\nm\np\na\nr\ne\nd\n \nw\ni\nt\nh\n \na\n \nl\ne\ns\ns\n \ng\ne\nn\ne\nr\na\nl\n \ns\no\nl\nu\nt\ni\no\nn\n \no\nf\n \nt\nh\ne\n \ns\na\nm\ne\n \np\nr\no\nb\nl\ne\nm\n \nw\nh\ni\nc\nh\n \nw\na\ns\n \ng\ni\nv\ne\nn\n \nb\ny\n \ng\nr\ne\ne\nn\n \n[\n5\n]\n.\n \nt\nh\ne\n \np\nr\ne\ns\ne\nn\nt\n \ns\no\nl\nu\nt\ni\no\nn\n,\n \ni\nn\n \nc\no\nn\nt\nr\na\ns\nt\n \nt\no\n \ng\nr\ne\ne\nn\n'\ns\n,\n \ni\ns\n \na\np\np\nl\ni\nc\na\nb\nl\ne\n \nt\no\n \nm\na\nt\nr\ni\nc\ne\ns\n \na\n \na\nn\nd\n \nb\n \nw\nh\ni\nc\nh\n \na\nr\ne\n \no\nf\n \nl\ne\ns\ns\n \nt\n", "0\n2\n0\n2\n\n \n\ny\na\nm\n3\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n4\n5\n0\n2\n0\n\n.\n\n0\n1\n9\n1\n:\nv\ni\nx\nr\na\n\nzero: memory optimizations toward training trillion\n\nparameter models\n\nsamyam rajbhandari\u2217, je\ufb00 rasley\u2217, olatunji ruwase, yuxiong he\n\n{samyamr, jerasley, olruwase, yuxhe}@microsoft.com\n\nabstract\n\nlarge deep learning models o\ufb00er signi\ufb01cant accuracy gains, but training billions to trillions of\nparameters is challenging. existing solutions such as data and model parallelisms exhibit funda-\nmental limitations to \ufb01t these models into limited device memory, while obtaining computation,\ncommunication and development e\ufb03ciency. we develop a novel solution, zero redundancy\noptimizer (zero), to optimize memory, vastly improving training speed while increasing the\nmodel size that can be e\ufb03ciently trained. zero eliminates memory redundancies in data- and\nmodel-parallel training while retaining low communication volume and high computational\ngranularity, allowing us to scale the model size proportional to the nu", "optical control of neuronal ion \nchannels and receptors\n\n 3*\n\n 2* and alexandre\u00a0mourot \n\n 1*, graham\u00a0c.\u00a0r.\u00a0ellis- davies \n\npierre\u00a0paoletti \nabstract | light- controllable tools provide powerful means to manipulate and interrogate  \nbrain function with relatively low invasiveness and high spatiotemporal precision. although \noptogenetic approaches permit neuronal excitation or inhibition at the network level, other \ntechnologies, such as optopharmacology (also known as photopharmacology) have emerged  \nthat provide molecular- level control by endowing light sensitivity to endogenous biomolecules.  \nin this review , we discuss the challenges and opportunities of photocontrolling native neuronal \nsignalling pathways, focusing on ion channels and neurotransmitter receptors. we describe \nexisting strategies for rendering receptors and channels light sensitive and provide an overview of \nthe neuroscientific insights gained from such approaches. at the crossroads of chemistry , protein \nengine", "mach learn (2016) 102:349\u2013391\ndoi 10.1007/s10994-015-5528-6\n\nsupersparse linear integer models for optimized medical\nscoring systems\nberk ustun1 \u00b7 cynthia rudin2\n\nreceived: 1 february 2015 / accepted: 5 august 2015 / published online: 5 november 2015\n\u00a9 the author(s) 2015\n\nabstract scoring systems are linear classi\ufb01cation models that only require users to add,\nsubtract and multiply a few small numbers in order to make a prediction. these models are in\nwidespread use by the medical community, but are dif\ufb01cult to learn from data because they\nneed to be accurate and sparse, have coprime integer coef\ufb01cients, and satisfy multiple oper-\national constraints. we present a new method for creating data-driven scoring systems called\na supersparse linear integer model (slim). slim scoring systems are built by using an\ninteger programming problem that directly encodes measures of accuracy (the 0\u20131 loss) and\nsparsity (the (cid:2)0-seminorm) while restricting coef\ufb01cients to coprime integers. slim can ", "an empirical investigation of catastrophic forgetting in\n\ngradient-based neural networks\n\nian j. goodfellow\nmehdi mirza\nda xiao\naaron courville\nyoshua bengio\n\nabstract\n\ncatastrophic forgetting is a problem faced\nby many machine learning models and al-\ngorithms. when trained on one task, then\ntrained on a second task, many machine\nlearning models \u201cforget\u201d how to perform the\n\ufb01rst task. this is widely believed to be a\nserious problem for neural networks. here,\nwe investigate the extent to which the catas-\ntrophic forgetting problem occurs for mod-\nern neural networks, comparing both estab-\nlished and recent gradient-based training al-\ngorithms and activation functions. we also\nexamine the e\ufb00ect of the relationship between\nthe \ufb01rst task and the second task on catas-\ntrophic forgetting. we \ufb01nd that it is always\nbest to train using the dropout algorithm\u2013\nthe dropout algorithm is consistently best at\nadapting to the new task, remembering the\nold task, and has the best tradeo\ufb00 curve be-\ntween ", "9\n1\n0\n2\n\n \nt\nc\no\n5\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n4\n0\n1\n7\n0\n\n.\n\n0\n1\n9\n1\n:\nv\ni\nx\nr\na\n\northogonal gradient descent for continual learning\n\nmehrdad farajtabar\n\ndeepmind\n\nnavid azizan1\n\ncaltech\n\nalex mott\ndeepmind\n\nang li\ndeepmind\n\nabstract\n\nneural networks are achieving state of the art\nand sometimes super-human performance on\nlearning tasks across a variety of domains.\nwhenever these problems require learning in\na continual or sequential manner, however,\nneural networks su\ufb00er from the problem of\ncatastrophic forgetting; they forget how to\nsolve previous tasks after being trained on a\nnew task, despite having the essential capac-\nity to solve both tasks if they were trained on\nboth simultaneously. in this paper, we pro-\npose to address this issue from a parameter\nspace perspective and study an approach to\nrestrict the direction of the gradient updates\nto avoid forgetting previously-learned data.\nwe present the orthogonal gradient descent\n(ogd) method, which accomplishes this goal\nby pr", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nrecurrent  neural  networks  as  versatile  tools  of\nneuroscience  research\nomri  barak\n\nrecurrent  neural  networks  (rnns)  are  a  class  of  computational\nmodels  that  are  often  used  as  a  tool  to  explain  neurobiological\nphenomena,  considering  anatomical,  electrophysiological  and\ncomputational  constraints.\nrnns  can  either  be  designed  to  implement  a  certain\ndynamical  principle,  or  they  can  be  trained  by  input\u2013output\nexamples.  recently,  there  has  been  large  progress  in  utilizing\ntrained  rnns  both  for  computational  tasks,  and  as  explanations\nof  neural  phenomena.  i  will  review  how  combining  trained  rnns\nwith  reverse  engineering  can  provide  an  alternative  framework\nfor  modeling  in  neuroscience,  potentially  serving  as  a  powerful\nhypothesis  generation  tool.\ndespite  the  recent  progress  and  potential  bene\ufb01ts,  there  are\nmany  fundamental  gaps  towards ", "understanding black-box predictions via in\ufb02uence functions\n\npang wei koh 1 percy liang 1\n\nabstract\n\nhow can we explain the predictions of a black-\nbox model? in this paper, we use in\ufb02uence func-\ntions \u2014 a classic technique from robust statis-\ntics \u2014 to trace a model\u2019s prediction through the\nlearning algorithm and back to its training data,\nthereby identifying training points most respon-\nsible for a given prediction. to scale up in\ufb02uence\nfunctions to modern machine learning settings,\nwe develop a simple, ef\ufb01cient implementation\nthat requires only oracle access to gradients and\nhessian-vector products. we show that even on\nnon-convex and non-differentiable models where\nthe theory breaks down, approximations to in\ufb02u-\nence functions can still provide valuable infor-\nmation. on linear models and convolutional neu-\nral networks, we demonstrate that in\ufb02uence func-\ntions are useful for multiple purposes: under-\nstanding model behavior, debugging models, de-\ntecting dataset errors, and even cr", "the journal of neuroscience, april 18, 2012 \u2022 32(16):5609 \u20135619 \u2022 5609\n\nbehavioral/systems/cognitive\n\nspatial profile of excitatory and inhibitory synaptic\nconnectivity in mouse primary auditory cortex\n\nrobert b. levy and alex d. reyes\ncenter for neural science, new york university, new york, new york 10003\n\nthe role of local cortical activity in shaping neuronal responses is controversial. among other questions, it is unknown how the diverse\nresponse patterns reported in vivo\u2014lateral inhibition in some cases, approximately balanced excitation and inhibition (co-tuning) in\nothers\u2014 compare to the local spread of synaptic connectivity. excitatory and inhibitory activity might cancel each other out, or, whether\none outweighs the other, receptive field properties might be substantially affected. as a step toward addressing this question, we used\nmultiple intracellular recording in mouse primary auditory cortical slices to map synaptic connectivity among excitatory pyramidal cells\nand the t", "neuronal cell-type classification: \nchallenges, opportunities and the  \npath forward\n\nhongkui zeng1 and joshua r.\u00a0sanes2\nabstract| neurons have diverse molecular, morphological, connectional and functional \nproperties. we believe that the only realistic way to manage this complexity \u2014 and thereby pave \nthe way for understanding the structure, function and development of brain circuits \u2014 is to \ngroup neurons into types, which can then be analysed systematically and reproducibly. however, \nneuronal classification has been challenging both technically and conceptually. new \nhigh-throughput methods have created opportunities to address the technical challenges \nassociated with neuronal classification by collecting comprehensive information about \nindividual cells. nonetheless, conceptual difficulties persist. borrowing from the field of species \ntaxonomy, we propose principles to be followed in the cell-type classification effort, including \nthe incorporation of multiple, quantitative feat", "neural population dynamics underlying motor\nlearning transfer\n\narticle\n\nhighlights\nd covert learning via a brain-machine interface transfers to\n\novert reaching behavior\n\nd covert learning systematically changes motor cortical\n\npreparatory activity\n\nd covert and overt movements share preparatory neural states\n\nand facilitate transfer\n\nd covert and overt movements engage a similar neural\n\ndynamical system\n\nauthors\n\nsaurabh vyas, nir even-chen,\nsergey d. stavisky, stephen i. ryu,\npaul nuyujukian, krishna v. shenoy\n\ncorrespondence\nsmvyas@stanford.edu\n\nin brief\nvyas et al. ask whether learning\n\u2018\u2018covertly,\u2019\u2019 without physical movements,\ncan transfer to overt behavior. by using\nvisuomotor perturbations, they show that\ncovert and overt movements derive from\na common neural substrate consisting of\nmotor cortical preparatory activity that\nfacilitates transfer of learning.\n\nvyas et al., 2018, neuron 97, 1177\u20131186\nmarch 7, 2018 \u00aa 2018 elsevier inc.\nhttps://doi.org/10.1016/j.neuron.2018.01.040\n\n\f", "physiol rev 86: 1033\u20131048, 2006;\ndoi:10.1152/physrev.00030.2005.\n\nspike timing-dependent plasticity:\n\nfrom synapse to perception\n\nyang dan and mu-ming poo\n\ndivision of neurobiology, department of molecular and cell biology, and helen wills neuroscience institute,\n\nuniversity of california, berkeley, california\n\ni. introduction\nii. cellular mechanisms underlying synaptic spike timing-dependent plasticity\n\niii. spike timing-dependent plasticity with complex spatiotemporal activity patterns\n\na. ltp window\nb. ltd window\nc. modulation of stdp by inhibitory inputs\n\na. dependence on dendritic location\nb. complex spike trains\n\niv. spike timing-dependent plasticity in vivo\n\na. electrical stimulation\nb. sensory stimulation\nc. natural stimuli\nd. persistence of stdp in vivo\n\nv. nonsynaptic aspects of spike timing-dependent plasticity\n\na. stdp of intrinsic neuronal excitability\nb. stdp of local dendritic excitability and synaptic integration\n\nvi. spread of synaptic spike timing-dependent plasticity", "letter\nsensory-evoked ltp driven by dendritic plateau\npotentials in vivo\n\ndoi:10.1038/nature13664\n\nfre\u00b4de\u00b4ric gambino1*{, ste\u00b4phane page`s1*, vassilis kehayas1,2, daniela baptista1, roberta tatti1,2, alan carleton1\n& anthony holtmaat1\n\nlong-term synaptic potentiation (ltp) is thought to be a key pro-\ncess in cortical synaptic network plasticity and memory formation1.\nhebbian forms of ltp depend on strong postsynaptic depolarization,\nwhich in many models is generated by action potentials that propa-\ngate back from the soma into dendrites2,3. however, local dendritic\ndepolarization has been shown to mediate these forms of ltp as well1,4,5.\nas pyramidal cells in supragranular layers of the somatosensory cor-\ntex spike infrequently6\u20138, it is unclear which of the two mechanisms\nprevails for those cellsinvivo.using whole-cell recordings in the mouse\nsomatosensory cortexinvivo, we demonstrate that rhythmic sensory\nwhisker stimulation efficiently induces synaptic ltp in layer 2/3 (l2/3)\npyrami", "deep learning and the information bottleneck principle\n\nnaftali tishby1,2\n\nnoga zaslavsky1\n\ninformation theoretic limits of\n\nabstract\u2014 deep neural networks (dnns) are analyzed via\nthe theoretical framework of the information bottleneck (ib)\nprinciple. we \ufb01rst show that any dnn can be quanti\ufb01ed by\nthe mutual information between the layers and the input and\noutput variables. using this representation we can calculate\nthe optimal\nthe dnn and\nobtain \ufb01nite sample generalization bounds. the advantage of\ngetting closer to the theoretical limit is quanti\ufb01able both by\nthe generalization bound and by the network\u2019s simplicity. we\nargue that both the optimal architecture, number of layers and\nfeatures/connections at each layer, are related to the bifurcation\npoints of the information bottleneck tradeoff, namely, relevant\ncompression of the input layer with respect to the output\nlayer. the hierarchical representations at the layered network\nnaturally correspond to the structural phase transitions a", "relative flatness and generalization\n\nhenning petzka\u2217\n\nlund university, sweden\n\nhenning.petzka@math.lth.se\n\nmichael kamp\u2217\n\ncispa helmholtz center for information security,\n\ngermany and monash university, australia\n\nmichael.kamp@monash.edu\n\nlinara adilova\n\nruhr university bochum, germany\n\nand fraunhofer iais\n\ncristian sminchisescu\nlund university, sweden\n\nand google research, switzerland\n\nmario boley\n\nmonash university, australia\n\nabstract\n\nflatness of the loss curve is conjectured to be connected to the generalization\nability of machine learning models, in particular neural networks. while it has\nbeen empirically observed that \ufb02atness measures consistently correlate strongly\nwith generalization, it is still an open theoretical problem why and under which\ncircumstances \ufb02atness is connected to generalization, in particular in light of\nreparameterizations that change certain \ufb02atness measures but leave generalization\nunchanged. we investigate the connection between \ufb02atness and generalizati", "letter\n\ncommunicated by mark mcdonnell\n\ndeep learning with dynamic spiking neurons and fixed\nfeedback weights\n\narash samadi\nars2023@med.cornell.edu\ndepartment of physiology, university of toronto, toronto, ontario,\nm5s 1a8, canada\n\ntimothy p. lillicrap\ntimothylillicrap@google.com\ngoogle deepmind, london, ec4a 3tw, u.k.\n\ndouglas b. tweed\ndouglas.tweed@utoronto.ca\ndepartment of physiology, university of toronto, toronto, ontario, m5s 1a8,\ncanada, and centre for vision research, york university, toronto, ontario,\nm3j 1pc, canada\n\nrecent work in computer science has shown the power of deep learn-\ning driven by the backpropagation algorithm in networks of arti\ufb01cial\nneurons. but real neurons in the brain are different from most of these\narti\ufb01cial ones in at least three crucial ways: they emit spikes rather than\ngraded outputs, their inputs and outputs are related dynamically rather\nthan by piecewise-smooth functions, and they have no known way to\ncoordinate arrays of synapses in separate for", "a large-scale model of the functioning brain\nchris eliasmith\nscience\ndoi: 10.1126/science.1225266\n\n, 1202 (2012);\n\n et al.\n\n338\n\n \n\nthis copy is for your personal, non-commercial use only.\n \n\nif you wish to distribute this article to others\ncolleagues, clients, or customers by \n\nclicking here.\n \n\n, you can order high-quality copies for your\n\n \n\nhere.\n\npermission to republish or repurpose articles or portions of articles\nfollowing the guidelines \n \nthe following resources related to this article are available online at\nwww.sciencemag.org (this information is current as of\n \n \na correction has been published for this article at:\nhttp://www.sciencemag.org/content/338/6113/1420.2.full.html\n \n\ndecember 14, 2012\n\n ): \n\n can be obtained by\n\nupdated information and services, \nversion of this article at: \nhttp://www.sciencemag.org/content/338/6111/1202.full.html\n \n\nincluding high-resolution figures, can be found in the online\n\nsupporting online material \nhttp://www.sciencemag.org/content/suppl/", "article\n\nhttps://doi.org/10.1038/s41467-022-34452-w\n\nreal-time brain-machine interface in\nnon-human primates achieves high-velocity\nprosthetic \ufb01nger movements using a shallow\nfeedforward neural network decoder\n\nreceived: 16 march 2022\n\naccepted: 25 october 2022\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\ncheck for updates\n\nmatthew s. willsey 1,2, samuel r. nason-tomaszewski\nhisham temmar\nparag g. patil1,2,4,5 & cynthia a. chestek 2,3,4,6,7\n\n2, matthew j. mender\n\n2, joseph t. costello 3,\n\n2, scott r. ensel2,\n\ndespite the rapid progress and interest in brain-machine interfaces that\nrestore motor function, the performance of prosthetic \ufb01ngers and limbs has\nyet to mimic native function. the algorithm that converts brain signals to a\ncontrol signal for the prosthetic device is one of the limitations in achieving\nrapid and realistic \ufb01nger movements. to achieve more realistic \ufb01nger move-\nments, we developed a shallow feed-forward neural network to decode real-\ntime two-de", "the journal of neuroscience, march 5, 2008 \u2022 28(10):2435\u20132446 \u2022 2435\n\ncellular/molecular\n\ndopamine receptor activation is required for\ncorticostriatal spike-timing-dependent plasticity\n\nverena pawlak1,2 and jason n. d. kerr2\n1department of cell physiology, max planck institute for medical research, 69120 heidelberg, germany, and 2network imaging group, max planck\ninstitute for biological cybernetics, 72076 tu\u00a8bingen, germany\n\nsingle action potentials (aps) backpropagate into the higher-order dendrites of striatal spiny projection neurons during cortically driven\n\u201cup\u201d states. the timing of these backpropagating aps relative to the arriving corticostriatal excitatory inputs determines changes in\ndendritic calcium concentration. the question arises to whether this spike-timing relative to cortical excitatory inputs can also induce\nsynaptic plasticity at corticostriatal synapses. here we show that timing of single postsynaptic aps relative to the cortically evoked epsp\ndetermines both the ", "a large-scale standardized physiological survey \nreveals functional organization of the mouse \nvisual cortex\n\n\u200a1,5*, jerome a. lecoq\u200a\n\n\u200a1,5*, michael a. buice\u200a\n\u200a1, nicholas cain\u200a\n\n\u200a1, carol thompson1, wayne wakeman1, jack waters\u200a\n\n\u200a1,5*, peter a. groblewski1,  \nsaskia e. j. de vries\u200a\ngabriel k. ocker1, michael oliver1, david feng\u200a\n\u200a1, peter ledochowitsch1,  \ndaniel millman1, kate roll1, marina garrett1, tom keenan1, leonard kuan1, stefan mihalas\u200a\nshawn olsen\u200a\n\u200a1, derric williams1, \nchris barber1, nathan berbesque1, brandon blanchard1, nicholas bowles1, shiella d. caldejon1, \nlinzy casal1, andrew cho1, sissy cross1, chinh dang1, tim dolbeare1, melise edwards1, \njohn galbraith1, nathalie gaudreault1, terri l. gilbert1, fiona griffin1, perry hargrave1, \nrobert howard1, lawrence huang1, sean jewell2, nika keller1, ulf knoblich\u200a\nrachael larsen1, chris lau1, eric lee\u200a\nkyla mace1, thuyanh nguyen1, jed perkins1, miranda robertson1, sam seid1, eric shea-brown1,3, \njianghong shi3, nathan sjoquis", "emerging properties in self-supervised vision transformers\n\nmathilde caron1,2\n\njulien mairal2\n\nhugo touvron1,3\n\nishan misra1\n\nherv\u00b4e jegou1\n\npiotr bojanowski1\n\narmand joulin1\n\n1\n2\n0\n2\n\n \n\ny\na\nm\n4\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n4\n9\n2\n4\n1\n\n.\n\n4\n0\n1\n2\n:\nv\ni\nx\nr\na\n\n1 facebook ai research\n\n2 inria\u2217\n\n3 sorbonne university\n\nfigure 1: self-attention from a vision transformer with 8 \u00d7 8 patches trained with no supervision. we look at the self-attention of\nthe [cls] token on the heads of the last layer. this token is not attached to any label nor supervision. these maps show that the model\nautomatically learns class-speci\ufb01c features leading to unsupervised object segmentations.\n\nabstract\n\n1. introduction\n\nin this paper, we question if self-supervised learning pro-\nvides new properties to vision transformer (vit) [19] that\nstand out compared to convolutional networks (convnets).\nbeyond the fact that adapting self-supervised methods to this\narchitecture works particularly well, we make the foll", "accepted to the 37th ieee symposium on security & privacy, ieee 2016. san jose, ca.\n\ndistillation as a defense to adversarial\n\nperturbations against deep neural networks\n\n1\n\nnicolas papernot\u2217, patrick mcdaniel\u2217, xi wu\u00a7, somesh jha\u00a7, and ananthram swami\u2021\n\n\u2217department of computer science and engineering, penn state university\n\n\u00a7computer sciences department, university of wisconsin-madison\n\n\u2021united states army research laboratory, adelphi, maryland\n\n{ngp5056,mcdaniel}@cse.psu.edu, {xiwu,jha}@cs.wisc.edu, ananthram.swami.civ@mail.mil\n\n6\n1\n0\n2\n\n \nr\na\n\n \n\nm\n4\n1\n\n \n \n]\n\nr\nc\n.\ns\nc\n[\n \n \n\n2\nv\n8\n0\n5\n4\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014deep learning algorithms have been shown to per-\nform extremely well on many classical machine learning prob-\nlems. however, recent studies have shown that deep learning,\nlike other machine learning techniques, is vulnerable to adver-\nsarial samples: inputs crafted to force a deep neural network\n(dnn) to provide adversary-selected outputs. such attacks can\nseriou", "neural networks 15 (2002) 535\u2013547\n\n2002 special issue\n\nwww.elsevier.com/locate/neunet\n\nactor \u2013 critic models of the basal ganglia:\n\nnew anatomical and computational perspectives\n\ndaphna joela,*, yael niva, eytan ruppinb\n\nadepartment of psychology, tel-aviv university, ramat-aviv, tel aviv 69978, israel\n\nbschools of medicine and mathematical sciences, tel-aviv university, tel aviv 69978, israel\n\nreceived 23 october 2001; revised 27 march 2002; accepted 27 march 2002\n\nabstract\n\na large number of computational models of information processing in the basal ganglia have been developed in recent years. prominent in\nthese are actor \u2013 critic models of basal ganglia functioning, which build on the strong resemblance between dopamine neuron activity and the\ntemporal difference prediction error signal in the critic, and between dopamine-dependent long-term synaptic plasticity in the striatum and\nlearning guided by a prediction error signal in the actor. we selectively review several actor \u2013 criti", "5\n1\n0\n2\n\n \n\np\ne\ns\n0\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n5\n0\n0\n3\n0\n\n.\n\n9\n0\n5\n1\n:\nv\ni\nx\nr\na\n\ncompatible value gradients for reinforcement learning\n\nof continuous deep policies\n\ndavid balduzzi\nschool of mathematics and statistics\nvictoria university of wellington\nwellington, new zealand\n\nmuhammad ghifary\nschool of engineering and computer science\nvictoria university of wellington\nwellington, new zealand\n\ndavid.balduzzi@vuw.ac.nz\n\nmuhammad.ghifary@ecs.vuw.ac.nz\n\nabstract\n\nthis paper proposes gprop, a deep reinforcement learning algorithm for continuous poli-\ncies with compatible function approximation. the algorithm is based on two innovations.\nfirstly, we present a temporal-di\ufb00erence based method for learning the gradient of the\nvalue-function. secondly, we present the deviator-actor-critic (dac) model, which com-\nprises three neural networks that estimate the value function, its gradient, and determine\nthe actor\u2019s policy respectively.\n\nwe evaluate gprop on two challenging tasks: a contextu", "9\n1\n0\n2\n\n \n\nv\no\nn\n4\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n5\n9\n1\n1\n\n.\n\n4\n0\n9\n1\n:\nv\ni\nx\nr\na\n\non exact computation with an in\ufb01nitely wide\n\nneural net\u2217\n\nsanjeev arora\u2020\n\nsimon s. du\u2021\n\nwei hu\u00a7\n\nzhiyuan li\u00b6\n\nruslan salakhutdinovk\n\nruosong wang\u2217\u2217\n\nabstract\n\nhow well does a classic deep net architecture like alexnet or vgg19 classify\non a standard dataset such as cifar-10 when its \u201cwidth\u201d\u2014 namely, number of\nchannels in convolutional layers, and number of nodes in fully-connected internal\nlayers \u2014 is allowed to increase to in\ufb01nity? such questions have come to the fore-\nfront in the quest to theoretically understand deep learning and its mysteries about\noptimization and generalization. they also connect deep learning to notions such\nas gaussian processes and kernels. a recent paper [jacot et al., 2018] introduced\nthe neural tangent kernel (ntk) which captures the behavior of fully-connected\ndeep nets in the in\ufb01nite width limit trained by gradient descent; this object was\nimplicit in some other recent", "learning interactive real-world simulators\n\nmengjiao yang\u2020,\u22c4, yilun du\u266e, kamyar ghasemipour\u22c4\njonathan tompson\u22c4, dale schuurmans\u22c4,\u2021, pieter abbeel\u2020\n\u2020uc berkeley, \u22c4google deepmind, \u266emit, \u2021university of alberta\n\nsherryy@{berkeley.edu, google.com}\n\nuniversal-simulator.github.io\n\n3\n2\n0\n2\n\n \nt\nc\no\n9\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\nv\n4\n1\n1\n6\n0\n\n.\n\n0\n1\n3\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\ngenerative models trained on internet data have revolutionized how text, image,\nand video content can be created. perhaps the next milestone for generative models\nis to simulate realistic experience in response to actions taken by humans, robots,\nand other interactive agents. applications of a real-world simulator range from\ncontrollable content creation in games and movies, to training embodied agents\npurely in simulation that can be directly deployed in the real world. we explore\nthe possibility of learning a universal simulator (unisim) of real-world interaction\nthrough generative modeling. we first make the important", "neuron\n\nperspective\n\nneuroscience needs behavior:\ncorrecting a reductionist bias\n\njohn w. krakauer,1,* asif a. ghazanfar,2 alex gomez-marin,3 malcolm a. maciver,4 and david poeppel5,6\n1departments of neurology, and neuroscience, johns hopkins university, baltimore, md 21287, usa\n2princeton neuroscience institute, departments of psychology and ecology & evolutionary biology, princeton university, princeton,\nnj 08540 usa\n3instituto de neurociencias, consejo superior de investigaciones cient\u0131\u00b4\ufb01cas & universidad miguel herna\u00b4 ndez, sant joan d\u2019alacant,\n03550 alicante, spain\n4neuroscience and robotics laboratory, department of neurobiology, department of mechanical engineering, northwestern university,\nevanston, il 60208, usa\n5department of psychology, new york university, new york, ny 10003, usa\n6neuroscience department, max-planck institute for empirical aesthetics, 60322 frankfurt, germany\n*correspondence: jkrakau1@jhmi.edu\nhttp://dx.doi.org/10.1016/j.neuron.2016.12.041\n\nthere are ever m", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nnew  light  on  cortical  neuropeptides  and  synaptic\nnetwork  plasticity\nstephen  j  smith1,  michael  hawrylycz1,  jean  rossier2 and\nuygar  su\u00a8 mbu\u00a8 l1\n\nneuropeptides,  members  of  a  large  and  evolutionarily  ancient\nfamily  of  proteinaceous  cell\u2013cell  signaling  molecules,  are\nwidely  recognized  as  extremely  potent  regulators  of  brain\nfunction  and  behavior.  at  the  cellular  level,  neuropeptides  are\nknown  to  act  mainly  via  modulation  of  ion  channel  and  synapse\nfunction,  but  functional  impacts  emerging  at  the  level  of\ncomplex  cortical  synaptic  networks  have  resisted  mechanistic\nanalysis.  new  \ufb01ndings  from  single-cell  rna-seq\ntranscriptomics  now  illuminate  intricate  patterns  of  cortical\nneuropeptide  signaling  gene  expression  and  new  tools  now\noffer  powerful  molecular  access  to  cortical  neuropeptide\nsignaling.  here  we  highlight  some  of  these  ne", "article\n\nhttps://doi.org/10.1038/s41467-022-28552-w\n\nopen\n\nfeedforward and feedback interactions between\nvisual cortical areas use different population\nactivity patterns\njo\u00e3o d. semedo1\u2709, anna i. jasper2, amin zandvakili2, aravind krishna\nchristian k. machens\n\n3,7\u2709, adam kohn2,4,5,7\u2709 & byron m. yu\n\n2, amir aschner2,\n\n1,6,7\u2709\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\nbrain function relies on the coordination of activity across multiple, recurrently connected\nbrain areas. for instance, sensory information encoded in early sensory areas is relayed to,\nand further processed by, higher cortical areas and then fed back. however, the way in which\nfeedforward and feedback signaling interact with one another is incompletely understood.\nhere we investigate this question by leveraging simultaneous neuronal population recordings\nin early and midlevel visual areas (v1\u2013v2 and v1\u2013v4). using a dimensionality reduction\napproach, we \ufb01nd that population interactions are feedforward-dominated shortly after sti-\nmulu", "articles\n\nvol 452 | 27 march 2008 | doi:10.1038/nature06725\n\ncompartmentalized dendritic plasticity\nand input feature storage in neurons\n\nattila losonczy1*, judit k. makara1* & jeffrey c. magee1\n\nalthough information storage in the central nervous system is thought to be primarily mediated by various forms of synaptic\nplasticity, other mechanisms, such as modifications in membrane excitability, are available. local dendritic spikes are\nnonlinear voltage events that are initiated within dendritic branches by spatially clustered and temporally synchronous\nsynaptic input. that local spikes selectively respond only to appropriately correlated input allows them to function as input\nfeature detectors and potentially as powerful information storage mechanisms. however, it is currently unknown whether\nany effective form of local dendritic spike plasticity exists. here we show that the coupling between local dendritic spikes and\nthe soma of rat hippocampal ca1 pyramidal neurons can be modified ", "a r t i c l e s\n\nrelease probability of hippocampal glutamatergic \nterminals scales with the size of the active zone\nnoemi holderith1, andrea lorincz1, gergely katona2, bal\u00e1zs r\u00f3zsa2, akos kulik3,4, masahiko watanabe5 & \nzoltan nusser1\n\n.\n\nd\ne\nv\nr\ne\ns\ne\nr\n \n\ns\nt\nh\ng\ni\nr\n \nl\nl\n\na\n\n \n \n.\n\nc\nn\n\ni\n \n,\n\na\nc\n\n \n\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n5\n1\n0\n2\n\u00a9\n\ncortical synapses have structural, molecular and functional heterogeneity; our knowledge regarding the relationship between \ntheir ultrastructural and functional parameters is still fragmented. here we asked how the neurotransmitter release probability \nand presynaptic [ca2+] transients relate to the ultrastructure of rat hippocampal glutamatergic axon terminals. two-photon ca2+ \nimaging\u2013derived optical quantal analysis and correlated electron microscopic reconstructions revealed a tight correlation between \nthe release probability and the active-zone area. peak amplitude of [ca2+] transients in single boutons also positively correlated \nwith the acti", "6266 \u2022 the journal of neuroscience, april 27, 2011 \u2022 31(17):6266 \u2013 6276\n\nbehavioral/systems/cognitive\n\nstimulus selectivity in dorsal and ventral prefrontal cortex\nafter training in working memory tasks\n\ntravis meyer,1,2 xue-lian qi,1 terrence r. stanford,1 and christos constantinidis1\n1department of neurobiology and anatomy, wake forest university school of medicine, winston-salem, north carolina 27157 and 2center for the neural\nbasis of cognition, carnegie mellon university, pittsburgh, pennsylvania 15213\n\nthe prefrontal cortex is known to represent different types of information in working memory. contrasting theories propose that the\ndorsal and ventral regions of the lateral prefrontal cortex are innately specialized for the representation of spatial and nonspatial\ninformation, respectively (goldman-rakic, 1996), or that the two regions are shaped by the demands of cognitive tasks imposed on them\n(miller, 2000). to resolve this issue, we recorded from neurons in the two regions, be", "bayesian latent structure discovery from\n\nmulti-neuron recordings\n\nscott w. linderman\ncolumbia university\n\nswl2133@columbia.edu\n\nryan p. adams\n\nharvard university and twitter\n\nrpa@seas.harvard.edu\n\njonathan w. pillow\nprinceton university\n\npillow@princeton.edu\n\nabstract\n\nneural circuits contain heterogeneous groups of neurons that differ in type, location,\nconnectivity, and basic response properties. however, traditional methods for\ndimensionality reduction and clustering are ill-suited to recovering the structure\nunderlying the organization of neural circuits. in particular, they do not take\nadvantage of the rich temporal dependencies in multi-neuron recordings and fail\nto account for the noise in neural spike trains. here we describe new tools for\ninferring latent structure from simultaneously recorded spike train data using a\nhierarchical extension of a multi-neuron point process model commonly known as\nthe generalized linear model (glm). our approach combines the glm with \ufb02exible\ngr", "course 9 \n\nirregular  activity  in large  networks  of \n\nneurons \n\nc.  van vreeswijk i and h.  sompolinsky 2'3 \n\n1 cnrs umr 8119,  universit~ paris 5 ren~ descartes,  45 rue des saints pkres, \n\n2racah institute of physics and center for neural computation,  hebrew  university,  jerusalem, \n\n75270 paris cedex 06, \n\n91904, israel (permanent address) \n\n3department of molecular and cellular biology,  harvard university,  cambridge,  ma 02138,  usa \n\nc c. chow, b.  gutkin, d. hansel,  c  meunier and j. dalibard,  eds. \nles houches,  session lxxx, 2003 \nmethods and models in neurophysics \nm~thodes et modkles en neurophysique \n(cid:14)9 2005 elsevier b. v. all rights reserved \n\n341 \n\n\f", "a r t i c l e s\n\nbump attractor dynamics in prefrontal cortex explains \nbehavioral precision in spatial working memory\nklaus wimmer1, duane q nykamp1,2, christos constantinidis3 & albert compte1\nprefrontal persistent activity during the delay of spatial working memory tasks is thought to maintain spatial location in memory. \na \u2018bump attractor\u2019 computational model can account for this physiology and its relationship to behavior. however, direct \nexperimental evidence linking parameters of prefrontal firing to the memory report in individual trials is lacking, and, to date, \nno demonstration exists that bump attractor dynamics underlies spatial working memory. we analyzed monkey data and found \nmodel-derived predictive relationships between the variability of prefrontal activity in the delay and the fine details of recalled \nspatial location, as evident in trial-to-trial imprecise oculomotor responses. our results support a diffusing bump representation \nfor spatial working memory instan", "probing the compositionality of intuitive functions\n\neric schulz\n\nuniversity college london\n\ne.schulz@cs.ucl.ac.uk\n\njoshua b. tenenbaum\n\nmit\n\njbt@mit.edu\n\ndavid duvenaud\n\nuniversity of toronto\n\nduvenaud@cs.toronto.edu\n\nmaarten speekenbrink\nuniversity college london\nm.speekenbrink@ucl.ac.uk\n\nsamuel j. gershman\nharvard university\n\ngershman@fas.harvard.edu\n\nabstract\n\nhow do people learn about complex functional structure? taking inspiration from\nother areas of cognitive science, we propose that this is accomplished by harnessing\ncompositionality: complex structure is decomposed into simpler building blocks.\nwe formalize this idea within the framework of bayesian regression using a gram-\nmar over gaussian process kernels. we show that participants prefer compositional\nover non-compositional function extrapolations, that samples from the human prior\nover functions are best described by a compositional model, and that people per-\nceive compositional functions as more predictable than their n", "recurrence is required to capture the representational\ndynamics of the human visual system\n\ntim c. kietzmanna,b,1, courtney j. spoerera, lynn k. a. s\u00f6rensenc, radoslaw m. cichyd, olaf hauka,\nand nikolaus kriegeskortee\n\namrc cognition and brain sciences unit, university of cambridge, cambridge cb2 7ef, united kingdom; bdonders institute for brain, cognition and\nbehaviour, radboud university, 6525 hr nijmegen, the netherlands; cdepartment of psychology, university of amsterdam, 1018 wd amsterdam,\nthe netherlands; ddepartment of education and psychology, freie universit\u00e4t berlin, 14195 berlin, germany; and edepartment of psychology, columbia\nuniversity, new york, ny 10027\n\nedited by terrence j. sejnowski, salk institute for biological studies, la jolla, ca, and approved september 17, 2019 (received for review april 8, 2019)\n\nthe human visual system is an intricate network of brain regions\nthat enables us to recognize the world around us. despite its\nabundant lateral and feedback connectio", "chu kiong loo   keem siah yap\nkok wai wong   andrew teoh\nkaizhu huang (eds.)\n\n \n\n4\n3\n8\n8\ns\nc\nn\nl\n\nneural\ninformation processing\n\n21st international conference, iconip 2014\nkuching, malaysia, november 3\u20136, 2014\nproceedings, part i\n\n \n1\n2\n3\n\f", "neural networks 115 (2019) 100\u2013123\n\ncontents lists available at sciencedirect\n\nneural networks\n\njournal homepage: www.elsevier.com/locate/neunet\n\nreview\nrecent advances in physical reservoir computing: a review\ngouhei tanaka a,b,\u2217, toshiyuki yamane c, jean benoit h\u00e9roux c, ryosho nakane a,b,\nnaoki kanazawa c, seiji takeda c, hidetoshi numata c, daiju nakano c, akira hirose a,b\na institute for innovation in international engineering education, graduate school of engineering, the university of tokyo, tokyo 113-8656, japan\nb department of electrical engineering and information systems, graduate school of engineering, the university of tokyo, tokyo 113-8656, japan\nc ibm research \u2013 tokyo, kanagawa 212-0032, japan\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 15 august 2018\nreceived in revised form 24 february 2019\naccepted 7 march 2019\navailable online 20 march 2019\n\nkeywords:\nneural networks\nmachine learning\nreservoir computing\nnonlinear dynamical systems\nneuromorphic", "9\n1\n0\n2\n\n \n\np\ne\ns\n0\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n0\n3\n6\n4\n0\n\n.\n\n9\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nmeta-learning with implicit gradients\n\naravind rajeswaran\u2217,1 chelsea finn\u2217,2\n\nsham kakade1\n\nsergey levine2\n\n1 university of washington seattle\n\n2 university of california berkeley\n\nabstract\n\na core capability of intelligent systems is the ability to quickly learn new tasks by\ndrawing on prior experience. gradient (or optimization) based meta-learning has\nrecently emerged as an effective approach for few-shot learning. in this formu-\nlation, meta-parameters are learned in the outer loop, while task-speci\ufb01c models\nare learned in the inner-loop, by using only a small amount of data from the cur-\nrent task. a key challenge in scaling these approaches is the need to differentiate\nthrough the inner loop learning process, which can impose considerable computa-\ntional and memory burdens. by drawing upon implicit differentiation, we develop\nthe implicit maml algorithm, which depends only on the solution to ", "b r i e f   co m m u n i c at i o n s\n\na hierarchy of intrinsic \ntimescales across primate cortex\njohn d murray1,2, alberto bernacchia2,3, david j freedman4, \nranulfo romo5,6, jonathan d wallis7,8, xinying cai9,10,  \ncamillo padoa-schioppa10, tatiana pasternak11,12, hyojung seo2, \ndaeyeol lee2 & xiao-jing wang1,2,9\n\nspecialization and hierarchy are organizing principles for \nprimate cortex, yet there is little direct evidence for how \ncortical areas are specialized in the temporal domain. we \nmeasured timescales of intrinsic fluctuations in spiking activity \nacross areas and found a hierarchical ordering, with sensory \nand prefrontal areas exhibiting shorter and longer timescales, \nrespectively. on the basis of our findings, we suggest that \nintrinsic timescales reflect areal specialization for task-relevant \ncomputations over multiple temporal ranges.\n\nhierarchy provides a parsimonious description of various functional \ndifferences across cortical areas. for instance, the sizes of spa", "cortical-like dynamics in recurrent circuits \noptimized for sampling-based probabilistic \ninference\n\nrodrigo echeveste\u200a\n\n\u200a1,2\u2009\u2709, laurence aitchison1, guillaume hennequin\u200a\n\n\u200a1,4 and m\u00e1t\u00e9 lengyel\u200a\n\n\u200a1,3,4\n\nsensory cortices display a suite of ubiquitous dynamical features, such as ongoing noise variability, transient overshoots and \noscillations,  that  have  so  far  escaped  a  common,  principled  theoretical  account.  we  developed  a  unifying  model  for  these \nphenomena  by  training  a  recurrent  excitatory\u2013inhibitory  neural  circuit  model  of  a  visual  cortical  hypercolumn  to  perform \nsampling-based  probabilistic  inference.  the  optimized  network  displayed  several  key  biological  properties,  including  divi-\nsive  normalization  and  stimulus-modulated  noise  variability,  inhibition-dominated  transients  at  stimulus  onset  and  strong \ngamma oscillations. these dynamical features had distinct functional roles in speeding up inferences and made predictions ", "biological learning in key-value memory networks\n\ndanil tyulmankov\u21e4\ncolumbia university\n\ndt2586@columbia.edu\n\nannapurna vadaparty\ncolumbia university\nstanford university\n\napvadaparty@gmail.com\n\nching fang\u21e4\n\ncolumbia university\n\nching.fang@columbia.edu\n\nguangyu robert yang\ncolumbia university\n\nmassachusetts institute of technology\n\nyanggr@mit.edu\n\nabstract\n\nin neuroscience, classical hop\ufb01eld networks are the standard biologically plausible\nmodel of long-term memory, relying on hebbian plasticity for storage and attractor\ndynamics for recall. in contrast, memory-augmented neural networks in machine\nlearning commonly use a key-value mechanism to store and read out memories\nin a single step. such augmented networks achieve impressive feats of memory\ncompared to traditional variants, yet their biological relevance is unclear. we\npropose an implementation of basic key-value memory that stores inputs using a\ncombination of biologically plausible three-factor plasticity rules. the same rules\na", "alpa: automating inter- and intra-operator \nparallelism for distributed deep learning\nlianmin zheng, zhuohan li, and hao zhang, uc berkeley; yonghao zhuang, \nshanghai jiao tong university; zhifeng chen and yanping huang, google;  \nyida wang, amazon web services; yuanzhong xu, google; danyang zhuo,  \n\nduke university; eric p. xing, mbzuai and carnegie mellon university;  \n\njoseph e. gonzalez and ion stoica, uc berkeley\n\nhttps://www.usenix.org/conference/osdi22/presentation/zheng-lianmin\n\nthis paper is included in the proceedings of the 16th usenix symposium on operating systems  design and implementation.july 11\u201313, 2022 \u2022 carlsbad, ca, usa978-1-939133-28-1open access to the proceedings of the  16th usenix symposium on operating  systems design and implementation  is sponsored by\f", "recurrent model-free rl can be a strong baseline for many pomdps\n\ntianwei ni 1 benjamin eysenbach 2 ruslan salakhutdinov 2\nhttps://github.com/twni2016/pomdp-baselines\n\n2\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n5\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n8\n3\n0\n5\n0\n\n.\n\n0\n1\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nmany problems in rl, such as meta-rl, robust\nrl, generalization in rl, and temporal credit as-\nsignment, can be cast as pomdps.\nin theory,\nsimply augmenting model-free rl with memory-\nbased architectures, such as recurrent neural net-\nworks, provides a general approach to solving\nall types of pomdps. however, prior work has\nfound that such recurrent model-free rl meth-\nods tend to perform worse than more specialized\nalgorithms that are designed for speci\ufb01c types\nof pomdps. this paper revisits this claim. we\n\ufb01nd that careful architecture and hyperparameter\ndecisions can often yield a recurrent model-free\nimplementation that performs on par with (and\noccasionally substantially better than) more so-\nphisticated recent techniques", "7\n1\n0\n2\n\n \nl\nu\nj\n \n\n0\n1\n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n2\nv\n6\n8\n2\n2\n0\n\n.\n\n7\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nemergence of locomotion behaviours\n\nin rich environments\n\nnicolas heess, dhruva tb, srinivasan sriram, jay lemmon, josh merel, greg wayne,\nyuval tassa, tom erez, ziyu wang, s. m. ali eslami, martin riedmiller, david silver\n\ndeepmind\n\nabstract\n\nthe reinforcement learning paradigm allows, in principle, for complex behaviours\nto be learned directly from simple reward signals.\nin practice, however, it is\ncommon to carefully hand-design the reward function to encourage a particular\nsolution, or to derive it from demonstration data. in this paper explore how a rich\nenvironment can help to promote the learning of complex behavior. speci\ufb01cally,\nwe train agents in diverse environmental contexts, and \ufb01nd that this encourages\nthe emergence of robust behaviours that perform well across a suite of tasks.\nwe demonstrate this principle for locomotion \u2013 behaviours that are known for\ntheir sensitivity to the choice ", "how important is weight symmetry in backpropagation?\n\nqianli liao and joel z. leibo and tomaso poggio\n\ncenter for brains, minds and machines, mcgovern institute\n\nmassachusetts institute of technology\n\n77 massachusetts ave., cambridge, ma, 02139, usa\n\n6\n1\n0\n2\n\n \n\nb\ne\nf\n4\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n7\n6\n0\n5\n0\n\n.\n\n0\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\ngradient backpropagation (bp) requires symmetric feedfor-\nward and feedback connections\u2014the same weights must be\nused for forward and backward passes. this \u201cweight trans-\nport problem\u201d (grossberg 1987) is thought to be one of the\nmain reasons to doubt bp\u2019s biologically plausibility. using\n15 different classi\ufb01cation datasets, we systematically investi-\ngate to what extent bp really depends on weight symmetry.\nin a study that turned out to be surprisingly similar in spirit to\nlillicrap et al.\u2019s demonstration (lillicrap et al. 2014) but or-\nthogonal in its results, our experiments indicate that: (1) the\nmagnitudes of feedback weights do not matter", "published as a conference paper at iclr 2021\n\nare neural nets modular? inspecting func-\ntional modularity through differentiable\nweight masks\n\nr\u00f3bert csord\u00e1s\nidsia / usi / supsi\nrobert@idsia.ch\n\nsjoerd van steenkiste\nidsia / usi / supsi\nsjoerd@idsia.ch\n\nj\u00fcrgen schmidhuber\nidsia / usi / supsi / nnaisense\njuergen@idsia.ch\n\nabstract\n\nneural networks (nns) whose subnetworks implement reusable functions are\nexpected to offer numerous advantages, including compositionality through ef-\n\ufb01cient recombination of functional building blocks, interpretability, preventing\ncatastrophic interference, etc. understanding if and how nns are modular could\nprovide insights into how to improve them. current inspection methods, however,\nfail to link modules to their functionality. in this paper, we present a novel method\nbased on learning binary weight masks to identify individual weights and subnets\nresponsible for speci\ufb01c functions. using this powerful tool, we contribute an exten-\nsive study of emerging m", "m\no\nc\n\n.\n\ne\nr\nu\nt\na\nn\n\n.\ni\n\nc\ns\no\nr\nu\ne\nn\n\n/\n/\n:\np\nt\nt\nh\n \n\u2022\n \n.\n\nc\nn\n\ni\n \n\na\nc\n\n \n\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n9\n9\n9\n1\n\u00a9\n\n \n\n \n\n\u00a9 1999 nature america inc. \u2022 http://neurosci.nature.com\n\narticles\n\npredictive coding in the visual cortex: \na functional interpretation of some\nextra-classical receptive-field effects\n\nrajesh p. n. rao1 and dana h. ballard2\n\n1 the salk institute, sloan center for theoretical neurobiology and computational neurobiology laboratory, 10010 n. torrey pines road, \n\nla jolla, california 92037, usa\n\n2 department of computer science, university of rochester, rochester, new york 14627-0226, usa\n\ncorrespondence should be addressed to r.p.n.r. (rao@salk.edu)\n\nwe describe a model of visual processing in which feedback connections from a higher- to a lower-\norder visual cortical area carry predictions of lower-level neural activities, whereas the feedforward\nconnections carry the residual errors between the predictions and the actual lower-level activities.\nwhen exposed to natur", "c o r t e x 9 8 ( 2 0 1 8 ) 2 4 9 e2 6 1\n\navailable online at www.sciencedirect.com\n\nsciencedirect\n\njournal homepage: www.elsevier.com/locate/cortex\n\nspecial issue: review\n\nvisual pathways from the perspective of cost\nfunctions and multi-task deep neural networks\n\nh. steven scholte a,b,*,1, max m. losch a,b,c,1, kandan ramakrishnan c,\nedward h.f. de haan a,b and sander m. bohte d\na department of psychology, university of amsterdam, amsterdam, the netherlands\nb amsterdam brain and cognition, university of amsterdam, amsterdam, the netherlands\nc informatics institute, university of amsterdam, amsterdam, the netherlands\nd cwi ml, amsterdam, the netherlands\n\na r t i c l e i n f o\n\na b s t r a c t\n\narticle history:\nreceived 14 january 2017\nreviewed 31 march 2017\nrevised 2 june 2017\naccepted 25 september 2017\npublished online 7 october 2017\n\nkeywords:\ndual-pathway\ndeep learning\ncost functions\nrepresentations\nvisual processing\n\nvision research has been shaped by the seminal insight that we ca", "on the local behavior of spaces of natural images\n\ngunnar carlsson\ntigran ishkhanov\n\nvin de silva\n\nafra zomorodian\n\nabstract\n\nin this study we concentrate on qualitative topological analysis of the local behavior of the\nspace of natural images. to this end, we use a space of 3 by 3 high-contrast patches m studied by\nmumford et al. we develop a theoretical model for the high-density 2-dimensional submanifold\nof m showing that it has the topology of the klein bottle. using our topological software package\nplex we experimentally verify our theoretical conclusions. we use polynomial representation\nto give coordinatization to various subspaces of m. we (cid:12)nd the best-(cid:12)tting embedding of the\nklein bottle into the ambient space of m. our results are currently being used in developing a\ncompression algorithm based on a klein bottle dictionary.\n\nkeywords : topology; natural images; manifold; (cid:12)ltration; klein bottle; persistent homology.\n\nintroduction\n\na natural image, such as", "deep reinforcement learning that matters\npeter henderson1\u2217, riashat islam1,2\u2217, philip bachman2\n\njoelle pineau1, doina precup1, david meger1\n\n1 mcgill university, montreal, canada\n2 microsoft maluuba, montreal, canada\n\n{peter.henderson,riashat.islam}@mail.mcgill.ca, phbachma@microsoft.com\n\n{jpineau,dprecup}@cs.mcgill.ca, dmeger@cim.mcgill.ca\n\n9\n1\n0\n2\n\n \n\nn\na\nj\n \n\n0\n3\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n6\n5\n6\n0\n\n.\n\n9\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nin recent years, signi\ufb01cant progress has been made in solving\nchallenging problems across various domains using deep re-\ninforcement learning (rl). reproducing existing work and\naccurately judging the improvements offered by novel meth-\nods is vital to sustaining this progress. unfortunately, repro-\nducing results for state-of-the-art deep rl methods is seldom\nstraightforward. in particular, non-determinism in standard\nbenchmark environments, combined with variance intrinsic\nto the methods, can make reported results tough to interpret.\nwithout signi\ufb01c", "jcb: mini-review\n\n cytoplasmic diffusion: molecular motors mix it up \n\n  clifford p.   brangwynne ,  1    gijsje h.   koenderink ,  1,2,3    frederick c.   mackintosh ,  4   and  david a.   weitz   1,2   \n\n  1 school of engineering and applied sciences and  2 department of physics, harvard university, cambridge, ma 02138 \n  3 foundation for fundamental research on matter institute for atomic and molecular physics, 1098 sj amsterdam, netherlands \n  4 department of physics and astronomy, vrije universiteit, 1081 hv amsterdam, netherlands    \n\ny\ng\no\nl\no\n\ni\n\nb\n\nl\nl\ne\nc\n\nf\no\n\nl\na\nn\nr\nu\no\nj\n\ne\nh\nt\n\nrandom motion within the cytoplasm gives rise to molec-\nular diffusion; this motion is essential to many biological \nprocesses.  however,  in  addition  to  thermal  brownian \nmotion, the cytoplasm also undergoes constant agitation \ncaused  by  the  activity  of  molecular  motors  and  other \nnonequilibrium cellular processes. here, we discuss re-\ncent work that suggests this activity can give ri", "2\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n3\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n6\n3\n0\n0\n1\n\n.\n\n3\n0\n2\n2\n:\nv\ni\nx\nr\na\n\non the generalization mystery in deep learning\n\nsatrajit chatterjee and piotr zielinski\n\nabstract. the generalization mystery in deep learning is the following: why do over-parameterized\nneural networks trained with gradient descent (gd) generalize well on real datasets even though\nthey are capable of \ufb01tting random datasets of comparable size? furthermore, from among all\nsolutions that \ufb01t the training data, how does gd \ufb01nd one that generalizes well (when such a\nwell-generalizing solution exists)?\n\nwe argue that the answer to both questions lies in the interaction of the gradients of di\ufb00erent\nexamples during training. intuitively, if the per-example gradient vectors are well-aligned, that is, if\nthey are coherent, then one may expect gd to be (algorithmically) stable, and hence generalize well.\nwe formalize this argument with an easy to compute and interpretable metric for coherence, and\nshow that the", "the journal of neuroscience, may 1, 2003 \u2022 23(9):3697\u20133714 \u2022 3697\n\nlearning input correlations through nonlinear temporally\nasymmetric hebbian plasticity\n\nr. gu\u00a8tig,1* r. aharonov,2* s. rotter,1 and haim sompolinsky2,3\n1institute of biology iii, university of freiburg, 79104 freiburg, germany, and 2interdisciplinary center for neural computation and 3racah institute of\nphysics, hebrew university, jerusalem 91904, israel\n\ntriggered by recent experimental results, temporally asymmetric hebbian (tah) plasticity is considered as a candidate model for the\nbiological implementation of competitive synaptic learning, a key concept for the experience-based development of cortical circuitry.\nhowever, because of the well known positive feedback instability of correlation-based plasticity, the stability of the resulting learning\nprocess has remained a central problem. plagued by either a runaway of the synaptic efficacies or a greatly reduced sensitivity to input\ncorrelations, the learning perform", "article\n\ncommunicated by rodney douglas\n\nreal-time computing without stable states: a new\nframework for neural computation based on perturbations\n\nwolfgang maass\nmaass@igi.tu-graz.ac.at\nthomas natschl\u00a8ager\ntnatschl@igi.tu-graz.ac.at\ninstitute for theoretical computer science, technische universit\u00a8at graz;\na-8010 graz, austria\n\nhenry markram\nhenry.markram@ep\ufb02.ch\nbrain mind institute, ecole polytechnique federale de lausanne,\nch-1015 lausanne, switzerland\n\na key challenge for neural modeling is to explain how a continuous\nstream of multimodal input from a rapidly changing environment can be\nprocessed by stereotypical recurrent circuits of integrate-and-\ufb01re neurons\nin real time. we propose a new computational model for real-time com-\nputing on time-varying input that provides an alternative to paradigms\nbased on turing machines or attractor neural networks. it does not require\na task-dependent construction of neural circuits. instead, it is based on\nprinciples of high-dimensional dynamica", "6\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n3\n1\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n3\n1\n8\n3\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ntowards an integration of deep learning and neuroscience\n\nadam h. marblestone\nmit media lab\ncambridge, ma 02139, usa\n\ngreg wayne\ngoogle deepmind\nlondon, ec4a 3tw, uk\n\nkonrad p. kording\nrehabilitation institute of chicago\nnorthwestern university\nchicago, il 60611, usa\n\neditor: tbn\n\namarbles@media.mit.edu\n\ngregwayne@google.com\n\nkoerding@gmail.com\n\nabstract\n\nneuroscience has focused on the detailed implementation of computation, studying neural codes,\ndynamics and circuits. in machine learning, however, arti\ufb01cial neural networks tend to eschew\nprecisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function,\noften using simple and relatively uniform initial architectures. two recent developments have\nemerged within machine learning that create an opportunity to connect these seemingly divergent\nperspectives. first, structured architectures are used, includin", "communicated by ronald williams \n\nrelating real-time backpropagation and \nbackpropagation-through-time: an application \nof  flow graph interreciprocity \n\nfransoise  beaufays \neric a.  wan \ndepartment  of  electrical engineering, stanford  university, \nstanford, c a  94305-4055 u s a  \n\nwe  show that  signal flow graph theory  provides  a  simple way  to  re- \nlate  two  popular  algorithms  used  for  adapting  dynamic  neural  net- \nworks,  real-time  backpropagation  and backpropagation-through-time. \nstarting with the flow graph for real-time  backpropagation,  we  use  a \nsimple transposition  to  produce  a  second  graph.  the  new  graph  is \nshown to be interreciprocal  with the original and to correspond to the \nbackpropagation-through-time  algorithm.  interreciprocity  provides  a \ntheoretical  argument to  verify  that  both  flow  graphs implement the \nsame overall weight update. \n\n1 introduction \n\ntwo adaptive  algorithms,  real-time  backpropagation  (rtbp) and back- \npr", "neural networks 132 (2020) 428\u2013446\n\ncontents lists available at sciencedirect\n\nneural networks\n\njournal homepage: www.elsevier.com/locate/neunet\n\nhigh-dimensional dynamics of generalization error in neural networks\nmadhu s. advani a,1, andrew m. saxe a,2,\u2217,1, haim sompolinsky a,b\n\na center for brain science, harvard university, cambridge, ma 02138, united states of america\nb edmond and lily safra center for brain sciences, hebrew university, jerusalem 91904, israel\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 4 january 2020\nreceived in revised form 18 august 2020\naccepted 24 august 2020\navailable online 5 september 2020\n\nkeywords:\nneural networks\ngeneralization error\nrandom matrix theory\n\n1. introduction\n\nwe perform an analysis of the average generalization dynamics of large neural networks trained using\ngradient descent. we study the practically-relevant \u2018\u2018high-dimensional\u2019\u2019 regime where the number of\nfree parameters in the network is on the order of or even lar", "page 1 of 77\n\narticles in press. j neurophysiol (october 10, 2007). doi:10.1152/jn.00364.2007 \n\nreinforcement learning with modulated \nspike timing-dependent synaptic plasticity\n\nrunning head: reinforcement learning with stdp\n\nmichael a. farries1\nadrienne l. fairhall2\n\n1university of texas at san antonio\ndept. of biology\none utsa circle\nsan antonio, tx 78249\n\n2 university of washington\ndept. of physiology and biophysics\n1959 ne pacific st., box 357290\nseattle, wa 98195-7290\n\ncorresponding author: michael a. farries\nemail: michael.farries@utsa.edu\n\n1\n\ncopyright \u00a9 2007 by the american physiological society.\n \n\n\f", "accurate online training of dynamical \nspiking neural networks through forward \npropagation through time\n\nhttps://doi.org/10.1038/s42256-023-00650-4\n\nreceived: 5 may 2022\n\naccepted: 30 march 2023\n\npublished online: 8 may 2023\n\n check for updates\n\nbojian yin\u2009\n\n \u20091 \n\n, federico corradi\u2009\n\n \u20092,3 & sander m. boht\u00e9\u2009\n\n \u20091,4,5\n\nwith recent advances in learning algorithms, recurrent networks of \nspiking neurons are achieving performance that is competitive with \nvanilla recurrent neural networks. however, these algorithms are limited \nto small networks of simple spiking neurons and modest-length temporal \nsequences, as they impose high memory requirements, have difficulty \ntraining complex neuron models and are incompatible with online learning. \nhere, we show how the recently developed forward-propagation through \ntime (fptt) learning combined with novel liquid time-constant spiking \nneurons resolves these limitations. applying fptt to networks of such \ncomplex spiking neurons, we demonstrate ", "2\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n3\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n3\n1\n4\n1\n\n.\n\n5\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nflashattention: fast and memory-e\ufb03cient exact attention\n\nwith io-awareness\n\ntri dao\u2020, daniel y. fu\u2020, stefano ermon\u2020, atri rudra\u2021, and christopher r\u00e9\u2020\n\n\u2020department of computer science, stanford university\n\n\u2021department of computer science and engineering, university at bu\ufb00alo, suny\n{trid,danfu}@cs.stanford.edu, ermon@stanford.edu, atri@buffalo.edu,\n\nchrismre@cs.stanford.edu\n\njune 24, 2022\n\nabstract\n\ntransformers are slow and memory-hungry on long sequences, since the time and memory complexity\nof self-attention are quadratic in sequence length. approximate attention methods have attempted\nto address this problem by trading o\ufb00 model quality to reduce the compute complexity, but often do\nnot achieve wall-clock speedup. we argue that a missing principle is making attention algorithms io-\naware\u2014accounting for reads and writes between levels of gpu memory. we propose flashattention,\nan io-aware exact atte", "9\n1\n0\n2\n\n \nc\ne\nd\n4\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n8\n5\n9\n4\n0\n\n.\n\n0\n1\n9\n1\n:\nv\ni\nx\nr\na\n\nstructured and deep similarity matching via\n\nstructured and deep hebbian networks\n\ndina obeid\n\nhugo ramambason\n\ncengiz pehlevan\n\njohn a. paulson school of engineering and applied sciences\n\nharvard university\n\ncambridge, ma, usa\n\n{dinaobeid@seas,hugo_ramambason@g,cpehlevan@seas}.harvard.edu\n\nabstract\n\nsynaptic plasticity is widely accepted to be the mechanism behind learning in\nthe brain\u2019s neural networks. a central question is how synapses, with access\nto only local information about the network, can still organize collectively and\nperform circuit-wide learning in an ef\ufb01cient manner. in single-layered and all-\nto-all connected neural networks, local plasticity has been shown to implement\ngradient-based learning on a class of cost functions that contain a term that aligns\nthe similarity of outputs to the similarity of inputs. whether such cost functions\nexist for networks with other architectures is not", "how to start training:\n\nthe effect of initialization and architecture\n\nboris hanin\n\ndepartment of mathematics\n\ntexas a& m university\ncollege station, tx, usa\nbhanin@math.tamu.edu\n\ndavid rolnick\n\ndepartment of mathematics\n\nmassachusetts institute of technology\n\ncambridge, ma, usa\ndrolnick@mit.edu\n\nabstract\n\nwe identify and study two common failure modes for early training in deep relu\nnets. for each, we give a rigorous proof of when it occurs and how to avoid it, for\nfully connected, convolutional, and residual architectures. we show that the \ufb01rst\nfailure mode, exploding or vanishing mean activation length, can be avoided by\ninitializing weights from a symmetric distribution with variance 2/fan-in and, for\nresnets, by correctly scaling the residual modules. we prove that the second failure\nmode, exponentially large variance of activation length, never occurs in residual\nnets once the \ufb01rst failure mode is avoided. in contrast, for fully connected nets, we\nprove that this failure mode can", "european journal of neuroscience, vol. 13, pp. 1984\u20131992, 2001\n\n\u00aa federation of european neuroscience societies\n\ninvolvement of the central nucleus of the amygdala and\nnucleus accumbens core in mediating pavlovian\nin\ufb02uences on instrumental behaviour\n\njeremy hall,* john a. parkinson,* thomas m. connor, anthony dickinson and barry j. everitt\ndepartment of experimental psychology, university of cambridge, downing street, cambridge, cb2 3eb. uk\n\nkeywords: pavlovian conditioning, pavlovian-to-instrumental transfer, motivation, rat\n\nabstract\n\npavlovian conditioned cues exert a powerful in\ufb02uence on instrumental actions directed towards a common reward, this is known\nas pavlovian-to-instrumental transfer (pit). the nucleus accumbens (nacc) has been hypothesized to function as an interface\nbetween limbic cortical structures required for associative conditioning, like the amygdala, and response mechanisms through\nwhich instrumental behaviour can be selected and performed. here we have used selec", "9\n1\n0\n2\n\n \nc\ne\nd\n4\n\n \n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n9\n4\n5\n8\n0\n\n.\n\n7\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nuniversality and individuality in neural dynamics\nacross large populations of recurrent networks\n\nniru maheswaranathan\u2217\ngoogle brain, google inc.\n\nmountain view, ca\nnirum@google.com\n\nalex h. williams\u2217\nstanford university\n\nstanford, ca\n\nahwillia@stanford.edu\n\nmatthew d. golub\nstanford university\n\nstanford, ca\n\nmgolub@stanford.edu\n\nsurya ganguli\n\nstanford university and google brain\nstanford, ca and mountain view, ca\n\nsganguli@stanford.edu\n\ndavid sussillo\u2020\n\ngoogle brain, google inc.\n\nmountain view, ca\n\nsussillo@google.com\n\nabstract\n\ntask-based modeling with recurrent neural networks (rnns) has emerged as a\npopular way to infer the computational function of different brain regions. these\nmodels are quantitatively assessed by comparing the low-dimensional neural rep-\nresentations of the model with the brain, for example using canonical correlation\nanalysis (cca). however, the nature of the detailed", "www.nature.com/npjscilearn\n\nopen\n\nreview article\nis plasticity of synapses the mechanism of long-term memory\nstorage?\n\nwickliffe c. abraham 1, owen d. jones1 and david l. glanzman2\n\nit has been 70 years since donald hebb published his formalized theory of synaptic adaptation during learning. hebb\u2019s seminal\nwork foreshadowed some of the great neuroscienti\ufb01c discoveries of the following decades, including the discovery of long-term\npotentiation and other lasting forms of synaptic plasticity, and more recently the residence of memories in synaptically connected\nneuronal assemblies. our understanding of the processes underlying learning and memory has been dominated by the view that\nsynapses are the principal site of information storage in the brain. this view has received substantial support from research in\nseveral model systems, with the vast majority of studies on the topic corroborating a role for synapses in memory storage. yet,\ndespite the neuroscience community\u2019s best efforts, we a", "\u0000\u0002\u0001\u0003\u0001\u0003\u0004\u0006\u0005\b\u0007\n\t\f\u000b\u000e\r\u0010\u000f\u0012\u0011\u0014\u0013\u0016\u0015\u0018\u0017\u0019\u0001\u001a\u000f\u0006\t\u001b\u000b\u000e\r\u001c\u0013\u001d\u0000\u001e\u0001\u001f\u0001\u0003\u0004 \u0005\u0014\u0007!\t\u001b\u000b\u000e\r\u0010\u000f\u0012\u0011#\"\u001e\u0011\u0014\t%$\u0003&'\u0005(\u0004 )*\u0011\b\u000b\u000e\u0011\u0014$+\u000f-,.\u0011\u0014\r(\u0004 $\u001f\t%$\u0003/\n\n021\u00063547683:953<;\u0012=\ns\u0010tvuxwzyg[\u0003\\*]_^a`5b:uct'uxde]gf<tvhaikj\u001bb5lx]gwxm%dnj\u001bf<m\u001bjpokf5deu\fqro\u0010\\tsuq5s2]_f<v:]gf\u001fwx\\zy\fi|{g}k~(q5oz\u007f\n\u0080a\u0081\n\\*]g^a`5b:uzj\flk\u008c:m%dnj\u001bfrm%j\u001c\u008dzj\u001b`<t_l\u008eux^ej\ff\u008fu\fqr\\ttvlxf5j\f\u0090_dnj%\u0091\u0093\u00928j\u001bhnhe]gf\u001fokf5de\u0094gj\u001blcw\u008edeu\u0095[_q5\u0096_\u0097_\u0097_\u0097e\u00985]glzypj\fw\u0014}\u0012\u0094_j\ffgb5j_q:\u0099udeu\u008eucw\u008ey<b5lz\u0090g\u009aaq5\u0099h}\u009by\u009c\u0096g\u009d:y'\u009e\n\n1\u0012\u0082\u0084\u0083*35\u0082\u0006\u0085r\u0086\n\n\u0081r\u0087\n\n>\u001b?a@2b\u0010ced2@gfh>\fi<j*kml2npopkq@an\u0006kmlar\n\n\u0088%npo:cen\u0089>gkenpbkluke\u008a5\u008b2l\n\n\u009f\u00a1\u00a0z\u00a2g\u00a3:\u00a45\u00a5\u00a7\u00a6p\u00a3\n\n\u00e2\u00e4\u00e1\n\n\u00e1\u00f1\u00e2\n\n\u00e1\u0093\u00e0\n\n\u00bc%\u00bd\n\n\u00b7%\u00e7\n\n\u00e0'\u00e1\u00e3\u00e2\n\n\u00e18\u00ee'\u00e2\n\n\u00bc\u0006\u00e55\u00b9v\u00e6\n\n\u00e7p\u00e5<\u00b9'\u00e6\n\nf\u00a9]_lcv:j\u001bl\u00aaux]\u00abwz]_hn\u0094_j\u00aclxj\ftvhnd\u00adw\u0095uxdnm\u00aelzj\fdef5\u00af\u00b0]_lcm%j%\u0091\n^aj\u001bf\u008fu\nhnj\ftvlxf5dnf5\u0090\u00aa`5lz]gy5hnj\u001b^\nw\u001bq*dmu\nd\u00adw\nm%lxdmuxdnm\ftvh\nux\u009a<t'uzt_`5`5lx]\u00b2\u00b1:de^\nt'uxjpt_he\u0090g]_lxdmux\u009a5^\nw\u00b3yrj\u00b4b<w\u008ej\u009cvh\u00b5\nf\u00aauz\u009a<dnw\u00b4`<tv`pj\u001bl\u009cq\u0012\u00b6tj\n`5lxj\fwzj\u001bf\u008fu(uz\u009a<j\u00b8\u00b7x\u00b9v\u00ba5\u00bb\u001b\u00bc\u001b\u00bd\u0016\u00be\n\u00b9'\u00baatvhn\u0090_]_lxdeuz\u009a5^\u00e8\u00b6\u0014\u009a5dnmc\u009a\n\u00bf'\u00e0'\u00e1\u00e3\u00e2\u00e4\u00bf\n\u00e9<frv5wkt_f\n\u00eaztv`<`5lz]\u00b2\u00b1:dn^\nt'uzj\fhe[g\u00ebz]g`:uzdn^\ntvh\u0012`r]ghm\u0091\nd\u00adm%[gq\u00b4\u0090_dn\u0094_j\ff\u00ect_m\fm%j\fwxw8ux]\u00e8t\u00ed\u00bdz\u00bc\u0016\u00bb\n\u00b9v\u00ba\u00f3\u00f2\u00b0\u00b6\u0014\u009a<dnmc\u009a\u009b", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nusing  computational  theory  to  constrain  statistical\nmodels  of  neural  data\nscott  w  linderman1 and  samuel  j  gershman2\n\ncomputational  neuroscience  is,  to  \ufb01rst  order,  dominated  by\ntwo  approaches:  the  \u2018bottom-up\u2019  approach,  which  searches\nfor  statistical  patterns  in  large-scale  neural  recordings,  and  the\n\u2018top-down\u2019  approach,  which  begins  with  a  theory  of\ncomputation  and  considers  plausible  neural  implementations.\nwhile  this  division  is  not  clear-cut,  we  argue  that  these\napproaches  should  be  much  more  intimately  linked.  from  a\nbayesian  perspective,  computational  theories  provide\nconstrained  prior  distributions  on  neural  data  \u2014  albeit  highly\nsophisticated  ones.  by  connecting  theory  to  observation  via  a\nprobabilistic  model,  we  provide  the  link  necessary  to  test,\nevaluate,  and  revise  our  theories  in  a  data-driven  and\nstatistically  rigoro", "0\n2\n0\n2\n\n \nt\nc\no\n7\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n5\n6\n7\n4\n1\n\n.\n\n0\n1\n0\n2\n:\nv\ni\nx\nr\na\n\npreprint\n\ndeep networks from the principle of rate reduction\n\nkwan ho ryan chan\u2020 yaodong yu\u2020 chong you\u2020 haozhi qi\u2020 john wright\u2021(cid:5) yi ma\u2020\n\n\u2020department of eecs, university of california, berkeley\n\u2021department of electrical engineering and data science institute, columbia university\n(cid:5)department of applied physics and applied mathematics, columbia university\n\nabstract\n\nthis work attempts to interpret modern deep (convolutional) networks from the\nprinciples of rate reduction and (shift) invariant classi\ufb01cation. we show that the\nbasic iterative gradient ascent scheme for optimizing the rate reduction of learned\nfeatures naturally leads to a multi-layer deep network, one iteration per layer.\nthe layered architectures, linear and nonlinear operators, and even parameters of\nthe network are all explicitly constructed layer-by-layer in a forward propagation\nfashion by emulating the gradient scheme. a", "biological credit assignment through\n\ndynamic inversion of feedforward networks\n\nwilliam f. podlaski\u2217\nchampalimaud research\n\nchristian k. machens\u2217\nchampalimaud research\n\nchampalimaud centre for the unknown\n\nchampalimaud centre for the unknown\n\n1400-038 lisbon, portugal\n\n1400-038 lisbon, portugal\n\nabstract\n\nlearning depends on changes in synaptic connections deep inside the brain. in\nmultilayer networks, these changes are triggered by error signals fed back from the\noutput, generally through a stepwise inversion of the feedforward processing steps.\nthe gold standard for this process \u2014 backpropagation \u2014 works well in arti\ufb01cial\nneural networks, but is biologically implausible. several recent proposals have\nemerged to address this problem, but many of these biologically-plausible schemes\nare based on learning an independent set of feedback connections. this complicates\nthe assignment of errors to each synapse by making it dependent upon a second\nlearning problem, and by \ufb01tting inversions r", "p\ne\nr\ng\na\nm\no\nn\n \np\ni\ni\n:\n \ns\n0\n0\n4\n2\n-\n6\n9\n8\n9\n(\n9\n7\n)\n0\n0\n1\n6\n9\n-\n7\n \nv\ni\ns\ni\no\nn\n \nr\ne\ns\n.\n,\n \nv\no\nl\n.\n \n3\n7\n,\n \nn\no\n.\n \n2\n3\n,\n \np\np\n.\n \n3\n3\n1\n1\n-\n3\n3\n2\n5\n,\n \n1\n9\n9\n7\n \n\u00a9\n \n1\n9\n9\n7\n \ne\nl\ns\ne\nv\ni\ne\nr\n \ns\nc\ni\ne\nn\nc\ne\n \nl\nt\nd\n.\n \na\nl\nl\n \nr\ni\ng\nh\nt\ns\n \nr\ne\ns\ne\nr\nv\ne\nd\n \np\nr\ni\nn\nt\ne\nd\n \ni\nn\n \ng\nr\ne\na\nt\n \nb\nr\ni\nt\na\ni\nn\n \n0\n0\n4\n2\n-\n6\n9\n8\n9\n/\n9\n7\n \n$\n1\n7\n.\n0\n0\n \n+\n \n0\n.\n0\n0\n \ns\np\na\nr\ns\ne\n \nc\no\nd\ni\nn\ng\n \nw\ni\nt\nh\n \na\nn\n \no\nv\ne\nr\nc\no\nm\np\nl\ne\nt\ne\n \nb\na\ns\ni\ns\n \ns\ne\nt\n:\n \na\n \ns\nt\nr\na\nt\ne\ng\ny\n \ne\nm\np\nl\no\ny\ne\nd\n \nb\ny\n \nv\n1\n \n?\n \nb\nr\nu\nn\no\n \na\n.\n \no\nl\ns\nh\na\nu\ns\ne\nn\n,\n$\n \nd\na\nv\ni\nd\n \nj\n.\n \nf\ni\ne\nl\nd\nt\n \nr\ne\nc\ne\ni\nv\ne\nd\n \n1\n6\n \nj\nu\nl\ny\n \n1\n9\n9\n6\n;\n \ni\nn\n \nr\ne\nv\ni\ns\ne\nd\n \nf\no\nr\nm\n \n2\n4\n \nd\ne\nc\ne\nm\nb\ne\nr\n \n1\n9\n9\n6\n \nt\nh\ne\n \ns\np\na\nt\ni\na\nl\n \nr\ne\nc\ne\np\nt\ni\nv\ne\n \nf\ni\ne\nl\nd\ns\n \no\nf\n \ns\ni\nm\np\nl\ne\n \nc\ne\nl\nl\ns\n \ni\nn\n \nm\na\nm\nm\na\nl\ni\na\nn\n \ns\nt\nr\ni\na\nt\ne\n \nc\no\nr\nt\ne\nx\n \nh\na\nv\ne\n \nb\ne\ne\nn\n \nr\ne\na\ns\no\nn\na\nb\nl\ny\n \nw\ne\nl\nl\n \nd\ne\ns\nc\nr\ni\nb\ne\nd\n \np\nh\ny\ns\ni\no\nl\no\ng\ni\nc\na\nl\nl\ny\n \na\nn\nd\n \nc\na\nn\n \nb\ne\n \nc\nh\na\nr\na\nc\nt\ne\nr\ni\n", "the  journal \n\nof  neuroscience, \n\nmay \n\n15,  1996, \n\n16(10):3351-3362 \n\nefficient  coding  of  natural  scenes \nnucleus:  experimental \n\ntest  of  a  computational \n\nin  the  lateral  geniculate \n\ntheory \n\nyang  dan,\u2019 \nlaboratories \n\nj.  atick, \n\njoseph \nof  lneurobiology \n\nand  r.  clay  reid\u2019 \n\nand  2computational  neuroscience, \n\nthe  rockefeller  university,  new  york,  new  york  70021 \n\ncorrelation \n\ntheory  suggests \n\ntheoretic \ninformation \n\ninformation \ninformation \n\nthe  lateral  geniculate \ninto  an  efficient \n\na  recent  computational \nin  the  retina  and \nrecode \n1990). \ntation  of  visual \ninefficient,  primarily  attributable \ntemporal \nfore,  that  the  retina  and  the  lgn  should \na  decorrelated \nspatial  and  temporal \ntested  directly \nof  the  lgn \nrecorded \ncat  to  natural, \nto  white-noise \n\nwhitens \nof  individual \nimages \n\ntime-varying \nstimuli.  although \n\ntemporally \nthe  responses \n\nform  or,  equivalently, \n\nin  natural  scenes. \n\npower  spectrum. \n\n", "cell metabolism\n\nreview\n\nthe role of advanced glycation end\nproducts in aging and metabolic diseases:\nbridging association and causality\n\njyotiska chaudhuri,1,* yasmin bains,2 sanjib guha,1 arnold kahn,1,3 david hall,1 neelanjan bose,1,3\nalejandro gugliucci,2,* and pankaj kapahi1,3,*\n1the buck institute for research on aging, 8001 redwood boulevard, novato, ca 94945, usa\n2touro university college of osteopathic medicine, glycation oxidation and research laboratory, vallejo, ca, 94592, usa\n3university of california, department of urology, 400 parnassus avenue, san francisco, ca 94143, usa\n*correspondence: jchaudhuri@buckinstitute.org (j.c.), alejandro.gugliucci@tu.edu (a.g.), pkapahi@buckinstitute.org (p.k.)\nhttps://doi.org/10.1016/j.cmet.2018.08.014\n\naccumulation of advanced glycation end products (ages) on nucleotides, lipids, and peptides/proteins are\nan inevitable component of the aging process in all eukaryotic organisms, including humans. to date, a\nsubstantial body of evidence sh", "supplementary figures and legends: \n \n \n \n \n \n \n \n \n\nsupplementary information\n\nprojections versus each other\n\n(250 ms of data) \n\ndoi:10.1038/nature11129\n\njpc2 projection versus time\n\n(all times)\n\ntarget\n\nmove onset\n\n200 ms\n\njpc1 projection versus time\n\n(all times)\n\ntarget\n\nmove onset\n\n200 ms\n\n \n \nsupplementary figure 1.  projection onto each jpca axis as a function of time.  data is shown for monkey j3.  the \nplot of the jpca plane uses the same format as in figure 3 of the main text.  the plots versus time use the same \nformat as for the individual-neuron psths in figure 2 of the main text (though here the vertical units are arbitrary).  \ntraces are colored red to green based on the level of preparatory activity for that projection.  this allows \nvisualization of \u2018tuning\u2019 with respect to the reach trajectories (inset).  direction \u2018tuning\u2019 is present but imperfect in \nthe projections, much as it is for the neurons upon which the projections are based. \n\nindeed, in many ways the projec", "biorxiv preprint \n\nhttps://doi.org/10.1101/798553\n; \n\ndoi: \nnot certified by peer review) is the author/funder. all rights reserved. no reuse allowed without permission. \n\nthis version posted november 6, 2020. \n\nthe copyright holder for this preprint (which was\n\nrecurrent neural network models of multi-area computation underlying\n\ndecision-making\n\nmichael kleinmana, chandramouli chandrasekaranb,c,d,e,\u2217, jonathan c. kaoa,f,\u2217\n\nadepartment of electrical and computer engineering, university of california, los angeles, ca, usa\nbdepartment of anatomy & neurobiology, boston university school of medicine, boston, ma, usa\n\ncdepartment of psychological and brain sciences, boston university, boston, ma, usa\n\ndcenter for systems neuroscience, boston university, boston, ma, usa\n\nedepartment of biomedical engineering, boston university, boston, ma, usa\n\nfneurosciences program, university of california, los angeles, ca, usa\n\nabstract\n\ncognition emerges from coordinated computations across multiple br", "journal of machine learning research 4 (2003) 119-155\n\nsubmitted 6/02; published 6/03\n\nthink globally, fit locally:\n\nunsupervised learning of low dimensional manifolds\n\nlawrence k. saul\ndepartment of computer and information science\nuniversity of pennsylvania\n200 south 33rd street\n557 moore school - grw\nphiladelphia, pa 19104-6389, usa\n\nsam t. roweis\ndepartment of computer science\nuniversity of toronto\n6 king\u2019s college road\npratt building 283\ntoronto, ontario m5s 3g4, canada\n\neditor: yoram singer\n\nlsaul@cis.upenn.edu\n\nroweis@cs.toronto.edu\n\nabstract\n\nthe problem of dimensionality reduction arises in many \ufb01elds of information processing, including\nmachine learning, data compression, scienti\ufb01c visualization, pattern recognition, and neural com-\nputation. here we describe locally linear embedding (lle), an unsupervised learning algorithm\nthat computes low dimensional, neighborhood preserving embeddings of high dimensional data.\nthe data, assumed to be sampled from an underlying manifold, ", "modeling behaviorally relevant neural dynamics \nenabled by preferential subspace identification\n\nomid g. sani\u200a\nmaryam m. shanechi\u200a\n\n\u200a1,3,4\u2009\u2709\n\n\u200a1, hamidreza abbaspourazad\u200a\n\n\u200a1, yan t. wong2,5, bijan pesaran\u200a\n\n\u200a2 and \n\nneural activity exhibits complex dynamics related to various brain functions, internal states and behaviors. understanding \nhow neural dynamics explain specific measured behaviors requires dissociating behaviorally relevant and irrelevant dynam-\nics, which is not achieved with current neural dynamic models as they are learned without considering behavior. we develop \npreferential subspace identification (psid), which is an algorithm that models neural activity while dissociating and prioritizing \nits behaviorally relevant dynamics. modeling data in two monkeys performing three-dimensional reach and grasp tasks, psid \nrevealed that the behaviorally relevant dynamics are significantly lower-dimensional than otherwise implied. moreover, psid \ndiscovered distinct rotational dy", "8\n1\n0\n2\n\n \nr\na\n\nm\n3\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n5\n6\n1\n0\n0\n\n.\n\n1\n1\n7\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2018\n\ndeep neural networks as gaussian processes\n\njaehoon lee\u2217\u2020, yasaman bahri\u2217\u2020, roman novak , samuel s. schoenholz,\njeffrey pennington, jascha sohl-dickstein\n\ngoogle brain\n{jaehlee, yasamanb, romann, schsam, jpennin, jaschasd}@google.com\n\nabstract\n\nit has long been known that a single-layer fully-connected neural network with an\ni.i.d. prior over its parameters is equivalent to a gaussian process (gp), in the limit\nof in\ufb01nite network width. this correspondence enables exact bayesian inference\nfor in\ufb01nite width neural networks on regression tasks by means of evaluating the\ncorresponding gp. recently, kernel functions which mimic multi-layer random\nneural networks have been developed, but only outside of a bayesian framework.\nas such, previous work has not identi\ufb01ed that these kernels can be used as co-\nvariance functions for gps and allow fully bayesian pr", "weight uncertainty in neural networks\n\ncharles blundell\njulien cornebise\nkoray kavukcuoglu\ndaan wierstra\ngoogle deepmind\n\ncblundell@google.com\njucor@google.com\nkorayk@google.com\nwierstra@google.com\n\nabstract\n\nwe introduce a new, ef\ufb01cient, principled and\nbackpropagation-compatible algorithm for learn-\ning a probability distribution on the weights of\na neural network, called bayes by backprop. it\nregularises the weights by minimising a com-\npression cost, known as the variational free en-\nergy or the expected lower bound on the marginal\nlikelihood. we show that this principled kind\nof regularisation yields comparable performance\nto dropout on mnist classi\ufb01cation. we then\ndemonstrate how the learnt uncertainty in the\nweights can be used to improve generalisation\nin non-linear regression problems, and how this\nweight uncertainty can be used to drive the\nexploration-exploitation trade-off in reinforce-\nment learning.\n\n1. introduction\nplain feedforward neural networks are prone to over\ufb01t-\nti", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nlinking  neural  responses  to  behavior  with\ninformation-preserving  population  vectors\ntatyana  o  sharpee  and  john  a  berkowitz\n\nall  systems  for  processing  signals,  both  arti\ufb01cial  and  within\nanimals,  must  obey  fundamental  statistical  laws  for  how\ninformation  can  be  processed.  we  discuss  here  recent  results\nusing  information  theory  that  provide  a  blueprint  for  building\ncircuits  where  signals  can  be  read-out  without  information  loss.\nmany  properties  that  are  necessary  to  build  information-\npreserving  circuits  are  actually  observed  in  real  neurons,  at\nleast  approximately.  one  such  property  is  the  use  of  logistic\nnonlinearity  for  relating  inputs  to  neural  response  probability.\nsuch  nonlinearities  are  common  in  neural  and  intracellular\nnetworks.  with  this  nonlinearity  type,  there  is  a  linear\ncombination  of  neural  responses  that", "line attractor dynamics in recurrent networks for sentiment classication\n\nniru maheswaranathan * 1 alex h. williams * 2 1 matthew d. golub 2 surya ganguli 2 1 david sussillo 1\n\nabstract\n\nrecurrent neural networks (rnns) are a pow-\nerful tool for modeling sequential data. despite\ntheir widespread usage, understanding how rnns\nsolve complex problems remains elusive. here,\nwe characterize how popular off-the-shelf archi-\ntectures (including lstms, grus, and vanilla\nrnns) perform document-level sentiment clas-\nsi\ufb01cation. despite their theoretical capacity to\nimplement complex, high-dimensional computa-\ntions, we \ufb01nd that all architectures converge to\nhighly interpretable, low-dimensional represen-\ntations. we identify a simple mechanism, inte-\ngration along an approximate line attractor, and\n\ufb01nd this mechanism present across rnn archi-\ntectures (including lstms, grus, and vanilla\nrnns). overall, these results demonstrate that\nsurprisingly universal and human interpretable\ncomputations can ", "ef\ufb01cient training of very deep neural networks for supervised hashing\n\nziming zhang, yuting chen and venkatesh saligrama\n\ndepartment of ece and division of systems engineering, boston university\n\n{zzhang14, yutingch, srv}@bu.edu\n\n6\n1\n0\n2\n\n \nr\np\na\n1\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n4\n2\n5\n4\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nin this paper, we propose training very deep neural net-\nworks (dnns) for supervised learning of hash codes. exist-\ning methods in this context train relatively \u201cshallow\u201d net-\nworks limited by the issues arising in back propagation (e.g.\nvanishing gradients) as well as computational ef\ufb01ciency.\nwe propose a novel and ef\ufb01cient training algorithm inspired\nby alternating direction method of multipliers (admm) that\novercomes some of these limitations. our method decom-\nposes the training process into independent layer-wise lo-\ncal updates through auxiliary variables. empirically we\nobserve that our training algorithm always converges and\nits computational complexity is", "0\n2\n0\n2\n\n \nt\nc\no\n3\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n1\n1\n0\n9\n0\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nimproved techniques for training score-based\n\ngenerative models\n\nyang song\n\ncomputer science department\n\nstanford university\n\nyangsong@cs.stanford.edu\n\nstefano ermon\n\ncomputer science department\n\nstanford university\n\nermon@cs.stanford.edu\n\nabstract\n\nscore-based generative models can produce high quality image samples comparable\nto gans, without requiring adversarial optimization. however, existing training\nprocedures are limited to images of low resolution (typically below 32 \u00d7 32),\nand can be unstable under some settings. we provide a new theoretical analysis\nof learning and sampling from score-based models in high dimensional spaces,\nexplaining existing failure modes and motivating new solutions that generalize\nacross datasets. to enhance stability, we also propose to maintain an exponential\nmoving average of model weights. with these improvements, we can scale score-\nbased generative models to va", "rigorous dynamical mean field theory for stochastic gradient descent methods \u2217\nc\u00b4edric gerbelot\u2020 , emanuele troiani\u2021 , francesca mignacco\u00a7 , florent krzakala\u00b6, and lenka\n\nzdeborov\u00b4a\u2021\n\nabstract. we prove closed-form equations for the exact high-dimensional asymptotics of a family of first order\ngradient-based methods, learning an estimator (e.g. m-estimator, shallow neural network, ...) from\nobservations on gaussian data with empirical risk minimization. this includes widely used algo-\nrithms such as stochastic gradient descent (sgd) or nesterov acceleration. the obtained equations\nmatch those resulting from the discretization of dynamical mean-field theory (dmft) equations from\nstatistical physics when applied to the corresponding gradient flow. our proof method allows us to\ngive an explicit description of how memory kernels build up in the effective dynamics, and to include\nnon-separable update functions, allowing datasets with non-identity covariance matrices. finally, we\nprovide num", "1\n2\n0\n2\n\n \n\nv\no\nn\n9\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n6\n4\n4\n0\n1\n\n.\n\n5\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nredunet: a white-box deep network from the principle of\n\nmaximizing rate reduction\u2217\n\nkwan ho ryan chan \u2020, (cid:5)\nyaodong yu \u2020, (cid:5)\nchong you \u2020, (cid:5)\nhaozhi qi \u2020\njohn wright \u2021\nyi ma \u2020\n\nryanchankh@berkeley.edu\n\nyyu@eecs.berkeley.edu\n\ncyou@berkeley.edu\n\nhqi@berkeley.edu\n\njohnwright@ee.columbia.edu\n\nyima@eecs.berkeley.edu\n\n\u2020 department of electrical engineering and computer sciences\n\nuniversity of california, berkeley, ca 94720-1776, usa\n\n\u2021 department of electrical engineering\n\ndepartment of applied physics and applied mathematics\ncolumbia university, new york, ny, 10027, usa\n\nabstract\n\nthis work attempts to provide a plausible theoretical framework that aims to interpret\nmodern deep (convolutional) networks from the principles of data compression and discrimi-\nnative representation. we argue that for high-dimensional multi-class data, the optimal\nlinear discriminative representation maximizes ", "learning dynamics of deep linear networks with\n\nmultiple pathways\n\njianghong shi\n\ndepartment of applied mathematics\n\nuniversity of washington\n\nseattle, wa 98195\n\njhshi@uw.edu\n\neric shea-brown\n\ndepartment of applied mathematics\n\nuniversity of washington\n\nseattle, wa 98195\n\netsb@uw.edu\n\nmichael a. buice\n\nallen institute mindscope program\n\nseattle, wa 98109\n\nmichaelbu@alleninstitute.org\n\nabstract\n\nnot only have deep networks become standard in machine learning, they are in-\ncreasingly of interest in neuroscience as models of cortical computation that capture\nrelationships between structural and functional properties. in addition they are a\nuseful target of theoretical research into the properties of network computation.\ndeep networks typically have a serial or approximately serial organization across\nlayers, and this is often mirrored in models that purport to represent computation\nin mammalian brains. there are, however, multiple examples of parallel pathways\nin mammalian brains. in some", "functional organization of the \nhippocampal longitudinal axis\n\nbryan a.\u00a0strange1,2, menno p.\u00a0witter3, ed s.\u00a0lein4 and edvard i.\u00a0moser3\n\nabstract | the precise functional role of the hippocampus remains a topic of much \ndebate. the dominant view is that the dorsal (or posterior) hippocampus is implicated in \nmemory and spatial navigation and the ventral (or anterior) hippocampus mediates \nanxiety-related behaviours. however, this \u2018dichotomy view\u2019 may need revision. gene \nexpression studies demonstrate multiple functional domains along the hippocampal \nlong axis, which often exhibit sharply demarcated borders. by contrast, anatomical \nstudies and electrophysiological recordings in rodents suggest that the long axis is \norganized along a gradient. together, these observations suggest a model in which \nfunctional long-axis gradients are superimposed on discrete functional domains.  \nthis model provides a potential framework to explain and test the multiple functions \nascribed to the hippoc", "nnu-net: a self-configuring method for deep \nlearning-based biomedical image segmentation\n\nfabian isensee1,2,6, paul f. jaeger1,6, simon a. a. kohl1,3, jens petersen1,4 and klaus h. maier-hein\u200a\n\n\u200a1,5\u2009\u2709\n\nbiomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field \nof deep learning. while semantic segmentation algorithms enable image analysis and quantification in many applications, the \ndesign of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. \nwe developed nnu-net, a deep learning-based segmentation method that automatically configures itself, including preprocess-\ning, network architecture, training and post-processing for any new task. the key design choices in this process are modeled as \na set of fixed parameters, interdependent rules and empirical decisions. without manual intervention, nnu-net surpasses most \nexisting approaches, including highl", "0\n2\n0\n2\n\n \nl\nu\nj\n \n\n0\n3\n\n \n \n]\n\n.\n\na\nn\nh\nt\na\nm\n\n[\n \n \n\n5\nv\n5\n7\n5\n5\n0\n\n.\n\n8\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nensemble kalman inversion: mean-\ufb01eld limit and convergence\nanalysis\n\nzhiyan ding \u00b7 qin li\n\nabstract ensemble kalman inversion (eki) has been\na very popular algorithm used in bayesian inverse prob-\nlems [22]. it samples particles from a prior distribu-\ntion, and introduces a motion to move the particles\naround in pseudo-time. as the pseudo-time goes to in-\n\ufb01nity, the method \ufb01nds the minimizer of the objective\nfunction, and when the pseudo-time stops at 1, the en-\nsemble distribution of the particles resembles, in some\nsense, the posterior distribution in the linear setting.\nthe ideas trace back further to ensemble kalman fil-\nter and the associated analysis [14,30], but to today,\nwhen viewed as a sampling method, why eki works,\nand in what sense with what rate the method converges\nis still largely unknown.\n\nin this paper, we analyze the continuous version of\neki, a coupled sde system, and prove", "2\n2\n0\n2\n\n \nr\na\n\n \n\nm\n8\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n6\n4\n3\n0\n\n.\n\n3\n0\n2\n2\n:\nv\ni\nx\nr\na\n\ntensor programs v:\n\ntuning large neural networks via\nzero-shot hyperparameter transfer\n\ngreg yang\u2217\u00d7 edward j. hu\u2217\u00d7\u2020\ndavid farhi\u25e6 nick ryder\u25e6\n\nigor babuschkin\u25e6\njakub pachocki\u25e6 weizhu chen\u00d7\n\nszymon sidor\u25e6 xiaodong liu\u00d7\njianfeng gao\u00d7\n\n\u00d7microsoft corporation\n\n\u25e6openai\n\nabstract\n\nhyperparameter (hp) tuning in deep learning is an expensive process, prohibitively\nso for neural networks (nns) with billions of parameters. we show that, in the\nrecently discovered maximal update parametrization (\u00b5p), many optimal hps\nremain stable even as model size changes. this leads to a new hp tuning paradigm\nwe call \u00b5transfer: parametrize the target model in \u00b5p, tune the hp indirectly on a\nsmaller model, and zero-shot transfer them to the full-sized model, i.e., without\ndirectly tuning the latter at all. we verify \u00b5transfer on transformer and resnet.\nfor example, 1) by transferring pretraining hps from a model of 13m para", "learnable latent embeddings for joint \nbehavioural and neural analysis\n\nhttps://doi.org/10.1038/s41586-023-06031-6\nreceived: 30 march 2022\naccepted: 28 march 2023\npublished online: 3 may 2023\nopen access\n\n check for updates\n\nsteffen schneider1,2, jin hwa lee1,2 & mackenzie weygandt mathis1\u2009\u2709\n\nmapping behavioural actions to neural activity is a fundamental goal of neuroscience. \nas our ability to record large neural and behavioural data increases, there is growing \ninterest in modelling neural dynamics during adaptive behaviours to probe neural \nrepresentations1\u20133. in particular, although neural latent embeddings can reveal \nunderlying correlates of behaviour, we lack nonlinear techniques that can explicitly \nand flexibly leverage joint behaviour and neural data to uncover neural dynamics3\u20135. \nhere, we fill this gap with a new encoding method, cebra, that jointly uses behavioural \nand neural data in a (supervised) hypothesis- or (self-supervised) discovery-driven \nmanner to produce both", "research article\n\na map of abstract relational knowledge in\nthe human hippocampal\u2013entorhinal cortex\nmona m garvert1,2*, raymond j dolan1,3, timothy ej behrens1,2\n\n1wellcome trust centre for neuroimaging, institute of neurology, university\ncollege london, london, united kingdom; 2oxford centre for functional mri of\nthe brain, nuffield department of clinical neurosciences, university of oxford,\noxford, united kingdom; 3max planck-ucl centre for computational psychiatry\nand ageing research, london, united kingdom\n\nabstract the hippocampal\u2013entorhinal system encodes a map of space that guides spatial\nnavigation. goal-directed behaviour outside of spatial navigation similarly requires a representation\nof abstract forms of relational knowledge. this information relies on the same neural system, but it\nis not known whether the organisational principles governing continuous maps may extend to the\nimplicit encoding of discrete, non-spatial graphs. here, we show that the human hippocampal\u2013\nentorh", "          \n\np1: ars/bta\ndecember 9, 1998\n\np2: ars/spd\n\nqc: ars\n\n17:38\n\nannual reviews\n\nar076-11\n\nannu. rev. neurosci. 1999. 22:241\u201359\n\ncopyright c(cid:176) 1999 by annual reviews. all rights reserved\n\nneural selection and\ncontrol of visually\nguided eye movements\n\njeffrey d. schall and kirk g. thompson\nvanderbilt vision research center, department of psychology, vanderbilt university,\nnashville, tennessee 37240; e-mail: jeffrey.d.schall@vanderbilt.edu\n\nkey words:\n\nsaccade, visual search, countermanding, eye \ufb01eld, attention, selection,\nreaction time\n\nabstract\n\nwe review neural correlates of perceptual and motor decisions, examining whether\nthe time they occupy explains the duration and variability of behavioral reaction\ntimes. the location of a salient target is identi\ufb01ed through a spatiotemporal evo-\nlution of visually evoked activation throughout the visual system. selection of\nthe target leads to stochastic growth of movement-related activity toward a \ufb01xed\nthreshold to generate the ga", "open\n\nreceived: 21 august 2017\naccepted: 19 february 2018\npublished: xx xx xxxx\n\ndeep residual network predicts \ncortical representation and \norganization of visual features for \nrapid categorization\n\nhaiguang wen2,3, junxing shi \n\n 2,3, wei chen4 & zhongming liu \n\n 1,2,3\n\nthe brain represents visual objects with topographic cortical patterns. to address how distributed \nvisual representations enable object categorization, we established predictive encoding models based \non a deep residual network, and trained them to predict cortical responses to natural movies. using \nthis predictive model, we mapped human cortical representations to 64,000 visual objects from 80 \ncategories with high throughput and accuracy. such representations covered both the ventral and \ndorsal pathways, reflected multiple levels of object features, and preserved semantic relationships \nbetween categories. in the entire visual cortex, object representations were organized into three \nclusters of categories: biol", "commentary \n\nthe  organization  of  the  basal \n\nganglia-thalamocortical \nopen  interconnected \n\ncircuits: \nrather  than \n\nclosed  segregated \n\np\ne\nr\ng\na\nm\no\nn\n \n0\n3\n0\n6\n-\n4\n5\n2\n2\n(\n9\n4\n)\n0\n0\n2\n8\n9\n-\n4\n \nn\ne\nu\nr\no\ns\nc\ni\ne\nn\nc\ne\n \nv\no\nl\n.\n \n6\n3\n,\n \nn\no\n.\n \n2\n,\n \n3\n6\n3\n-\n3\n7\n9\n,\n \n1\n9\n9\n4\n \np\np\n.\n \ne\nl\ns\ne\nv\ni\ne\nr\n \ns\nc\ni\ne\nn\nc\ne\n \nl\nt\nd\n \nc\no\np\ny\nr\ni\ng\nh\nt\n \n0\n \n1\n9\n9\n4\n \ni\nb\nr\no\n \np\nr\ni\nn\nt\ne\nd\n \ni\nn\n \ng\nr\ne\na\nt\n \nb\nr\ni\nt\na\ni\nn\n.\n \na\nl\nl\n \nr\ni\ng\nh\nt\ns\n \nr\ne\ns\ne\nr\nv\ne\nd\n \n0\n3\n0\n6\n-\n4\n5\n2\n2\n/\n9\n4\n \n$\n7\n.\n0\n0\n \n+\n \n0\n.\n0\n0\n \nd\n.\n \nj\no\ne\nl\n \na\nn\nd\n \ni\n.\n \nw\ne\ni\nn\ne\nr\n*\n \nd\ne\np\na\nr\nt\nm\ne\nn\nt\n \no\nf\n \np\ns\ny\nc\nh\no\nl\no\ng\ny\n,\n \nt\ne\nl\n \na\nv\ni\nv\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n,\n \nr\na\nm\na\nt\n \na\nv\ni\nv\n \n6\n9\n9\n7\n8\n,\n \nt\ne\nl\n \na\nv\ni\nv\n,\n \ni\ns\nr\na\ne\nl\n \na\nb\ns\nt\nr\na\nc\nt\n-\na\nn\na\nt\no\nm\ni\nc\na\nl\n \nf\ni\nn\nd\ni\nn\ng\ns\n \ni\nn\n \np\nr\ni\nm\na\nt\ne\ns\n \na\nn\nd\n \nr\no\nd\ne\nn\nt\ns\n \nh\na\nv\ne\n \nl\ne\nd\n \nt\no\n \na\n \nd\ne\ns\nc\nr\ni\np\nt\ni\no\nn\n \no\nf\n \ns\ne\nv\ne\nr\na\nl\n \np\na\nr\na\nl\nl\ne\nl\n \ns\ne\ng\nr\ne\ng\na\nt\ne\nd\n \nb\na\ns\na\nl\n \ng\na\nn\ng\nl\ni\na\n-\nt\nh\na\nl\na\nm\no\n", "neuroimage 85 (2014) 656\u2013666\n\ncontents lists available at sciencedirect\n\nneuroimage\n\nj o u r n a l h o m e p a ge : w ww . e l s e v i e r . c o m/ l o c a t e / y n i m g\n\nreview\ntheta rhythm and the encoding and retrieval of space and time\nmichael e. hasselmo \u204e, chantal e. stern\ncenter for memory and brain, department of psychology and graduate program for neuroscience, boston university, 2 cummington mall, boston, ma, 02215, usa\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\nphysiological data demonstrates theta frequency oscillations associated with memory function and spatial\nbehavior. modeling and data from animals provide a perspective on the functional role of theta rhythm,\nincluding correlations with behavioral performance and coding by timing of spikes relative to phase of oscil-\nlations. data supports a theorized role of theta rhythm in setting the dynamics for encoding and retrieval\nwithin cortical circuits. recent data also supports models showing how network and cellular theta", "letters to nature\n\n(3.0\u20135.0 mm diameter) for chronic recording was drilled through the cranioplastic cap\nand the underlying skull.\n\n11. ono, t., nishijo, h. & uwano, t. amygdala role in conditioned associative learning. prog. neurobiol.\n\n46, 401\u2013422 (1995).\n\nbehavioural task\nthe rats were required to lick a spout to obtain rewards of 0.3 m sucrose solution or icss\n(0.5-s train of 100-hz, 0.3-ms capacitor-coupled negative square wave pulses). we\nmonitored licking behaviour with a photoelectric sensor. the threshold level for icss to\nmaintain the licking behaviour in the operant task was determined in behavioural tests\nbefore recording. in this study, the thresholds ranged from 65 to 110 ma. the licking\nfrequency depended on the method of reward delivery, that is, low for icss and high for a\nliquid reward. this was probably because icss was delivered only once at the \ufb01rst lick even\nif the rat licked several times, whereas liquid was delivered continuously during a 2-s\nreward period, alth", "elifesciences.org\n\nresearch advance\n\na cortical disinhibitory circuit for\nenhancing adult plasticity\nyu fu1*\u2020, megumi kaneko1\u2020, yunshuo tang2,3,4, arturo alvarez-buylla2,3,\nmichael p stryker1*\n\n1center for integrative neuroscience, department of physiology, university of\ncalifornia, san francisco, san francisco, united states; 2department of\nneurological surgery, university of california, san francisco, san francisco,\nunited states; 3the eli and edythe broad center of regeneration medicine, university\nof california, san francisco, san francisco, united states; 4medical scientist training\nprogram, biomedical science program, university of california, san francisco, san\nfrancisco, united states\n\nabstract the adult brain continues to learn and can recover from injury, but the elements and\noperation of the neural circuits responsible for this plasticity are not known. in previous work, we\nhave shown that locomotion dramatically enhances neural activity in the visual cortex (v1) of the\nmous", "ieee transactions on systems, man, and cybernetics\u2014part b: cybernetics, vol. 32, no. 6, december 2002\n\n711\n\nvarieties of learning automata: an overview\n\nm. a. l. thathachar, fellow, ieee, and p. s. sastry, senior member, ieee\n\nabstract\u2014automata models of learning systems introduced\nin the 1960s were popularized as learning automata (la) in\na survey paper in 1974 [1]. since then, there have been many\nfundamental advances in the theory as well as applications of\nthese learning models. in the past few years, the structure of la\nhas been modified in several directions to suit different applica-\ntions. concepts such as parameterized learning automata (pla),\ngeneralized learning automata (gla), and continuous action-set\nlearning automata (cala) have been proposed, analyzed, and\napplied to solve many significant learning problems. furthermore,\ngroups of la forming teams and feedforward networks have\nbeen shown to converge to desired solutions under appropriate\nlearning algorithms. modules of ", "article\nshared and distinct transcriptomic cell \ntypes across neocortical areas\n\nhttps://doi.org/10.1038/s41586-018-0654-5\n\nbosiljka tasic1*, zizhen yao1,4, lucas t. graybuck1,4, kimberly a. smith1,4, thuc nghi nguyen1, darren bertagnolli1, jeff goldy1, \nemma garren1, michael n. economo2, sarada viswanathan2, osnat penn1, trygve bakken1, vilas menon1,2, jeremy miller1,  \nolivia fong1, karla e. hirokawa1, kanan lathia1, christine rimorin1, michael tieu1, rachael larsen1, tamara casper1,  \neliza barkan1, matthew kroll1, sheana parry1, nadiya v. shapovalova1, daniel hirschstein1, julie pendergraft1,  \nheather a. sullivan3, tae kyung kim1, aaron szafer1, nick dee1, peter groblewski1, ian wickersham3, ali cetin1, julie a. harris1, \nboaz p. levi1, susan m. sunkin1, linda madisen1, tanya l. daigle1, loren looger2, amy bernard1, john phillips1, ed lein1,  \nmichael hawrylycz1, karel svoboda2, allan r. jones1, christof koch1 & hongkui zeng1\n\nthe neocortex contains a multitude of cell types that ", "dynamics of blood flow and oxygenation changes \nduring brain activation: the balloon model \n\nrichard b. buxton, eric c. wong, lawrence r. frank \n\na biomechanical model is presented for the dynamic changes \nin  deoxyhemoglobin content  during  brain  activation.  the \nmodel  incorporates  the  conflicting  effects  of  dynamic \nchanges in both blood oxygenation and blood volume. calcu- \nlations based on the model show pronounced transients in the \ndeoxyhemoglobin content and  the  blood  oxygenation level \ndependent (bold) signal measured with functional mri, in- \ncluding  initial dips  and  overshoots and  a  prolonged post- \nstimulus undershoot of the bold signal. furthermore, these \ntransient effects can occur in the presence of tight coupling of \ncerebral blood flow  and oxygen metabolism throughout the \nactivation period. an  initial test of the model against experi- \nmental measurements of  flow  and  bold  changes during  a \nfinger-tapping task showed good agreement. \nkey words: ", "7\n1\n0\n2\n\n \nr\na\n\n \n\nm\n1\n2\n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n2\nv\n6\n0\n9\n3\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nmassive exploration of neural machine translation\n\narchitectures\n\ndenny britz\u2217\u2020, anna goldie\u2217, minh-thang luong, quoc le\n\n{dennybritz,agoldie,thangluong,qvl}@google.com\n\ngoogle brain\n\nabstract\n\nthe \ufb01rst\n\nneural machine translation (nmt) has\nshown remarkable progress over the past\nfew years with production systems now\nbeing deployed to end-users. one major\ndrawback of current architectures is that\nthey are expensive to train, typically re-\nquiring days to weeks of gpu time to\nconverge. this makes exhaustive hyper-\nparameter search, as is commonly done\nwith other neural network architectures,\nprohibitively expensive.\nin this work,\nwe present\nlarge-scale analy-\nsis of nmt architecture hyperparameters.\nwe report empirical results and variance\nnumbers for several hundred experimental\nruns, corresponding to over 250,000 gpu\nhours on the standard wmt english to\ngerman translation task. our experiments\nlead", "preprint. under review.\n\n1\n\nmodularizing deep learning via pairwise learning\n\nwith kernels\n\nshiyu duan, shujian yu, member, ieee, and jos\u00b4e c. pr\u00b4\u0131ncipe, life fellow, ieee\n\n0\n2\n0\n2\n\n \n\np\ne\ns\n0\n1\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n1\n4\n5\n5\n0\n\n.\n\n5\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\u2014by rede\ufb01ning the conventional notions of layers,\nwe present an alternative view on \ufb01nitely wide, fully trainable\ndeep neural networks as stacked linear models in feature spaces,\nleading to a kernel machine interpretation. based on this con-\nstruction, we then propose a provably optimal modular learning\nframework for classi\ufb01cation that does not require between-\nmodule backpropagation. this modular approach brings new\ninsights into the label requirement of deep learning: it leverages\nonly implicit pairwise labels (weak supervision) when learning\nthe hidden modules. when training the output module, on\nthe other hand, it requires full supervision but achieves high\nlabel ef\ufb01ciency, needing as few as 10 randomly selected ", "random features for large-scale kernel machines\n\nali rahimi and ben recht\n\nabstract\n\nto accelerate the training of kernel machines, we propose to map the input data\nto a randomized low-dimensional feature space and then apply existing fast linear\nmethods. our randomized features are designed so that the inner products of the\ntransformed data are approximately equal to those in the feature space of a user\nspeci\ufb01ed shift-invariant kernel. we explore two sets of random features, provide\nconvergence bounds on their ability to approximate various radial basis kernels,\nand show that in large-scale classi\ufb01cation and regression tasks linear machine\nlearning algorithms that use these features outperform state-of-the-art large-scale\nkernel machines.\n\n1 introduction\n\nkernel machines such as the support vector machine are attractive because they can approximate\nany function or decision boundary arbitrarily well with enough training data. unfortunately, meth-\nods that operate on the kernel matrix (", "expectation-induced modulation of metastable \nactivity underlies faster coding of sensory stimuli\n\nl.\u00a0mazzucato1,2, g.\u00a0la\u00a0camera\u200a\n\n\u200a1,3* and a.\u00a0fontanini\u200a\n\n\u200a1,3*\n\nsensory stimuli can be recognized more rapidly when they are expected. this phenomenon depends on expectation affect-\ning the cortical processing of sensory information. however, the mechanisms responsible for the effects of expectation on \nsensory circuits remain elusive. in the present study, we report a novel computational mechanism underlying the expectation-\ndependent acceleration of coding observed in the gustatory cortex of alert rats. we use a recurrent spiking network model with \na clustered architecture capturing essential features of cortical activity, such as its intrinsically generated metastable dynam-\nics. relying on network theory and computer simulations, we propose that expectation exerts its function by modulating the \nintrinsically generated dynamics preceding taste delivery. our model\u2019s predictions were c", "article\n\nreceived 27 jun 2016 | accepted 14 sep 2016 | published 27 oct 2016\n\ndoi: 10.1038/ncomms13239\n\nopen\n\nreorganization between preparatory and\nmovement population responses in motor cortex\ngamaleldin f. elsayed1,2,*, antonio h. lara2,*, matthew t. kaufman3, mark m. churchland2,4,5,6\n& john p. cunningham1,4,7\n\nthe underlying computational strategies at\n\nneural populations can change the computation they perform on very short timescales.\nalthough such \ufb02exibility is common,\nthe\npopulation level remain unknown. to address this gap, we examined population responses in\nmotor cortex during reach preparation and movement. we found that there exist exclusive\nand orthogonal population-level subspaces dedicated to preparatory and movement\ncomputations. this orthogonality yielded a reorganization in response correlations: the set of\nneurons with shared response properties changed completely between preparation and\nmovement. thus, the same neural population acts, at different times, as two se", "manhattan rule training for memristive crossbar \n\ncircuit pattern classifiers \n\nelham zamanidoost, farnood m. bayat, dmitri strukov \n\nelectrical and computer engineering department \n\nuniversity of california santa barbara \n\nsanta barbara, ca, usa \n\n{elham, farnoodmb, strukov}@ece.ucsb.edu \n\n \n\nabstract\u2014we  investigated  batch  and  stochastic  manhattan \nrule  algorithms  for  training  multilayer  perceptron  classifiers \nimplemented  with  memristive  crossbar  circuits.  in  manhattan \nrule  training,  the  weights  are  updated  only  using  sign \ninformation  of  classical  backpropagation  algorithm.  the  main \nadvantage  of  manhattan  rule  is  its  simplicity,  which  leads  to \nmore  compact  hardware  implementation  and  faster  training \ntime. additionally, in case of stochastic training, manhattan rule \nallows  performing  all  weight  updates  in  parallel,  which  further \nspeeds  up  the training procedure.  the tradeoff for  simplicity is \nslightly  worse \nexample, \n", "an introduction to deep\nreinforcement learning\n\nfull text available at: http://dx.doi.org/10.1561/2200000071\f", "original research\npublished: 10 february 2021\ndoi: 10.3389/fnins.2021.629892\n\nlearning without feedback:\nfixed random learning signals\nallow for feedforward training\nof deep neural networks\n\ncharlotte frenkel 1,2*\u2020, martin lefebvre 2*\u2020 and david bol 2\n\n1 institute of neuroinformatics, university of z\u00fcrich and eth z\u00fcrich, zurich, switzerland, 2 icteam institute, universit\u00e9\ncatholique de louvain, louvain-la-neuve, belgium\n\nwhile the backpropagation of error algorithm enables deep neural network training, it\nimplies (i) bidirectional synaptic weight transport and (ii) update locking until the forward\nand backward passes are completed. not only do these constraints preclude biological\nplausibility, but they also hinder the development of low-cost adaptive smart sensors at\nthe edge, as they severely constrain memory accesses and entail buffering overhead. in\nthis work, we show that the one-hot-encoded labels provided in supervised classi\ufb01cation\nproblems, denoted as targets, can be viewed as", "22. j. f. kelly, k. g. horton, glob. ecol. biogeogr. 25, 1159\u20131165 (2016).\n\nbiological sciences research council grant bb/j004286/1 to j.w.c.\ndata are available at dryad at http://dx.doi.org/10.5061/dryad.6kt29.\n\ntables s1 to s8\nreferences (23\u201349)\n\nacknowledgments\ng.h.\u2019s visiting scholarship was funded by the china scholarship\ncouncil. we acknowledge the support provided by cost\u2014european\ncooperation in science and technology through the action es1305\n\u201cenram.\u201d the project was supported by uk biotechnology and\n\nsupplementary materials\nwww.sciencemag.org/content/354/6319/1584/suppl/dc1\nmaterials and methods\nfigs. s1 to s8\n\n27 june 2016; accepted 22 november 2016\n10.1126/science.aah4379\n\nresearch | reports\n\nbrain research\n\nactive cortical dendrites\nmodulate perception\n\nnaoya takahashi,1 thomas g. oertner,2 peter hegemann,3 matthew e. larkum1*\n\nthere is as yet no consensus concerning the neural basis of perception and how it operates\nat a mechanistic level. we found that ca2+ activity in th", "8\n1\n0\n2\n\n \n\nb\ne\nf\n \n5\n1\n\n \n \n]\nt\ni\n.\ns\nc\n[\n \n \n\n1\nv\n8\n0\n4\n5\n0\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nworkshop track - iclr 2018\n\n\u201cdependency bottleneck\u201d in auto-encoding\narchitectures: an empirical study\n\ndenny wu\u22171, yixiu zhao\u22171, yao-hung hubert tsai\u22172, makoto yamada3, ruslan salakhutdinov2\n1computational biology department, carnegie mellon university\n2machine learning department, carnegie mellon university\n3riken aip, jst presto\nyiwu1@andrew.cmu.edu, yixiuz@andrew.cmu.edu,\nyaohungt@cs.cmu.edu, makoto.yamada@riken.jp, rsalakhu@cs.cmu.edu\n\nabstract\n\nrecent works investigated the generalization properties in deep neural networks\n(dnns) by studying the information bottleneck in dnns. however, the mea-\nsurement of the mutual information (mi) is often inaccurate due to the density\nestimation. to address this issue, we propose to measure the dependency instead\nof mi between layers in dnns. speci\ufb01cally, we propose to use hilbert-schmidt\nindependence criterion (hsic) as the dependency measure, which can me", "research article\nidentifying control ensembles for information\nprocessing within the cortico-basal ganglia-\nthalamic circuit\n\ncatalina vichid1,2, matthew clappid3,4, jonathan e. rubinid4,5\u262f*,\ntimothy verstynenid3,4\u262f*\n1 dept. de matem\u00e0tiques i inform\u00e0tica, universitat de les illes balears, palma, spain, 2 institute of applied\ncomputing and community code, palma, spain, 3 department of psychology & neuroscience institute,\ncarnegie mellon university, pittsburgh, pennsylvania, united states of america, 4 center for the neural basis\nof cognition, pittsburgh, pennsylvania, united states of america, 5 department of mathematics, university of\npittsburgh, pittsburgh, pennsylvania, united states of america\n\n\u262f these authors contributed equally to this work.\n* jonrubin@pitt.edu (jr); timothyv@andrew.cmu.edu (tv)\n\nabstract\n\nin situations featuring uncertainty about action-reward contingencies, mammals can flexibly\nadopt strategies for decision-making that are tuned in response to environmental chan", "psychopharmacology (2007) 191:507\u2013520\ndoi 10.1007/s00213-006-0502-4\n\noriginal investigation\n\ntonic dopamine: opportunity costs\nand the control of response vigor\n\nyael niv & nathaniel d. daw & daphna joel &\npeter dayan\n\nreceived: 27 april 2006 / accepted: 28 june 2006 / published online: 10 october 2006\n# springer-verlag 2006\n\ninfluence over\n\nabstract\nrationale dopamine neurotransmission has long been\nknown to exert a powerful\nthe vigor,\nstrength, or rate of responding. however, there exists no\nclear understanding of the computational foundation for this\neffect; predominant accounts of dopamine\u2019s computational\nfunction focus on a role for phasic dopamine in controlling\nthe discrete selection between different actions and have\nnothing to say about response vigor or indeed the free-\noperant tasks in which it is typically measured.\nobjectives we seek to accommodate free-operant behav-\nioral tasks within the realm of models of optimal control\nand thereby capture how dopaminergic and motivat", "p\ne\nr\ng\na\nm\no\nn\n \np\ni\ni\n:\n \ns\n0\n0\n4\n2\n-\n6\n9\n8\n9\n(\n9\n7\n)\n0\n0\n1\n2\n1\n-\n1\n \nv\ni\ns\ni\no\nn\n \nr\ne\ns\n.\n,\n \nv\no\nl\n.\n \n3\n7\n,\n \nn\no\n.\n \n2\n3\n,\n \np\np\n.\n \n3\n3\n2\n7\n-\n3\n3\n3\n8\n,\n \n1\n9\n9\n7\n \n\u00a9\n \n1\n9\n9\n7\n \ne\nl\ns\ne\nv\ni\ne\nr\n \ns\nc\ni\ne\nn\nc\ne\n \nl\nt\nd\n.\n \na\nl\nl\n \nr\ni\ng\nh\nt\ns\n \nr\ne\ns\ne\nr\nv\ne\nd\n \np\nr\ni\nn\nt\ne\nd\n \ni\nn\n \ng\nr\ne\na\nt\n \nb\nr\ni\nt\na\ni\nn\n \n0\n0\n4\n2\n-\n6\n9\n8\n9\n/\n9\n7\n \n$\n1\n7\n.\n0\n0\n \n+\n \n0\n.\n0\n0\n \nt\nh\ne\n \n\"\ni\nn\nd\ne\np\ne\nn\nd\ne\nn\nt\n \nc\no\nm\np\no\nn\ne\nn\nt\ns\n\"\n \no\nf\n \ns\nc\ne\nn\ne\ns\n \na\nr\ne\n \ne\nd\ng\ne\n \nf\ni\nl\nt\ne\nr\ns\n \na\nn\nt\nh\no\nn\ny\n \nj\n.\n \nb\ne\nl\nl\n,\n*\nt\n \nt\ne\nr\nr\ne\nn\nc\ne\n \nj\n.\n \ns\ne\nj\nn\no\nw\ns\nk\ni\n*\n \nr\ne\nc\ne\ni\nv\ne\nd\n \n1\n6\n \nj\nu\nl\ny\n \n1\n9\n9\n6\n;\n \ni\nn\n \nr\ne\nv\ni\ns\ne\nd\n \nf\no\nr\nm\n \n9\n \na\np\nr\ni\nl\n \n1\n9\n9\n7\n \nn\na\nt\nu\nr\na\nl\n \ni\nt\n \nh\na\ns\n \np\nr\ne\nv\ni\no\nu\ns\nl\ny\n \nb\ne\ne\nn\n \ns\nu\ng\ng\ne\ns\nt\ne\nd\n \nt\nh\na\nt\n \nn\ne\nu\nr\no\nn\ns\n \nw\ni\nt\nh\n \nl\ni\nn\ne\n \na\nn\nd\n \ne\nd\ng\ne\n \ns\ne\nl\ne\nc\nt\ni\nv\ni\nt\ni\ne\ns\n \nf\no\nu\nn\nd\n \ni\nn\n \np\nr\ni\nm\na\nr\ny\n \nv\ni\ns\nu\na\nl\n \nc\no\nr\nt\ne\nx\n \no\nf\n \nc\na\nt\ns\n \na\nn\nd\n \nm\no\nn\nk\ne\ny\ns\n \nf\no\nr\nm\n \na\n \ns\np\na\nr\ns\ne\n,\n \nd\ni\ns\nt\nr\ni\nb\nu\nt\ne\n", "towards understanding mixture of experts in deep\n\nlearning\n\nzixiang chen\u2217 and yihe deng\u2020 and yue wu\u2021 and quanquan gu\u00a7 and yuanzhi li\u00b6\n\nabstract\n\nthe mixture-of-experts (moe) layer, a sparsely-activated model controlled by a router, has\nachieved great success in deep learning. however, the understanding of such architecture remains\nelusive. in this paper, we formally study how the moe layer improves the performance of neural\nnetwork learning and why the mixture model will not collapse into a single model. our empirical\nresults suggest that the cluster structure of the underlying problem and the non-linearity of the\nexpert are pivotal to the success of moe. to further understand this, we consider a challenging\nclassi\ufb01cation problem with intrinsic cluster structures, which is hard to learn using a single\nexpert. yet with the moe layer, by choosing the experts as two-layer nonlinear convolutional\nneural networks (cnns), we show that the problem can be learned successfully. furthermore,\nour", "1\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n4\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n4\n3\n1\n0\n\n.\n\n6\n0\n1\n2\n:\nv\ni\nx\nr\na\n\ndecision transformer: reinforcement\n\nlearning via sequence modeling\n\nlili chen\u2217,1, kevin lu\u2217,1, aravind rajeswaran2, kimin lee1,\n\naditya grover2, michael laskin1, pieter abbeel1, aravind srinivas\u2020,1, igor mordatch\u2020,3\n\n\u2217equal contribution \u2020equal advising\n\n1uc berkeley 2facebook ai research 3google brain\n\n{lilichen, kzl}@berkeley.edu\n\nabstract\n\nwe introduce a framework that abstracts reinforcement learning (rl) as a se-\nquence modeling problem. this allows us to draw upon the simplicity and scalabil-\nity of the transformer architecture, and associated advances in language modeling\nsuch as gpt-x and bert. in particular, we present decision transformer, an\narchitecture that casts the problem of rl as conditional sequence modeling. un-\nlike prior approaches to rl that \ufb01t value functions or compute policy gradients,\ndecision transformer simply outputs the optimal actions by leveraging a causally\nmasked t", "\f", "hybrid speech recognition with deep bidirectional lstm\n\nalex graves, navdeep jaitly and abdel-rahman mohamed\n\nuniversity of toronto\n\ndepartment of computer science\n\n6 king\u2019s college rd. toronto, m5s 3g4, canada\n\nabstract\n\ndeep bidirectional lstm (dblstm) recurrent neural net-\nworks have recently been shown to give state-of-the-art per-\nformance on the timit speech database. however, the re-\nsults in that work relied on recurrent-neural-network-speci\ufb01c\nobjective functions, which are dif\ufb01cult to integrate with exist-\ning large vocabulary speech recognition systems. this paper\ninvestigates the use of dblstm as an acoustic model in a\nstandard neural network-hmm hybrid system. we \ufb01nd that a\ndblstm-hmm hybrid gives equally good results on timit\nas the previous work.\nit also outperforms both gmm and\ndeep network benchmarks on a subset of the wall street jour-\nnal corpus. however the improvement in word error rate over\nthe deep network is modest, despite a great increase in frame-\nlevel accura", "article\n\nhttps://doi.org/10.1038/s41467-020-19788-5\n\nopen\n\nremembrance of things practiced with fast and\nslow learning in cortical and subcortical pathways\n\njames m. murray\n\n1,2\u2709 & g. sean escola\n\n1,3\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\nthe learning of motor skills unfolds over multiple timescales, with rapid initial gains in\nperformance followed by a longer period in which the behavior becomes more re\ufb01ned,\nhabitual, and automatized. while recent lesion and inactivation experiments have provided\nhints about how various brain areas might contribute to such learning, their precise roles and\nthe neural mechanisms underlying them are not well understood. in this work, we propose\nneural- and circuit-level mechanisms by which motor cortex, thalamus, and striatum support\nmotor learning. in this model, the combination of fast cortical learning and slow subcortical\nlearning gives rise to a covert learning process through which control of behavior is gradually\ntransferred from cortical to subcortica", "article\n\nreceived 3 feb 2016 | accepted 29 oct 2016 | published 13 dec 2016\n\ndoi: 10.1038/ncomms13749\n\nopen\n\nmaking brain\u2013machine interfaces robust to\nfuture neural variability\ndavid sussillo1,2,*, sergey d. stavisky3,*, jonathan c. kao1,*, stephen i. ryu1,4 & krishna v. shenoy1,2,3,5,6,7\n\na major hurdle to clinical translation of brain\u2013machine interfaces (bmis) is that current\ndecoders, which are trained from a small quantity of recent data, become ineffective when\nneural recording conditions subsequently change. we tested whether a decoder could be\nmade more robust to future neural variability by training it to handle a variety of recording\nconditions sampled from months of previously collected data as well as synthetic training\ndata perturbations. we developed a new multiplicative recurrent neural network bmi decoder\nthat successfully learned a large variety of neural-to-kinematic mappings and became more\nrobust with larger training data sets. here we demonstrate that when tested wi", "neuron\n\nreview\n\nneuroscience-inspired arti\ufb01cial intelligence\n\ndemis hassabis,1,2,* dharshan kumaran,1,3 christopher summer\ufb01eld,1,4 and matthew botvinick1,2\n1deepmind, 5 new street square, london, uk\n2gatsby computational neuroscience unit, 25 howland street, london, uk\n3institute of cognitive neuroscience, university college london, 17 queen square, london, uk\n4department of experimental psychology, university of oxford, oxford, uk\n*correspondence: dhcontact@google.com\nhttp://dx.doi.org/10.1016/j.neuron.2017.06.011\n\nthe \ufb01elds of neuroscience and arti\ufb01cial intelligence (ai) have a long and intertwined history. in more recent\ntimes, however, communication and collaboration between the two \ufb01elds has become less commonplace.\nin this article, we argue that better understanding biological brains could play a vital role in building intelligent\nmachines. we survey historical interactions between the ai and neuroscience \ufb01elds and emphasize current\nadvances in ai that have been inspired by the s", "computational prediction of methylation status\nin human genomic sequences\n\nrajdeep das*, nevenka dimitrova\u2020, zhenyu xuan*, robert a. rollins\u2021, fatemah haghighi\u00a7, john r. edwards\u00a7\u00b6,\njingyue ju\u00a7\u00b6, timothy h. bestor\u2021, and michael q. zhang*\u50a8\n\n*cold spring harbor laboratory, cold spring harbor, ny 11724; \u2020philips research, 345 scarborough road, briarcliff manor, ny 10510; \u2021department\nof genetics and development, college of physicians and surgeons of columbia university, new york, ny 10032; and \u00a7columbia genome center and\n\u00b6department of chemical engineering, columbia university, new york, ny 10032\n\ncommunicated by michael h. wigler, cold spring harbor laboratory, cold spring harbor, ny, april 12, 2006 (received for review october 25, 2005)\n\nepigenetic effects in mammals depend largely on heritable\ngenomic methylation patterns. we describe a computational pat-\ntern recognition method that is used to predict the methylation\nlandscape of human brain dna. this method can be applied both\nto cpg i", "5\n1\n0\n2\n\n \nr\np\na\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n1\n4\n6\n0\n0\n\n.\n\n4\n0\n5\n1\n:\nv\ni\nx\nr\na\n\na probabilistic theory of deep learning\n\nankit b. patel, tan nguyen, richard g. baraniuk\n\ndepartment of electrical and computer engineering\n\nrice university\n\n{abp4, mn15, richb}@rice.edu\n\napril 2, 2015\n\na grand challenge in machine learning is the development of computational al-\ngorithms that match or outperform humans in perceptual inference tasks such\nas visual object and speech recognition. the key factor complicating such tasks\nis the presence of numerous nuisance variables, for instance, the unknown\nobject position, orientation, and scale in object recognition or the unknown\nvoice pronunciation, pitch, and speed in speech recognition. recently, a new\nbreed of deep learning algorithms have emerged for high-nuisance inference\ntasks; they are constructed from many layers of alternating linear and nonlin-\near processing units and are trained using large-scale algorithms and massive\namounts of tr", "article\n\ndoi:10.1038/nature14251\n\nbranch-specific dendritic ca21 spikes\ncause persistent synaptic plasticity\n\njoseph cichon1 & wen-biao gan1\n\nthe brain has an extraordinary capacity for memory storage, but how it stores new information without disrupting\npreviously acquired memories remains unknown. here we show that different motor learning tasks induce dendritic\nca21 spikes on different apical tuft branches of individual layer v pyramidal neurons in the mouse motor cortex. these\ntask-related, branch-specific ca21 spikes cause long-lasting potentiation of postsynaptic dendritic spines active at the\ntime of spike generation. when somatostatin-expressing interneurons are inactivated, different motor tasks frequently\ninduce ca21 spikes on the same branches. on those branches, spines potentiated during one task are depotentiated when\nthey are active seconds before ca21 spikes induced by another task. concomitantly, increased neuronal activity and\nperformance improvement after learning one", "asynchronous methods for deep reinforcement learning\n\nvolodymyr mnih1\nadri\u00e0 puigdom\u00e8nech badia1\nmehdi mirza1,2\nalex graves1\ntim harley1\ntimothy p. lillicrap1\ndavid silver1\nkoray kavukcuoglu 1\n1 google deepmind\n2 montreal institute for learning algorithms (mila), university of montreal\n\nvmnih@google.com\nadriap@google.com\nmirzamom@iro.umontreal.ca\ngravesa@google.com\ntharley@google.com\ncountzero@google.com\ndavidsilver@google.com\nkorayk@google.com\n\nabstract\nconceptually\n\na\n\nsimple\n\npropose\n\nand\nwe\nlightweight\nframework for deep reinforce-\nment learning that uses asynchronous gradient\ndescent for optimization of deep neural network\ncontrollers. we present asynchronous variants of\nfour standard reinforcement learning algorithms\nand show that parallel actor-learners have a\nstabilizing effect on training allowing all four\nmethods to successfully train neural network\ncontrollers. the best performing method, an\nasynchronous variant of actor-critic, surpasses\nthe current state-of-the-art on the a", "the journal of neuroscience, march 1, 2001, 21(5):1635\u20131644\n\naccumulation of hippocampal place fields at the goal location in\nan annular watermaze task\n\nstig a. hollup,1 sturla molden,1 james g. donnett,2 may-britt moser,1 and edvard i. moser1\n1department of psychology, norwegian university of science and technology, 7491 trondheim, norway, and 2axona\nltd., st. albans, herts al3 6eu, united kingdom\n\nto explore the plastic representation of information in spatially\nselective hippocampal pyramidal neurons, we made multiple\nsingle-unit recordings in rats trained to \ufb01nd a hidden platform at\na constant location in a hippocampal-dependent annular wa-\ntermaze task. hippocampal pyramidal cells exhibited place-\nrelated \ufb01ring in the watermaze. place \ufb01elds tended to accumu-\nlate near the platform, even in probe trials without immediate\nescape. the percentage of cells with peak activity around the\nhidden platform was more than twice the percentage \ufb01ring in\nequally large areas elsewhere in the aren", "j neurophysiol 124: 312\u2013329, 2020.\nfirst published june 24, 2020; doi:10.1152/jn.00158.2020.\n\nresearch article nervous system pathophysiology\n\ndelta oscillations are a robust biomarker of dopamine depletion severity and\nmotor dysfunction in awake mice\n\nx timothy c. whalen,1,2 x amanda m. willard,1,2,3 x jonathan e. rubin,1,4 and aryn h. gittis1,2\n1center for the neural basis of cognition, carnegie mellon university, pittsburgh, pennsylvania; 2neuroscience institute and\ndepartment of biological sciences, carnegie mellon university, pittsburgh, pennsylvania; 3department of biology and\ngeosciences, clarion university, clarion, pennsylvania; and 4department of mathematics, university of pittsburgh,\npittsburgh, pennsylvania\n\nsubmitted 30 march 2020; accepted in \ufb01nal form 17 june 2020\n\nwhalen tc, willard am, rubin je, gittis ah. delta oscilla-\ntions are a robust biomarker of dopamine depletion severity and motor\ndysfunction in awake mice. j neurophysiol 124: 312\u2013329, 2020. first\npublished ju", "learning recurrent neural networks with hessian-free optimization\n\njames martens\nilya sutskever\nuniversity of toronto, canada\n\njmartens@cs.toronto.edu\nilya@cs.utoronto.ca\n\nabstract\n\nin this work we resolve the long-outstanding\nproblem of how to effectively train recurrent neu-\nral networks (rnns) on complex and dif\ufb01cult\nsequence modeling problems which may con-\ntain long-term data dependencies. utilizing re-\ncent advances in the hessian-free optimization\napproach (martens, 2010), together with a novel\ndamping scheme, we successfully train rnns on\ntwo sets of challenging problems. first, a col-\nlection of pathological synthetic datasets which\nare known to be impossible for standard op-\ntimization approaches (due to their extremely\nlong-term dependencies), and second, on three\nnatural and highly complex real-world sequence\ndatasets where we \ufb01nd that our method sig-\nni\ufb01cantly outperforms the previous state-of-the-\nart method for training neural sequence mod-\nels:\nthe long short-term memor", "neuron, volume 81 \n\nsupplemental information \n\nlearning by the dendritic \n\nprediction of somatic spiking \n\nrobert urbanczik and walter senn \n\n\f", "surrogate gradient learning  \nin spiking neural networks\nbringing the power of gradient-based optimization  \nto spiking neural networks\n\ns piking neural networks (snns) are nature\u2019s versatile solu-\n\ntion to fault-tolerant, energy-efficient signal processing. to \ntranslate these benefits into hardware, a growing number of \nneuromorphic spiking nn processors have attempted to emulate \nbiological nns. these developments have created an imminent \nneed  for  methods  and  tools  that  enable  such  systems  to  solve \nreal-world signal processing problems. like conventional nns, \nsnns  can  be  trained  on  real,  domain-specific  data;  however, \ntheir training requires the overcoming of a number of challenges \nlinked  to  their  binary  and  dynamical  nature.  this  article  elu-\ncidates  step-by-step  the  problems  typically  encountered  when \ntraining snns and guides the reader through the key concepts \nof synaptic plasticity and data-driven learning in the spiking set-\nting. accordi", "hebbian  synapses \n\nin  visual  cortex \n\nthe  journal \n\nof  neuroscience, \n\nmarch \n\n1994, \n\n14(3): \n\n1634-l \n\n645 \n\nalfred0  kirkwood \nbrown  university  department \nrhode \n\nisland  02912 \n\nand  mark  f.  bear \n\nof  neuroscience \n\nand  institute \n\nfor  brain  and  neural  systems,  brown  university,  providence, \n\n(ltp) \nburst \nby  theta \nof  the  cortical \niv.  this  synaptic \nfield  potentials \n\nin  slices  of  rat  visual  cortex \n\nthat  reliable \n\nof  synaptic \n\nresponses \n\nin \n\nlayer \n\nlong- \nill \nto  a \n\nstimulation \nthickness, \nplasticity \nand \n\nwas \nintracellular \n\ndelivered \ncorresponding \nreflected \nepsps \nresponses \n\nin \nin \n\nill,  but  was  not  observed \nsuggesting \nill  neurons. \n\nin  the  intracellular \na  preferential \ntetanus-induced \n\ninput \n\nspecific, \n\nand  was  blocked \n\nof  an  nmda \n\nreceptor \n\nantagonist \n\nof  nitric  oxide \n\nresponses \n\nin  addition, \n\nalso  be  produced \n\nsynthase). \ncould \nsynaptic \n\nintracellular \nin  this  circuit \n\nstimulation \ndepolariza", "j neurophysiol 98: 2038 \u20132057, 2007.\nfirst published july 25, 2007; doi:10.1152/jn.01311.2006.\n\nmodel of birdsong learning based on gradient estimation by dynamic\nperturbation of neural conductances\n\nila r. fiete,1,2 michale s. fee,3,5 and h. sebastian seung4,5\n1kavli institute for theoretical physics, university of california, santa barbara, santa barbara; 2center for theoretical biological\nphysics, university of california, san diego, la jolla, california; and 3mcgovern institute for brain research, 4howard hughes medical\ninstitute, and 5brain and cognitive sciences department, massachusetts institute of technology, cambridge, massachusetts\n\nsubmitted 14 december 2006; accepted in \ufb01nal form 13 july 2007\n\nfiete ir, fee ms, seung hs. model of birdsong learning based on\ngradient estimation by dynamic perturbation of neural conductances.\nj neurophysiol 98: 2038 \u20132057, 2007. first published july 25, 2007;\ndoi:10.1152/jn.01311.2006. we propose a model of songbird learning\nthat focuses on a", "the journal of neuroscience, may 27, 2015 \u2022 35(21):8145\u2013 8157 \u2022 8145\n\nbehavioral/cognitive\n\nreinforcement learning in multidimensional environments\nrelies on attention mechanisms\n\nx yael niv,1 x reka daniel,1 andra geana,1 samuel j. gershman,2 yuan chang leong,3 angela radulescu,1\nand robert c. wilson4\n1department of psychology and neuroscience institute, princeton university, princeton, new jersey 08540, 2department of brain and cognitive sciences,\nmassachusetts institute of technology, cambridge, massachusetts 02139, 3department of psychology, stanford university, stanford, california 94305, and\n4department of psychology and cognitive science program, university of arizona, tucson, arizona 85721\n\nin recent years, ideas from the computational field of reinforcement learning have revolutionized the study of learning in the brain,\nfamously providing new, precise theories of how dopamine affects learning in the basal ganglia. however, reinforcement learning\nalgorithms are notorious for n", "0\n2\n0\n2\n\n \nr\na\n\n \n\nm\n7\n2\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n3\n9\n8\n2\n0\n\n.\n\n7\n0\n9\n1\n:\nv\ni\nx\nr\na\n\ninvariant risk minimization\n\nmartin arjovsky, l\u00b4eon bottou, ishaan gulrajani, david lopez-paz\n\n1\n\nintroduction\n\nmachine learning su\ufb00ers from a fundamental problem. while machines are able to\nlearn complex prediction rules by minimizing their training error, data are often\nmarred by selection biases, confounding factors, and other peculiarities [49, 48, 23].\nas such, machines justi\ufb01ably inherit these data biases. this limitation plays an\nessential role in the situations where machine learning fails to ful\ufb01ll the promises of\narti\ufb01cial intelligence. more speci\ufb01cally, minimizing training error leads machines into\nrecklessly absorbing all the correlations found in training data. understanding which\npatterns are useful has been previously studied as a correlation-versus-causation\ndilemma, since spurious correlations stemming from data biases are unrelated to the\ncausal explanation of interest [31, 27", "research\n\nneuroscience\n\nlearning and attention reveal a\ngeneral relationship between\npopulation activity and behavior\n\na. m. ni, d. a. ruff, j. j. alberts, j. symmonds, m. r. cohen*\n\nprior studies have demonstrated that correlated variability changes with cognitive processes\nthat improve perceptual performance. we tested whether correlated variability covaries\nwith subjects\u2019 performance\u2014whether performance improves quickly with attention or slowly\nwith perceptual learning. we found a single, consistent relationship between correlated\nvariability and behavioral performance, regardless of the time frame of correlated variability\nchange. this correlated variability was oriented along the dimensions in population space\nused by the animal on a trial-by-trial basis to make decisions. that subjects\u2019 choices\nwere predicted by specific dimensions that were aligned with the correlated variability\naxis clarifies long-standing paradoxes about the relationship between shared variability\nand behavio", "unified rational protein engineering with \nsequence-based deep representation learning\n\nethan c. alley1,2,6, grigory khimulya6,7, surojit biswas1,3,6, mohammed alquraishi\u200a\ngeorge m. church\u200a\n\n\u200a1,5*\n\n\u200a4 and \n\nrational protein engineering requires a holistic understanding of protein function. here, we apply deep learning to unlabeled \namino-acid sequences to distill the fundamental features of a protein into a statistical representation that is semantically rich \nand structurally, evolutionarily and biophysically grounded. we show that the simplest models built on top of this unified rep-\nresentation (unirep) are broadly applicable and generalize to unseen regions of sequence space. our data-driven approach \npredicts the stability of natural and de\u00a0novo designed proteins, and the quantitative function of molecularly diverse mutants, \ncompetitively with the state-of-the-art methods. unirep further enables two orders of magnitude efficiency improvement in \na protein engineering task. unirep", "prevalence of neural collapse during the terminal\nphase of deep learning training\n\nvardan papyana,1\n\n, x. y. hanb,1\n\n, and david l. donohoa,2\n\nadepartment of statistics, stanford university, stanford, ca 94305-4065; and bschool of operations research and information engineering, cornell\nuniversity, ithaca, ny 14850\n\ncontributed by david l. donoho, august 18, 2020 (sent for review july 22, 2020; reviewed by helmut boelsckei and st \u00b4ephane mallat)\n\nmodern practice for training classi\ufb01cation deepnets involves a ter-\nminal phase of training (tpt), which begins at the epoch where\ntraining error \ufb01rst vanishes. during tpt, the training error stays\neffectively zero, while training loss is pushed toward zero. direct\nmeasurements of tpt, for three prototypical deepnet architec-\ntures and across seven canonical classi\ufb01cation datasets, expose\na pervasive inductive bias we call neural collapse (nc), involv-\ning four deeply interconnected phenomena. (nc1) cross-example\nwithin-class variability of la", "3\n2\n0\n2\n\n \nl\nu\nj\n \n\n7\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n1\n9\n6\n8\n0\n\n.\n\n7\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nfaster attention with better parallelism and work partitioning\n\nflashattention-2:\n\ntri dao1,2\n\n1department of computer science, princeton university\n2department of computer science, stanford university\n\ntrid@cs.stanford.edu\n\njuly 18, 2023\n\nabstract\n\nscaling transformers to longer sequence lengths has been a major problem in the last several years,\npromising to improve performance in language modeling and high-resolution image understanding, as\nwell as to unlock new applications in code, audio, and video generation. the attention layer is the\nmain bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in\nthe sequence length. flashattention [5] exploits the asymmetric gpu memory hierarchy to bring\nsignificant memory saving (linear instead of quadratic) and runtime speedup (2-4\u00d7 compared to optimized\nbaselines), with no approximation. however, flashattention is stil", "neurons learn by predicting future activity\n\nartur luczak\u200a\n\n\u200a1\u2009\u2709, bruce l. mcnaughton1,2 and yoshimasa kubo1\n\nunderstanding how the brain learns may lead to machines with human-like intellectual capacities. it was previously proposed \nthat the brain may operate on the principle of predictive coding. however, it is still not well understood how a predictive system \ncould be implemented in the brain. here we demonstrate that the ability of a single neuron to predict its future activity may \nprovide an effective learning mechanism. interestingly, this predictive learning rule can be derived from a metabolic principle, \nwhereby neurons need to minimize their own synaptic activity (cost) while maximizing their impact on local blood supply by \nrecruiting other neurons. we show how this mathematically derived learning rule can provide a theoretical connection between \ndiverse types of brain-inspired algorithm, thus offering a step towards the development of a general theory of neuronal learn-", "direct voxel grid optimization:\n\nsuper-fast convergence for radiance fields reconstruction\n\ncheng sun*,\u2020\n\nmin sun*,\u2021\n\nchengsun@gapp.nthu.edu.tw\n\nsunmin@ee.nthu.edu.tw\n\nhwann-tzong chen*,\u00a7\nhtchen@cs.nthu.edu.tw\n\n2\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n3\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n5\n1\n2\n1\n1\n\n.\n\n1\n1\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe present a super-fast convergence approach to recon-\nstructing the per-scene radiance \ufb01eld from a set of images\nthat capture the scene with known poses. this task, which is\noften applied to novel view synthesis, is recently revolution-\nized by neural radiance field (nerf) for its state-of-the-art\nquality and \ufb02exibility. however, nerf and its variants re-\nquire a lengthy training time ranging from hours to days for\na single scene. in contrast, our approach achieves nerf-\ncomparable quality and converges rapidly from scratch in\nless than 15 minutes with a single gpu. we adopt a represen-\ntation consisting of a density voxel grid for scene geometry\nand a feature voxel grid with a shal", "the double- edged sword of memory \nretrieval\n\n 1\u2009\u2709 and magdalena\u00a0abel2\n\nhenry\u00a0l.\u00a0roediger\u00a0iii \nabstract | accurately retrieving information from memory boosts later retrieval. however, \nretrieving memories can also open a window to errors when erroneous information is retrieved  \nor when new information is encoded during retrieval. similarly, the process of retrieval can \ninfluence recall of related information, either inhibiting or facilitating it depending upon the \nsituation. in addition, retrieving or attempting to retrieve information can facilitate encoding  \nof new information, regardless of whether the new information is correct or incorrect. in this \nreview, we provide selective coverage of the influences of memory retrieval in three distinct \narenas: effects on the retrieved information itself, effects on retrieval of related information, and \neffects on information encoded just after an event is retrieved. consideration of both positive \nand negative effects of retrieval in ", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nbrain  structure  and  dynamics  across  scales:  in  search\nof  rules\nxiao-jing  wang1,2 and  henry  kennedy3,4\n\nlouis  henry  sullivan,  the  father  of  skyscrapers,  famously  stated\n\u2018form  ever  follows  function\u2019.  in  this  short  review,  we  will  focus\non  the  relationship  between  form  (structure)  and  function\n(dynamics)  in  the  brain.  we  summarize  recent  advances  on  the\nquanti\ufb01cation  of  directed-  and  weighted-mesoscopic\nconnectivity  of  mammalian  cortex,  the  exponential  distance\nrule  for  mesoscopic  and  microscopic  circuit  wiring,  a  spatially\nembedded  random  model  of  inter-areal  cortical  networks,  and\na  large-scale  dynamical  circuit  model  of  money\u2019s  cortex  that\ngives  rise  to  a  hierarchy  of  timescales.  these  \ufb01ndings\ndemonstrate  that  inter-areal  cortical  networks  are  dense\n(hence  such  concepts  as  \u2018small-world\u2019  need  to  be  re\ufb01ned  when\napplied  to  the ", "0\n2\n0\n2\n\n \nl\nu\nj\n \n\n3\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n9\n0\n5\n2\n0\n\n.\n\n0\n1\n9\n1\n:\nv\ni\nx\nr\na\n\nremind your neural network to prevent\n\ncatastrophic forgetting\n\ntyler l. hayes1,(cid:63), kushal ka\ufb02e2,(cid:63), robik shrestha1,(cid:63),\n\nmanoj acharya1, and christopher kanan1,3,4\n\n1 rochester institute of technology, rochester ny 14623, usa\n\n{tlh6792,rss9369,ma7583,kanan}@rit.edu\n2 adobe research, san jose ca 95110, usa\n\nkkafle@adobe.com\n\n3 paige, new york ny 10036, usa\n\n4 cornell tech, new york ny 10044, usa\n\nabstract. people learn throughout life. however, incrementally updat-\ning conventional neural networks leads to catastrophic forgetting. a com-\nmon remedy is replay, which is inspired by how the brain consolidates\nmemory. replay involves \ufb01ne-tuning a network on a mixture of new and\nold instances. while there is neuroscienti\ufb01c evidence that the brain re-\nplays compressed memories, existing methods for convolutional networks\nreplay raw images. here, we propose remind, a brain-inspired approa", "long short-term memory and learning-to-learn in\n\nnetworks of spiking neurons\n\nguillaume bellec*, darjan salaj*, anand subramoney*, robert legenstein & wolfgang maass\n\ninstitute for theoretical computer science\n\n{bellec,salaj,subramoney,legenstein,maass}@igi.tugraz.at\n\ngraz university of technology, austria\n\n* equal contributions\n\nabstract\n\nrecurrent networks of spiking neurons (rsnns) underlie the astounding comput-\ning and learning capabilities of the brain. but computing and learning capabilities\nof rsnn models have remained poor, at least in comparison with arti\ufb01cial neural\nnetworks (anns). we address two possible reasons for that. one is that rsnns\nin the brain are not randomly connected or designed according to simple rules,\nand they do not start learning as a tabula rasa network. rather, rsnns in the\nbrain were optimized for their tasks through evolution, development, and prior\nexperience. details of these optimization processes are largely unknown. but\ntheir functional contribut", "reverse engineering learned optimizers\nreveals known and novel mechanisms\n\nniru maheswaranathan\u2217\ngoogle research, brain team\n\nniru@hey.com\n\ndavid sussillo\u2217\n\ngoogle research, brain team\n\nluke metz\n\nruoxi sun\n\ngoogle research, brain team\n\ngoogle research, brain team\n\nlmetz@google.com\n\nruoxis@google.com\n\njascha sohl-dickstein\n\ngoogle research, brain team\n\njaschasd@google.com\n\nabstract\n\nlearned optimizers are parametric algorithms that can themselves be trained to\nsolve optimization problems. in contrast to baseline optimizers (such as momentum\nor adam) that use simple update rules derived from theoretical principles, learned\noptimizers use \ufb02exible, high-dimensional, nonlinear parameterizations. although\nthis can lead to better performance, their inner workings remain a mystery. how\nis a given learned optimizer able to outperform a well tuned baseline? has it\nlearned a sophisticated combination of existing optimization techniques, or is it\nimplementing completely new behavior? in this work", "2\n2\n0\n2\n\n \nt\nc\no\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n2\n0\n5\n2\n0\n\n.\n\n0\n1\n0\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2021\n\ndenoising diffusion implicit models\n\njiaming song, chenlin meng & stefano ermon\nstanford university\n{tsong,chenlin,ermon}@cs.stanford.edu\n\nabstract\n\ndenoising diffusion probabilistic models (ddpms) have achieved high qual-\nity image generation without adversarial training, yet they require simulating a\nmarkov chain for many steps in order to produce a sample. to accelerate sam-\npling, we present denoising diffusion implicit models (ddims), a more ef\ufb01cient\nclass of iterative implicit probabilistic models with the same training procedure as\nddpms. in ddpms, the generative process is de\ufb01ned as the reverse of a particular\nmarkovian diffusion process. we generalize ddpms via a class of non-markovian\ndiffusion processes that lead to the same training objective. these non-markovian\nprocesses can correspond to generative processes that are deterministic, giving ri", "letter\n\ncommunicated by rajesh rao\n\noptimal spike-timing-dependent plasticity for precise action\npotential firing in supervised learning\n\njean-pascal p\ufb01ster\njean-pascal.p\ufb01ster@ep\ufb02.ch\ntaro toyoizumi\ntaro@sat.t.u-tokyo.ac.jp\ndavid barber\ndavid.barber@idiap.ch\nwulfram gerstner\nwulfram.gerstner@ep\ufb02.ch\nlaboratory of computational neuroscience, school of computer and communication\nsciences and brain-mind institute, ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n(epfl), ch-1015 lausanne, switzerland\n\nin timing-based neural codes, neurons have to emit action potentials\nat precise moments in time. we use a supervised learning paradigm to\nderive a synaptic update rule that optimizes by gradient ascent the like-\nlihood of postsynaptic \ufb01ring at one or several desired \ufb01ring times. we\n\ufb01nd that the optimal strategy of up- and downregulating synaptic ef\ufb01-\ncacies depends on the relative timing between presynaptic spike arrival\nand desired postsynaptic \ufb01ring. if the presynaptic spike arrives before\nthe desi", "a mean \ufb01eld view of the landscape of two-layer\nneural networks\n\nsong meia, andrea montanarib,c,1, and phan-minh nguyenb\n\nainstitute for computational and mathematical engineering, stanford university, stanford, ca 94305; bdepartment of electrical engineering, stanford\nuniversity, stanford, ca 94305; and cdepartment of statistics, stanford university, stanford, ca 94305\n\nedited by peter j. bickel, university of california, berkeley, ca, and approved june 21, 2018 (received for review april 16, 2018)\n\nmultilayer neural networks are among the most powerful models\nin machine learning, yet the fundamental reasons for this suc-\ncess defy mathematical understanding. learning a neural network\nrequires optimizing a nonconvex high-dimensional objective (risk\nfunction), a problem that is usually attacked using stochastic gra-\ndient descent (sgd). does sgd converge to a global optimum of\nthe risk or only to a local optimum? in the former case, does this\nhappen because local minima are absent or be", "e\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\ne\nr\nu\nt\na\nn\nm\no\nc\n.\ne\nr\nu\nt\na\nn\nw\nw\nw\n\n/\n\n.\n\n/\n/\n:\np\nt\nt\nh\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n2\n0\n0\n2\n\u00a9\n\n \n\narticles\n\noptimal feedback control as a\ntheory of motor coordination\n\nemanuel todorov1 and michael i. jordan2\n\n1 department of cognitive science, university of california, san diego, 9500 gilman dr., la jolla, california 92093-0515, usa\n2 division of computer science and department of statistics, university of california, berkeley, 731 soda hall #1776, berkeley, california \n\n94720-1776, usa\n\ncorrespondence should be addressed to e.t. (todorov@cogsci.ucsd.edu)\n\npublished online 28 october 2002; doi:10.1038/nn963\n\na central problem in motor control is understanding how the many biomechanical degrees of\nfreedom are coordinated to achieve a common goal. an especially puzzling aspect of coordination is\nthat behavioral goals are achieved reliably and repeatedly with movements rarely reproducible in\ntheir detail. existing theoretical framewor", "vision research 41 (2001) 1409\u20131422\n\nwww.elsevier.com/locate/visres\n\nthe lateral occipital complex and its role in object recognition\n\nkalanit grill-spector a,*, zoe kourtzi a, nancy kanwisher a,b\n\na department of brain and cogniti6e sciences, massachusetts institute of technology, cambridge, ma 02139, usa\n\nb massachusetts general hospital, nmr center, charlestown, ma 02129, usa\n\nreceived 14 february 2001\n\nabstract\n\nhere we review recent \ufb01ndings that reveal the functional properties of extra-striate regions in the human visual cortex that are\ninvolved in the representation and perception of objects. we characterize both the invariant and non-invariant properties of these\nregions and we discuss the correlation between activation of these regions and recognition. overall, these results indicate that the\nlateral occipital complex plays an important role in human object recognition. \u00a9 2001 published by elsevier science ltd.\n\nkeywords: object recognition; functional magnetic resonance imagi", "physical review e 69, 066138 (2004)\n\nestimating mutual information\n\nalexander kraskov, harald st\u00f6gbauer, and peter grassberger\n\njohn-von-neumann institute for computing, forschungszentrum j\u00fclich, d-52425 j\u00fclich, germany\n\n(received 28 may 2003; published 23 june 2004)\n\nwe present two classes of improved estimators for mutual information msx,yd, from samples of random\npoints distributed according to some joint probability density msx, yd. in contrast to conventional estimators\nbased on binnings, they are based on entropy estimates from k-nearest neighbor distances. this means that they\nare data ef\ufb01cient (with k=1 we resolve structures down to the smallest possible scales), adaptive (the resolution\nis higher where data are more numerous), and have minimal bias. indeed, the bias of the underlying entropy\nestimates is mainly due to nonuniformity of the density at the smallest resolved scale, giving typically sys-\ntematic errors which scale as functions of k/n for n points. numerically, we \ufb01", "proc. natl. acad. sci. usa\nvol. 96, pp. 1083\u20131087, february 1999\nneurobiology\n\nthe role of presynaptic activity in monocular deprivation:\ncomparison of homosynaptic and heterosynaptic mechanisms\n\nbrian s. blais*, harel z. shouval, and leon n. cooper\ndepartments of physics and neuroscience and institute for brain and neural systems, brown university, providence ri 02912\n\ncontributed by leon n cooper, november 3, 1998\n\nabstract\nalthough investigations in computational\nneuroscience have been extensive, the opportunity (that has\nmade such a marked difference in physical sciences) to test\ndetailed and subtle quantitative consequences of a theory\nagainst experimental results is rare. in this paper, we outline\na testable consequence of two contrasting theories of synaptic\nplasticity applied to the disconnection in visual cortex of the\nclosed eye in monocular deprivation. this disconnection is\nsometimes thought to be the consequence of a process that\nstems from a competition of inputs for a li", "the journal of neuroscience, december 1, 2000, 20(23):8812\u20138821\n\nstable hebbian learning from spike timing-dependent plasticity\n\nm. c. w. van rossum,1 g. q. bi,2 and g. g. turrigiano1\n1brandeis university, department of biology, waltham, massachusetts 02454-9110, and 2university of california at san\ndiego, department of biology, la jolla, california 92093-0357\n\nwe explore a synaptic plasticity model that incorporates recent\n\ufb01ndings that potentiation and depression can be induced by\nprecisely timed pairs of synaptic events and postsynaptic spikes.\nin addition we include the observation that strong synapses\nundergo relatively less potentiation than weak synapses,\nwhereas depression is independent of synaptic strength. after\nrandom stimulation, the synaptic weights reach an equilibrium\ndistribution which is stable, unimodal, and has positive skew.\nthis weight distribution compares favorably to the distributions\nof quantal amplitudes and of receptor number observed exper-\nimentally in cent", "8\n1\n0\n2\n\n \nt\nc\no\n0\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n6\nv\n8\n3\n4\n2\n0\n\n.\n\n6\n0\n5\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2016\n\nhigh-dimensional continuous control using\ngeneralized advantage estimation\n\njohn schulman, philipp moritz, sergey levine, michael i. jordan and pieter abbeel\ndepartment of electrical engineering and computer science\nuniversity of california, berkeley\n{joschu,pcmoritz,levine,jordan,pabbeel}@eecs.berkeley.edu\n\nabstract\n\npolicy gradient methods are an appealing approach in reinforcement learning be-\ncause they directly optimize the cumulative reward and can straightforwardly be\nused with nonlinear function approximators such as neural networks. the two\nmain challenges are the large number of samples typically required, and the dif\ufb01-\nculty of obtaining stable and steady improvement despite the nonstationarity of the\nincoming data. we address the \ufb01rst challenge by using value functions to substan-\ntially reduce the variance of policy gradient estimates at the", "9\n1\n0\n2\n\n \nr\na\n\nm\n3\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n4\n4\n1\n6\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nkernel machines that adapt to gpus for\n\neffective large batch training\n\nsiyuan ma 1 mikhail belkin 1\n\nabstract\n\nmodern machine learning models are typically trained using stochastic gradient descent (sgd) on massively\nparallel computing resources such as gpus. increasing mini-batch size is a simple and direct way to utilize the\nparallel computing capacity. for small batch an increase in batch size results in the proportional reduction in the\ntraining time, a phenomenon known as linear scaling. however, increasing batch size beyond a certain value\nleads to no further improvement in training time. in this paper we develop the \ufb01rst analytical framework that\nextends linear scaling to match the parallel computing capacity of a resource. the framework is designed for a\nclass of classical kernel machines. it automatically modi\ufb01es a standard kernel machine to output a mathemati-\ncally equivalent predicti", "cerebral cortex, april 2018;28: 1396\u20131415\n\ndoi: 10.1093/cercor/bhx339\nadvance access publication date: 28 december 2017\noriginal article\n\no r i g i n a l a r t i c l e\nmulticontact co-operativity in spike-timing\u2013\ndependent structural plasticity stabilizes networks\nmoritz deger1,2, alexander seeholzer1 and wulfram gerstner1\n\n1school of computer and communication sciences and school of life sciences, brain mind institute, \u00e9cole\npolytechnique f\u00e9d\u00e9rale de lausanne, 1015 lausanne epfl, switzerland and 2institute for zoology, faculty of\nmathematics and natural sciences, university of cologne, 50674 cologne, germany\n\naddress correspondence to postal address: laboratory of computational neuroscience, lcn epfl \u2013 batiment aab (station 15), ch-1015 lausanne epfl,\nswitzerland. email: wulfram.gerstner@ep\ufb02.ch\n\nabstract\nexcitatory synaptic connections in the adult neocortex consist of multiple synaptic contacts, almost exclusively formed on\ndendritic spines. changes of spine volume, a correlate of sy", "pilco: a model-based and data-e\ufb03cient approach to policy search\n\nmarc peter deisenroth\ndepartment of computer science & engineering, university of washington, usa\n\nmarc@cs.washington.edu\n\ncarl edward rasmussen\ndepartment of engineering, university of cambridge, uk\n\ncer54@cam.ac.uk\n\nabstract\n\nin this paper, we introduce pilco, a practi-\ncal, data-e\ufb03cient model-based policy search\nmethod. pilco reduces model bias, one of\nthe key problems of model-based reinforce-\nment learning, in a principled way. by learn-\ning a probabilistic dynamics model and ex-\nplicitly incorporating model uncertainty into\nlong-term planning, pilco can cope with\nvery little data and facilitates learning from\nscratch in only a few trials. policy evaluation\nis performed in closed form using state-of-\nthe-art approximate inference. furthermore,\npolicy gradients are computed analytically\nfor policy improvement. we report unprece-\ndented learning e\ufb03ciency on challenging and\nhigh-dimensional control tasks.\n\n1. introducti", "a uni\ufb01ed architecture for natural language processing:\n\ndeep neural networks with multitask learning\n\nronan collobert\njason weston\nnec labs america, 4 independence way, princeton, nj 08540 usa\n\ncollober@nec-labs.com\njasonw@nec-labs.com\n\nabstract\n\nwe describe a single convolutional neural net-\nwork architecture that, given a sentence, out-\nputs a host of language processing predic-\ntions: part-of-speech tags, chunks, named en-\ntity tags, semantic roles, semantically similar\nwords and the likelihood that the sentence\nmakes sense (grammatically and semanti-\ncally) using a language model. the entire\nnetwork is trained jointly on all these tasks\nusing weight-sharing, an instance of multitask\nlearning. all the tasks use labeled data ex-\ncept the language model which is learnt from\nunlabeled text and represents a novel form of\nsemi-supervised learning for the shared tasks.\nwe show how both multitask learning and\nsemi-supervised learning improve the general-\nization of the shared tasks, result", "nonlinear ica using auxiliary variables\n\nand generalized contrastive learning\n\naapo hyv\u00a8arinen 1,2\n\nhiroaki sasaki 3,1\n\nrichard e. turner 4\n\n1 the gatsby unit\n\nucl, uk\n\n2 dept. of cs and hiit\nuniv. helsinki, finland\n\n3div. of info. sci.\n\nnaist, japan\n\n4 univ. cambridge &\nmicrosoft research, uk\n\nabstract\n\nnonlinear ica is a fundamental problem\nfor unsupervised representation learning, em-\nphasizing the capacity to recover the underly-\ning latent variables generating the data (i.e.,\nidenti\ufb01ability). recently, the very \ufb01rst iden-\nti\ufb01ability proofs for nonlinear ica have been\nproposed, leveraging the temporal structure\nof the independent components. here, we\npropose a general framework for nonlinear\nica, which, as a special case, can make use of\ntemporal structure. it is based on augment-\ning the data by an auxiliary variable, such\nas the time index, the history of the time se-\nries, or any other available information. we\npropose to learn nonlinear ica by discrimi-\nnating between true augm", "2\n2\n0\n2\n\n \nl\nu\nj\n \n\n1\n2\n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n6\nv\n0\n8\n0\n2\n0\n\n.\n\n1\n1\n1\n2\n:\nv\ni\nx\nr\na\n\nan explanation of in-context learning as implicit\n\nbayesian inference\n\nsang michael xie\nstanford university\n\nxie@cs.stanford.edu\n\npercy liang\n\nstanford university\n\naditi raghunathan\nstanford university\n\naditir@stanford.edu\n\ntengyu ma\n\nstanford university\n\npliang@cs.stanford.edu\n\ntengyuma@cs.stanford.edu\n\nabstract\n\nlarge language models (lms) such as gpt-3 have the surprising ability to do in-context learning, where\nthe model learns to do a downstream task simply by conditioning on a prompt consisting of input-output\nexamples. the lm learns from these examples without being explicitly pretrained to learn. thus, it is unclear what\nenables in-context learning. in this paper, we study how in-context learning can emerge when pretraining\ndocuments have long-range coherence. here, the lm must infer a latent document-level concept to generate\ncoherent next tokens during pretraining. at test time, in-context ", "beyond accuracy: generalization properties of\nbio-plausible temporal credit assignment rules\n\nyuhan helena liu1,2,3,*, arna ghosh4,5, blake a. richards4,5,6,7, eric shea-brown1,2,3, and\n\nguillaume lajoie5,7,8,*\n\n1department of applied mathematics, university of washington, seattle, wa, usa\n\n2allen institute for brain science, 615 westlake ave n, seattle wa, usa\n\n3computational neuroscience center, university of washington, seattle, wa, usa\n\n4school of computer science, mcgill university, montreal, qc, canada\n\n5mila - quebec ai institute, montreal, qc, canada\n\n6department of neurology and neurosurgery, montreal neurological institute, mcgill university,\n\n7canada cifar ai chair, cifar, toronto, on, canada\n\n8dept. de math\u00e9matiques et statistiques, universit\u00e9 de montr\u00e9al, montreal, qc, canada\n\n*correspondence: hyliu24@uw.edu, g.lajoie@umontreal.ca\n\nmontreal, qc, canada\n\nabstract\n\nto unveil how the brain learns, ongoing work seeks biologically-plausible ap-\nproximations of gradient descent ", "letters to nature\n\nwith empty vector were used as controls. we isolated agp fractions from the culture\nmedium of transgenic by-2 cells by using the speci\ufb01c interaction of agps with b glcy3 and\nadded them into zinnia microbead cultures to monitor the activity of xylogen.\n\nanalysis of xylogen\nxylogen was puri\ufb01ed from the agp fraction, which was obtained from the conditioned\nmedium of the zinnia xylogenic culture by using the speci\ufb01c interaction of agps with\nb glcy3. the agp fraction was boiled for 10 min and centrifuged at 10,000 g for 20 min.\nthe supernatant was applied onto concanavalin-a\u2013sepharose column (amersham\npharmacia biotech) equilibrated with 10 mm kh2po4-koh (ph 7.2) containing\n150 mm nacl. the column was washed with the same buffer, and xylogen was eluted with\nthe same buffer containing 0.2 m methyl-a -d-glucoside. puri\ufb01ed xylogen was\ndeglycosylated by tri\ufb02uoromethanesulphonic acid24. the resulting 16k polypeptide was\nsequenced by a g1005a protein sequencing system (hewlett ", "published as a conference paper at iclr 2021\n\nscore-based generative modeling through\nstochastic differential equations\n\nyang song\u02da\nstanford university\nyangsong@cs.stanford.edu\n\njascha sohl-dickstein\ngoogle brain\njaschasd@google.com\n\ndiederik p. kingma\ngoogle brain\ndurk@google.com\n\nabhishek kumar\ngoogle brain\nabhishk@google.com\n\nstefano ermon\nstanford university\nermon@cs.stanford.edu\n\nben poole\ngoogle brain\npooleb@google.com\n\nabstract\n\ncreating noise from data is easy; creating data from noise is generative modeling.\nwe present a stochastic differential equation (sde) that smoothly transforms a com-\nplex data distribution to a known prior distribution by slowly injecting noise, and a\ncorresponding reverse-time sde that transforms the prior distribution back into the\ndata distribution by slowly removing the noise. crucially, the reverse-time sde\ndepends only on the time-dependent gradient \ufb01eld (a.k.a., score) of the perturbed\ndata distribution. by leveraging advances in score-based gene", "9\n1\n0\n2\n\n \nr\na\n\nm\n4\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n5\n3\n6\n3\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2019\n\nthe lottery ticket hypothesis:\nfinding sparse, trainable neural networks\n\njonathan frankle\nmit csail\njfrankle@csail.mit.edu\n\nmichael carbin\nmit csail\nmcarbin@csail.mit.edu\n\nabstract\n\nneural network pruning techniques can reduce the parameter counts of trained net-\nworks by over 90%, decreasing storage requirements and improving computational\nperformance of inference without compromising accuracy. however, contemporary\nexperience is that the sparse architectures produced by pruning are dif\ufb01cult to train\nfrom the start, which would similarly improve training performance.\n\nwe \ufb01nd that a standard pruning technique naturally uncovers subnetworks whose\ninitializations made them capable of training effectively. based on these results, we\narticulate the lottery ticket hypothesis: dense, randomly-initialized, feed-forward\nnetworks contain subnetworks (winning tick", "right for the wrong reasons: diagnosing syntactic heuristics in\n\nnatural language inference\n\nr. thomas mccoy,1 ellie pavlick,2 & tal linzen1\n\n1department of cognitive science, johns hopkins university\n\n2department of computer science, brown university\n\ntom.mccoy@jhu.edu, ellie pavlick@brown.edu, tal.linzen@jhu.edu\n\n9\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n4\n2\n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n4\nv\n7\n0\n0\n1\n0\n\n.\n\n2\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\na machine learning system can score well on\na given test set by relying on heuristics that are\neffective for frequent example types but break\ndown in more challenging cases. we study this\nissue within natural language inference (nli),\nthe task of determining whether one sentence\nentails another. we hypothesize that statisti-\ncal nli models may adopt three fallible syn-\ntactic heuristics: the lexical overlap heuristic,\nthe subsequence heuristic, and the constituent\nheuristic. to determine whether models have\nadopted these heuristics, we introduce a con-\ntrolled evaluation set cal", "optimal errors and phase transitions in\nhigh-dimensional generalized linear models\n\njean barbiera,b,1,2, florent krzakalab,1, nicolas macrisc,1, l \u00b4eo miolaned,1,2, and lenka zdeborov \u00b4ae,1\n\naquantitative life sciences, international center for theoretical physics, 34151 trieste, italy; blaboratoire de physique de l\u2019ecole normale sup \u00b4erieure,\nuniversit \u00b4e paris-sciences-et-lettres, centre national de la recherche scienti\ufb01que, sorbonne universit \u00b4e, universit \u00b4e paris-diderot, sorbonne paris cit \u00b4e, 75005\nparis, france; ccommunication theory laboratory, school of computer and communication sciences, ecole polytechnique f \u00b4ed \u00b4erale de lausanne, ch-1015\nlausanne, switzerland; dd \u00b4epartement d\u2019informatique de l\u2019ecole normale sup \u00b4erieure, universit \u00b4e paris-sciences-et-lettres, centre national de la recherche\nscienti\ufb01que, inria, 75005 paris, france; and einstitut de physique th \u00b4eorique, centre national de la recherche scienti\ufb01que et commissariat `a l\u2019energie\natomique, universit \u00b4e paris", "the impact of traditional neuroimaging methods on\nthe spatial localization of cortical areas\n\ntimothy s. coalsona, david c. van essena,1, and matthew f. glassera,b,1\n\nadepartment of neuroscience, washington university school of medicine, st. louis, mo 63110; and bst. luke\u2019s hospital, st. louis, mo 63017\n\ncontributed by david c. van essen, may 17, 2018 (sent for review january 29, 2018; reviewed by alexander l. cohen, james v. haxby, and martin i. sereno)\n\nin\nlocalizing human brain functions is a long-standing goal\nsystems neuroscience. toward this goal, neuroimaging studies\nhave traditionally used volume-based smoothing, registered data\nto volume-based standard spaces, and reported results relative to\nvolume-based parcellations. a novel 360-area surface-based corti-\ncal parcellation was recently generated using multimodal data\nfrom the human connectome project, and a volume-based version\nof this parcellation has frequently been requested for use with\ntraditional volume-based analyses. ", "maximum likelihood from incomplete data via the em algorithm \nauthor(s): a. p. dempster, n. m. laird and d. b. rubin \nsource: journal of the royal statistical society. series b (methodological), 1977, vol. 39, \nno. 1 (1977), pp. 1-38\n \npublished by: wiley for the royal statistical society \n\nstable url: https://www.jstor.org/stable/2984875\n\njstor is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide \nrange of content in a trusted digital archive. we use information technology and tools to increase productivity and \nfacilitate new forms of scholarship. for more information about jstor, please contact support@jstor.org. \n \nyour use of the jstor archive indicates your acceptance of the terms & conditions of use, available at \nhttps://about.jstor.org/terms\n\nroyal statistical society and wiley are collaborating with jstor to digitize, preserve and \nextend access to journal of the royal statistical society. series b (methodological)\n\nt", "towards causal representation learning\n\nbernhard sch\u00f6lkopf \u2020, francesco locatello \u2020, stefan bauer (cid:63), nan rosemary ke (cid:63), nal kalchbrenner\n\nanirudh goyal, yoshua bengio\n\n1\n\nabstract\u2014the two \ufb01elds of machine learning and graphical\ncausality arose and developed separately. however, there is now\ncross-pollination and increasing interest in both \ufb01elds to bene\ufb01t\nfrom the advances of the other. in the present paper, we review\nfundamental concepts of causal inference and relate them to\ncrucial open problems of machine learning, including transfer\nand generalization, thereby assaying how causality can contribute\nto modern machine learning research. this also applies in the\nopposite direction: we note that most work in causality starts\nfrom the premise that the causal variables are given. a central\nproblem for ai and causality is, thus, causal representation\nlearning, the discovery of high-level causal variables from low-\nlevel observations. finally, we delineate some implications o", "the journal of neuroscience, june 15, 1999, 19(12):5066\u20135073\n\ndifferential neural responses during performance of matching and\nnonmatching to sample tasks at two delay intervals\n\nrebecca elliott1 and raymond j. dolan1,2\n1wellcome department of cognitive neurology, institute of neurology, london wc1n 3bg, united kingdom, and\n2royal free hospital school of medicine, london nw3 2pf, united kingdom\n\nvisual short-term memory in humans and animals is frequently\nassessed using delayed matching to sample (dmts) and de-\nlayed nonmatching to sample (dnmts) tasks across variable\ndelay intervals. although these tasks depend on certain com-\nmon mechanisms, there are behavioral differences between\nthem, and neuroimaging provides a means of assessing explic-\nitly whether this is underpinned by differences at a neural level.\nfindings of delay-dependent de\ufb01cits, after lesions in humans\nand animals, suggest that the neural implementation of these\ntasks may also critically depend on the delay interval. i", "european journal of neuroscience\n\neuropean journal of neuroscience, vol. 35, pp. 1024\u20131035, 2012\n\ndoi:10.1111/j.1460-9568.2011.07980.x\n\nhow much of reinforcement learning is working memory,\nnot reinforcement learning? a behavioral, computational,\nand neurogenetic analysis\n\nanne g. e. collins and michael j. frank\ndepartment of cognitive, linguistic and psychological sciences, brown institute for brain science, brown university, providence,\nri, usa\n\nkeywords: basal ganglia, capacity, human, prefrontal cortex, striatum\n\nabstract\n\ninstrumental\nlearning involves corticostriatal circuitry and the dopaminergic system. this system is typically modeled in the\nreinforcement learning (rl) framework by incrementally accumulating reward values of states and actions. however, human\nlearning also implicates prefrontal cortical mechanisms involved in higher level cognitive functions. the interaction of these\nsystems remains poorly understood, and models of human behavior often ignore working memory (w", "a r t i c l e s\n\nmultilaminar networks of cortical neurons integrate \ncommon inputs from sensory thalamus\nnicol\u00e1s a morgenstern, jacques bourg & leopoldo petreanu\nneurons in the thalamorecipient layers of sensory cortices integrate thalamic and recurrent cortical input. cortical neurons \nform fine-scale, functionally cotuned networks, but whether interconnected cortical neurons within a column process common \nthalamocortical inputs is unknown. we tested how local and thalamocortical connectivity relate to each other by analyzing \ncofluctuations of evoked responses in cortical neurons after photostimulation of thalamocortical axons. we found that connected \npairs of pyramidal neurons in layer (l) 4 of mouse visual cortex share more inputs from the dorsal lateral geniculate nucleus than \nnonconnected pairs. vertically aligned connected pairs of l4 and l2/3 neurons were also preferentially contacted by the same \nthalamocortical axons. our results provide a circuit mechanism for the observ", "kernel measures of conditional dependence\n\nkenji fukumizu\n\ninstitute of statistical mathematics\n4-6-7 minami-azabu, minato-ku\n\ntokyo 106-8569 japan\n\narthur gretton\n\nmax-planck institute for biological cybernetics\nspemannstra\u00dfe 38, 72076 t\u00a8ubingen, germany\narthur.gretton@tuebingen.mpg.de\n\nfukumizu@ism.ac.jp\n\nxiaohai sun\n\nmax-planck institute for biological cybernetics\nspemannstra\u00dfe 38, 72076 t\u00a8ubingen, germany\n\nxiaohi@tuebingen.mpg.de\n\nbernhard sch\u00a8olkopf\n\nmax-planck institute for biological cybernetics\nspemannstra\u00dfe 38, 72076 t\u00a8ubingen, germany\n\nbernhard.schoelkopf@tuebingen.mpg.de\n\nabstract\n\nwe propose a new measure of conditional dependence of random variables, based\non normalized cross-covariance operators on reproducing kernel hilbert spaces.\nunlike previous kernel dependence measures, the proposed criterion does not de-\npend on the choice of kernel in the limit of in\ufb01nite data, for a wide class of ker-\nnels. at the same time, it has a straightforward empirical estimate with good\nc", "cerebral cortex, august 2020;30: 4361\u20134380\n\ndoi: 10.1093/cercor/bhaa023\nadvance access publication date: 3 april 2020\noriginal article\n\no r i g i n a l a r t i c l e\na domain-general cognitive core defined in\nmultimodally parcellated human cortex\nmoataz assem 1, matthew f. glasser2,3,4, david c. van essen2 and\njohn duncan1,5\n1mrc cognition and brain sciences unit, university of cambridge, cambridge, cb2 7ef, uk, 2department of\nneuroscience, washington university in st. louis, saint louis, mo 63110, usa, 3department of radiology,\nwashington university in st. louis, saint louis, mo 63110, usa, 4st. luke\u2019s hospital, saint louis, mo 63017,\nusa and 5department of experimental psychology, university of oxford, oxford ox1 3ud, uk\n\naddress correspondence to moataz assem, mrc cognition and brain sciences unit, university of cambridge, 15 chaucer road, cambridge cb2 7ef, uk.\nemail: moataz.assem@mrc-cbu.cam.ac.uk.\n\nabstract\nnumerous brain imaging studies identified a domain-general or \u201cmultiple-d", "stochastic solutions for linear inverse problems\n\nusing the prior implicit in a denoiser\n\nzahra kadkhodaie\n\ncenter for data science,\n\nnew york university\n\nzk388@nyu.edu\n\neero p. simoncelli\n\ncenter for neural science, and\n\ncourant inst. of mathematical sciences,\n\nnew york university\n\nflatiron institute, simons foundation\n\neero.simoncelli@nyu.edu\n\nabstract\n\ndeep neural networks have provided state-of-the-art solutions for problems such\nas image denoising, which implicitly rely on a prior probability model of natural\nimages. two recent lines of work \u2013 denoising score matching and plug-and-play \u2013\npropose methodologies for drawing samples from this implicit prior and using it to\nsolve inverse problems, respectively. here, we develop a parsimonious and robust\ngeneralization of these ideas. we rely on a classic statistical result that shows\nthe least-squares solution for removing additive gaussian noise can be written\ndirectly in terms of the gradient of the log of the noisy signal density. w", "an oscillatory interference model of grid cell firing\n\nneil burgess,1,2* caswell barry,1\u20133 and john o\u2019keefe2,3\n\nhippocampus 17:801\u2013812 (2007)\n\nabstract: we expand upon our proposal that the oscillatory inter-\nference mechanism proposed for the phase precession effect in place\ncells underlies the grid-like \ufb01ring pattern of dorsomedial entorhinal grid\ncells (o\u2019keefe and burgess (2005) hippocampus 15:853\u2013866). the origi-\nnal one-dimensional interference model is generalized to an appropriate\ntwo-dimensional mechanism. speci\ufb01cally, dendritic subunits of layer ii\nmedial entorhinal stellate cells provide multiple linear interference\npatterns along different directions, with their product determining the\n\ufb01ring of the cell. connection of appropriate speed- and direction- de-\npendent inputs onto dendritic subunits could result from an unsuper-\nvised learning rule which maximizes postsynaptic \ufb01ring (e.g. competi-\ntive learning). these inputs cause the intrinsic oscillation of subunit\nmembrane po", "cortical areas interact through a communication\nsubspace\n\narticle\n\nhighlights\nd visual cortical areas interact through a communication\n\nsubspace (cs)\n\nd the cs de\ufb01nes which activity patterns in a source area relate\n\nto downstream activity\n\nd the largest activity patterns in a source area are not matched\n\nto the cs\n\nd the cs allows for selective and \ufb02exible routing of population\n\nsignals between areas\n\nauthors\n\njoa\u02dc o d. semedo, amin zandvakili,\nchristian k. machens, byron m. yu,\nadam kohn\n\ncorrespondence\njsemedo@cmu.edu\n\nin brief\nmost brain functions require the selective\nand \ufb02exible routing of neuronal activity\nbetween cortical areas. using paired\npopulation recordings from multiple\nvisual cortical areas, semedo et al. \ufb01nd a\npopulation-level mechanism that can\nachieve this routing, termed a\ncommunication subspace.\n\nsemedo et al., 2019, neuron 102, 249\u2013259\napril 3, 2019 \u00aa 2019 elsevier inc.\nhttps://doi.org/10.1016/j.neuron.2019.01.026\n\n\f", "manifold clustering\n\nrichard souvenir and robert pless\nwashington university in st. louis\n\ndepartment of computer science and engineering\n\ncampus box 1045, one brookings drive, st. louis, mo 63130\n\n{rms2, pless}@cse.wustl.edu\n\nabstract\n\nmanifold learning has become a vital tool in data driven\nmethods for interpretation of video, motion capture, and\nhandwritten character data when they lie on a low dimen-\nsional, non-linear manifold. this work extends manifold\nlearning to classify and parameterize unlabeled data which\nlie on multiple, intersecting manifolds. this approach sig-\nni\ufb01cantly increases the domain to which manifold learn-\ning methods can be applied, allowing parameterization of\nexample manifolds such as \ufb01gure eights and intersecting\npaths which are quite common in natural data sets. this\napproach introduces several technical contributions which\nmay be of broader interest, including node-weighted multi-\ndimensional scaling and a fast algorithm for weighted low-\nrank approximati", "synaptic neuroscience\ntiming is not everything: neuromodulation opens the  \nstdp gate\n\nreview article\npublished: 25 october 2010\ndoi: 10.3389/fnsyn.2010.00146\n\nverena pawlak1*, jeffery r. wickens 2, alfredo kirkwood 3 and jason n. d. kerr1*\n\n1  network imaging group, max planck institute for biological cybernetics, tuebingen, germany\n2  neurobiology research unit, okinawa institute of science and technology, okinawa, japan\n3  mind/brain institute and department of neurosciences, johns hopkins university, baltimore, md, usa\n\nedited by:\nwulfram gerstner, ecole polytechnique \nf\u00e9d\u00e9rale de lausanne, switzerland\nreviewed by:\nmarkus diesmann, riken brain \nscience institute, japan\nwulfram gerstner, ecole polytechnique \nf\u00e9d\u00e9rale de lausanne, switzerland\nhenning sprekeler , ecole polytechnique \nf\u00e9d\u00e9rale de lausanne, switzerland \nnicolas fremaux, ecole polytechnique \nf\u00e9d\u00e9rale de lausanne, switzerland\nbotond szatm\u00e1ry , brain corporation, \nusa\n*correspondence:\nverena pawlak and jason n. d. kerr, \nn", "review\nreinforcement  learning,  fast  and  slow\n\nmatthew  botvinick,1,2,* sam  ritter,1,3 jane  x.  wang,1 zeb  kurth-nelson,1,2 charles  blundell,1 and\ndemis  hassabis1,2\n\ndeep  reinforcement  learning  (rl)  methods  have  driven  impressive  advances  in\narti\ufb01cial  intelligence  in  recent  years,  exceeding  human  performance  in  domains\nranging  from  atari  to  go  to  no-limit  poker.  this  progress  has  drawn  the  atten-\ntion  of  cognitive  scientists  interested  in  understanding  human  learning.  how-\never,  the  concern  has  been  raised  that  deep  rl  may  be  too  sample-inef\ufb01cient \n\u2013\nthat  is,  it  may  simply  be  too  slow \n  to  provide  a  plausible  model  of  how  humans\nlearn.  in  the  present  review,  we  counter  this  critique  by  describing  recently\ndeveloped  techniques  that  allow  deep  rl  to  operate  more  nimbly,  solving\nproblems  much  more  quickly  than  previous  methods.  although  these  techni-\nques  were  developed  in  an  ai  ", "overcoming catastrophic forgetting in\nneural networks\n\njames kirkpatricka,1, razvan pascanua, neil rabinowitza, joel venessa, guillaume desjardinsa, andrei a. rusua,\nkieran milana, john quana, tiago ramalhoa, agnieszka grabska-barwinskaa, demis hassabisa, claudia clopathb,\ndharshan kumarana, and raia hadsella\n\nadeepmind, london ec4 5tw, united kingdom; and bbioengineering department, imperial college london, london sw7 2az, united kingdom\n\nedited by james l. mcclelland, stanford university, stanford, ca, and approved february 13, 2017 (received for review july 19, 2016)\n\nthe ability to learn tasks in a sequential fashion is crucial to the\ndevelopment of arti\ufb01cial intelligence. until now neural networks\nhave not been capable of this and it has been widely thought that\ncatastrophic forgetting is an inevitable feature of connectionist\nmodels. we show that it is possible to overcome this limitation\nand train networks that can maintain expertise on tasks that they\nhave not experienced for a", "neuron\n\narticle\n\nhippocampal swr activity predicts\ncorrect decisions during the initial learning\nof an alternation task\n\nannabelle c. singer,1,2 margaret f. carr,2,3 mattias p. karlsson,2,4 and loren m. frank2,*\n1mcgovern institute for brain research and mit media lab, mit, cambridge, ma 02139, usa\n2center for integrative neuroscience and department of physiology, university of california, san francisco, ca 94143-0444, usa\n3cnc program, stanford university, stanford, ca 94305, usa\n4janelia farms research campus, hhmi, ashburn, va 20147, usa\n*correspondence: loren@phy.ucsf.edu\nhttp://dx.doi.org/10.1016/j.neuron.2013.01.027\n\nsummary\n\nthe hippocampus frequently replays memories of\npast experiences during sharp-wave ripple (swr)\nevents. these events can represent spatial trajecto-\nries extending from the animal\u2019s current location to\ndistant locations, suggesting a role in the evaluation\nof upcoming choices. while swrs have been linked\nto learning and memory, the speci\ufb01c role of awake\nrepla", "universal value function approximators\n\ntom schaul\ndan horgan\nkarol gregor\ndavid silver\ngoogle deepmind, 5 new street square, ec4a 3tw london\n\nschaul@google.com\nhorgan@google.com\nkarolg@google.com\ndavidsilver@google.com\n\nabstract\n\nvalue functions are a core component of rein-\nforcement learning systems. the main idea is\nto to construct a single function approximator\nv (s; \u03b8) that estimates the long-term reward from\nany state s, using parameters \u03b8.\nin this paper\nwe introduce universal value function approx-\nimators (uvfas) v (s, g; \u03b8) that generalise not\njust over states s but also over goals g. we de-\nvelop an ef\ufb01cient technique for supervised learn-\ning of uvfas, by factoring observed values into\nseparate embedding vectors for state and goal,\nand then learning a mapping from s and g to\nthese factored embedding vectors. we show how\nthis technique may be incorporated into a re-\ninforcement learning algorithm that updates the\nuvfa solely from observed rewards. finally, we\ndemonstrate tha", "innovative methodology\n\nj neurophysiol 93: 1074 \u20131089, 2005.\nfirst published september 8, 2004; doi:10.1152/jn.00697.2004.\n\na point process framework for relating neural spiking activity to spiking\nhistory, neural ensemble, and extrinsic covariate effects\n\nwilson truccolo,1 uri t. eden, 2,3 matthew r. fellows,1 john p. donoghue,1 and emery n. brown2,3\n1neuroscience department, brown university, providence, rhode island; 2neuroscience statistics research laboratory, department of\nanesthesia and critical care, massachusetts general hospital, boston; and 3division of health sciences and technology, harvard\nmedical school/massachusetts institute of technology, cambridge, massachusetts\n\nsubmitted 8 july 2004; accepted in \ufb01nal form 23 august 2004\n\ntruccolo, wilson, uri t. eden, matthew r. fellows, john p.\ndonoghue, and emery n. brown. a point process framework for\nrelating neural spiking activity to spiking history, neural ensemble,\nand extrinsic covariate effects. j neurophysiol 93: 1074 \u20131", "in the format provided by the authors and unedited.\n\nprefrontal cortex as a meta-reinforcement \nlearning system\n\njane x. wang\u200a\njoel z. leibo1, demis hassabis1,4 and matthew botvinick\u200a\n\n\u200a1,5, zeb kurth-nelson1,2,5, dharshan kumaran1,3, dhruva tirumala1, hubert soyer1,  \n\n\u200a1,4*\n\n1deepmind, london, uk. 2max planck-ucl centre for computational psychiatry and ageing research, university college london, london, uk. 3institute \nof cognitive neuroscience, university college london, london, uk. 4gatsby computational neuroscience unit,  university college london, london, uk.  \n5these authors contributed equally: jane x. wang and zeb kurth-nelson. *e-mail: botvinick@google.com\n\nnature neuroscience | www.nature.com/natureneuroscience\n\narticleshttps://doi.org/10.1038/s41593-018-0147-8supplementary information\u00a9 2018 nature america inc., part of springer nature. all rights reserved.\f", "21st international conference on pattern recognition (icpr 2012)\nnovember 11-15, 2012. tsukuba, japan\n\n978-4-9906441-1-6 \u00a92012 iapr\n\n3288\n\nconvolutionalneuralnetworksappliedtohousenumbersdigitclassi\ufb01cationpierresermanet,soumithchintalaandyannlecunthecourantinstituteofmathematicalsciences-newyorkuniversity{sermanet,soumith,yann}@cs.nyu.eduabstractweclassifydigitsofreal-worldhousenumbersus-ingconvolutionalneuralnetworks(convnets).con-vnetsarehierarchicalfeaturelearningneuralnet-workswhosestructureisbiologicallyinspired.un-likemanypopularvisionapproachesthatarehand-designed,convnetscanautomaticallylearnauniquesetoffeaturesoptimizedforagiventask.weaug-mentedthetraditionalconvnetarchitecturebylearningmulti-stagefeaturesandbyusinglppoolingandestab-lishanewstate-of-the-artof95.10%accuracyonthesvhndataset(48%errorimprovement).furthermore,weanalyzethebene\ufb01tsofdifferentpoolingmethodsandmulti-stagefeaturesinconvnets.thesourcecodeandatutorialareavailableateblearn.sf.net.1.introductioncharacterreco", "0\n2\n0\n2\n\n \n\nb\ne\nf\n0\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n1\n6\n5\n3\n0\n\n.\n\n0\n1\n9\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2020\n\ndeep network classification by scattering\nand homotopy dictionary learning\n\njohn zarka, louis thiry, tom\u00e1s angles\nd\u00e9partement d\u2019informatique de l\u2019ens, ens, cnrs, psl university, paris, france\n{john.zarka,louis.thiry,tomas.angles}@ens.fr\n\nst\u00e9phane mallat\ncoll\u00e8ge de france, paris, france\nflatiron institute, new york, usa\n\nabstract\n\nwe introduce a sparse scattering deep convolutional neural network, which pro-\nvides a simple model to analyze properties of deep representation learning for\nclassi\ufb01cation. learning a single dictionary matrix with a classi\ufb01er yields a higher\nclassi\ufb01cation accuracy than alexnet over the imagenet 2012 dataset. the net-\nwork \ufb01rst applies a scattering transform that linearizes variabilities due to ge-\nometric transformations such as translations and small deformations. a sparse\n(cid:96)1 dictionary coding reduces intra-class variab", "888\n\nieee transactions on pattern analysis and machine intelligence, vol. 22, no. 8, august 2000\n\nnormalized cuts and image segmentation\n\njianbo shi and jitendra malik, member, ieee\n\nabstract\u2014we propose a novel approach for solving the perceptual grouping problem in vision. rather than focusing on local features\nand their consistencies in the image data, our approach aims at extracting the global impression of an image. we treat image\nsegmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. the\nnormalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the\ngroups. we show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this\ncriterion. we have applied this approach to segmenting static images, as well as motion sequences, and found the results to be very\nencouraging.\n\nindex terms\u2014groupi", "7\n1\n0\n2\n\n \n\ny\na\nm\n \n1\n3\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n6\n4\n1\n1\n1\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nsuperspike: supervised learning in multi-layer\n\nspiking neural networks\n\nf. zenke\n\ndepartment of applied physics\n\nstanford university\nstanford, ca 94305\n\nfzenke@stanford.edu\n\ns. ganguli\n\ndepartment of applied physics\n\nstanford university\nstanford, ca 94305\n\nsganguli@stanford.edu\n\nabstract\n\na vast majority of computation in the brain is performed by spiking neural networks.\ndespite the ubiquity of such spiking, we currently lack an understanding of how\nbiological spiking neural circuits learn and compute in-vivo, as well as how we can\ninstantiate such capabilities in arti\ufb01cial spiking circuits in-silico. here we revisit\nthe problem of supervised learning in temporally coding multi-layer spiking neural\nnetworks. first, by using a surrogate gradient approach, we derive superspike,\na nonlinear voltage-based three factor learning rule capable of training multi-\nlayer networks of deterministic inte", "anatomical substrates for functional\ncolumns in macaque monkey primary\nvisual cortex\n\njennifer s. lund1, alessandra angelucci1 and paul c. bressloff2\n\n1moran eye center, university of utah, 50 north medical drive,\nsalt lake city, ut 84132, usa and 2department of\nmathematics, university of utah, 155 s. 1400 e. salt lake city,\nut 84112, usa\n\nin this review we re-examine the concept of a cortical column in\nmacaque primary visual cortex, and consider to what extent a\nfunctionally defined column reflects any sort of anatomical entity\nthat subdivides cortical territory. functional studies have shown that\ncolumns relating to different response properties are mapped in\ncortex at different spatial scales. we suggest that these properties\nfirst emerge in mid-layer 4c through a combination of thalamic\nafferent inputs and local\nintracortical circuitry, and are then\ntransferred to other layers in a columnar fashion, via interlaminar\nrelays, where additional processing occurs. however, several\nprope", "8\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n4\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n2\n2\n5\n9\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\na provably correct algorithm for deep learning\n\nthat actually works\n\neran malach and shai shalev-shwartz\n\nschool of computer science, the hebrew university, israel\n\nabstract\n\nwe describe a layer-by-layer algorithm for training deep convolutional networks,\nwhere each step involves gradient updates for a two layer network followed by a\nsimple clustering algorithm. our algorithm stems from a deep generative model\nthat generates images level by level, where lower resolution images correspond to\nlatent semantic classes. we analyze the convergence rate of our algorithm assuming\nthat the data is indeed generated according to this model (as well as additional\nassumptions). while we do not pretend to claim that the assumptions are realistic\nfor natural images, we do believe that they capture some true properties of real data.\nfurthermore, we show that our algorithm actually works in practice (on the cifar\nd", "j comput neurosci (2006) 21:35\u201349\ndoi 10.1007/s10827-006-7074-5\n\npredicting spike timing of neocortical pyramidal neurons\nby simple threshold models\nrenaud jolivet \u00b7 alexander rauch \u00b7\nhans-rudolf l\u00a8uscher \u00b7 wulfram gerstner\n\nreceived: 26 september 2005 / revised: 21 december 2005 / accepted: 11 january 2006 / published online: 22 april 2006\nc(cid:1) springer science + business media, llc 2006\n\nabstract neurons generate spikes reliably with millisec-\nond precision if driven by a \ufb02uctuating current\u2014is it then\npossible to predict the spike timing knowing the input? we\ndetermined parameters of an adapting threshold model using\ndata recorded in vitro from 24 layer 5 pyramidal neurons\nfrom rat somatosensory cortex, stimulated intracellularly\nby a \ufb02uctuating current simulating synaptic bombardment\nin vivo. the model generates output spikes whenever the\nmembrane voltage (a \ufb01ltered version of the input current)\nreaches a dynamic threshold. we \ufb01nd that for input currents\nwith large \ufb02uctuation am", "2\n2\n0\n2\n\n \n\nb\ne\nf\n2\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n1\n3\n1\n0\n0\n\n.\n\n8\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nsimple, fast, and flexible framework for matrix completion\n\nwith in\ufb01nite width neural networks\n\nadityanarayanan radhakrishnan1 george stefanakis1 mikhail belkin 2 caroline uhler1,3\n\nfebruary 23, 2022\n\nabstract\n\nmatrix completion problems arise in many applications including recommendation systems, computer\nvision, and genomics. increasingly larger neural networks have been successful in many of these appli-\ncations, but at considerable computational costs. remarkably, taking the width of a neural network to\nin\ufb01nity allows for improved computational performance. in this work, we develop an in\ufb01nite width neural\nnetwork framework for matrix completion that is simple, fast, and \ufb02exible. simplicity and speed come\nfrom the connection between the in\ufb01nite width limit of neural networks and kernels known as neural\ntangent kernels (ntk). in particular, we derive the ntk for fully connected and convolutional ", "hypothesis and theory\npublished: 14 september 2016\ndoi: 10.3389/fncom.2016.00094\n\ntoward an integration of deep\nlearning and neuroscience\n\nadam h. marblestone 1*, greg wayne 2 and konrad p. kording 3\n\n1 synthetic neurobiology group, massachusetts institute of technology, media lab, cambridge, ma, usa, 2 google\ndeepmind, london, uk, 3 rehabilitation institute of chicago, northwestern university, chicago, il, usa\n\nneuroscience has focused on the detailed implementation of computation, studying\nneural codes, dynamics and circuits. in machine learning, however, arti\ufb01cial neural\nnetworks tend to eschew precisely designed codes, dynamics or circuits in favor of\nbrute force optimization of a cost function, often using simple and relatively uniform\ninitial architectures. two recent developments have emerged within machine learning\nthat create an opportunity to connect these seemingly divergent perspectives. first,\nstructured architectures are used, including dedicated systems for attention, re", "ne43ch01_fishell\n\narjats.cls\n\njune 19, 2020\n\n7:39\n\nannu. rev. neurosci. 2020. 43:1\u201330\n\nfirst published as a review in advance on\njuly 12, 2019\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nhttps://doi.org/10.1146/annurev-neuro-070918-\n050421\n\ncopyright \u00a9 2020 by annual reviews.\nall rights reserved\n\nannual review of neuroscience\ninterneuron types as attractors\nand controllers\n\ngord fishell1,2,3 and adam kepecs4,5\n1department of neurobiology, blavatnik institute, harvard medical school, boston,\nmassachusetts 02115, usa; email: gordon_fishell@hms.harvard.edu\n2stanley center for psychiatric research, broad institute, cambridge, massachusetts 02142,\nusa\n3center for genomics and systems biology, new york university abu dhabi, abu dhabi,\nunited arab emirates\n4cold spring harbor laboratory, cold spring harbor, new york 11724, usa\n5department of neuroscience, washington university in st. louis, st. louis, missouri 63130,\nusa; email: akepecs@wustl.edu\n\nkeywords\ninterne", "synaptic  plasticity:  ltp and  ltd \n\nmark  f bear and  robert c malenka \n\nbrown  university, \n\nprovidence  and  university  of  california,  san  francisco,  usa \n\n(ltp) \n\nlong-term  potentiation \nhigh-frequency \nrecent  evidence  suggests  that  induction  of  ltp  may  require, \npostsynaptic  ca*+  entry,  activation  of  metabotropic  glutamate \n\nfollows  brief, \nthe  hippocampus  and  neocortex. \nto \n\nis  a  synaptic  enhancement \n\nelectrical  stimulation \n\nthat \n\nin \n\nin  addition \nreceptors  and the generation  of  diffusible \n\nintercellular  messengers.  a  new  form  of  synaptic \n(ltd)  has  also  recently  been \n\nlong-term  depression \n\nplasticity,  homosynaptic \ndocumented,  which, \ncurrent  work  suggests  that  this  ltd \nis  a  reversal  of  ltp,  and  vice  versa,  and \nthat  the  mechanisms  of  ltp  and  ltd  may  converge  at  the  level  of  specific \n\nlike  ltp,  requires  ca*+  entry  through \n\nintroduction \n\nis  investigated \n\nstimulation,  and  usually \n\nit is w", "inferring single-trial neural population dynamics \nusing sequential auto-encoders\n\n\u200a1,2,3,4,5*, daniel\u00a0j.\u00a0o\u2019shea\u200a\n\nchethan\u00a0pandarinath\u200a\nsergey\u00a0d.\u00a0stavisky3,4,5,6, jonathan\u00a0c.\u00a0kao4,8, eric\u00a0m.\u00a0trautmann6, matthew\u00a0t.\u00a0kaufman6,22, \nstephen\u00a0i.\u00a0ryu4,9, leigh\u00a0r.\u00a0hochberg10,11,12, jaimie\u00a0m.\u00a0henderson3,5, krishna\u00a0v.\u00a0shenoy4,5,13,14,15,16, \nl.\u00a0f.\u00a0abbott17,18,19 and david\u00a0sussillo\u200a\n\n\u200a4,6, jasmine\u00a0collins7,20, rafal\u00a0jozefowicz7,21, \n\n\u200a4,5,7*\n\nneuroscience is experiencing a revolution in which simultaneous recording of thousands of neurons is revealing population \ndynamics that are not apparent from single-neuron responses. this structure is typically extracted from data averaged across \nmany  trials,  but  deeper  understanding  requires  studying  phenomena  detected  in  single  trials,  which  is  challenging  due  to \nincomplete sampling of the neural population, trial-to-trial variability, and fluctuations in action potential timing. we introduce \nlatent factor analysis via dynamical systems,", "the tradeoffs of large scale learning\n\nl\u00b4eon bottou\n\nnec laboratories of america\nprinceton, nj 08540, usa\n\nolivier bousquet\n\ngoogle z\u00a8urich\n\n8002 zurich, switzerland\n\nleon@bottou.org\n\nolivier.bousquet@m4x.org\n\nabstract\n\nthis contribution develops a theoretical framework that takes into account the\neffect of approximate optimization on learning algorithms. the analysis shows\ndistinct tradeoffs for the case of small-scale and large-scale learning problems.\nsmall-scale learning problems are subject to the usual approximation\u2013estimation\ntradeoff. large-scale learning problems are subject to a qualitatively different\ntradeoff involving the computational complexity of the underlying optimization\nalgorithms in non-trivial ways.\n\n1 motivation\n\nthe computational complexity of learning algorithms has seldom been taken into account by the\nlearning theory. valiant [1] states that a problem is \u201clearnable\u201d when there exists a probably approx-\nimatively correct learning algorithm with polynomial comp", "reports\n\nidly, had immediate, major, and visible impacts\n_\non the island\ns biota and physical landscape, and\nbegan investing in monumental architecture and\nstatuary within the first century or two of set-\ntlement. although still poorly dated, monumen-\ntal architecture and statuary are known from\nislands, such as the societies, marquesas, and\naustral islands, from perhaps as early as 1200\na.d. nearly immediate building of monuments,\ncarving giant statues, and transporting them to\nevery corner of the island may have been cultural\ninvestments, homologous to forms elsewhere in\neastern polynesia, that mediated against over-\npopulation and resource shortfalls in an unpre-\ndictable environment. such a model would help\nto explain the success of ancient polynesians on\ntiny, remote rapa nui (27). demographic and\ncultural collapse resulted from european con-\ntact beginning in 1722 a.d. with the devas-\ntating consequences of newly introduced old\nworld diseases to a nonimmune polynesian pop-\nulatio", "9\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n9\n2\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n4\nv\n8\n3\n4\n5\n\n.\n\n0\n1\n3\n1\n:\nv\ni\nx\nr\na\n\nvariational bayesian inference\n\nfor linear and logistic regression\n\njan drugowitsch\n\nabstract. the article describe the model, derivation, and implementation\nof variational bayesian inference for linear and logistic regression, both with\nand without automatic relevance determination. it has the dual function of\nacting as a tutorial for the derivation of variational bayesian inference for\nsimple models, as well as documenting, and providing brief examples for the\nmatlab/octave functions that implement this inference. these functions are\nfreely available online.\n\n1. introduction\n\nlinear and logistic regression are essential workhorses of statistical analysis,\nwhose bayesian treatment has received much recent attention (gelman et al., 2013;\nbishop, 2006; murphy, 2012; hastie et al., 2011). these allow specifying the a-priori\nuncertainty and infer a-posteriori uncertainty about regression coe\ufb03cients ", "how recurrent networks implement contextual processing in sentiment analysis\n\nniru maheswaranathan * 1 david sussillo * 1\n\n0\n2\n0\n2\n\n \nr\np\na\n7\n1\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n1\nv\n3\n1\n0\n8\n0\n\n.\n\n4\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nneural networks have a remarkable capacity for\ncontextual processing\u2014using recent or nearby in-\nputs to modify processing of current input. for\nexample, in natural language, contextual process-\ning is necessary to correctly interpret negation\n(e.g. phrases such as \u201cnot bad\u201d). however, our\nability to understand how networks process con-\ntext is limited. here, we propose general meth-\nods for reverse engineering recurrent neural net-\nworks (rnns) to identify and elucidate contextual\nprocessing. we apply these methods to under-\nstand rnns trained on sentiment classi\ufb01cation.\nthis analysis reveals inputs that induce contextual\neffects, quanti\ufb01es the strength and timescale of\nthese effects, and identi\ufb01es sets of these inputs\nwith similar properties. additionally, we analyze\nc", "neuroimage 106 (2015) 222\u2013237\n\ncontents lists available at sciencedirect\n\nneuroimage\n\nj ou r n a l h o m e p a ge : w ww . e l s e v i e r . c o m / l oc a te / yn i mg\n\na computational analysis of the neural bases of bayesian inference\nantonio kolossa a, bruno kopp b,\u204e, tim fingscheidt a\na institute for communications technology, technische universit\u00e4t braunschweig, schleinitzstr. 22, 38106 braunschweig, germany\nb department of neurology, hannover medical school, carl-neuberg-str. 1, 30625 hannover, germany\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\naccepted 2 november 2014\navailable online 8 november 2014\n\nkeywords:\nevent-related potentials\nsingle-trial eeg\nfree-energy principle\nbayesian brain\nsurprise\nprobability weighting\n\nempirical support for the bayesian brain hypothesis, although of major theoretical importance for cognitive neu-\nroscience, is surprisingly scarce. this hypothesis posits simply that neural activities code and compute bayesian\nprobabilities. here,", "journal of computational physics 378 (2019) 686\u2013707\n\ncontents lists available at sciencedirect\n\njournal  of  computational  physics\n\nwww.elsevier.com/locate/jcp\n\nphysics-informed  neural  networks:  a  deep  learning \nframework  for  solving  forward  and  inverse  problems  involving \nnonlinear  partial  differential  equations\nm. raissi a, p. perdikaris b,\u2217, g.e. karniadakis a\n\na division of applied mathematics, brown university, providence, ri, 02912, usa\nb department of mechanical engineering and applied mechanics, university of pennsylvania, philadelphia, pa, 19104, usa\n\na  r  t  i  c  l  e \n\ni  n  f  o\n\na  b  s  t  r  a  c  t\n\narticle history:\nreceived 13 june 2018\nreceived in revised form 26 october 2018\naccepted 28 october 2018\navailable online 3 november 2018\n\nkeywords:\ndata-driven scienti\ufb01c computing\nmachine learning\npredictive modeling\nrunge\u2013kutta methods\nnonlinear dynamics\n\nwe introduce physics-informed neural networks \u2013 neural networks that are trained to solve \nsupervised", "neuroimage 80 (2013) 105\u2013124\n\ncontents lists available at sciverse sciencedirect\n\nneuroimage\n\nj o u r n a l h o m e p a ge : w ww . e l s e v i e r . c o m/ l o c a t e / y n i m g\n\nthe minimal preprocessing pipelines for the human connectome project\nmatthew f. glasser a,\u204e, stamatios n. sotiropoulos b, j. anthony wilson c, timothy s. coalson a,\nbruce fischl d,e, jesper l. andersson b, junqian xu f,g, saad jbabdi b, matthew webster b,\njonathan r. polimeni d, david c. van essen a, mark jenkinson b for the wu-minn hcp consortium\na department of anatomy and neurobiology, washington university medical school, 660 s. euclid avenue, st. louis, mo 63110, usa\nb centre for functional magnetic resonance imaging of the brain (fmrib), university of oxford, oxford, uk\nc mallinckrodt institute of radiology, washington university medical school, usa\nd athinoula a martinos center for biomedical imaging, department of radiology, harvard medical school/mass. general hospital, boston, ma, usa\ne computer s", "original research\npublished: 08 october 2015\ndoi: 10.3389/fncom.2015.00120\n\nan algorithm to predict the\nconnectome of neural microcircuits\n\nmichael w. reimann, james g. king, eilif b. muller, srikanth ramaswamy and\nhenry markram *\n\nblue brain project, \u00e9cole polytechnique f\u00e9d\u00e9rale de lausanne (epfl) biotech campus, geneva, switzerland\n\nexperimentally mapping synaptic connections, in terms of the numbers and locations of\ntheir synapses and estimating connection probabilities, is still not a tractable task, even\nfor small volumes of tissue. in fact, the six layers of the neocortex contain thousands\nof unique types of synaptic connections between the many different types of neurons,\nof which only a handful have been characterized experimentally. here we present a\ntheoretical framework and a data-driven algorithmic strategy to digitally reconstruct\nthe complete synaptic connectivity between the different types of neurons in a small\nwell-de\ufb01ned volume of tissue\u2014the micro-scale connectome of ", "article\ntransparency of arti\ufb01cial intelligence in healthcare: insights\nfrom professionals in computing and healthcare worldwide\njose bernal 1,2,3\n\nand claudia mazo 4,*,\u2020\n\n1 german center for neurodegenerative diseases (dzne), 39120 magdeburg, germany\n2\n\ninstitute of cognitive neurology and dementia research, otto-von-guericke university magdeburg,\n39120 magdeburg, germany\n\n3 centre for clinical brain sciences, the university of edinburgh, edinburgh eh16 4sb, uk\n4 dcu school of computing, dublin city university, d09 dxa0 dublin, ireland\n* correspondence: claudia.mazo@dcu.ie\n\u2020 current address: ucd school of computer science, university college dublin, d04 v1w8 dublin, ireland.\n\nabstract: although it is widely assumed that arti\ufb01cial intelligence (ai) will revolutionise healthcare\nin the near future, considerable progress must yet be made in order to gain the trust of healthcare\nprofessionals and patients. improving ai transparency is a promising avenue for addressing such\ntrust issues. ho", "the  journal \n\nof  neuroscience, \n\nfebruary \n\n1990, \n\n70(2): \n\n436-447 \n\nhead-direction \nmoving  rats. \n\ncells  recorded \n\nthe  postsubiculum \nii.  effects  of  environmental  manipulations \n\nfrom \n\nin  freely \n\njeffrey  s.  taube,  robert  u.  muller,  and  james  b.  ranck, \n\njr. \n\ndepartment \n\nof  physiology,  suny  health  sciences  center  at  brooklyn,  brooklyn,  new  york  11203 \n\ncharacteristics \n\nof  postsubicular \n\nhead-direc- \n\n(taube \n\nfiring \n\nthe  discharge \ntion  cells \nvious  paper \nin \nthe \nchanges \nwere \ndiameter \nwas \nto  the \nthe  major  orienting \n\nrecorded \n\ntaped \n\nin  a  fixed  environment \n\nwere  described \n\net  al.,  1990).  this  paper \n\nproperties \nin  the  animal\u2019s \n\nof  head-direction \nenvironment. \n\nfrom \n\nrats  as  they  moved \n\ngray  cylinder. \n\na  white \n\nfreely \ncard,  occupying \n\nhead-direction \n\nin  the  pre- \nchanges \nfollowing \n\nreports \ncells \n\ncells \nin  a  76-cm- \n100\u201d  of  arc, \nas \n\ninside  wall  of  the  cylinder \n\nand  served \n\nspatial \n\nc", "i an update to this article is included at the end\n\nneuron\n\narticle\n\nneuromodulators control the polarity of\nspike-timing-dependent synaptic plasticity\n\ngeun hee seol,1,2,6 jokubas ziburkus,1,3,6 shiyong huang,1 lihua song,4 in tae kim,4 kogo takamiya,5\nrichard l. huganir,5 hey-kyoung lee,4 and alfredo kirkwood1,5,*\n1the mind/brain institute, johns hopkins university, baltimore, md 21218, usa\n2department of basic nursing science, korea university, seoul, korea\n3department of biology and biochemistry, university of houston, houston, tx 77204-5001, usa\n4department of biology, university of maryland, college park, md 20742, usa\n5department of neuroscience, johns hopkins university, baltimore, md 21205, usa\n6these authors contributed equally to this work.\n*correspondence: kirkwood@jhu.edu\ndoi 10.1016/j.neuron.2007.08.013\n\nsummary\n\nnear coincidental pre- and postsynaptic action\npotentials induce associative long-term poten-\ntiation (ltp) or long-term depression (ltd),\ndepending on the order", "0\n2\n0\n2\n\n \nc\ne\nd\n6\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n2\n8\n7\n0\n1\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nai feynman 2.0: pareto-optimal symbolic regression\n\nexploiting graph modularity\n\nsilviu-marian udrescu1, andrew tan1, jiahai feng1, orisvaldo neto1, tailin wu2 & max tegmark1,3\n\n1mit dept. of physics and institute for ai & fundamental interactions, cambridge, ma, usa\n\n2stanford dept. of computer science, palo alto, ca, usa\n\n3theiss research, la jolla, ca, usa\n\n1{sudrescu, aktan, fjiahai, oris,tegmark}@mit.edu, 2tailin@cs.stanford.edu\n\nabstract\n\nwe present an improved method for symbolic regression that seeks to \ufb01t data to\nformulas that are pareto-optimal, in the sense of having the best accuracy for a given\ncomplexity. it improves on the previous state-of-the-art by typically being orders\nof magnitude more robust toward noise and bad data, and also by discovering many\nformulas that stumped previous methods. we develop a method for discovering\ngeneralized symmetries (arbitrary modularity in the compu", "article\n\nhttps://doi.org/10.1038/s41467-021-23103-1\n\nopen\n\nspectral bias and task-model alignment explain\ngeneralization in kernel regression and in\ufb01nitely\nwide neural networks\n\nabdulkadir canatar\n\n1,2, blake bordelon2,3 & cengiz pehlevan\n\n2,3\u2709\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\na theoretical understanding of generalization remains an open problem for many machine\nlearning models, including deep networks where overparameterization leads to better per-\nformance, contradicting the conventional wisdom from classical statistics. here, we inves-\ntigate generalization error for kernel regression, which, besides being a popular machine\nlearning method, also describes certain in\ufb01nitely overparameterized neural networks. we use\ntechniques from statistical mechanics to derive an analytical expression for generalization\nerror applicable to any kernel and data distribution. we present applications of our theory to\nreal and synthetic datasets, and for many kernels including those that arise from train", "letter\n\ncommunicated by joshua b. tenenbaum\n\nlaplacian eigenmaps for dimensionality reduction and data\nrepresentation\n\nmikhail belkin\nmisha@math.uchicago.edu\ndepartment of mathematics, university of chicago, chicago, il 60637, u.s.a.\n\npartha niyogi\nniyogi@cs.uchicago.edu\ndepartment of computer science and statistics, university of chicago,\nchicago, il 60637 u.s.a.\n\none of the central problems in machine learning and pattern recognition\nis to develop appropriate representations for complex data. we consider\nthe problem of constructing a representation for data lying on a low-\ndimensional manifold embedded in a high-dimensional space. drawing\non the correspondence between the graph laplacian, the laplace beltrami\noperator on the manifold, and the connections to the heat equation, we\npropose a geometrically motivated algorithm for representing the high-\ndimensional data. the algorithm provides a computationally ef\ufb01cient ap-\nproach to nonlinear dimensionality reduction that has locality-pr", "a r t i c l e s\n\na category-free neural population supports evolving \ndemands during decision-making\ndavid raposo1\u20133, matthew t kaufman1,3 & anne k churchland1\nthe posterior parietal cortex (ppc) receives diverse inputs and is involved in a dizzying array of behaviors. these many behaviors \ncould rely on distinct categories of neurons specialized to represent particular variables or could rely on a single population of \nppc neurons that is leveraged in different ways. to distinguish these possibilities, we evaluated rat ppc neurons recorded during \nmultisensory decisions. newly designed tests revealed that task parameters and temporal response features were distributed \nrandomly across neurons, without evidence of categories. this suggests that ppc neurons constitute a dynamic network that \nis decoded according to the animal\u2019s present needs. to test for an additional signature of a dynamic network, we compared \nmoments when behavioral demands differed: decision and movement. our new st", "rsif.royalsocietypublishing.org\n\nresearch\n\ncite this article: friston k. 2013 life as we\nknow it. j r soc interface 10: 20130475.\nhttp://dx.doi.org/10.1098/rsif.2013.0475\n\nreceived: 27 may 2013\naccepted: 12 june 2013\n\nsubject areas:\nbiomathematics\n\nkeywords:\nautopoiesis, self-organization, active inference,\nfree energy, ergodicity, random attractor\n\nauthor for correspondence:\nkarl friston\ne-mail: k.friston@ucl.ac.uk\n\nlife as we know it\n\nkarl friston\n\nthe wellcome trust centre for neuroimaging, institute of neurology, queen square, london wc1n 3bg, uk\n\nthis paper presents a heuristic proof (and simulations of a primordial soup)\nsuggesting that life\u2014or biological self-organization\u2014is an inevitable and\nemergent property of any (ergodic) random dynamical system that possesses\na markov blanket. this conclusion is based on the following arguments: if\nthe coupling among an ensemble of dynamical systems is mediated by\nshort-range forces, then the states of remote systems must be conditionally\n", "1\n2\n0\n2\n\n \n\nv\no\nn\n0\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n4\n6\n4\n5\n0\n\n.\n\n1\n1\n1\n2\n:\nv\ni\nx\nr\na\n\nare transformers more robust than cnns?\n\nyutong bai1\n\njieru mei1 alan yuille1 cihang xie2\n\n2 university of california, santa cruz\n1johns hopkins university\n{ytongbai, meijieru, alan.l.yuille, cihangxie306}@gmail.com\n\nabstract\n\ntransformer emerges as a powerful tool for visual recognition. in addition to\ndemonstrating competitive performance on a broad range of visual benchmarks,\nrecent works also argue that transformers are much more robust than convolutions\nneural networks (cnns). nonetheless, surprisingly, we \ufb01nd these conclusions\nare drawn from unfair experimental settings, where transformers and cnns are\ncompared at different scales and are applied with distinct training frameworks.\nin this paper, we aim to provide the \ufb01rst fair & in-depth comparisons between\ntransformers and cnns, focusing on robustness evaluations.\nwith our uni\ufb01ed training setup, we \ufb01rst challenge the previous belief that\ntra", "published in proceedings of 2nd international conference on knowledge discovery and data mining (kdd-96)\n\na density-based algorithm for discovering clusters\n\nin large spatial databases with noise\n\nmartin ester, hans-peter kriegel, j\u00f6rg sander, xiaowei xu\n\ninstitute for computer science, university of munich\n\noettingenstr. 67, d-80538 m\u00fcnchen, germany\n\n{ester | kriegel | sander | xwxu}@informatik.uni-muenchen.de\n\nabstract\n\nclustering algorithms are attractive for the task of class iden-\nti\ufb01cation  in  spatial  databases.  however,  the  application  to\nlarge spatial databases rises the following requirements for\nclustering  algorithms:  minimal  requirements  of  domain\nknowledge to determine the input parameters, discovery of\nclusters with arbitrary shape and good ef\ufb01ciency on large da-\ntabases. the well-known clustering algorithms offer no solu-\ntion to the combination of these requirements. in this paper,\nwe present the new clustering algorithm dbscan relying on\na density-based notio", "lora: low-rank adaptation of large lan-\nguage models\n\nphillip wallis\n\nyelong shen\u2217\nshean wang\n\nedward hu\u2217\nyuanzhi li\nmicrosoft corporation\n{edwardhu, yeshe, phwallis, zeyuana,\nyuanzhil, swang, luw, wzchen}@microsoft.com\nyuanzhil@andrew.cmu.edu\n(version 2)\n\nlu wang\n\nweizhu chen\n\nzeyuan allen-zhu\n\n1\n2\n0\n2\n\n \nt\nc\no\n6\n1\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n2\nv\n5\n8\n6\n9\n0\n\n.\n\n6\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nan important paradigm of natural language processing consists of large-scale pre-\ntraining on general domain data and adaptation to particular tasks or domains. as\nwe pre-train larger models, full \ufb01ne-tuning, which retrains all model parameters,\nbecomes less feasible. using gpt-3 175b as an example \u2013 deploying indepen-\ndent instances of \ufb01ne-tuned models, each with 175b parameters, is prohibitively\nexpensive. we propose low-rank adaptation, or lora, which freezes the pre-\ntrained model weights and injects trainable rank decomposition matrices into each\nlayer of the transformer architecture, greatly r", "machine learning, 37, 183\u2013233 (1999)\nc(cid:176) 1999 kluwer academic publishers. manufactured in the netherlands.\n\nan introduction to variational methods\nfor graphical models\n\nmichael i. jordan\ndepartment of electrical engineering and computer sciences and department of statistics,\nuniversity of california, berkeley, ca 94720, usa\n\njordan@cs.berkeley.edu\n\nzoubin ghahramani\ngatsby computational neuroscience unit, university college london wc1n 3ar, uk\n\nzoubin@gatsby.ucl.ac.uk\n\ntommi s. jaakkola\narti\ufb01cial intelligence laboratory, mit, cambridge, ma 02139, usa\n\nlawrence k. saul\nat&t labs\u2013research, florham park, nj 07932, usa\n\neditor: david heckerman\n\ntommi@ai.mit.edu\n\nlsaul@research.att.edu\n\nabstract. this paper presents a tutorial introduction to the use of variational methods for inference and learning\nin graphical models (bayesian networks and markov random \ufb01elds). we present a number of examples of graphical\nmodels, including the qmr-dt database, the sigmoid belief network, the boltzm", "show and tell: a neural image caption generator\n\noriol vinyals\n\ngoogle\n\nalexander toshev\n\ngoogle\n\nsamy bengio\n\ngoogle\n\ndumitru erhan\n\ngoogle\n\nvinyals@google.com\n\ntoshev@google.com\n\nbengio@google.com\n\ndumitru@google.com\n\n5\n1\n0\n2\n\n \nr\np\na\n0\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n5\n5\n5\n4\n\n.\n\n1\n1\n4\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nautomatically describing the content of an image is a\nfundamental problem in arti\ufb01cial intelligence that connects\ncomputer vision and natural language processing. in this\npaper, we present a generative model based on a deep re-\ncurrent architecture that combines recent advances in com-\nputer vision and machine translation and that can be used\nto generate natural sentences describing an image. the\nmodel is trained to maximize the likelihood of the target de-\nscription sentence given the training image. experiments\non several datasets show the accuracy of the model and the\n\ufb02uency of the language it learns solely from image descrip-\ntions. our model is often quite accurate, whic", "4\n1\n0\n2\n\n \nt\nc\no\n8\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n4\nv\n8\n2\n8\n7\n\n.\n\n4\n0\n4\n1\n:\nv\ni\nx\nr\na\n\ndeep learning in neural networks: an overview\n\ntechnical report idsia-03-14 / arxiv:1404.7828 v4 [cs.ne] (88 pages, 888 references)\n\nistituto dalle molle di studi sull\u2019intelligenza arti\ufb01ciale\n\nj\u00a8urgen schmidhuber\n\nthe swiss ai lab idsia\n\nuniversity of lugano & supsi\ngalleria 2, 6928 manno-lugano\n\nswitzerland\n\n8 october 2014\n\nabstract\n\nin recent years, deep arti\ufb01cial neural networks (including recurrent ones) have won numerous\ncontests in pattern recognition and machine learning. this historical survey compactly summarises\nrelevant work, much of it from the previous millennium. shallow and deep learners are distin-\nguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal\nlinks between actions and effects. i review deep supervised learning (also recapitulating the history\nof backpropagation), unsupervised learning, reinforcement learning & evolutionary computati", "adversarial score matching and improved\nsampling for image generation\n\nanonymous authors\npaper under double-blind review\n\nabstract\n\ndenoising score matching with annealed langevin sampling (dsm-als) has\nrecently found success in generative modeling. the approach works by \ufb01rst\ntraining a neural network to estimate the score of a distribution, and then using\nlangevin dynamics to sample from the data distribution assumed by the score\nnetwork. despite the convincing visual quality of samples, this method appears to\nperform worse than generative adversarial networks (gans) under the fr\u00e9chet\ninception distance, a standard metric for generative models. we show that this\napparent gap vanishes when denoising the \ufb01nal langevin samples using the score\nnetwork. in addition, we propose two improvements to dsm-als: 1) consistent\nannealed sampling as a more stable alternative to annealed langevin sampling,\nand 2) a hybrid training formulation, composed of both denoising score matching\nand adversarial", "3\n2\n0\n2\n\n \n\nv\no\nn\n \n1\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n9\n6\n8\n0\n1\n\n.\n\n1\n1\n3\n2\n:\nv\ni\nx\nr\na\n\n| yuhanhelenaliu1,3\n\noriginal article\njournal section\nevolutionaryalgorithmsasanalternativeto\nbackpropagationforsupervisedtrainingof\nbiophysicalneuralnetworksandneuralodes\njameshazelden1,3\n| elishlizerman1,2,3*\n| ericshea-brown1,2,3,4*\n1appliedmathematicsandcomputational\nneurosciencecenter,universityof\nwashington,seattle,wa,98195,united\nstates\n2electrical&computerengineering,\nuniversityofwashington,seattle,wa,\n98195,unitedstates\n3computationalneurosciencecenter,\nuniversityofwashington,seattle,wa,\n98195,unitedstates\n4physiology&biophysics,universityof\nwashington,seattle,wa,98195,united\nstates\n*theseauthorsshareseniorauthorship\ncorrespondence\njameshazelden,appliedmathematics,\nuniversityofwashington,seattle,wa,\n98195,unitedstates\nemail: jhazelde@uw.edu\n\ntrainingnetworksconsistingofbiophysicallyaccurateneu-\nronmodelscouldallowfornewinsightsintohowbraincir-\ncuits can organize and solve tasks. we b", "the forget-me-not process\n\nkieran milan\u2020, joel veness\u2020, james kirkpatrick, demis hassabis\n\ngoogle deepmind\n\n{kmilan,aixi,kirkpatrick,demishassabis}@google.com\n\nanna koop, michael bowling\n\nuniversity of alberta\n\n{anna,bowling}@cs.ualberta.ca\n\nabstract\n\nwe introduce the forget-me-not process, an ef\ufb01cient, non-parametric meta-\nalgorithm for online probabilistic sequence prediction for piecewise stationary,\nrepeating sources. our method works by taking a bayesian approach to partition-\ning a stream of data into postulated task-speci\ufb01c segments, while simultaneously\nbuilding a model for each task. we provide regret guarantees with respect to piece-\nwise stationary data sources under the logarithmic loss, and validate the method\nempirically across a range of sequence prediction and task identi\ufb01cation problems.\n\n1\n\nintroduction\n\nmodeling non-stationary temporal data sources is a fundamental problem in signal processing,\nstatistical data compression, quantitative \ufb01nance and model-based reinfor", "the journal of neuroscience, may 15, 2001, 21(10):3646\u20133655\n\ncoding speci\ufb01city in cortical microcircuits: a multiple-electrode\nanalysis of primate prefrontal cortex\n\nchristos constantinidis, matthew n. franowicz, and patricia s. goldman-rakic\nsection of neurobiology, yale school of medicine, new haven, connecticut 06510\n\nneurons with directional speci\ufb01cities are active in the prefrontal\ncortex (pfc) during tasks that require spatial working memory.\nalthough the coordination of neuronal activity in pfc is thought\nto be maintained by a network of recurrent connections, direct\nphysiological evidence regarding such networks is sparse. to\ngain insight into the functional organization of the working\nmemory system in vivo, we recorded simultaneously from mul-\ntiple neurons spaced 0.2\u20131 mm apart in monkeys performing an\noculomotor delayed response task. we used cross-correlation\nanalysis and characterized the effective connectivity between\nneurons in relation to their spatial and temporal resp", "letter\n\ncommunicated by michael shadlen\n\nthe effect of correlated variability on the accuracy of a\npopulation code\n\nl. f. abbott\nvolen center and department of biology, brandeis university, waltham, ma 02454-\n9110, u.s.a.\n\n\u2217\npeter dayan\ndepartment of brain and cognitive sciences, massachusetts institute of technology,\ncambridge, ma 02139, u.s.a.\n\nwe study the impact of correlated neuronal \ufb01ring rate variability on the\naccuracy with which an encoded quantity can be extracted from a pop-\nulation of neurons. contrary to widespread belief, correlations in the\nvariabilities of neuronal \ufb01ring rates do not, in general, limit the increase\nin coding accuracy provided by using large populations of encoding neu-\nrons. furthermore, in some cases, but not all, correlations improve the\naccuracy of a population code.\n\n1 introduction\n\nin population coding schemes, the activities of a number of neurons jointly\nencode the value of a quantity. a frequently touted advantage of popula-\ntion coding is that ", "journal of machine learning research 11 (2010) 625-660\n\nsubmitted 8/09; published 2/10\n\nwhy does unsupervised pre-training help deep learning?\n\ndumitru erhan\u2217\nyoshua bengio\naaron courville\npierre-antoine manzagol\npascal vincent\nd\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle\nuniversit\u00b4e de montr\u00b4eal\n2920, chemin de la tour\nmontr\u00b4eal, qu\u00b4ebec, h3t 1j8, canada\n\ndumitru.erhan@umontreal.ca\nyoshua.bengio@umontreal.ca\naaron.courville@umontreal.ca\npierre-antoine.manzagol@umontreal.ca\npascal.vincent@umontreal.ca\n\nsamy bengio\ngoogle research\n1600 amphitheatre parkway\nmountain view, ca, 94043, usa\n\neditor: l\u00b4eon bottou\n\nbengio@google.com\n\nabstract\n\nmuch recent research has been devoted to learning algorithms for deep architectures such as deep\nbelief networks and stacks of auto-encoder variants, with impressive results obtained in several\nareas, mostly on vision and language data sets. the best results obtained on supervised learning\ntasks involve an unsupervised learning component, ", "open\n\nsmall-world propensity and \nweighted brain networks\n\nsarah feldt muldoon1,2,3, eric w. bridgeford1,4 & danielle s. bassett1,5\n\nreceived: 23 june 2015\naccepted: 03 february 2016\npublished: 25 february 2016\n\nquantitative descriptions of network structure can provide fundamental insights into the function of \ninterconnected complex systems. small-world structure, diagnosed by high local clustering yet short \naverage path length between any two nodes, promotes information flow in coupled systems, a key \nfunction that can differ across conditions or between groups. however, current techniques to quantify \nsmall-worldness are density dependent and neglect important features such as the strength of network \nconnections, limiting their application in real-world systems. here, we address both limitations with a \nnovel metric called the small-world propensity (swp). in its binary instantiation, the swp provides an \nunbiased assessment of small-world structure in networks of varying densiti", "0\n2\n0\n2\n\n \n\ng\nu\na\n3\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n4\n3\n9\n8\n0\n\n.\n\n3\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nnerf: representing scenes as\n\nneural radiance fields for view synthesis\n\nben mildenhall1(cid:63)\n\npratul p. srinivasan1(cid:63) matthew tancik1(cid:63)\n\njonathan t. barron2 ravi ramamoorthi3 ren ng1\n\n1uc berkeley\n\n2google research\n\n3uc san diego\n\nabstract. we present a method that achieves state-of-the-art results\nfor synthesizing novel views of complex scenes by optimizing an under-\nlying continuous volumetric scene function using a sparse set of input\nviews. our algorithm represents a scene using a fully-connected (non-\nconvolutional) deep network, whose input is a single continuous 5d coor-\ndinate (spatial location (x, y, z) and viewing direction (\u03b8, \u03c6)) and whose\noutput is the volume density and view-dependent emitted radiance at\nthat spatial location. we synthesize views by querying 5d coordinates\nalong camera rays and use classic volume rendering techniques to project\nthe output colors and dens", "journal of the american statistical association\n\nissn: 0162-1459 (print) 1537-274x (online) journal homepage: https://www.tandfonline.com/loi/uasa20\n\nvariational inference: a review for statisticians\n\ndavid m. blei, alp kucukelbir & jon d. mcauliffe\n\nto cite this article: david m. blei, alp kucukelbir & jon d. mcauliffe (2017) variational inference:\na review for statisticians, journal of the american statistical association, 112:518, 859-877,\ndoi: 10.1080/01621459.2017.1285773\n\nto link to this article:  https://doi.org/10.1080/01621459.2017.1285773\n\nview supplementary material \n\npublished online: 13 jul 2017.\n\nsubmit your article to this journal \n\narticle views: 40093\n\nview related articles \n\nview crossmark data\n\nciting articles: 1138 view citing articles \n\nfull terms & conditions of access and use can be found at\n\nhttps://www.tandfonline.com/action/journalinformation?journalcode=uasa20\n\n\f", "6028 \u2022 the journal of neuroscience, july 4, 2018 \u2022 38(27):6028 \u2013 6044\n\nbehavioral/cognitive\n\ndeep neural networks for modeling visual perceptual\nlearning\n\nx li k. wenliang1 and aaron r. seitz2\n1gatsby computational neuroscience unit, university college london, london w1t 4jg, united kingdom and 2department of psychology, university of\ncalifornia\u2013riverside, riverside, california 92521\n\nunderstanding visual perceptual learning (vpl) has become increasingly more challenging as new phenomena are discovered with novel\nstimuli and training paradigms. although existing models aid our knowledge of critical aspects of vpl, the connections shown by these\nmodels between behavioral learning and plasticity across different brain areas are typically superficial. most models explain vpl as\nreadout from simple perceptual representations to decision areas and are not easily adaptable to explain new findings. here, we show that\na well -known instance of deep neural network (dnn), whereas not designed sp", "ne41ch19_josselyn\n\nari\n\n24 may 2018\n\n10:32\n\nannual review of neuroscience\nmemory allocation:\nmechanisms and function\nsheena a. josselyn1,2,3,4,5 and paul w. frankland1,2,3,4,6\n1department of psychology, university of toronto, ontario m5s 3g3, canada;\nemail: sheena.josselyn@sickkids.ca, paul.frankland@sickkids.ca\n2program in neurosciences & mental health, hospital for sick children, toronto,\nontario m5g 1x8, canada\n3department of physiology, university of toronto, ontario m5s 1a8, canada\n4institute of medical sciences, university of toronto, ontario m5s 1a8, canada\n5brain, mind & consciousness program, canadian institute for advanced research (cifar),\ntoronto, ontario m5g 1m1, canada\n6child & brain development program, canadian institute for advanced research (cifar),\ntoronto, ontario m5g 1m1, canada\n\nkeywords\nmemory, engram, allocation, neuronal excitability, fear, reward\n\nabstract\nmemories for events are thought to be represented in sparse, distributed\nneuronal ensembles (or engrams).", "neuroimage 93 (2014) 165\u2013175\n\ncontents lists available at sciencedirect\n\nneuroimage\n\nj o u r n a l h o m e p a ge : w ww . e l s e v i e r . c o m/ l o c a t e / y n i m g\n\nreview\ntrends and properties of human cerebral cortex:\ncorrelations with cortical myelin content\nmatthew f. glasser a,1, manu s. goyal b, todd m. preuss e,f,g, marcus e. raichle a,b,c,d, david c. van essen a,\u204e\na department of anatomy and neurobiology, washington university school of medicine, 660 s. euclid avenue, st. louis, mo 63110, usa\nb department of radiology, washington university school of medicine, 660 s. euclid avenue, st. louis, mo 63110, usa\nc department of neurology, washington university school of medicine, 660 s. euclid avenue, st. louis, mo 63110, usa\nd department of biomedical engineering, washington university school of medicine, 660 s. euclid avenue, st. louis, mo 63110, usa\ne division of neuropharmacology and neurologic diseases, emory university, atlanta, ga 30329, usa\nf center for translational ", "cl concentrations in the sajama ice core, and to\na number of other pedological and geomorpho-\nlogical features indicative of long-term dry cli-\nmates (8, 11\u201314, 18). this decline in human\nactivity around the altiplano paleolakes is seen\nin most caves, with early and late occupations\nseparated by largely sterile mid-holocene sed-\niments. however, a few sites, including the\ncaves of tulan-67 and tulan-68, show that\npeople did not completely disappear from the\narea. all of the sites of sporadic occupation\nare located near wetlands in valleys, near\nlarge springs, or where lakes turned into wet-\nlands and subsistence resources were locally\nstill available despite a generally arid climate\n(7, 8, 19, 20).\n\narchaeological data from surrounding ar-\neas suggest that the silencio arqueolo\u00b4gico\napplies best to the most arid areas of the\ncentral andes, where aridity thresholds for\nearly societies were critical. in contrast, a\nweaker expression is to be expected in the\nmore humid highlands of northe", "2\n2\n0\n2\n\n \n\np\ne\ns\n9\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n8\n8\n9\n4\n1\n\n.\n\n9\n0\n2\n2\n:\nv\ni\nx\nr\na\n\ndreamfusion: text-to-3d using 2d diffusion\n\nben poole1, ajay jain2, jonathan t. barron1, ben mildenhall1\n1google research, 2uc berkeley\n{pooleb, barron, bmild}@google.com, ajayj@berkeley.edu\n\nabstract\n\nrecent breakthroughs in text-to-image synthesis have been driven by diffusion\nmodels trained on billions of image-text pairs. adapting this approach to 3d synthe-\nsis would require large-scale datasets of labeled 3d data and ef\ufb01cient architectures\nfor denoising 3d data, neither of which currently exist. in this work, we circum-\nvent these limitations by using a pretrained 2d text-to-image diffusion model to\nperform text-to-3d synthesis. we introduce a loss based on probability density\ndistillation that enables the use of a 2d diffusion model as a prior for optimization\nof a parametric image generator. using this loss in a deepdream-like procedure,\nwe optimize a randomly-initialized 3d model (a neural", "biorxiv preprint \nthe copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is \n\nhttps://doi.org/10.1101/2022.03.17.484712\n; \n\nthis version posted march 19, 2022. \n\ndoi: \n\nmade available under a\n\ncc-by 4.0 international license\n\n.\n\nthe combination of hebbian and predictive plasticity learns\n\ninvariant object representations in deep sensory networks\n\nmanu srinath halvagal1,2 and friedemann zenke1,*\n\n1friedrich miescher institute for biomedical research, 4058 basel, switzerland\n\n2faculty of natural sciences, university of basel, 4033 basel, switzerland\n\n*correspondence: friedemann.zenke@fmi.ch\n\nabstract\n\ndiscriminating distinct objects and concepts from sensory stimuli is essential for sur-\nvival. our brains accomplish this feat by forming meaningful internal representations in\ndeep sensory networks with plastic synaptic connections. experience-dependent plasticity\npres", "neuron\n\narticle\n\nrewarded outcomes enhance reactivation\nof experience in the hippocampus\n\nannabelle c. singer1,* and loren m. frank1\n1w.m. keck center for integrative neuroscience and department of physiology, university of california, san francisco, san francisco,\nca 94143-0444, usa\n*correspondence: annabelle.singer@ucsf.edu\ndoi 10.1016/j.neuron.2009.11.016\n\nsummary\n\nremembering experiences that lead to reward is\nessential for survival. the hippocampus is required\nfor forming and storing memories of events and pla-\nces, but the mechanisms that associate speci\ufb01c\nexperiences with rewarding outcomes are not under-\nstood. event memory storage is thought to depend\non the reactivation of previous experiences during\nhippocampal sharp wave ripples (swrs). we used\na sequence switching task that allowed us to\nexamine the interaction between swrs and reward.\nwe compared swr activity after animals traversed\nspatial trajectories and either received or did not\nreceive a reward. here, we show that r", "\f", "6\n1\n0\n2\n\n \n\np\ne\ns\n3\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n8\n5\n1\n5\n0\n\n.\n\n9\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nreal-time single image and video super-resolution using an ef\ufb01cient\n\nsub-pixel convolutional neural network\n\nwenzhe shi1, jose caballero1, ferenc husz\u00b4ar1, johannes totz1, andrew p. aitken1,\n\nrob bishop1, daniel rueckert1, zehan wang1\n\n1twitter\n\n1{wshi,jcaballero,fhuszar,jtotz,aitken,rbishop,zehanw}@twitter.com\n\nabstract\n\nrecently, several models based on deep neural networks\nhave achieved great success in terms of both reconstruction\naccuracy and computational performance for single image\nsuper-resolution. in these methods, the low resolution (lr)\ninput image is upscaled to the high resolution (hr) space\nusing a single \ufb01lter, commonly bicubic interpolation, before\nreconstruction. this means that the super-resolution (sr)\noperation is performed in hr space. we demonstrate that\nthis is sub-optimal and adds computational complexity. in\nthis paper, we present the \ufb01rst convolutional neural network\n(cnn", "8\n1\n0\n2\n\n \n\ng\nu\na\n9\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n7\n5\n3\n3\n0\n\n.\n\n8\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nerror forward-propagation: reusing feedforward connec-\ntions to propagate errors in deep learning\n\nadam a. kohan\n\nuniversity of massachusetts amherst\n\nedward a. rietman\n\nuniversity of massachusetts amherst\n\nhava t. siegelmann\n\nuniversity of massachusetts amherst\n\nakohan@cs.umass.edu\n\nerietman@umass.edu\n\nhava@cs.umass.edu\n\nabstract\n\nwe introduce error forward-propagation, a biologically plausible mechanism to propagate error\nfeedback forward through the network. architectural constraints on connectivity are virtually\neliminated for error feedback in the brain; systematic backward connectivity is not used or needed\nto deliver error feedback. feedback as a means of assigning credit to neurons earlier in the forward\npathway for their contribution to the \ufb01nal output is thought to be used in learning in the brain. how the\nbrain solves the credit assignment problem is unclear. in machine learning, error back", "structure discovery in nonparametric regression through\n\ncompositional kernel search\n\ndavid duvenaud\u2217\u2020\njames robert lloyd\u2217\u2020\nroger grosse\u2021\njoshua b. tenenbaum\u2021\nzoubin ghahramani\u2020\n\nabstract\n\ndespite its importance, choosing the struc-\ntural form of the kernel\nin nonparametric\nregression remains a black art. we de\ufb01ne\na space of kernel structures which are built\ncompositionally by adding and multiplying a\nsmall number of base kernels. we present a\nmethod for searching over this space of struc-\ntures which mirrors the scienti\ufb01c discovery\nprocess. the learned structures can often\ndecompose functions into interpretable com-\nponents and enable long-range extrapolation\non time-series datasets. our structure search\nmethod outperforms many widely used ker-\nnels and kernel combination methods on a\nvariety of prediction tasks.\n\n1. introduction\n\nkernel-based nonparametric models, such as support\nvector machines and gaussian processes (gps), have\nbeen one of the dominant paradigms for supervised\nmach", "article\n\nhttps://doi.org/10.1038/s41467-019-08350-7\n\nopen\n\nmedial geniculate body and primary auditory\ncortex differentially contribute to striatal sound\nrepresentations\nliang chen1, xinxing wang1, shaoyu ge1 & qiaojie xiong\n\n1\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\nthe dorsal striatum has emerged as a key region in sensory-guided, reward-driven decision\nmaking. a posterior sub-region of the dorsal striatum, the auditory striatum, receives con-\nvergent projections from both auditory thalamus and auditory cortex. how these pathways\ncontribute to auditory striatal activity and function remains largely unknown. here we show\nthat chemogenetic inhibition of the projections from either the medial geniculate body (mgb)\nor primary auditory cortex (acx) to auditory striatum in mice impairs performance in an\nauditory frequency discrimination task. while recording striatal sound responses, we \ufb01nd\nthat transiently silencing the mgb projection reduced sound responses across a wide-range\nof frequencies in ", "2\n2\n0\n2\n\n \n\nb\ne\nf\n7\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n7\n8\n6\n0\n0\n\n.\n\n4\n0\n9\n1\n:\nv\ni\nx\nr\na\n\non the power and limitations of random features\n\nfor understanding neural networks\n\nohad shamir\ngilad yehudai\nweizmann institute of science\n\n{gilad.yehudai,ohad.shamir}@weizmann.ac.il\n\nabstract\n\nrecently, a spate of papers have provided positive theoretical results for training over-parameterized\nneural networks (where the network size is larger than what is needed to achieve low error). the key\ninsight is that with suf\ufb01cient over-parameterization, gradient-based methods will implicitly leave some\ncomponents of the network relatively unchanged, so the optimization dynamics will behave as if those\ncomponents are essentially \ufb01xed at their initial random values. in fact, \ufb01xing these explicitly leads to the\nwell-known approach of learning with random features (e.g. [26, 28]). in other words, these techniques\nimply that we can successfully learn with neural networks, whenever we can successfully learn w", "dimensionality reduction of calcium-imaged \nneuronal population activity\n\nhttps://doi.org/10.1038/s43588-022-00390-2\n\nreceived: 10 march 2022\n\naccepted: 5 december 2022\n\npublished online: 29 december 2022\n\n check for updates\n\ntze hui koh1,2, william e. bishop2,3,4, takashi kawashima4,5, brian b. jeon1,2, \nranjani srinivasan1,6, yu mu\u2009\nmisha b. ahrens\u2009\n\n \u20097, ziqiang wei4, sandra j. kuhlman\u2009\n\n, steven m. chase\u2009\n\n & byron m. yu\u2009\n\n \u20098,9, \n \u20091,8,10,11 \n\n \u20091,8,11 \n\n \u20094 \n\ncalcium imaging has been widely adopted for its ability to record from large \nneuronal populations. to summarize the time course of neural activity, \ndimensionality-reduction methods, which have been applied extensively to \npopulation spiking activity, may be particularly useful. however, it is unclear \nwhether the dimensionality-reduction methods applied to spiking activity \nare appropriate for calcium imaging. we thus carried out a systematic \nstudy of design choices based on standard dimensionality-reduction \nmethods. we ", "the heavily connected brain\n\nreview summary\n\ncortical high-density counterstream \narchitectures\n\nnikola t. markov, m\u00e1ria ercsey-ravasz, david c. van essen, kenneth knoblauch, \nzolt\u00e1n toroczkai,* henry kennedy*\n\nread the full article online\nhttp://dx.doi.org/10.1126/science.1238406\n\ncite this article as n. t. markov et al., \nscience 342, 1238406 (2013). \ndoi: 10.1126/science.1238406\n\nbackground: the cerebral cortex is divisible into many individual areas, each exhibiting distinct \nconnectivity pro\ufb01 les, architecture, and physiological characteristics. interactions among cortical \nareas underlie higher sensory, motor, and cognitive functions. graph theory provides an important \nframework for understanding network properties of the interareal weighted and directed connectiv-\nity matrix reported in recent studies.\n\nadvances: we derive an exponential distance rule that predicts many binary and weighted features \nof the cortical network, including ef\ufb01 ciency of information transfer, the high", "the role of acetylcholine in learning and memory\nmichael e hasselmo\n\npharmacological data clearly indicate that both muscarinic and\nnicotinic acetylcholine receptors have a role in the encoding of\nnew memories. localized lesions and antagonist infusions\ndemonstrate the anatomical locus of these cholinergic effects,\nand computational modeling links the function of cholinergic\nmodulation to speci\ufb01c cellular effects within these regions.\nacetylcholine has been shown to increase the strength of\nafferent input relative to feedback, to contribute to theta rhythm\noscillations, activate intrinsic mechanisms for persistent\nspiking, and increase the modi\ufb01cation of synapses. these\neffects might enhance different types of encoding in different\ncortical structures. in particular, the effects in entorhinal and\nperirhinal cortex and hippocampus might be important for\nencoding new episodic memories.\n\naddresses\ncenter for memory and brain, boston university, 2 cummington street,\nboston, ma 02215, usa\n\n", "deep boltzmann machines\n\nruslan salakhutdinov\n\ndepartment of computer science\n\nuniversity of toronto\n\nrsalakhu@cs.toronto.edu\n\ngeoffrey hinton\n\ndepartment of computer science\n\nuniversity of toronto\nhinton@cs.toronto.edu\n\nabstract\n\nwe present a new learning algorithm for boltz-\nmann machines that contain many layers of hid-\nden variables. data-dependent expectations are\nestimated using a variational approximation that\ntends to focus on a single mode, and data-\nindependent expectations are approximated us-\ning persistent markov chains. the use of two\nquite different techniques for estimating the two\ntypes of expectation that enter into the gradient\nof the log-likelihood makes it practical to learn\nboltzmann machines with multiple hidden lay-\ners and millions of parameters. the learning can\nbe made more ef\ufb01cient by using a layer-by-layer\n\u201cpre-training\u201d phase that allows variational in-\nference to be initialized with a single bottom-\nup pass. we present results on the mnist and\nnorb datase", "pattern recognition 46 (2013) 2175\u20132186\n\ncontents lists available at sciverse sciencedirect\n\npattern recognition\n\njournal homepage: www.elsevier.com/locate/pr\n\npassage method for nonlinear dimensionality reduction of data\non multi-cluster manifolds\n\ndeyu meng a,n, yee leung b, zongben xu a\na institute for information and system sciences and ministry of education key lab for intelligent networks and network security, xi\u2019an jiaotong university, xi\u2019an 710049, pr china\nb department of geography and resource management, the chinese university of hong kong, hong kong, pr china\n\na r t i c l e i n f o\n\na b s t r a c t\n\narticle history:\nreceived 5 august 2011\nreceived in revised form\n27 november 2012\naccepted 24 january 2013\navailable online 8 february 2013\n\nkeywords:\nmanifold learning\nmulti-cluster manifolds\nnonlinear dimensionality reduction\npassage method\n\nnonlinear dimensionality reduction of data lying on multi-cluster manifolds is a crucial issue in\nmanifold learning research. an effectiv", "t i m e l i n e\n\nfrom the neuron doctrine to  \nneural networks\n\nbe a useful paradigm, or act as guideposts, to \nunderstand many brain computations. this \narticle does not provide an exhaustive review \nbut instead illustrates with a small number \nof examples the transition between these two \nparadigms of neuroscience.\n\nrafael yuste\n\nabstract | for over a century, the neuron doctrine \u2014 which states that the neuron \nis the structural and functional unit of the nervous system \u2014 has provided a \nconceptual foundation for neuroscience. this viewpoint reflects its origins in a time \nwhen the use of single-neuron anatomical and physiological techniques was \nprominent. however, newer multineuronal recording methods have revealed that \nensembles of neurons, rather than individual cells, can form physiological units and \ngenerate emergent functional properties and states. as a new paradigm for \nneuroscience, neural network models have the potential to incorporate knowledge \nacquired with single-ne", "neuron, vol. 46, 681\u2013692, may 19, 2005, copyright \u00a92005 by elsevier inc. doi 10.1016/j.neuron.2005.04.026\n\nuncertainty, neuromodulation, and attention\n\nangela j. yu* and peter dayan\ngatsby computational neuroscience unit\n17 queen square\nlondon wc1n 3ar\nunited kingdom\n\nsummary\n\nuncertainty in various forms plagues our interactions\nwith the environment. in a bayesian statistical frame-\nwork, optimal inference and prediction, based on un-\nreliable observations in changing contexts, require\nthe representation and manipulation of different\nforms of uncertainty. we propose that the neuro-\nmodulators acetylcholine and norepinephrine play a\nmajor role in the brain\u2019s implementation of these un-\ncertainty computations. acetylcholine signals ex-\npected uncertainty, coming from known unreliability\nof predictive cues within a context. norepinephrine\nsignals unexpected uncertainty, as when unsignaled\ncontext switches produce strongly unexpected ob-\nservations. these uncertainty signals interact to e", "8\n1\n0\n2\n\n \nr\np\na\n0\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n3\n9\n8\n6\n0\n\n.\n\n4\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nastudyonoverfittingindeepreinforcementlearningchiyuanzhangchiyuan@google.comoriolvinyalsvinyals@google.comremimunosmunos@google.comsamybengiobengio@google.comabstractrecentyearshavewitnessedsignificantprogressesindeepreinforcementlearning(rl).empoweredwithlargescaleneuralnetworks,carefullydesignedarchitectures,noveltrainingalgorithmsandmassivelyparallelcomputingdevices,researchersareabletoattackmanychallengingrlproblems.however,inmachinelearning,moretrainingpowercomeswithapotentialriskofmoreoverfitting.asdeeprltechniquesarebeingappliedtocriticalproblemssuchashealthcareandfinance,itisimportanttounderstandthegeneralizationbehaviorsofthetrainedagents.inthispaper,weconductasystematicstudyofstandardrlagentsandfindthattheycouldoverfitinvariousways.moreover,overfittingcouldhappen\u201crobustly\u201d:commonlyusedtechniquesinrlthataddstochasticitydonotnecessarilypreventordetectoverfitting.inparticular,thesameagentsand", "training deep spiking neural networks using\n\nbackpropagation\n\njun haeng lee\u2217\u2020, tobi delbruck\u2020, michael pfeiffer\u2020\n\n\u2217samsung advanced institute of technology, samsung electronics\n\u2020institute of neuroinformatics, university of zurich and eth zurich\n\njunhaeng2.lee@samsung.com\n\n{tobi, pfeiffer}@ini.uzh.ch\n\n6\n1\n0\n2\n\n \n\ng\nu\na\n1\n3\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n2\n8\n7\n8\n0\n\n.\n\n8\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\ndeep spiking neural networks (snns) hold great potential for improving the latency and energy\nef\ufb01ciency of deep neural networks through event-based computation. however, training such\nnetworks is dif\ufb01cult due to the non-differentiable nature of asynchronous spike events. in this\npaper, we introduce a novel technique, which treats the membrane potentials of spiking neurons\nas differentiable signals, where discontinuities at spike times are only considered as noise. this\nenables an error backpropagation mechanism for deep snns, which works directly on spike\nsignals and membrane potentials. thus,", "5\n1\n0\n2\n\n \n\nv\no\nn\n0\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n0\n8\n6\n7\n0\n\n.\n\n7\n0\n5\n1\n:\nv\ni\nx\nr\na\n\ntraining recurrent networks online\n\nwithout backtracking\n\nyann ollivier\n\ncorentin tallec\n\nguillaume charpiat\n\nabstract\n\nwe introduce the \u201cnobacktrack\u201d algorithm to train the parameters\nof dynamical systems such as recurrent neural networks. this algorithm\nworks in an online, memoryless setting, thus requiring no backpropaga-\ntion through time, and is scalable, avoiding the large computational\nand memory cost of maintaining the full gradient of the current state\nwith respect to the parameters.\n\nthe algorithm essentially maintains, at each time, a single search\ndirection in parameter space. the evolution of this search direction\nis partly stochastic and is constructed in such a way to provide, at\nevery time, an unbiased random estimate of the gradient of the loss\nfunction with respect to the parameters. because the gradient estimate\nis unbiased, on average over time the parameter is updated as it shou", "nmda receptor-dependent long-term\npotentiation and long-term depression\n(ltp/ltd)\n\nchristian lu\u00a8 scher1 and robert c. malenka2\n\n1department of basic neurosciences and clinic of neurology, university of geneva and geneva university\nhospital, 1211 geneva, switzerland\n2nancy pritzker laboratory, department of psychiatry and behavioral sciences, stanford university school of\nmedicine, palo alto, california 94305-5453\n\ncorrespondence: christian.luscher@unige.ch and malenka@stanford.edu\n\nlong-term potentiation and long-term depression (ltp/ltd) can be elicited by activating\nn-methyl-d-aspartate (nmda)-type glutamate receptors, typically by the coincident activity\nof pre- and postsynaptic neurons. the early phases of expression are mediated by a redistri-\nbution of ampa-type glutamate receptors: more receptors are added to potentiate the\nsynapse or receptors are removed to weaken synapses. with time, structural changes\nbecome apparent, which in general require the synthesis of new proteins. t", "neuron\n\nperspective\n\nhow does the brain solve\nvisual object recognition?\n\njames j. dicarlo,1,* davide zoccolan,2 and nicole c. rust3\n1department of brain and cognitive sciences and mcgovern institute for brain research, massachusetts institute of technology,\ncambridge, ma 02139, usa\n2cognitive neuroscience and neurobiology sectors, international school for advanced studies (sissa), trieste, 34136, italy\n3department of psychology, university of pennsylvania, philadelphia, pa 19104, usa\n*correspondence: dicarlo@mit.edu\ndoi 10.1016/j.neuron.2012.01.010\n\nmounting evidence suggests that \u2018core object recognition,\u2019 the ability to rapidly recognize objects despite\nsubstantial appearance variation, is solved in the brain via a cascade of re\ufb02exive, largely feedforward\ncomputations that culminate in a powerful neuronal representation in the inferior temporal cortex. however,\nthe algorithm that produces this solution remains poorly understood. here we review evidence ranging\nfrom individual neuron", "7\n1\n0\n2\n\n \n\nb\ne\nf\n7\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n7\n4\n2\n2\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\npublishedasaconferencepaperaticlr2017q-prop:sample-efficientpolicygradientwithanoff-policycriticshixianggu123,timothylillicrap4,zoubinghahramani16,richarde.turner1,sergeylevine35sg717@cam.ac.uk,countzero@google.com,zoubin@eng.cam.ac.uk,ret26@cam.ac.uk,svlevine@eecs.berkeley.edu1universityofcambridge,uk2maxplanckinstituteforintelligentsystems,t\u00a8ubingen,germany3googlebrain,usa4deepmind,uk5ucberkeley,usa6uberailabs,usaabstractmodel-freedeepreinforcementlearning(rl)methodshavebeensuccessfulinawidevarietyofsimulateddomains.however,amajorobstaclefacingdeeprlintherealworldistheirhighsamplecomplexity.batchpolicygradientmethodsofferstablelearning,butatthecostofhighvariance,whichoftenrequireslargebatches.td-stylemethods,suchasoff-policyactor-criticandq-learning,aremoresample-ef\ufb01cientbutbiased,andoftenrequirecostlyhyperparametersweepstostabilize.inthiswork,weaimtodevelopmethodsthatcombinethestabilityofpolicyg", "0\n2\n0\n2\n\n \nr\na\n\nm\n3\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n4\nv\n4\n6\n1\n2\n0\n\n.\n\n2\n1\n9\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2020\n\nplug and play language models: a simple\napproach to controlled text generation\n\nsumanth dathathri \u2217\u2217\ncms, caltech\n\nandrea madotto \u2217\nhkust\n\njanice lan\nuber ai\n\neric frank\nuber ai\n\npiero molino\nuber ai\n\njason yosinski \u2020\u2020\nuber ai\n\njane hung\nuber ai\n\nrosanne liu \u2020\nuber ai\n\ndathathris@gmail.com, amadotto@connect.ust.hk\n{janlan, jane.hung, mysterefrank, piero, yosinski, rosanne}@uber.com\n\nabstract\n\nlarge transformer-based language models (lms) trained on huge text corpora\nhave shown unparalleled generation capabilities. however, controlling attributes\nof the generated language (e.g. switching topic or sentiment) is dif\ufb01cult without\nmodifying the model architecture or \ufb01ne-tuning on attribute-speci\ufb01c data and en-\ntailing the signi\ufb01cant cost of retraining. we propose a simple alternative: the plug\nand play language model (pplm) for controllable language generation,", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nbrain\u2013computer  interfaces  for  dissecting  cognitive\nprocesses  underlying  sensorimotor  control\nmatthew  d  golub1,3,  steven  m  chase2,3,  aaron  p  batista3,4,5\nand  byron  m  yu1,2,3\n\nabstract\n\nsensorimotor  control  engages  cognitive  processes  such  as\nprediction,  learning,  and  multisensory  integration.\nunderstanding  the  neural  mechanisms  underlying  these\ncognitive  processes  with  arm  reaching  is  challenging  because\nwe  currently  record  only  a  fraction  of  the  relevant  neurons,  the\narm  has  nonlinear  dynamics,  and  multiple  modalities  of  sensory\nfeedback  contribute  to  control.  a  brain\u2013computer  interface\n(bci)  is  a  well-de\ufb01ned  sensorimotor  loop  with  key  simplifying\nadvantages  that  address  each  of  these  challenges,  while\nengaging  similar  cognitive  processes.  as  a  result,  bci  is\nbecoming  recognized  as  a  powerful  tool  for  basic  scienti\ufb01c\nstudies  of  se", "letter\nfully integrated silicon probes for high-density \nrecording of neural activity\n\ndoi:10.1038/nature24636\n\njames j. jun1*, nicholas a. steinmetz2,3,4*, joshua h. siegle5*, daniel j. denman5*, marius bauza6,7*, brian barbarits1*, \nalbert k. lee1*, costas a. anastassiou5,8, alexandru andrei9, \u00e7a\u011fatay ayd\u0131n10,11, mladen barbic1, timothy j. blanche5,12, \nvincent bonin9,10,11,13, jo\u00e3o couto10,11, barundeb dutta9, sergey l. gratiy5, diego a. gutnisky1, michael h\u00e4usser3,14, bill karsh1, \npeter ledochowitsch5, carolina mora lopez9, catalin mitelut5,8, silke musa9, michael okun2,3,15, marius pachitariu2,3, \njan putzeys9, p. dylan rich1, cyrille rossant2,3, wei-lung sun1, karel svoboda1, matteo carandini4, kenneth d. harris2,3, \nchristof koch5, john o\u2019keefe6,7 & timothy d. harris1\n\nsensory, motor and cognitive operations involve the coordinated \naction of large neuronal populations across multiple brain regions \nin both superficial and deep structures1,2. existing extracellular \nprobes reco", "communication dynamics in complex \nbrain networks\n\nandrea avena-koenigsberger1, bratislav misic2 and olaf sporns1,3\nabstract | neuronal signalling and communication underpin virtually all aspects of brain activity \nand function. network science approaches to modelling and analysing the dynamics of \ncommunication on networks have proved useful for simulating functional brain connectivity and \npredicting emergent network states. this review surveys important aspects of communication \ndynamics in brain networks. we begin by sketching a conceptual framework that views \ncommunication dynamics as a necessary link between the empirical domains of structural and \nfunctional connectivity. we then consider how different local and global topological attributes of \nstructural networks support potential patterns of network communication, and how the \ninteractions between network topology and dynamic models can provide additional insights and \nconstraints. we end by proposing that communication dyna", "a r t i c l e s\n\ncardinal rules: visual orientation perception reflects \nknowledge of environmental statistics\nahna r girshick1,2, michael s landy1,2 & eero p simoncelli1\u20134\nhumans are good at performing visual tasks, but experimental measurements have revealed substantial biases in the perception \nof basic visual attributes. an appealing hypothesis is that these biases arise through a process of statistical inference, in which \ninformation from noisy measurements is fused with a probabilistic model of the environment. however, such inference is optimal \nonly if the observer\u2019s internal model matches the environment. we found this to be the case. we measured performance in \nan orientation-estimation task and found that orientation judgments were more accurate at cardinal (horizontal and vertical) \norientations. judgments made under conditions of uncertainty were strongly biased toward cardinal orientations. we estimated \nobservers\u2019 internal models for orientation and found that they matc", "pruning of memories by context-based prediction error\n\nghootae kima,b, jarrod a. lewis-peacockc,d, kenneth a. normana,b, and nicholas b. turk-brownea,b,1\n\nadepartment of psychology and bprinceton neuroscience institute, princeton university, princeton, nj 08544; and cdepartment of psychology and dinstitute\nfor neuroscience, university of texas at austin, austin, tx 78712\n\nedited by daniel l. schacter, harvard university, cambridge, ma, and approved may 8, 2014 (received for review october 16, 2013)\n\nthe capacity of long-term memory is thought to be virtually\nunlimited. however, our memory bank may need to be pruned\nregularly to ensure that the information most important for\nbehavior can be stored and accessed efficiently. using functional\nmagnetic resonance imaging of the human brain, we report the\ndiscovery of a context-based mechanism for determining which\nmemories to prune. specifically, when a previously experienced\ncontext is reencountered, the brain automatically generates\npredic", "discriminative clustering by regularized\n\ninformation maximization\n\nryan gomes\n\ngomes@vision.caltech.edu\n\nandreas krause\n\nkrausea@caltech.edu\n\npietro perona\n\nperona@vision.caltech.edu\n\ncalifornia institute of technology\n\npasadena, ca 91106\n\nabstract\n\nis there a principled way to learn a probabilistic discriminative classi\ufb01er from an\nunlabeled data set? we present a framework that simultaneously clusters the data\nand trains a discriminative classi\ufb01er. we call it regularized information maxi-\nmization (rim). rim optimizes an intuitive information-theoretic objective func-\ntion which balances class separation, class balance and classi\ufb01er complexity. the\napproach can \ufb02exibly incorporate different likelihood functions, express prior as-\nsumptions about the relative size of different classes and incorporate partial labels\nfor semi-supervised learning. in particular, we instantiate the framework to un-\nsupervised, multi-class kernelized logistic regression. our empirical evaluation\nindicates ", "gradient descent converges to minimizers\n\njason d. lee\u266f\n\n, max simchowitz\u266f, michael i. jordan\u266f\u2020, and benjamin recht\u266f\u2020\n\n\u266fdepartment of electrical engineering and computer sciences\n\n6\n1\n0\n2\n\n \nr\na\n\nm\n4\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n5\n1\n9\n4\n0\n\n.\n\n2\n0\n6\n1\n:\nv\ni\nx\nr\na\n\n\u2020department of statistcs\n\nuniversity of california, berkeley\n\nmarch 7, 2016\n\nabstract\n\nwe show that gradient descent converges to a local minimizer, almost surely with random initializa-\n\ntion. this is proved by applying the stable manifold theorem from dynamical systems theory.\n\nkeywords: gradient descent, smooth optimization, saddle points, local minimum, dynamical systems.\n\n1 introduction\n\nsaddle points have long been regarded as a tremendous obstacle for continuous optimization. there are\nmany well known examples when worst case initialization of gradient descent provably converge to saddle\npoints [20, section 1.2.3], and hardness results which show that \ufb01nding even a local minimizer of non-\nconvex functions is np-h", "0\n0\n0\n2\n\n \nr\np\na\n4\n2\n\n \n\n \n \n]\nn\na\n-\na\nt\na\nd\n\n.\ns\nc\ni\ns\ny\nh\np\n[\n \n \n\n1\nv\n7\n5\n0\n4\n0\n0\n0\n/\ns\nc\ni\ns\ny\nh\np\n:\nv\ni\nx\nr\na\n\nthe information bottleneck method\n\nnaftali tishby,1,2 fernando c. pereira,3 and william bialek1\n\n1nec research institute, 4 independence way\nprinceton, new jersey 08540\n2institute for computer science, and\ncenter for neural computation\nhebrew university\njerusalem 91904, israel\n3at&t shannon laboratory\n180 park avenue\nflorham park, new jersey 07932\n\n30 september 1999\n\nwe de\ufb01ne the relevant information in a signal x \u2208 x as being the in-\nformation that this signal provides about another signal y \u2208 y . examples\ninclude the information that face images provide about the names of the peo-\nple portrayed, or the information that speech sounds provide about the words\nspoken. understanding the signal x requires more than just predicting y, it\nalso requires specifying which features of x play a role in the prediction. we\nformalize this problem as that of \ufb01nding a short code for x th", "survey of spiking in the mouse visual system \nreveals functional hierarchy\n\nhttps://doi.org/10.1038/s41586-020-03171-x\nreceived: 23 december 2019\naccepted: 9 december 2020\npublished online: 20 january 2021\n\n check for updates\n\njoshua h. siegle1,6\u2009\u2709, xiaoxuan jia1,6\u2009\u2709, s\u00e9verine durand1, sam gale1, corbett bennett1, \nnile graddis1, greggory heller1, tamina k. ramirez1, hannah choi1,2, jennifer a. luviano1, \npeter a. groblewski1, ruweida ahmed1, anton arkhipov1, amy bernard1, yazan n. billeh1, \ndillan brown1, michael a. buice1, nicolas cain1, shiella caldejon1, linzy casal1, andrew cho1, \nmaggie chvilicek1, timothy c. cox3, kael dai1, daniel j. denman1,4, saskia e. j. de vries1, \nroald dietzman1, luke esposito1, colin farrell1, david feng1, john galbraith1, marina garrett1, \nemily c. gelfand1, nicole hancock1, julie a. harris1, robert howard1, brian hu1, ross hytnen1, \nramakrishnan iyer1, erika jessett1, katelyn johnson1, india kato1, justin kiggins1, \nsophie lambert1, jerome lecoq1, pete", "research\n\nthe graphical brain: belief propagation\n\nand active inference\n\nkarl j. friston1, thomas parr1, and bert de vries2,3\n\n1wellcome trust centre for neuroimaging, institute of neurology, university college london, united kingdom\n\n2eindhoven university of technology, department of electrical engineering, eindhoven, the netherlands\n\n3gn hearing, eindhoven, the netherlands\n\nkeywords: bayesian, neuronal, connectivity, factor graphs, free energy, belief propagation,\nmessage passing\n\na n o p e n a c c e s s\n\nj o u r n a l\n\nabstract\n\nthis paper considers functional integration in the brain from a computational perspective.\nwe ask what sort of neuronal message passing is mandated by active inference\u2014and what\nimplications this has for context-sensitive connectivity at microscopic and macroscopic\nlevels. in particular, we formulate neuronal processing as belief propagation under deep\ngenerative models. crucially, these models can entertain both discrete and continuous states,\nleading to dis", "scalarized multi-objective reinforcement learning:\n\nnovel design techniques\n\nkristof van moffaert\n\nmadalina m. drugan\n\nann now\u00b4e\n\ndepartment of computer science\n\ndepartment of computer science\n\ndepartment of computer science\n\nvrije universiteit brussel\n\nvrije universiteit brussel\n\nvrije universiteit brussel\n\npleinlaan 2, 1050 brussels, belgium\n\npleinlaan 2, 1050 brussels, belgium\n\npleinlaan 2, 1050 brussels, belgium\n\nemail: kvmoffae@vub.ac.be\n\nemail: mdrugan@vub.ac.be\n\nemail: anowe@vub.ac.be\n\nabstract\u2014in multi-objective problems, it is key to \ufb01nd com-\npromising solutions that balance different objectives. the linear\nscalarization function is often utilized to translate the multi-\nobjective nature of a problem into a standard, single-objective\nproblem. generally, it is noted that such as linear combination\ncan only \ufb01nd solutions in convex areas of the pareto front,\ntherefore making the method inapplicable in situations where\nthe shape of the front is not known beforehand, as is often\nth", "decoding cognition from spontaneous \nneural activity\n\n 1,3,4,7\n\n 3,5, timothy\u00a0e.\u00a0j.\u00a0behrens4,6 \n\n 1,2,3\u2009\u2709, matthew\u00a0m.\u00a0nour3,4, nicolas\u00a0w.\u00a0schuck \n\nyunzhe\u00a0liu \nand raymond\u00a0j.\u00a0dolan \nabstract | in human neuroscience, studies of cognition are rarely grounded in non- task- evoked, \n\u2018spontaneous\u2019 neural activity. indeed, studies of spontaneous activity tend to focus predominantly \non intrinsic neural patterns (for example, resting- state networks). taking a \u2018representation- rich\u2019 \napproach bridges the gap between cognition and resting- state communities: this approach  \nrelies on decoding task- related representations from spontaneous neural activity, allowing \nquantification of the representational content and rich dynamics of such activity. for example,  \nif we know the neural representation of an episodic memory, we can decode its subsequent \nreplay during rest. we argue that such an approach advances cognitive research beyond a focus \non immediate task demand and provides insight into t", "2\n2\n0\n2\n\n \n\ny\na\nm\n0\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n8\n9\n1\n5\n0\n\n.\n\n5\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nreducing activation recomputation\n\nin large transformer models\n\nvijay korthikanti, jared casper, sangkug lym, lawrence mcafee, michael andersch,\n\nmohammad shoeybi, and bryan catanzaro\n\nnvidia\n\nabstract\n\ntraining large transformer models is one of the most important computational challenges of\nmodern ai. in this paper, we show how to signi\ufb01cantly accelerate training of large transformer\nmodels by reducing activation recomputation. activation recomputation is commonly used to work\naround memory capacity constraints. rather than storing activations for backpropagation, they\nare traditionally recomputed, which saves memory but adds redundant compute. in this work, we\nshow most of this redundant compute is unnecessary because we can reduce memory consumption\nsu\ufb03ciently without it. we present two novel yet very simple techniques: sequence parallelism\nand selective activation recomputation. in conjunction", "8\n1\n0\n2\n\n \n\nn\na\nj\n \n\n0\n3\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n5\n7\n4\n8\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\novercoming catastrophic forgetting by\n\nincremental moment matching\n\nsang-woo lee1, jin-hwa kim1, jaehyun jun1, jung-woo ha2, and byoung-tak zhang1,3\n\nseoul national university1\n\nclova ai research, naver corp2\n\nsurromind robotics3\n\n{slee,jhkim,jhjun}@bi.snu.ac.kr jungwoo.ha@navercorp.com\n\nbtzhang@bi.snu.ac.kr\n\nabstract\n\ncatastrophic forgetting is a problem of neural networks that loses the information\nof the \ufb01rst task after training the second task. here, we propose a method, i.e. in-\ncremental moment matching (imm), to resolve this problem. imm incrementally\nmatches the moment of the posterior distribution of the neural network which is\ntrained on the \ufb01rst and the second task, respectively. to make the search space\nof posterior parameter smooth, the imm procedure is complemented by various\ntransfer learning techniques including weight transfer, l2-norm of the old and the\nnew parameter, and a varian", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/40042593\n\nresponse acquisition by humans with delayed reinforcement\n\narticle\u00a0\u00a0in\u00a0\u00a0journal of the experimental analysis of behavior \u00b7 may 2009\n\ndoi: 10.1901/jeab.2009.91-377\u00a0\u00b7\u00a0source: pubmed\n\nreads\n71\n\ncitations\n20\n\n1 author:\n\nhiroto okouchi\nosaka kyoiku university\n\n51 publications\u00a0\u00a0\u00a0359 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nall content following this page was uploaded by hiroto okouchi on 30 october 2014.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "psychological review\n1990, vot97, no. 3, 332-361\n\ncopyright 1990 by the american psychological association, inc.\n0033-295x/90/m0.75\n\non the control of automatic processes: a parallel  distributed\n\nprocessing account of the stroop  effect\n\njonathan d. cohen\n\ncarnegie mellon university\n\nuniversity of pittsburgh\n\nstanford university\n\nkevin dunbar\nmcgill university\n\njames l. mcclelland\ncarnegie mellon university\n\ntraditional views of automatic!ty are in need of revision. for example, automaticity often has been\ntreated  as an all-or-none phenomenon, and traditional  theories  have held that automatic processes\nare independent of attention. yet recent empirical data suggest that automatic processes are continu-\nous, and furthermore are subject to attentional control. a model of attention is presented to  address\nthese issues. within a parallel distributed processing framework, it is proposed  that the attributes of\nautomaticity depend on the strength of a processing pathway and that strengt", "root mean square layer normalization\n\nbiao zhang1 rico sennrich2,1\n\n1school of informatics, university of edinburgh\n\n2institute of computational linguistics, university of zurich\n\nb.zhang@ed.ac.uk, sennrich@cl.uzh.ch\n\n9\n1\n0\n2\n\n \nt\nc\no\n6\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n7\n6\n4\n7\n0\n\n.\n\n0\n1\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nlayer normalization (layernorm) has been successfully applied to various deep\nneural networks to help stabilize training and boost model convergence because\nof its capability in handling re-centering and re-scaling of both inputs and weight\nmatrix. however, the computational overhead introduced by layernorm makes\nthese improvements expensive and signi\ufb01cantly slows the underlying network, e.g.\nrnn in particular. in this paper, we hypothesize that re-centering invariance in\nlayernorm is dispensable and propose root mean square layer normalization, or\nrmsnorm. rmsnorm regularizes the summed inputs to a neuron in one layer ac-\ncording to root mean square (rms), giving the model r", "trust region policy optimization\n\n7\n1\n0\n2\n\n \nr\np\na\n0\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n7\n7\n4\n5\n0\n\n.\n\n2\n0\n5\n1\n:\nv\ni\nx\nr\na\n\njohn schulman\nsergey levine\nphilipp moritz\nmichael jordan\npieter abbeel\nuniversity of california, berkeley, department of electrical engineering and computer sciences\n\njoschu@eecs.berkeley.edu\nslevine@eecs.berkeley.edu\npcmoritz@eecs.berkeley.edu\njordan@cs.berkeley.edu\npabbeel@cs.berkeley.edu\n\nabstract\n\nwe describe an iterative procedure for optimizing\npolicies, with guaranteed monotonic improve-\nment. by making several approximations to the\ntheoretically-justi\ufb01ed procedure, we develop a\npractical algorithm, called trust region policy\noptimization (trpo). this algorithm is similar\nto natural policy gradient methods and is effec-\ntive for optimizing large nonlinear policies such\nas neural networks. our experiments demon-\nstrate its robust performance on a wide variety\nof tasks: learning simulated robotic swimming,\nhopping, and walking gaits; and playing atari\ngames us", "dueling network architectures for deep reinforcement learning\n\nziyu wang\ntom schaul\nmatteo hessel\nhado van hasselt\nmarc lanctot\nnando de freitas\ngoogle deepmind, london, uk\n\nziyu@google.com\nschaul@google.com\nmtthss@google.com\nhado@google.com\nlanctot@google.com\nnandodefreitas@google.com\n\nabstract\n\nin recent years there have been many successes\nof using deep representations in reinforcement\nlearning. still, many of these applications use\nconventional architectures, such as convolutional\nnetworks, lstms, or auto-encoders. in this pa-\nper, we present a new neural network architec-\nture for model-free reinforcement learning. our\ndueling network represents two separate estima-\ntors: one for the state value function and one for\nthe state-dependent action advantage function.\nthe main bene\ufb01t of this factoring is to general-\nize learning across actions without imposing any\nchange to the underlying reinforcement learning\nalgorithm. our results show that this architec-\nture leads to better policy ", "9\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n1\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n8\n8\n6\n4\n0\n\n.\n\n6\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nan improved analysis of training over-parameterized\n\ndeep neural networks\n\ndifan zou\u2217 and quanquan gu\u2020\n\nabstract\n\na recent line of research has shown that gradient-based algorithms with random initialization\ncan converge to the global minima of the training loss for over-parameterized (i.e., su\ufb03ciently\nwide) deep neural networks. however, the condition on the width of the neural network to\nensure the global convergence is very stringent, which is often a high-degree polynomial in\nthe training sample size n (e.g., o(n24)). in this paper, we provide an improved analysis of the\nglobal convergence of (stochastic) gradient descent for training deep neural networks, which only\nrequires a milder over-parameterization condition than previous work in terms of the training\nsample size and other problem-dependent parameters. the main technical contributions of our\nanalysis include (a) a tighter gradient lower", "biorxiv preprint \n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\nhttps://doi.org/10.1101/2022.12.07.519455\n; \n\nthis version posted december 7, 2022. \n\nthe copyright holder for this preprint\n\ndoi: \n\navailable under a\n\ncc-by 4.0 international license\n.\n\n 1  mega-scale movie-fields in the mouse visuo-hippocampal network chinmay s. purandare1-3\u2020\u2709 and mayank r. mehta2-4\u2709  1 department of bioengineering, ucla, los angeles, ca, usa. 2 w.m. keck center for neurophysics, department of physics and astronomy, ucla, los angeles, ca, usa. 5 3 department of neurology, ucla, los angeles, ca, usa. 4 department of electrical and computer engineering, ucla, los angeles, ca, usa. \u2020 present address \u2013 department of physiology, ucsf, san francisco, ca, usa  \u2709e-mail: chinmay.purandare@gmail.com, mayankmehta@ucla.edu  date of first submission (at science): june 6 2022 10 abstract: natural behavior often invol", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nneural  circuits  as  computational  dynamical  systems\ndavid  sussillo\n\nmany  recent  studies  of  neurons  recorded  from  cortex  reveal\ncomplex  temporal  dynamics.  how  such  dynamics  embody  the\ncomputations  that  ultimately  lead  to  behavior  remains  a\nmystery.  approaching  this  issue  requires  developing  plausible\nhypotheses  couched  in  terms  of  neural  dynamics.  a  tool  ideally\nsuited  to  aid  in  this  question  is  the  recurrent  neural  network\n(rnn).  rnns  straddle  the  \ufb01elds  of  nonlinear  dynamical  systems\nand  machine  learning  and  have  recently  seen  great  advances  in\nboth  theory  and  application.  i  summarize  recent  theoretical  and\ntechnological  advances  and  highlight  an  example  of  how  rnns\nhelped  to  explain  perplexing  high-dimensional\nneurophysiological  data  in  the  prefrontal  cortex.\n\naddresses\ndepartment  of  electrical  engineering  and  neurosciences  pr", "neural tangent kernel:\n\nconvergence and generalization in neural networks\n\narthur jacot\n\n\u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\narthur.jacot@netopera.net\n\nimperial college london and \u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\nfranck gabriel\n\nfranckrgabriel@gmail.com\n\ncl\u00b4ement hongler\n\n\u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\nclement.hongler@gmail.com\n\nabstract\n\nat initialization, arti\ufb01cial neural networks (anns) are equivalent to gaussian\nprocesses in the in\ufb01nite-width limit (12; 9), thus connecting them to kernel methods.\nwe prove that the evolution of an ann during training can also be described by a\nkernel: during gradient descent on the parameters of an ann, the network function\nf\u03b8 (which maps input vectors to output vectors) follows the kernel gradient of the\nfunctional cost (which is convex, in contrast to the parameter cost) w.r.t. a new\nkernel: the neural tangent kernel (ntk). this kernel is central to describe the\ngeneralization features of anns. while the ntk is r", "report\n\nreal-time readout of large-scale unsorted neural\nensemble place codes\n\ngraphical abstract\n\nauthors\n\nsile hu, davide ciliberti,\nandres d. grosmark, ...,\nmatthew a. wilson, fabian kloosterman,\nzhe chen\n\ncorrespondence\nfabian.kloosterman@nerf.be (f.k.),\nzhe.chen@nyulangone.org (z.c.)\n\nin brief\nthe hippocampal and neocortical\nneuronal ensembles encode rich spatial\ninformation in navigation. hu et al.\ndevelop computational techniques that\naccommodate real-time decoding and\nassessment of large-scale unsorted\nneural ensemble place codes during\nrunning behavior and sleep.\n\nhighlights\nd spike-sorting-free decoding reconstructs the rat\u2019s position\n\nwith ultrafast speed\n\nd gpu-powered population decoding signi\ufb01cantly speeds up\n\nmulti-core cpu-based system\n\nd gpu computing empowers real-time assessment of decoded\n\n\u2018\u2018memory replay\u2019\u2019 candidates\n\nd open-source software toolkit supports closed-loop content-\n\ntriggered intervention\n\nhu et al., 2018, cell reports 25, 2635\u20132642\ndecember 4, 2018 \u00aa ", "9\n1\n0\n2\n\n \nl\nu\nj\n \n\n5\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n9\n4\n6\n2\n0\n\n.\n\n7\n0\n9\n1\n:\nv\ni\nx\nr\na\n\njournal of machine learning research vv (20yy) pp\n\nsubmitted 06/2019; published mm/20yy\n\na uni\ufb01ed framework of online learning algorithms for\n\ntraining recurrent neural networks\n\nowen marschall\ncenter for neural science\nnew york university\nnew york, ny 10006, usa\n\nkyunghyun cho\nnew york university\nfacebook ai research\ncifar azrieli global scholar\n\ncristina savin\ncenter for neural science\ncenter for data science\nnew york university\n\neditor:\n\noem214@nyu.edu\n\nkyunghyun.cho@nyu.edu\n\ncsavin@nyu.edu\n\nabstract\n\nwe present a framework for compactly summarizing many recent results in e\ufb03cient and/or\nbiologically plausible online training of recurrent neural networks (rnn). the framework\norganizes algorithms according to several criteria: (a) past vs. future facing, (b) tensor\nstructure, (c) stochastic vs. deterministic, and (d) closed form vs. numerical. these axes\nreveal latent conceptual connections among sev", "6\n1\n0\n2\n\n \nr\na\n\n \n\nm\n1\n2\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n6\n3\n9\n5\n0\n\n.\n\n9\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nstdp as presynaptic activity times rate of\n\nchange of postsynaptic activity\n\nyoshua bengio1, thomas mesnard, asja fischer,\n\nsaizheng zhang, and yuhuai wu\n\nmontreal institute for learning algorithms, university of montreal,\n\nmontreal, qc, h3c 3j7\n1cifar senior fellow\n\nabstract\n\nwe introduce a weight update formula that is expressed only in terms\nof \ufb01ring rates and their derivatives and that results in changes consistent\nwith those associated with spike-timing dependent plasticity (stdp) rules\nand biological observations, even though the explicit timing of spikes is not\nneeded. the new rule changes a synaptic weight in proportion to the prod-\nuct of the presynaptic \ufb01ring rate and the temporal rate of change of activ-\nity on the postsynaptic side. these quantities are interesting for studying\ntheoretical explanation for synaptic changes from a machine learning per-\nspective. in particular, if neural dy", "usa\n\n(cid:88)\n\nj\n\nsupplementary materials: cell-type-speci\ufb01c neuromodulation\nguides synaptic credit assignment in a spiking neural network\n\nyuhan helena liu1,2,3,*, stephen smith2,4, stefan mihalas1,2,3, eric shea-brown1,2,3, and\n\nuygar s\u00fcmb\u00fcl2,*\n\n1department of applied mathematics, university of washington, seattle, wa, usa\n\n2allen institute, 615 westlake ave n, seattle wa, usa\n\n3computational neuroscience center, university of washington, seattle, wa, usa\n4department of molecular and cellular physiology, stanford university, stanford ca,\n\n*correspondence: hyliu24@uw.edu, uygars@alleninstitute.org\n\nsupplementary note 1 \u2013 online modulatory signaling for leaky out-\n\nput\n\nas mentioned in methods, we allow leaky outputs\n\nyk,t = \u03ba yk,t\u22121 +\n\nwout\n\nkj\n\nzj,t + bout\n\nk\n\n,\n\nwhere the output at the current step depends on the previous time step through the leak constant \u03ba.\n\nbased on this leaky output and loss e =(cid:80)\n(cid:88)\n\n\u2202e\n\u2202zj,t\n\n=\n\nk\n\nwou t\n\nkj\n\nt(cid:48)\u2265t\n\nk,t \u2212 yk,t)2, we have the", "task-dependent changes in the large-scale dynamics\nand necessity of cortical regions\n\narticle\n\nhighlights\nd mice navigated in virtual reality while performing one of three\n\nrelated tasks\n\nd all dorsal cortex contributed to memory-dependent but not\n\nvisually guided navigation\n\nd higher task demands induced decorrelation of whole-cortex\n\nca2+ activity patterns\n\nd a modular rnn model suggested that differences in\n\ncomputations can explain results\n\nauthors\n\nlucas pinto, kanaka rajan,\nbrian depasquale,\nstephan y. thiberge, david w. tank,\ncarlos d. brody\n\ncorrespondence\ndwtank@princeton.edu (d.w.t.),\nbrody@princeton.edu (c.d.b.)\n\nin brief\npinto et al. show that more complex\nperceptual decisions engage more\ndiverse and spatially distributed\ncomputations across the cortex than\nsimpler decisions, even when sensory\nstimuli and motor output are held\nconstant.\n\npinto et al., 2019, neuron 104, 810\u2013824\nnovember 20, 2019 \u00aa 2019 elsevier inc.\nhttps://doi.org/10.1016/j.neuron.2019.08.025\n\n\f", "published as a conference paper at iclr 2020\n\nunbiased contrastive divergence algorithm\nfor training energy-based latent variable\nmodels\n\nyixuan qiu\ndepartment of statistics and data science\ncarnegie mellon university\npittsburgh, pa 15213, usa\nyixuanq@andrew.cmu.edu\n\nlingsong zhang & xiao wang\ndepartment of statistics\npurdue university\nwest lafayette, in 47907, usa\n{lingsong, wangxiao}@purdue.edu\n\nabstract\n\nthe contrastive divergence algorithm is a popular approach to training energy-\nbased latent variable models, which has been widely used in many machine learn-\ning models such as the restricted boltzmann machines and deep belief nets. de-\nspite its empirical success, the contrastive divergence algorithm is also known to\nhave biases that severely affect its convergence. in this article we propose an un-\nbiased version of the contrastive divergence algorithm that completely removes its\nbias in stochastic gradient methods, based on recent advances on unbiased markov\nchain monte carlo me", "ageometricperspectiveonoptimalrepresentationsforreinforcementlearningmarcg.bellemare1,willdabney2,robertdadashi1,adrienalitaiga1,3,pablosamuelcastro1,nicolasleroux1,daleschuurmans1,4,torlattimore2,clarelyle5abstractweproposeanewperspectiveonrepresentationlearninginreinforcementlearningbasedongeometricpropertiesofthespaceofvaluefunctions.weleveragethisperspectivetoprovideformalevidenceregardingtheusefulnessofvaluefunctionsasauxiliarytasks.ourformulationconsidersadaptingtherepresentationtomini-mizethe(linear)approximationofthevaluefunctionofallstationarypoliciesforagivenenvironment.weshowthatthisoptimizationreducestomakingaccuratepredictionsregardingaspecialclassofvaluefunctionswhichwecalladversarialvaluefunctions(avfs).wedemonstratethatusingvaluefunctionsasauxiliarytaskscorrespondstoanexpected-errorrelaxationofourformulation,withavfsanaturalcandidate,andidentifyacloserelationshipwithproto-valuefunctions(mahadevan,2005).wehighlightcharacteristicsofavfsandtheirusefulnessasauxiliarytasksin", "beyond backprop: online alternating minimization with auxiliary variables\n\n9\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n5\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n4\nv\n7\n7\n0\n9\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nanna choromanska* 1 benjamin cowen* 1 sadhana kumaravel* 2 ronny luss* 2 mattia rigotti* 2\n\nirina rish* 2 brian kingsbury 2 paolo diachille 2 viatcheslav gurev 2 ravi tejwani 3 djallel bouneffouf 2\n\nabstract\n\ndespite signi\ufb01cant recent advances in deep neural\nnetworks, training them remains a challenge due\nto the highly non-convex nature of the objective\nfunction. state-of-the-art methods rely on error\nbackpropagation, which suffers from several well-\nknown issues, such as vanishing and exploding\ngradients, inability to handle non-differentiable\nnonlinearities and to parallelize weight-updates\nacross layers, and biological implausibility. these\nlimitations continue to motivate exploration of al-\nternative training algorithms, including several re-\ncently proposed auxiliary-variable methods which\nbreak the complex nested obj", "research article\n\nsomato-dendritic synaptic plasticity and\nerror-backpropagation in active dendrites\nmathieu schiess1*, robert urbanczik1*, walter senn1,2*\n\n1 department of physiology, university of bern, bern, switzerland, 2 center for cognition, learning and\nmemory, university of bern, bern, switzerland\n\n* mathieu.schiess@hesge.ch (ms); urbanczik@pyl.unibe.ch (ru); senn@pyl.unibe.ch (ws)\n\na11111\n\nabstract\n\nin the last decade dendrites of cortical neurons have been shown to nonlinearly combine\nsynaptic inputs by evoking local dendritic spikes. it has been suggested that these nonline-\narities raise the computational power of a single neuron, making it comparable to a 2-layer\nnetwork of point neurons. but how these nonlinearities can be incorporated into the synaptic\nplasticity to optimally support learning remains unclear. we present a theoretically derived\nsynaptic plasticity rule for supervised and reinforcement learning that depends on the timing\nof the presynaptic, the dendritic a", "a transcriptomic axis predicts state \nmodulation of cortical interneurons\n\nhttps://doi.org/10.1038/s41586-022-04915-7\nreceived: 26 october 2021\naccepted: 27 may 2022\npublished online: 6 july 2022\nopen access\n\n check for updates\n\nst\u00e9phane bugeon1\u2009\u2709, joshua duffield1, mario dipoppa1,2, anne ritoux1, isabelle prankerd1, \ndimitris nicoloutsopoulos1, david orme1, maxwell shinn1, han peng3, hamish forrest1, \naiste viduolyte1, charu bai reddy1,4, yoh isogai5, matteo carandini4 & kenneth d. harris1\u2009\u2709\n\ntranscriptomics has revealed that cortical inhibitory neurons exhibit a great diversity \nof fine molecular subtypes1\u20136, but it is not known whether these subtypes have \ncorrespondingly diverse patterns of activity in the living brain. here we show that \ninhibitory subtypes in primary visual cortex (v1) have diverse correlates with brain \nstate, which are organized by a single factor: position along the main axis of \ntranscriptomic variation. we combined in\u00a0vivo two-photon calcium imaging of mouse", "7\n1\n0\n2\n\n \nl\nu\nj\n \n\n5\n2\n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n3\nv\n2\n2\n1\n3\n0\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nconvolutionalsequencetosequencelearningjonasgehringmichaelaulidavidgrangierdenisyaratsyannn.dauphinfacebookairesearchabstracttheprevalentapproachtosequencetosequencelearningmapsaninputsequencetoavariablelengthoutputsequenceviarecurrentneuralnet-works.weintroduceanarchitecturebaseden-tirelyonconvolutionalneuralnetworks.1com-paredtorecurrentmodels,computationsoverallelementscanbefullyparallelizedduringtrainingtobetterexploitthegpuhardwareandoptimiza-tioniseasiersincethenumberofnon-linearitiesis\ufb01xedandindependentoftheinputlength.ouruseofgatedlinearunitseasesgradientpropaga-tionandweequipeachdecoderlayerwithasep-arateattentionmodule.weoutperformtheaccu-racyofthedeeplstmsetupofwuetal.(2016)onbothwmt\u201914english-germanandwmt\u201914english-frenchtranslationatanorderofmagni-tudefasterspeed,bothongpuandcpu.1.introductionsequencetosequencelearninghasbeensuccessfulinmanytaskssuchasmachinetranslation,speechrecogni-tio", "journal of vision (2008) 8(8):11, 1\u201318\n\nhttp://journalofvision.org/8/8/11/\n\n1\n\ntopological analysis of population activity\nin visual cortex\n\ngurjeet singh\n\nfacundo memoli\n\ntigran ishkhanov\n\nguillermo sapiro\n\ngunnar carlsson\n\ndario l. ringach\n\ninstitute for computational and mathematical engineering,\nstanford university, stanford, ca, usa\n\ndepartment of mathematics, stanford university,\nstanford, ca, usa\n\ndepartment of mathematics, stanford university,\nstanford, ca, usa\n\ndepartment of electrical and computer engineering,\nuniversity of minnesota, minnesota, mn, usa\n\ndepartment of mathematics, stanford university,\nstanford, ca, usa\n\ndepartments of neurobiology and psychology,\njules stein eye institute, david geffen school of medicine,\nuniversity of california, los angeles, ca, usa\n\ninformation in the cortex is thought to be represented by the joint activity of neurons. here we describe how fundamental\nquestions about neural representation can be cast in terms of the topological structure ", "9\n1\n0\n2\n\n \nl\nu\nj\n \n\n5\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n4\n7\n3\n6\n0\n\n.\n\n7\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nwhat does it mean to understand a neural network?\n\ntimothy p. lillicrap & konrad p. kording\n\njuly 2019\n\nabstract\n\nwe can de\ufb01ne a neural network that can learn to recognize objects in less than 100 lines\nof code. however, after training, it is characterized by millions of weights that contain the\nknowledge about many object types across visual scenes. such networks are thus dramatically\neasier to understand in terms of the code that makes them than the resulting properties, such\nas tuning or connections. in analogy, we conjecture that rules for development and learning in\nbrains may be far easier to understand than their resulting properties. the analogy suggests\nthat neuroscience would bene\ufb01t from a focus on learning and development.\n\nintroduction\n\nwhen we build networks that solve image recognition at human-like performance [18] or are strong\nat playing the game of go [30] we ended up using a few c", "7\n1\n0\n2\n\n \n\ng\nu\na\n6\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n5\n6\n1\n8\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nsupervised learning based on temporal coding in\n\nspiking neural networks\n\ndepartment of bioengineering, jacobs school of engineering\n\nhesham mostafa\n\ninstitute of neural computation\n\nuc san diego, la jolla, ca 92093 usa\n\nemail: hmmostafa@ucsd.edu\n\nabstract\n\ngradient descent training techniques are remarkably successful in training analog-\nvalued arti\ufb01cial neural networks (anns). such training techniques, however,\ndo not transfer easily to spiking networks due to the spike generation hard non-\nlinearity and the discrete nature of spike communication. we show that in a\nfeedforward spiking network that uses a temporal coding scheme where information\nis encoded in spike times instead of spike rates, the network input-output relation is\ndifferentiable almost everywhere. moreover, this relation is piece-wise linear after\na transformation of variables. methods for training anns thus carry directly to the\nt", "\u2029\n\ninteractions\u2029between\u2029intrinsic\u2029and\u2029stimulus\u00adevoked\u2029\u2029\n\nactivity\u2029in\u2029recurrent\u2029neural\u2029networks\u2029\n\ndepartment\u2029of\u2029physiology\u2029and\u2029cellular\u2029biophysics\u2029\ncolumbia\u2029university\u2029college\u2029of\u2029physicians\u2029and\u2029surgeons\u2029\n\n\u2029\nl.f.\u2029abbott\u2029and\u2029kanaka\u2029rajan\u2029\n\u2029\ndepartment\u2029of\u2029neuroscience\u2029\nnew\u2029york,\u2029ny\u202910032\u20102695\u2029usa\u2029\n\u2029and\u2029\n\u2029\nhaim\u2029sompolinsky\u2029\n\u2029\nracah\u2029institute\u2029of\u2029physics\u2029\nhebrew\u2029university\u2029\njerusalem,\u2029israel\u2029\n\ninterdisciplinary\u2029center\u2029for\u2029neural\u2029computation\u2029\n\nintroduction\u2029\n\n\u2029\u2029\n\u2029trial\u2010to\u2010trial\u2029variability\u2029is\u2029an\u2029essential\u2029feature\u2029of\u2029neural\u2029responses,\u2029but\u2029its\u2029source\u2029is\u2029a\u2029\nsubject\u2029of\u2029active\u2029debate.\u2029\u2029response\u2029variability\u2029(mast\u2029and\u2029victor,\u20291991;\u2029arieli\u2029et\u2029al.,\u2029\n1995\u2029&\u20291996;\u2029anderson\u2029et\u2029al.,\u20292000\u2029&\u20292001;\u2029kenet\u2029et\u2029al.,\u20292003;\u2029petersen\u2029et\u2029al.,\u20292003a\u2029\n&\u2029b;\u2029fiser,\u2029chiu\u2029and\u2029weliky,\u20292004;\u2029maclean\u2029et\u2029al.,\u20292005;\u2029yuste\u2029et\u2029al.,\u20292005;\u2029vincent\u2029et\u2029\nal.,\u20292007)\u2029is\u2029often\u2029treated\u2029as\u2029random\u2029noise,\u2029generated\u2029either\u2029by\u2029other\u2029brain\u2029areas,\u2029or\u2029\nby\u2029stochastic\u2029processes\u2029within\u2029the\u2029circuitry\u2029being\u2029studied.\u2029\u2029we\u2029call\u2029such\u2029sources\u2029of\u2029\nvariabilit", "research | reports\n\nand the bosonic field, v is the quartic coupling\namong the bosonic fields, l is the linear coupling\nbetween the bosonic field and the orthorhombic\nlattice distortion d, and q is the momentum trans-\nfer within one brillouin zone. minimizing the ac-\ntion with respect to d, we arrive at d \u00bc l dh i=cs,\nwhere cs is the shear modulus. in other words,\nthe orthorhombic lattice distortion is proportional\nto the nematic order parameter dh i; and both are\nexpected to develop nonzero expectation values\nbelow ts (12\u201314). however, the nematic field d\nundergoes fluctuations in the tetragonal phase\nabove ts while lattice distortion d remains zero.\nthese fluctuations will be observable in dynamic\nquantities, such as the finite-energy spin fluctua-\ntions, and in transport measurements. we there-\nfore conclude that the scale t\u2217, below which we\nobserve anisotropy of low-energy spin fluctuations\n(figs. 3e, 4e, and 4f) and where the resistivity\nanisotropy is observed (fig. 3e, right inse", "towards biologically plausible\n\nconvolutional networks\n\nroman pogodin\ngatsby unit, ucl\n\nroman.pogodin.17@ucl.ac.uk\n\nyash mehta\n\ngatsby unit, ucl\n\ny.mehta@ucl.ac.uk\n\n2\n2\n0\n2\n\n \n\nn\na\nj\n \n\n5\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n1\n3\n0\n3\n1\n\n.\n\n6\n0\n1\n2\n:\nv\ni\nx\nr\na\n\ntimothy p. lillicrap\n\ndeepmind; complex, ucl\n\ncountzero@google.com\n\npeter e. latham\ngatsby unit, ucl\n\npel@gatsby.ucl.ac.uk\n\nabstract\n\nconvolutional networks are ubiquitous in deep learning. they are particularly\nuseful for images, as they reduce the number of parameters, reduce training time,\nand increase accuracy. however, as a model of the brain they are seriously prob-\nlematic, since they require weight sharing \u2013 something real neurons simply cannot\ndo. consequently, while neurons in the brain can be locally connected (one of\nthe features of convolutional networks), they cannot be convolutional. locally\nconnected but non-convolutional networks, however, signi\ufb01cantly underperform\nconvolutional ones. this is troublesome for studies tha", "6\n1\n0\n2\n\n \n\ny\na\nm\n1\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n7\n5\n0\n6\n0\n\n.\n\n4\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nhierarchical deep reinforcement learning:\n\nintegrating temporal abstraction and\n\nintrinsic motivation\n\ntejas d. kulkarni\u2217\n\nkarthik r. narasimhan\u2217\n\nbcs, mit\n\ntejask@mit.edu\n\ncsail, mit\n\nkarthikn@mit.edu\n\nardavan saeedi\n\ncsail, mit\n\nardavans@mit.edu\n\njoshua b. tenenbaum\n\nbcs, mit\n\njbt@mit.edu\n\nabstract\n\nlearning goal-directed behavior in environments with sparse feedback is\na major challenge for reinforcement learning algorithms. the primary\ndi\ufb03culty arises due to insu\ufb03cient exploration, resulting in an agent being\nunable to learn robust value functions. intrinsically motivated agents can\nexplore new behavior for its own sake rather than to directly solve problems.\nsuch intrinsic behaviors could eventually help the agent solve tasks posed\nby the environment. we present hierarchical-dqn (h-dqn), a framework\nto integrate hierarchical value functions, operating at di\ufb00erent temporal\nscales, with intrinsi", "appendix for credit assignment through\n\nbroadcasting a global error vector\n\ndavid g. clark, l.f. abbott, sueyeon chung\n\ncontents\na supplementary \ufb01gures\nb formulation of vnns using vector input units\nc assumption in gevb sign match proof\nd alternative derivation of gevb\ne gradient alignment angle and relative standard deviation\nf concentration of relative standard deviation in wide networks\ng architectures\nh global error-vector broadcasting in convolutional networks\ni training\nj direct feedback alignment\nk t-sne\nl potential negative societal impacts\nm summary of mathematical results\n\na supplementary \ufb01gures\n\n1\n4\n4\n4\n4\n5\n6\n6\n7\n7\n7\n7\n7\n\nfigure 1: mnist learning curves. nonnegative-constrained networks have a \u201c+\u201d in the name of the\nlearning rule. thus, \u201cgevb+\u201d corresponds to gevb in vnns. solid line: test error. dashed line:\ntrain error. truncated curves re\ufb02ect early stopping due to zero train error. error bars are standard\ndeviations across \ufb01ve runs.\n\n1\n\n02550751001251501750123456error (%)", "7476 \u2022 the journal of neuroscience, july 11, 2007 \u2022 27(28):7476 \u20137481\n\nbehavioral/systems/cognitive\n\ninduction of long-term memory by exposure to novelty\nrequires protein synthesis: evidence for a behavioral\ntagging\n\ndiego moncada1 and hayde\u00b4e viola1,2\n1instituto de biolog\u0131\u00b4a celular y neurociencias, facultad de medicina, and 2departamento de fisiolog\u0131\u00b4a, biolog\u0131\u00b4a molecular y celular, facultad de ciencias\nexactas y naturales, universidad de buenos aires, 1121 buenos aires, argentina\n\na behavioral analog of the synaptic tagging and capture process, a key property of synaptic plasticity, has been predicted recently. here,\nwe demonstrate that weak inhibitory avoidance training, which induces short- but not long-term memory (ltm), can be consolidated\ninto ltm by an exploration to a novel, but not a familiar, environment occurring close in time to the training session. this memory-\npromoting effect caused by novelty depends on activation of dopamine d1/d5 receptors and requires newly synth", "self-training with noisy student improves imagenet classi\ufb01cation\n\nqizhe xie\u2217 1, minh-thang luong1, eduard hovy2, quoc v. le1\n1google research, brain team, 2carnegie mellon university\n\n{qizhex, thangluong, qvl}@google.com, hovy@cmu.edu\n\n0\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n9\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n2\n5\n2\n4\n0\n\n.\n\n1\n1\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe present noisy student training, a semi-supervised\nlearning approach that works well even when labeled data\nis abundant. noisy student training achieves 88.4% top-\n1 accuracy on imagenet, which is 2.0% better than the\nstate-of-the-art model that requires 3.5b weakly labeled\ninstagram images. on robustness test sets, it improves\nimagenet-a top-1 accuracy from 61.0% to 83.7%, reduces\nimagenet-c mean corruption error from 45.7 to 28.3, and\nreduces imagenet-p mean \ufb02ip rate from 27.8 to 12.2.\n\nnoisy student training extends the idea of self-training\nand distillation with the use of equal-or-larger student mod-\nels and noise added to the student during learning", "review\n\nrepresentational  geometry:\nintegrating  cognition,  computation,\nand  the  brain\nnikolaus  kriegeskorte1 and  rogier  a.  kievit1,2\n\n1 medical  research  council,  cognition  and  brain  sciences  unit,  cambridge,  uk\n2 department  of  psychological  methods,  university  of  amsterdam,  amsterdam,  the  netherlands\n\nthe  cognitive  concept  of  representation  plays  a  key  role\nin  theories  of  brain  information  processing.  however,\nlinking  neuronal  activity  to  representational  content  and\ncognitive  theory  remains  challenging.  recent  studies\nhave  characterized  the  representational  geometry  of\nneural  population  codes  by  means  of  representational\ndistance  matrices,  enabling  researchers  to  compare\nrepresentations  across  stages  of  processing  and  to  test\ncognitive  and  computational  theories.  representational\ngeometry  provides  a  useful  intermediate  level  of  descrip-\ntion,  capturing  both  the  information  represented  in  a\nneur", "1\n2\n0\n2\n\n \n\nb\ne\nf\n5\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n7\n2\n6\n2\n1\n\n.\n\n2\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nhow to represent part-whole hierarchies\n\nin a neural network\n\ngeo\ufb00rey hinton\ngoogle research\n\n&\n\nthe vector institute\n\n&\n\ndepartment of computer science\n\nuniversity of toronto\n\nfebruary 22, 2021\n\nabstract\n\nthis paper does not describe a working system. instead, it presents a\nsingle idea about representation which allows advances made by several\ndi\ufb00erent groups to be combined into an imaginary system called glom1.\nthe advances include transformers, neural \ufb01elds, contrastive representa-\ntion learning, distillation and capsules. glom answers the question: how\ncan a neural network with a \ufb01xed architecture parse an image into a part-\nwhole hierarchy which has a di\ufb00erent structure for each image? the idea\nis simply to use islands of identical vectors to represent the nodes in the\nparse tree. if glom can be made to work, it should signi\ufb01cantly improve\nthe interpretability of the representations produced b", "language can boost otherwise unseen objects\ninto visual awareness\n\ngary lupyana,1 and emily j. wardb\n\nauniversity of wisconsin-madison, madison, wi 53706; and byale university, new haven, ct 06520\n\nedited by james. l. mcclelland, stanford university, stanford, ca, and approved june 25, 2013 (received for review february 20, 2013)\n\nlinguistic labels (e.g., \u201cchair\u201d) seem to activate visual properties of\nthe objects to which they refer. here we investigated whether\nlanguage-based activation of visual representations can affect\nthe ability to simply detect the presence of an object. we used\ncontinuous \ufb02ash suppression to suppress visual awareness of fa-\nmiliar objects while they were continuously presented to one eye.\nparticipants made simple detection decisions, indicating whether\nthey saw any image. hearing a verbal label before the simple de-\ntection task changed performance relative to an uninformative cue\nbaseline. valid labels improved performance relative to no-label\nbaseline trials", "physiological reviews\nvol. 81, no. 3, july 2001\n\nprinted in u.s.a.\n\ncerebellar long-term depression: characterization,\n\nsignal transduction, and functional roles\n\nmasao ito\n\nbrain science institute, riken, wako, saitama, japan\n\ni. introduction (historical background)\n\na. dissection of neuronal network in 1960s\nb. exploration of synaptic plasticity in 1970s to 1980s\nc. system approach in 1970s to 1980s\nd. discovery of signal transduction and cognitive function in 1990s\n\nii. characterization of cerebellar long-term depression\n\na. induction of ltd\nb. features as synaptic plasticity\n\niii. signal transduction: initial processes\n\na. first messengers\nb. receptors\nc. ion channels\nd. g proteins\ne. phospholipases\n\niv. signal transduction: further processes\n\na. ion concentrations\nb. second messengers\nc. protein kinases and protein phosphatases\nd. other factors\ne. inactivation of ampa receptor\nf. chemical network for ltd\n\nv. models for functional roles of long-term depression\n\na. corticonuclear mi", "bayesian learning via stochastic gradient langevin dynamics\n\nmax welling\nwelling@ics.uci.edu\nd. bren school of information and computer science, university of california, irvine, ca 92697-3425, usa\n\nyee whye teh\ngatsby computational neuroscience unit, ucl, 17 queen square, london wc1n 3ar, uk\n\nywteh@gatsby.ucl.ac.uk\n\nabstract\n\nin this paper we propose a new framework\nfor learning from large scale datasets based\non iterative learning from small mini-batches.\nby adding the right amount of noise to a\nstandard stochastic gradient optimization al-\ngorithm we show that the iterates will con-\nverge to samples from the true posterior dis-\ntribution as we anneal the stepsize. this\nseamless transition between optimization and\nbayesian posterior sampling provides an in-\nbuilt protection against over\ufb01tting. we also\npropose a practical method for monte carlo\nestimates of posterior statistics which moni-\ntors a \u201csampling threshold\u201d and collects sam-\nples after it has been surpassed. we apply\nthe met", "gradient-based learning applied\nto document recognition\n\nyann lecun, member, ieee, l \u00b4eon bottou, yoshua bengio, and patrick haffner\n\ninvited paper\n\nmultilayer neural networks trained with the back-propagation\nalgorithm constitute the best example of a successful gradient-\nbased learning technique. given an appropriate network\narchitecture, gradient-based learning algorithms can be used\nto synthesize a complex decision surface that can classify\nhigh-dimensional patterns, such as handwritten characters, with\nminimal preprocessing. this paper reviews various methods\napplied to handwritten character recognition and compares them\non a standard handwritten digit recognition task. convolutional\nneural networks, which are speci\ufb01cally designed to deal with\nthe variability of two dimensional (2-d) shapes, are shown to\noutperform all other techniques.\n\nneural network.\noptical character recognition.\nprincipal component analysis.\nradial basis function.\n\nnn\nocr\npca\nrbf\nrs-svm reduced-set support ve", "3\n2\n0\n2\n\n \nt\nc\no\n2\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n3\n1\n5\n8\n0\n\n.\n\n0\n1\n3\n2\n:\nv\ni\nx\nr\na\n\npreprint under review\n\nhow connectivity structure shapes rich and\nlazy learning in neural circuits\n\nyuhan helena liu1,2,3,*, aristide baratin4, jonathan cornford3,5, stefan mihalas1,2, eric\n\nshea-brown1,2, and guillaume lajoie3,6,7,*\n\n1university of washington, seattle, wa, usa\n\n2allen institute for brain science, seattle wa, usa\n3mila - quebec ai institute, montreal, qc, canada\n4samsung - sait ai lab, montreal, qc, canada\n\n5mcgill university, montreal, qc, canada\n\n6canada cifar ai chair, cifar, toronto, on, canada\n\n7universit\u00e9 de montr\u00e9al, montreal, qc, canada\n\n*correspondence: hyliu24@uw.edu, g.lajoie@umontreal.ca\n\nabstract\n\nin theoretical neuroscience, recent work leverages deep learning tools to explore\nhow some network attributes critically influence its learning dynamics. notably,\ninitial weight distributions with small (resp.\nlarge) variance may yield a rich\n(resp. lazy) regime, where signific", "model-based value expansion\n\nfor ef\ufb01cient model-free reinforcement learning\n\n8\n1\n0\n2\n\n \n\nb\ne\nf\n8\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n1\n0\n1\n0\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nvladimir feinberg 1 alvin wan 1 ion stoica 1 michael i. jordan 1 joseph e. gonzalez 1 sergey levine 1\n\nabstract\n\nrecent model-free reinforcement learning algo-\nrithms have proposed incorporating learned dy-\nnamics models as a source of additional data\nwith the intention of reducing sample complexity.\nsuch methods hold the promise of incorporating\nimagined data coupled with a notion of model\nuncertainty to accelerate the learning of contin-\nuous control tasks. unfortunately, they rely on\nheuristics that limit usage of the dynamics model.\nwe present model-based value expansion, which\ncontrols for uncertainty in the model by only al-\nlowing imagination to \ufb01xed depth. by enabling\nwider use of learned dynamics models within a\nmodel-free reinforcement learning algorithm, we\nimprove value estimation, which, in turn, reduces\nthe s", "how does batch normalization help optimization?\n\nshibani santurkar\u2217\n\ndimitris tsipras\u2217\n\nandrew ilyas\u2217\n\naleksander m \u02dbadry\n\nmit\n\nmit\n\nmit\n\nmit\n\nshibani@mit.edu\n\ntsipras@mit.edu\n\nailyas@mit.edu\n\nmadry@mit.edu\n\nabstract\n\nbatch normalization (batchnorm) is a widely adopted technique that enables\nfaster and more stable training of deep neural networks (dnns). despite its\npervasiveness, the exact reasons for batchnorm\u2019s effectiveness are still poorly\nunderstood. the popular belief is that this effectiveness stems from controlling\nthe change of the layers\u2019 input distributions during training to reduce the so-called\n\u201cinternal covariate shift\u201d. in this work, we demonstrate that such distributional\nstability of layer inputs has little to do with the success of batchnorm. instead,\nwe uncover a more fundamental impact of batchnorm on the training process: it\nmakes the optimization landscape signi\ufb01cantly smoother. this smoothness induces\na more predictive and stable behavior of the gradients, allow", "a biologically plausible learning rule for\n\ndeep learning in the brain\n\n9\n1\n0\n2\n\n \nl\nu\nj\n \n\n2\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n8\n6\n7\n1\n0\n\n.\n\n1\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nisabella pozzi\n\nvision & cognition group\n\nnetherlands institute for neuroscience\n\namsterdam, the netherlands\n\ni.pozzi@nin.knaw.nl\n\nsander m. boht\u00e9\n\nmachine learning group\n\ncentrum wiskunde & informatica\n\namsterdam, the netherlands\n\ns.m.bohte@cwi.nl\n\npieter r. roelfsema\n\nvision & cognition group\n\nnetherlands institute for neuroscience\n\namsterdam, the netherlands\np.roelfsema@nin.knaw.nl\n\nabstract\n\nintelligence is our ability to learn appropriate responses to new stimuli and situations, and signi\ufb01cant\nprogress has been made in understanding how animals learn tasks by trial-and-error learning. the\nsuccess of deep learning in end-to-end learning on a wide range of complex tasks is now fuelling the\nsearch for similar deep learning principles in the brain. while most work has focused on biologically\nplausible variants of error-backpropaga", "b\ni\no\nl\n.\n \nc\ny\nb\ne\nr\nn\ne\nt\ni\nc\ns\n \n2\n7\n,\n \n7\n7\n \n8\n7\n \n(\n1\n9\n7\n7\n)\n \nb\ni\no\nl\no\ng\ni\nc\na\nl\n \nc\ny\nb\ne\nr\nn\ne\nt\ni\nc\ns\n \n(cid:14)\n9\n \nb\ny\n \ns\np\nr\ni\nn\ng\ne\nr\n-\nv\ne\nr\nl\na\ng\n \n1\n9\n7\n7\n \nd\ny\nn\na\nm\ni\nc\ns\n \no\nf\n \np\na\nt\nt\ne\nr\nn\n \nf\no\nr\nm\na\nt\ni\no\nn\n \ni\nn\n \nl\na\nt\ne\nr\na\nl\n-\ni\nn\nh\ni\nb\ni\nt\ni\no\nn\n \nt\ny\np\ne\n \nn\ne\nu\nr\na\nl\n \nf\ni\ne\nl\nd\ns\n*\n \ns\nh\nu\nn\n-\ni\nc\nh\ni\n \na\nm\na\nr\ni\n*\n*\n \nt\nh\ne\n \nc\ne\nn\nt\ne\nr\n \nf\no\nr\n \ns\ny\ns\nt\ne\nm\ns\n \nn\ne\nu\nr\no\ns\nc\ni\ne\nn\nc\ne\n,\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n \no\nf\n \nm\na\ns\ns\na\nc\nh\nu\ns\ne\nt\nt\ns\n,\n \na\nm\nh\ne\nr\ns\nt\n,\n \nm\na\n,\n \nu\ns\na\n \na\nb\ns\nt\nr\na\nc\nt\n.\n \nt\nh\ne\n \nd\ny\nn\na\nm\ni\nc\ns\n \no\nf\n \np\na\nt\nt\ne\nr\nn\n \nf\no\nr\nm\na\nt\ni\no\nn\n \ni\ns\n \ns\nt\nu\nd\ni\ne\nd\n \nf\no\nr\n \nl\na\nt\ne\nr\na\nl\n-\ni\nn\nh\ni\nb\ni\nt\no\nn\n \nt\ny\np\ne\n \nh\no\nm\no\ng\ne\nn\ne\no\nu\ns\n \nn\ne\nu\nr\na\nl\n \nf\ni\ne\nl\nd\ns\n \nw\ni\nt\nh\n \ng\ne\nn\ne\nr\na\nl\n \nc\no\nn\nn\ne\nc\nt\ni\no\nn\ns\n.\n \nn\ne\nu\nr\na\nl\n \nf\ni\ne\nl\nd\ns\n \nc\no\nn\ns\ni\ns\nt\ni\nn\ng\n \no\nf\n \ns\ni\nn\ng\nl\ne\n \nl\na\ny\ne\nr\n \na\nr\ne\n \nf\ni\nr\ns\nt\n \nt\nr\ne\na\nt\ne\nd\n,\n \na\nn\nd\n \ni\nt\n \ni\ns\n \np\nr\no\nv\ne\nd\n \nt\nh\na\nt\n \nt\nh\ne\nr\ne\n \na\nr\ne\n \nf\ni\nv\ne\n \nt\ny\np\ne\ns\n \no\nf", "8360 \u2022 the journal of neuroscience, august 9, 2006 \u2022 26(32):8360 \u2013 8367\n\nbehavioral/systems/cognitive\n\nthe role of the ventromedial prefrontal cortex in abstract\nstate-based inference during decision making in humans\n\nalan n. hampton,1 peter bossaerts,1,2 and john p. o\u2019doherty1,2\n1computation and neural systems program and 2division of humanities and social sciences, california institute of technology, pasadena, california 91125\n\nmany real-life decision-making problems incorporate higher-order structure, involving interdependencies between different stimuli,\nactions, and subsequent rewards. it is not known whether brain regions implicated in decision making, such as the ventromedial\nprefrontal cortex (vmpfc), use a stored model of the task structure to guide choice (model-based decision making) or merely learn action\nor state values without assuming higher-order structure as in standard reinforcement learning. to discriminate between these possibil-\nities, we scanned human subjects wit", "forum\n\nplanning  as  inference\n\nmatthew  botvinick1 and  marc  toussaint2\n\n1 princeton  neuroscience  institute  and  department  of  psychology,  princeton  university,  princeton,  nj,  usa\n2 department  of  mathematics  and  computer  science,  freie  universita\u00a8 t  berlin,  berlin,  germany\n\nrecent  developments  in  decision-making  research  are\nbringing  the  topic  of  planning  back  to  center  stage  in\ncognitive  science.  this  renewed  interest  reopens  an  old,\nbut  still  unanswered  question:  how  exactly  does  plan-\nning  happen?  what  are  the  underlying  information  pro-\ncessing  operations  and  how  are  they  implemented  in  the\nbrain?  although  a  range  of \ninteresting  possibilities\nexists,  recent  work  has  introduced  a  potentially  trans-\nformative  new \nis\naccomplished  through  probabilistic  inference.\n\nidea,  according  to  which  planning \n\nbehavioral  and  neuroscienti\ufb01c  data  on  reward-based  deci-\nsion  making  increasingly  point  to  ", "article\n\ndoi:10.1038/nature11129\n\nneural population dynamics during\nreaching\n\nmark m. churchland1,2,3*, john p. cunningham4,5*, matthew t. kaufman2,3, justin d. foster2, paul nuyujukian6,7,\nstephen i. ryu2,8 & krishna v. shenoy2,3,6,9\n\nmost theories of motor cortex have assumed that neural activity represents movement parameters. this view derives\nfrom what is known about primary visual cortex, where neural activity represents patterns of light. yet it is unclear how\nwell the analogy between motor and visual cortex holds. single-neuron responses in motor cortex are complex, and\nthere is marked disagreement regarding which movement parameters are represented. a better analogy might be with\nother motor systems, where a common principle is rhythmic neural activity. here we find that motor cortex responses\nduring reaching contain a brief but strong oscillatory component, something quite unexpected for a non-periodic\nbehaviour. oscillation amplitude and phase followed naturally from the pre", "0\n2\n0\n2\n\n \n\nv\no\nn\n6\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n0\n5\n3\n0\n1\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nkernel methods through the roof:\nhandling billions of points e\ufb03ciently\n\ngiacomo meanti1, luigi carratino1, lorenzo rosasco1,2,3, and alessandro rudi4\n\n1malga, dibris, universit\u00e0 degli studi di genova, genova, italy\n2center for brains, minds and machines, mit, cambridge, ma, usa\n\n3istituto italiano di tecnologia, genova, italy\n\n4inria - d\u00e9partement d\u2019informatique de l\u2019\u00e9cole normale sup\u00e9rieure - psl research university,\n\nparis, france\n\ngiacomo.meanti@edu.unige.it luigi.carratino@dibris.unige.it lorenzo.rosasco@unige.it\n\nalessandro.rudi@inria.fr\n\nabstract\n\nkernel methods provide an elegant and principled approach to nonparametric learning,\nbut so far could hardly be used in large scale problems, since na\u00efve implementations\nscale poorly with data size. recent advances have shown the bene\ufb01ts of a number of\nalgorithmic ideas, for example combining optimization, numerical linear algebra and\nrandom project", "synaptic neuroscience\nvoltage and spike timing interact in stdp \u2013 a unified model\n\noriginal research article\npublished: 21 july 2010\ndoi: 10.3389/fnsyn.2010.00025\n\nclaudia clopath1,2* and wulfram gerstner1\n\n1  laboratory of computational neuroscience, brain-mind institute, ecole polytechnique f\u00e9d\u00e9rale de lausanne, lausanne, switzerland\n2  laboratory of neurophysics and physiology, unite mixte de recherche 8119, centre national de la recherche scientifique, universit\u00e9 paris descartes, paris, france\n\na phenomenological model of synaptic plasticity is able to account for a large body of experimental \ndata on spike-timing-dependent plasticity (stdp). the basic ingredient of the model is the \ncorrelation of presynaptic spike arrival with postsynaptic voltage. the local membrane voltage \nis used twice: a first term accounts for the instantaneous voltage and the second one for a \nlow-pass filtered voltage trace. spike-timing effects emerge as a special case. we hypothesize \nthat the voltage d", "7\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n6\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n2\n6\n2\n7\n0\n\n.\n\n5\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nmeasuring neural net robustness with constraints\n\nosbert bastani\nstanford university\n\nobastani@cs.stanford.edu\n\ndimitrios vytiniotis\nmicrosoft research\n\ndimitris@microsoft.com\n\nyani ioannou\n\nuniversity of cambridge\n\nyai20@cam.ac.uk\n\naditya v. nori\n\nmicrosoft research\n\nadityan@microsoft.com\n\nleonidas lampropoulos\nuniversity of pennsylvania\nllamp@seas.upenn.edu\n\nantonio criminisi\nmicrosoft research\n\nantcrim@microsoft.com\n\nabstract\n\ndespite having high accuracy, neural nets have been shown to be susceptible to\nadversarial examples, where a small perturbation to an input can cause it to become\nmislabeled. we propose metrics for measuring the robustness of a neural net and\ndevise a novel algorithm for approximating these metrics based on an encoding of\nrobustness as a linear program. we show how our metrics can be used to evaluate\nthe robustness of deep neural nets with experiments on the mnist and cifa", "neuron\n\narticle\n\nmatching categorical object representations\nin inferior temporal cortex of man and monkey\n\nnikolaus kriegeskorte,1,* marieke mur,1,2 douglas a. ruff,1 roozbeh kiani,3 jerzy bodurka,1,4 hossein esteky,5,6\nkeiji tanaka,7 and peter a. bandettini1,4\n1section on functional imaging methods, laboratory of brain and cognition, national institute of mental health, national institutes of health,\nbethesda, md 20892-1148, usa\n2department of cognitive neuroscience, faculty of psychology, maastricht university, 6229 er maastricht, the netherlands\n3department of neurobiology and behavior, university of washington, seattle, wa 98195-7270, usa\n4functional magnetic resonance imaging facility, national institute of mental health, national institutes of health, bethesda,\nmd 20892-1148, usa\n5research group for brain and cognitive sciences, school of medicine, shaheed beheshti university, tehran, iran\n6school of cognitive sciences, institute for studies in theoretical physics and mathematic", "letter\n\ncommunicated by yoshua bengio\n\nan approximation of the error backpropagation\nalgorithm in a predictive coding network\nwith local hebbian synaptic plasticity\n\njames c. r. whittington\njames.whittington@ndcn.ox.ac.uk\nmrc brain network dynamics unit, university of oxford, oxford, ox1 3th, u.k.,\nand fmrib centre, nuf\ufb01eld department of clinical neurosciences, university\nof oxford, john radcliffe hospital, oxford, ox3 9du, u.k.\n\nrafal bogacz\nrafal.bogacz@ndcn.ox.ac.uk\nmrc brain network dynamics unit, university of oxford, oxford ox1 3th, u.k.,\nand nuf\ufb01eld department of clinical neurosciences, university of oxford,\njohn radcliffe hospital, oxford ox3 9du, u.k.\n\nto ef\ufb01ciently learn from feedback, cortical networks need to update\nsynaptic weights on multiple levels of cortical hierarchy. an effective and\nwell-known algorithm for computing such changes in synaptic weights\nis the error backpropagation algorithm. however, in this algorithm, the\nchange in synaptic weights is a complex functi", "7\n1\n0\n2\n\n \n\nv\no\nn\n \n5\n\n \n \n]\n\n.\n\nc\no\nh\nt\na\nm\n\n[\n \n \n\n2\nv\n2\n1\n4\n0\n1\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\ngradient descent can take exponential time to\n\nescape saddle points\n\nsimon s. du\n\ncarnegie mellon university\n\nssdu@cs.cmu.edu\n\nchi jin\n\nuniversity of california, berkeley\n\nchijin@berkeley.edu\n\njason d. lee\n\nuniversity of southern california\n\njasonlee@marshall.usc.edu\n\nmichael i. jordan\n\nuniversity of california, berkeley\njordan@cs.berkeley.edu\n\nbarnab\u00e1s p\u00f3czos\n\ncarnegie mellon university\nbapoczos@cs.cmu.edu\n\naarti singh\n\ncarnegie mellon university\naartisingh@cmu.edu\n\nabstract\n\nalthough gradient descent (gd) almost always escapes saddle points asymptot-\nically [lee et al., 2016], this paper shows that even with fairly natural random\ninitialization schemes and non-pathological functions, gd can be signi\ufb01cantly\nslowed down by saddle points, taking exponential time to escape. on the other\nhand, gradient descent with perturbations [ge et al., 2015, jin et al., 2017] is not\nslowed down by saddle point", "neuron\n\narticle\n\nmodeling the spatial reach of the lfp\n\nhenrik linde\u00b4 n,1,2 tom tetzlaff,1,3 tobias c. potjans,3,4,5 klas h. pettersen,1,6 sonja gru\u00a8 n,3,7,8 markus diesmann,3,4,8,9\nand gaute t. einevoll1,6,*\n1department of mathematical sciences and technology, norwegian university of life sciences, n-1432 a\u02da s, norway\n2department of computational biology, school of computer science and communication, royal institute of technology (kth),\n10044 stockholm, sweden\n3institute of neuroscience and medicine (inm-6), computational and systems neuroscience, research center ju\u00a8 lich,\n52425 ju\u00a8 lich, germany\n4brain and neural systems team, riken computational science research program, wako, saitama 351-0198 japan\n5faculty of biology iii, albert-ludwigs-university freiburg, 79104 freiburg, germany\n6centre for integrative genetics (cigene), norwegian university of life sciences, n-1432 a\u02da s, norway\n7theoretical systems neurobiology rtwh aachen university, 52062 aachen, germany\n8riken brain science ", "a simple framework for contrastive learning of visual representations\n\nting chen 1 simon kornblith 1 mohammad norouzi 1 geoffrey hinton 1\n\nabstract\n\nthis paper presents simclr: a simple framework\nfor contrastive learning of visual representations.\nwe simplify recently proposed contrastive self-\nsupervised learning algorithms without requiring\nspecialized architectures or a memory bank. in\norder to understand what enables the contrastive\nprediction tasks to learn useful representations,\nwe systematically study the major components of\nour framework. we show that (1) composition of\ndata augmentations plays a critical role in de\ufb01ning\neffective predictive tasks, (2) introducing a learn-\nable nonlinear transformation between the repre-\nsentation and the contrastive loss substantially im-\nproves the quality of the learned representations,\nand (3) contrastive learning bene\ufb01ts from larger\nbatch sizes and more training steps compared to\nsupervised learning. by combining these \ufb01ndings,\nwe are abl", "provably faster gradient descent via long steps\n\nbenjamin grimmer\u2217\n\nabstract\n\nthis work establishes provably faster convergence rates for gradient descent in smooth convex\noptimization via a computer-assisted analysis technique. our theory allows nonconstant stepsize\npolicies with frequent long steps potentially violating descent by analyzing the overall effect of\nmany iterations at once rather than the typical one-iteration inductions used in most first-order\nmethod analyses. we show that long steps, which may increase the objective value in the short\nterm, lead to provably faster convergence in the long term. a conjecture towards proving a faster\no(1/t log t) rate for gradient descent is also motivated along with simple numerical validation.\n\n3\n2\n0\n2\n\n \nl\nu\nj\n \n\n0\n2\n\n \n \n]\n\n.\n\nc\no\nh\nt\na\nm\n\n[\n \n \n\n4\nv\n4\n2\n3\n6\n0\n\n.\n\n7\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nintroduction\n\n1\nthis work proposes a new analysis technique for gradient descent, establishing provably better\nconvergence rates for smooth, convex opt", "training-free uncertainty estimation for\ndense regression: sensitivity as a surrogate\n\nlu mi1, hao wang2, yonglong tian1, hao he1, nir n shavit1\n\n1 mit csail\n\n2 rutgers university\n\nlumi@mit.edu\n\nabstract\n\nuncertainty estimation is an essential step in the evaluation\nof the robustness for deep learning models in computer vi-\nsion, especially when applied in risk-sensitive areas. how-\never, most state-of-the-art deep learning models either fail\nto obtain uncertainty estimation or need significant modifica-\ntion (e.g., formulating a proper bayesian treatment) to obtain\nit. most previous methods are not able to take an arbitrary\nmodel off the shelf and generate uncertainty estimation with-\nout retraining or redesigning it. to address this gap, we per-\nform a systematic exploration into training-free uncertainty\nestimation for dense regression, an unrecognized yet impor-\ntant problem, and provide a theoretical construction justify-\ning such estimations. we propose three simple and scalable\n", "learning deep resnet blocks sequentially using boosting theory\n\nfurong huang 1 jordan t. ash 2 john langford 3 robert e. schapire 3\n\nabstract\n\nwe prove a multi-channel telescoping sum boost-\ning theory for the resnet architectures which si-\nmultaneously creates a new technique for boost-\ning over features (in contrast to labels) and pro-\nvides a new algorithm for resnet-style architec-\ntures. our proposed training algorithm, boostres-\nnet, is particularly suitable in non-differentiable\narchitectures. our method only requires the rela-\ntively inexpensive sequential training of t \u201cshal-\nlow resnets\u201d. we prove that the training error\ndecays exponentially with the depth t if the weak\nmodule classi\ufb01ers that we train perform slightly\nbetter than some weak baseline. in other words,\nwe propose a weak learning condition and prove\na boosting theory for resnet under the weak\nlearning condition. a generalization error bound\nbased on margin theory is proved and suggests\nthat resnet could be resista", "1\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n8\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n9\n0\n4\n3\n0\n\n.\n\n0\n1\n0\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2021\n\nlearning mesh-based simulation\nwith graph networks\n\ntobias pfaff\u2217, meire fortunato\u2217, alvaro sanchez-gonzalez\u2217, peter w. battaglia\ndeepmind, london, uk\n{tpfaff,meirefortunato,alvarosg,peterbattaglia}@google.com\n\nabstract\n\nmesh-based simulations are central to modeling complex physical systems in\nmany disciplines across science and engineering. mesh representations sup-\nport powerful numerical integration methods and their resolution can be adapted\nto strike favorable trade-offs between accuracy and ef\ufb01ciency. however, high-\ndimensional scienti\ufb01c simulations are very expensive to run, and solvers and pa-\nrameters must often be tuned individually to each system studied. here we intro-\nduce meshgraphnets, a framework for learning mesh-based simulations us-\ning graph neural networks. our model can be trained to pass messages on a mesh\ngraph and to adapt the ", "journal of machine learning research 18 (2017) 1-52\n\nsubmitted 10/16; revised 6/17; published 7/17\n\nconvolutional neural networks analyzed via\n\nconvolutional sparse coding\n\nvardan papyan*\ndepartment of computer science\ntechnion - israel institute of technology\ntechnion city, haifa 32000, israel\n\nyaniv romano*\ndepartment of electrical engineering\ntechnion - israel institute of technology\ntechnion city, haifa 32000, israel\n\nmichael elad\ndepartment of computer science\ntechnion - israel institute of technology\ntechnion city, haifa 32000, israel\n\neditor: kevin murphy\n\nvardanp@campus.technion.ac.il\n\nyromano@tx.technion.ac.il\n\nelad@cs.technion.ac.il\n\nabstract\n\nconvolutional neural networks (cnn) have led to many state-of-the-art results spanning\nthrough various \ufb01elds. however, a clear and profound theoretical understanding of the\nforward pass, the core algorithm of cnn, is still lacking. in parallel, within the wide \ufb01eld of\nsparse approximation, convolutional sparse coding (csc) has gained in", "23 jan 2002 14:1\n\nar\n\nar148-13.tex ar148-13.sgm\n\nlatex2e(2001/05/10)\n\np1: gjc\n\n10.1146/annurev.physiol.64.092501.114547\n\nannu. rev. physiol. 2002. 64:355\u2013405\ndoi: 10.1146/annurev.physiol.64.092501.114547\ncopyright c(cid:176) 2002 by annual reviews. all rights reserved\n\nshort-term synaptic plasticity\n\nrobert s. zucker\ndepartment of molecular and cell biology, university of california, berkeley,\ncalifornia 94720; e-mail: zucker@socrates.berkeley.edu\n\nwade g. regehr\ndepartment of neurobiology, harvard medical school, boston, massachusetts 02115;\ne-mail: wregehr@hms.harvard.edu\n\nsynapse, facilitation, post-tetanic potentiation, depression,\n\nkey words\naugmentation, calcium\nn abstract synaptic transmission is a dynamic process. postsynaptic responses\nwax and wane as presynaptic activity evolves. this prominent characteristic of chemi-\ncal synaptic transmission is a crucial determinant of the response properties of synapses\nand, in turn, of the stimulus properties selected by neural networks ", "distributional reinforcement learning with linear function\n\napproximation\n\n9\n1\n0\n2\n\n \n\nb\ne\nf\n8\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n9\n4\n1\n3\n0\n\n.\n\n2\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nmarc g. bellemare\n\nnicolas le roux\n\npablo samuel castro\n\nsubhodeep moitra\n\ngoogle brain\n\nabstract\n\ndespite many algorithmic advances, our the-\noretical understanding of practical distribu-\ntional reinforcement learning methods re-\nmains limited. one exception is rowland\net al. (2018)\u2019s analysis of the c51 algorithm\nin terms of the cram\u00b4er distance, but their\nresults only apply to the tabular setting and\nignore c51\u2019s use of a softmax to produce\nnormalized distributions. in this paper we\nadapt the cram\u00b4er distance to deal with arbi-\ntrary vectors. from it we derive a new dis-\ntributional algorithm which is fully cram\u00b4er-\nbased and can be combined to linear func-\ntion approximation, with formal guarantees\nin the context of policy evaluation.\nin al-\nlowing the model\u2019s prediction to be any real\nvector, we lose the probabilistic int", "serotonin modulation of cortical neurons and networks\npau celada 1,2, m. victoria puig 3 and francesc artigas 1,2*\n\n1 department of neurochemistry and neuropharmacology, institut d\u2019 investigacions biom\u00e8diques de barcelona (csic), idibaps, barcelona, spain\n2 centro de investigaci\u00f3n biom\u00e9dica en red de salud mental (cibersam), madrid, spain\n3 the picower institute for learning and memory, massachusetts institute of technology, cambridge, ma, usa\n\nreview article\npublished: 19 april 2013\ndoi: 10.3389/fnint.2013.00025\n\nedited by:\nkongfatt wong-lin, university of\nulster, northern ireland\nreviewed by:\nrodrigo n. romcy-pereira,\nuniversidade federal do rio grande\ndo norte, brazil\naaron gruber, university of\nlethbridge, canada\n*correspondence:\nfrancesc artigas, department of\nneurochemistry and\nneuropharmacology, institut d\u2019\ninvestigacions biom\u00e8diques de\nbarcelona (csic), idibaps, rossell\u00f3\n161, 6th \ufb02oor, 08036 barcelona,\nspain.\ne-mail: fapnqi@iibb.csic.es\n\nthe serotonergic pathways originating in", "journal of machine learning research 11 (2010) 3371-3408\n\nsubmitted 5/10; published 12/10\n\nstacked denoising autoencoders: learning useful representations in\n\na deep network with a local denoising criterion\n\npascal vincent\nd\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle\nuniversit\u00b4e de montr\u00b4eal\n2920, chemin de la tour\nmontr\u00b4eal, qu\u00b4ebec, h3t 1j8, canada\nhugo larochelle\ndepartment of computer science\nuniversity of toronto\n10 king\u2019s college road\ntoronto, ontario, m5s 3g4, canada\n\npascal.vincent@umontreal.ca\n\nlarocheh@cs.toronto.edu\n\nisabelle lajoie\nyoshua bengio\npierre-antoine manzagol\nd\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle\nuniversit\u00b4e de montr\u00b4eal\n2920, chemin de la tour\nmontr\u00b4eal, qu\u00b4ebec, h3t 1j8, canada\n\nisabelle.lajoie.1@umontreal.ca\nyoshua.bengio@umontreal.ca\npierre-antoine.manzagol@umontreal.ca\n\neditor: l\u00b4eon bottou\n\nabstract\n\nwe explore an original strategy for building deep networks, based on stacking layers of denoising\nautoencoders which are tr", "article\n\nlinking connectivity, dynamics, and computations\nin low-rank recurrent neural networks\n\nhighlights\nd we study network models characterized by minimal\n\nconnectivity structures\n\nauthors\n\nfrancesca mastrogiuseppe,\nsrdjan ostojic\n\nd for such models, low-dimensional dynamics can be directly\n\ninferred from connectivity\n\ncorrespondence\nsrdjan.ostojic@ens.fr\n\nd computations emerge from distributed and mixed\n\nrepresentations\n\nd implementing speci\ufb01c tasks yields predictions linking\n\nconnectivity and computations\n\nin brief\nneural recordings show that cortical\ncomputations rely on low-dimensional\ndynamics over distributed\nrepresentations. how are these\ngenerated by the underlying\nconnectivity? mastrogiuseppe et al. use a\ntheoretical approach to infer low-\ndimensional dynamics and computations\nfrom connectivity and produce\npredictions linking connectivity and\nfunctional properties of neurons.\n\nmastrogiuseppe & ostojic, 2018, neuron 99, 609\u2013623\naugust 8, 2018 \u00aa 2018 elsevier inc.\nhttps://do", "7\n1\n0\n2\n\n \n\np\ne\ns\n7\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n4\n6\n8\n3\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nevolutionstrategiesasascalablealternativetoreinforcementlearningtimsalimansjonathanhoxichenszymonsidorilyasutskeveropenaiabstractweexploretheuseofevolutionstrategies(es),aclassofblackboxoptimizationalgorithms,asanalternativetopopularmdp-basedrltechniquessuchasq-learningandpolicygradients.experimentsonmujocoandatarishowthatesisaviablesolutionstrategythatscalesextremelywellwiththenumberofcpusavailable:byusinganovelcommunicationstrategybasedoncommonrandomnumbers,ouresimplementationonlyneedstocommunicatescalars,makingitpossibletoscaletooverathousandparallelworkers.thisallowsustosolve3dhumanoidwalkingin10minutesandobtaincompetitiveresultsonmostatarigamesafteronehouroftraining.inaddition,wehighlightseveraladvantagesofesasablackboxoptimizationtechnique:itisinvarianttoactionfrequencyanddelayedrewards,tolerantofextremelylonghorizons,anddoesnotneedtemporaldiscountingorvaluefunctionapproximation.1introducti", "article\n\ndynamic interaction between reinforcement\nlearning and attention in multidimensional\nenvironments\n\nhighlights\nd attention constrains reinforcement learning processes to\n\nrelevant task dimensions\n\nauthors\n\nyuan chang leong, angela radulescu,\nreka daniel, vivian dewoskin, yael niv\n\nd learned values of stimulus features drive shifts in the focus of\n\nattention\n\nd attention biases value and reward prediction error signals in\n\nthe brain\n\ncorrespondence\nycleong@stanford.edu (y.c.l.),\nangelar@princeton.edu (a.r.),\nyael@princeton.edu (y.n.)\n\nd dynamic control of attention is associated with activity in\n\nfrontoparietal network\n\nin brief\nleong, radulescu et al. used eye tracking\nand fmri to empirically measure\n\ufb02uctuations of attention in a\nmultidimensional decision-making task.\nthe authors demonstrate that decision\nmaking in multidimensional environments\nis facilitated by a bidirectional interaction\nbetween attention and trial-and-error\nreinforcement learning processes.\n\nleong et al., 20", "j neurophysiol 100: 3445\u20133457, 2008.\nfirst published october 1, 2008; doi:10.1152/jn.90833.2008.\n\ninnovative methodology\n\ntoward optimal target placement for neural prosthetic devices\n\njohn p. cunningham,1 byron m. yu,1,4,5 vikash gilja,2 stephen i. ryu,3 and krishna v. shenoy1,4\n1departments of electrical engineering, 2computer science, and 3neurosurgery, and 4neurosciences program, stanford university,\nstanford, california; and 5gatsby computational neuroscience unit university college london, london, united kingdom\n\nsubmitted 30 july 2008; accepted in \ufb01nal form 19 september 2008\n\ncunningham jp, yu bm, gilja v, ryu si, shenoy kv. toward\noptimal target placement for neural prosthetic devices. j neuro-\nphysiol 100: 3445\u20133457, 2008. first published october 1, 2008;\ndoi:10.1152/jn.90833.2008. neural prosthetic systems have been de-\nsigned to estimate continuous reach trajectories (motor prostheses)\nand to predict discrete reach targets (communication prostheses). in\nthe latter case, reac", " \n\n \n\nwww.sciencemag.org/content/357/6355/1033/suppl/dc1 \n\n \n \n \n \n\nsupplementary materials for \n\nbehavioral time scale synaptic plasticity underlies ca1 place fields \n\nkatie c. bittner, aaron d. milstein, christine grienberger, sandro romani, jeffrey c. magee* \n\n*corresponding author. email: mageej@janelia.hhmi.org \n\npublished 8 september 2017, science 357, 1033 (2017) \n\ndoi: 10.1126/science.aan3846 \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n \n\n \n \nthis pdf file includes: \n \n\nmaterials and methods \nfigs. s1 to s9 \nreferences \n\n\f", "reports\n\npronounced than in drbp nulls (5, 14). function-\nally, drbp and bruchpilot phenotypes appear\nsimilar: both demonstrate decreased and desyn-\nchronized evoked sv release with atypical short-\nterm facilitation. however, the deficits in evoked\nsv release are much more severe in drbp nulls\nthan in bruchpilot nulls [i.e., release occurs at 5%\nversus 30% (5) of the respective wild-type level].\ndrbp levels were clearly reduced in bruchpilot\nmutants (fig. s7), whereas gross bruchpilot lev-\nels were not altered in drbp mutants (fig. 2b).\ngiven that even a partial loss of drbp causes\nmarked reduction in sv release (fig. 3a), deficits\nin bruchpilot mutants might be explained, at least\nin part, by a concomitant loss of drbp, and\ndrbp probably serves functions beyond the\nstructural and ca2+ channel\u2013clustering roles of\nbruchpilot.\n\ntaken together, we identified drbp as a cen-\ntral part of the az cytomatrix. how, in detail,\ndrbp functionally integrates into this protein\nnetwork is subject to ", "published as a conference paper at iclr 2018\n\ndeep rewiring: training very sparse deep net-\nworks\n\nguillaume bellec, david kappel, wolfgang maass & robert legenstein\ninstitute for theoretical computer science\ngraz university of technology\naustria\n{bellec,kappel,maass,legenstein}@igi.tugraz.at\n\nabstract\n\nneuromorphic hardware tends to pose limits on the connectivity of deep networks\nthat one can run on them. but also generic hardware and software implementa-\ntions of deep learning run more ef\ufb01ciently for sparse networks. several methods\nexist for pruning connections of a neural network after it was trained without con-\nnectivity constraints. we present an algorithm, deep r, that enables us to train\ndirectly a sparsely connected neural network. deep r automatically rewires the\nnetwork during supervised training so that connections are there where they are\nmost needed for the task, while its total number is all the time strictly bounded.\nwe demonstrate that deep r can be used to train ver", "1\n2\n0\n2\n\n \n\ng\nu\na\n2\n\n \n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n0\n1\n2\n1\n0\n\n.\n\n8\n0\n1\n2\n:\nv\ni\nx\nr\na\n\n| chethanpandarinath2\n\noriginal article\nneural data science / analysis\nrepresentationlearningforneuralpopulation\nactivitywithneuraldatatransformers\njoelye1\n1schoolofinteractivecomputing,georgia\ninstituteoftechnology,atlanta,ga,usa\n2wallaceh.coulterdepartmentof\nbiomedicalengineeringanddepartmentof\nneurosurgery,emoryuniversityand\ngeorgiainstituteoftechnology,atlanta,\nga,usa\ncorrespondence\nemail: joelye9@gmail.com\nfundinginformation\nthisworkwassupportedbytheemory\nneuromodulationandtechnology\ninnovationcenter(entice),nsfncs\n1835364,darpa\npa-18-02-04-ini-fp-021,niheunice\nkennedyshrivernichdk12hd073945,\nthealfredp.sloanfoundation,andthe\nsimonsfoundationaspartofthe\nsimons-emoryinternationalconsortiumon\nmotorcontrol.\n\nneural population activity is theorized to re\u0002ect an under-\nlyingdynamicalstructure. thisstructurecanbeaccurately\ncaptured using state space models with explicit dynamics,\nsuch as those ", "3\n2\n0\n2\n\n \n\nn\na\nj\n \n\n4\n1\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n3\n2\n8\n0\n0\n\n.\n\n6\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nbeyond accuracy: generalization properties of\nbio-plausible temporal credit assignment rules\n\nyuhan helena liu1,2,3,*, arna ghosh4,5, blake a. richards4,5,6,7, eric shea-brown1,2,3, and\n\nguillaume lajoie5,7,8,*\n\n1department of applied mathematics, university of washington, seattle, wa, usa\n\n2allen institute for brain science, 615 westlake ave n, seattle wa, usa\n\n3computational neuroscience center, university of washington, seattle, wa, usa\n\n4school of computer science, mcgill university, montreal, qc, canada\n\n5mila - quebec ai institute, montreal, qc, canada\n\n6department of neurology and neurosurgery, montreal neurological institute, mcgill university,\n\n7canada cifar ai chair, cifar, toronto, on, canada\n\n8dept. de math\u00e9matiques et statistiques, universit\u00e9 de montr\u00e9al, montreal, qc, canada\n\n*correspondence: hyliu24@uw.edu, g.lajoie@umontreal.ca\n\nmontreal, qc, canada\n\nabstract\n\nto unveil how the brain", "a r t i c l e s\n\ngaba promotes the competitive selection of dendritic \nspines by controlling local ca2+ signaling\ntatsuya hayama1,2,6, jun noguchi1,2,6, satoshi watanabe1,2, noriko takahashi1,2, akiko hayashi-takagi1\u20133, \ngraham c r ellis-davies4, masanori matsuzaki2,3,5 & haruo kasai1,2\n\nactivity-dependent competition of synapses plays a key role in neural organization and is often promoted by gaba; however, its \ncellular bases are poorly understood. excitatory synapses of cortical pyramidal neurons are formed on small protrusions known as \ndendritic spines, which exhibit structural plasticity. we used two-color uncaging of glutamate and gaba in rat hippocampal ca1 \npyramidal neurons and found that spine shrinkage and elimination were markedly promoted by the activation of gabaa receptors \nshortly before action potentials. gabaergic inhibition suppressed bulk increases in cytosolic ca2+ concentrations, whereas it \npreserved the ca2+ nanodomains generated by nmda-type receptors, both of", "a\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\nt\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nhhs public access\nauthor manuscript\nj math psychol. author manuscript; available in pmc 2019 november 19.\n\npublished in final edited form as:\nj math psychol. 2017 february ; 76(b): 65\u201379. doi:10.1016/j.jmp.2016.01.001.\n\napproaches to analysis in model-based cognitive neuroscience\n\nbrandon m. turnera,*, birte u. forstmannb, bradley c. lovec, thomas j. palmerid, leendert \nvan maanenb\na department of psychology, the ohio state university\n\nb department of psychology, university of amsterdam\n\nc department of psychology, university college, london\n\nd department of psychology, vanderbilt university\n\nabstract\n\nour understanding of cognition has been advanced by two traditionally nonoverlapping and non-\ninteracting groups. mathematical psychologists rely on behavioral data to evaluate formal models \nof cognition, whereas cognitive neuroscienti", "text categorization with support vector\nmachines: learning with many relevant\n\nfeatures\n\nthorsten joachims\n\nuniversit\u0007at dortmund\n\ninformatik ls\b, baroper str. \u0003\u0000\u0001\n\n\u0004\u0004\u0002\u0002\u0001 dortmund, germany\n\nabstract. this paper explores the use of support vector machines\n\u001csvms\u001d for learning text classi\u0000ers from examples. it analyzes the par-\nticular properties of learning with text data and identi\u0000es why svms\nare appropriate for this task. empirical results support the theoretical\n\u0000ndings. svms achieve substantial improvements over the currently best\nperforming methods and behave robustly over a variety of di\u0000erent learn-\ning tasks. furthermore, they are fully automatic, eliminating the need\nfor manual parameter tuning.\n\n\u0001\n\nintroduction\n\nwith the rapid growth of online information, text categorization has become one\nof the key techniques for handling and organizing text data. text categorization\ntechniques are used to classify news stories, to \u0000nd interesting information on\nthe www, and to guide a user", "7\n1\n0\n2\n\n \n\nv\no\nn\n4\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n4\n2\n2\n7\n0\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\naide: an algorithm for measuring the accuracy of\n\nprobabilistic inference algorithms\n\nmarco f. cusumano-towner\nprobabilistic computing project\n\nmassachusetts institute of technology\n\nmarcoct@mit.edu\n\nvikash k. mansinghka\n\nprobabilistic computing project\n\nmassachusetts institute of technology\n\nvkm@mit.edu\n\nabstract\n\napproximate probabilistic inference algorithms are central to many \ufb01elds. exam-\nples include sequential monte carlo inference in robotics, variational inference\nin machine learning, and markov chain monte carlo inference in statistics. a\nkey problem faced by practitioners is measuring the accuracy of an approximate\ninference algorithm on a speci\ufb01c data set. this paper introduces the auxiliary\ninference divergence estimator (aide), an algorithm for measuring the accuracy of\napproximate inference algorithms. aide is based on the observation that inference\nalgorithms can be treated as pro", "letter\n\ncommunicated by terrence j. sejnowski\n\nrelating stdp to bcm\n\neugene m. izhikevich\neugene.izhikevich@nsi.edu,www.nsi.edu/users/izhikevich\nniraj s. desai\ndesai@nsi.edu\nthe neurosciences institute, san diego, ca, 92121, u.s.a.\n\nwe demonstrate that the bcm learning rule follows directly from stdp\nwhen pre- and postsynaptic neurons \u0005re uncorrelated or weakly corre-\nlated poisson spike trains, and only nearest-neighbor spike interactions\nare taken into account.\n\n1 introduction\n\nover the past several years, there has been increasing interest in a novel\nform of hebbian synaptic plasticity called spike-timing-dependent plasticity\n(stdp; markram, lubke, frotscher, & sakmann, 1997; bi & poo, 1998; de-\nbanne, gahwiler, & thomson, 1998; feldman, 2000; sjostrom, turrigiano, &\nnelson, 2001; froemke & dan, 2002), in which the temporal order of presy-\nnaptic and postsynaptic spikes determines whether a synapse is potentiated\nor depressed (see figure 1a). while experiments to date have given us ", "a r t i c l e s\n\ncortical activity in the null space: permitting \npreparation without movement\nmatthew t kaufman1\u20133, mark m churchland4\u20137, stephen i ryu2,8 & krishna v shenoy1,2,9,10\nneural circuits must perform computations and then selectively output the results to other circuits. yet synapses do not change \nradically at millisecond timescales. a key question then is: how is communication between neural circuits controlled? in motor \ncontrol, brain areas directly involved in driving movement are active well before movement begins. muscle activity is some readout \nof neural activity, yet it remains largely unchanged during preparation. here we find that during preparation, while the monkey \nholds still, changes in motor cortical activity cancel out at the level of these population readouts. motor cortex can thereby prepare \nthe movement without prematurely causing it. further, we found evidence that this mechanism also operates in dorsal premotor \ncortex, largely accounting for how pr", "supplementary materials for: a solution to the learning\n\ndilemma for recurrent networks of spiking neurons\n\nguillaume bellec1,\u25e6, franz scherr1,\u25e6, anand subramoney1,\n\nelias hajek1, darjan salaj1, robert legenstein1, wolfgang maass1,\u2217\n\n1institute of theoretical computer science, graz university of technology,\n\nin\ufb00eldgasse 16b, graz, austria\n\n\u2217 to whom correspondence should be addressed; e-mail: maass@igi.tugraz.at.\n\n\u25e6 equal contributions.\n\ncontents\n\nsupplementary figures\n\nfigure 1 performance comparison of bptt and e-prop on an extension of the episodic memory\n\ntask from [1]\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nfigure 2 comparison of learning algorithms for training lsnns on the timit task . . . . . . . .\nfigure 3 performance of e-prop on the framewise timit task for lsnns without recurrent\n\nconnections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nfigure 4 lstm networks trained with bptt and ", "published as a conference paper at iclr 2020\n\nimplementing inductive bias for different\nnavigation tasks through diverse rnn attr-\nractors\n\ntie xu, omri barak\nrappaport faculty of medicine and network biology research laboratory\ntechnion, israel institute of technology\nhaifa, 320003, israel\nfexutie@gmail.com, omri.barak@gmail.com\n\nabstract\n\nnavigation is crucial for animal behavior and is assumed to require an internal rep-\nresentation of the external environment, termed a cognitive map. the precise form\nof this representation is often considered to be a metric representation of space.\nan internal representation, however, is judged by its contribution to performance\non a given task, and may thus vary between different types of navigation tasks.\nhere we train a recurrent neural network that controls an agent performing several\nnavigation tasks in a simple environment. to focus on internal representations, we\nsplit learning into a task-agnostic pre-training stage that modi\ufb01es internal co", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/266656356\n\nevolving deep unsupervised convolutional networks for vision-based\nreinforcement learning\n\narticle \u00b7 july 2014\n\ndoi: 10.1145/2576768.2598358\n\ncitations\n91\n\n3 authors, including:\n\nfaustino gomez\nnnaisense sa\n\n71 publications\u00a0\u00a0\u00a09,850 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n3,379\n\nall content following this page was uploaded by faustino gomez on 29 march 2016.\n\nthe user has requested enhancement of the downloaded file.\n\n\f", "supplementary information\n\ndoi:10.1038/nature12160\n\nsupplementary methods\n\nm.1 de\ufb01nition and properties of dimensionality\n\nconsider an experiment with a discrete set of c di\ufb00erent experimental conditions, correspond-\ning to all distinct combinations of task-relevant variables. for example, in the experiment\nthat we analyzed, the animal has to remember a sequence of two visual cues. the \ufb01rst and\nthe second cue can be any of four visual objects, and the second cue cannot be the same\nas the \ufb01rst one, giving a total of 4 \u00d7 3 = 12 di\ufb00erent types of cue combinations. moreover,\nthere are two possible ways of testing the memory of the animal (recognition or recall task),\nand hence the total c is 12 \u00d7 2 = 24.\nwe now consider the neuronal activity of n neurons in one particular time bin. specif-\nically, for each experimental condition we consider the vectors whose components are the\nmean \ufb01ring rates of all neurons. these are a total of c distinct vectors, each of which is\nrepresented by a point ", "on the importance of initialization and momentum in deep learning\n\nilya sutskever1\njames martens\ngeorge dahl\ngeo\u21b5rey hinton\n\nabstract\n\ndeep and recurrent neural networks (dnns\nand rnns respectively) are powerful mod-\nels that were considered to be almost impos-\nsible to train using stochastic gradient de-\nscent with momentum.\nin this paper, we\nshow that when stochastic gradient descent\nwith momentum uses a well-designed random\ninitialization and a particular type of slowly\nincreasing schedule for the momentum pa-\nrameter, it can train both dnns and rnns\n(on datasets with long-term dependencies) to\nlevels of performance that were previously\nachievable only with hessian-free optimiza-\ntion. we \ufb01nd that both the initialization\nand the momentum are crucial since poorly\ninitialized networks cannot be trained with\nmomentum and well-initialized networks per-\nform markedly worse when the momentum is\nabsent or poorly tuned.\nour success training these models suggests\nthat previous attempts to tr", "learning function from structure in neuromorphic \nnetworks\n\nlaura e. su\u00e1rez\u200a\n\n\u200a1,2, blake a. richards\u200a\n\n\u200a1,2,3,4, guillaume lajoie\u200a\n\n\u200a2,3,5 and bratislav misic1\u2009\u2709\n\nthe connection patterns of neural circuits in the brain form a complex network. collective signalling within the network manifests \nas patterned neural activity and is thought to support human cognition and adaptive behaviour. recent technological advances \npermit macroscale reconstructions of biological brain networks. these maps, termed connectomes, display multiple non-random \narchitectural features, including heavy-tailed degree distributions, segregated communities and a densely interconnected core. yet, \nhow computation and functional specialization emerge from network architecture remains unknown. here we reconstruct human \nbrain connectomes using in vivo diffusion-weighted imaging and use reservoir computing to implement connectomes as artificial \nneural networks. we then train these neuromorphic networks to learn a ", "focused review\n\nsilencing the critics: understanding the effects \nof cocaine sensitization on dorsolateral and ventral \nstriatum in the context of an actor/critic model\n\nyuji takahashi1, geoffrey schoenbaum1 and yael niv2,3,\u2020\n\n1  department of anatomy and neurobiology, school of medicine, university of maryland, baltimore, md, usa.\n2  psychology department, princeton university, princeton, nj, usa.\n3  princeton neuroscience institute, princeton university, princeton, nj, usa.\n\nedited by:\nsidney a. simon, duke \nuniversity, usa\n\nreviewed by:\nrui m. costa, national \ninstitutes of health, usa\nbernard w. balleine, \nuniversity of california \nlos angeles, usa\n\n\u2020correspondence: \n\nyael niv will be assistant \nprofessor at the princeton \nneuroscience institute and \nthe psychology department \nat princeton university \nbeginning this fall. currently \na postdoctoral fellow \nat princeton, she received \nher phd from the hebrew \nuniversity in jerusalem, \nafter conducting her \ndoctoral research there at ", "5\n1\n0\n2\n\n \nr\na\n\nm\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n7\n6\n1\n3\n0\n\n.\n\n2\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nbatch normalization: accelerating deep network training by\n\nreducing internal covariate shift\n\nsergey ioffe\n\nchristian szegedy\n\ngoogle inc., sioffe@google.com\n\ngoogle inc., szegedy@google.com\n\nabstract\n\ntraining deep neural networks is complicated by the fact\nthat the distribution of each layer\u2019s inputs changes during\ntraining, as the parameters of the previous layers change.\nthis slows down the training by requiring lower learning\nrates and careful parameter initialization, and makes it no-\ntoriously hard to train models with saturating nonlineari-\nties. we refer to this phenomenon as internal covariate\nshift, and address the problem by normalizing layer in-\nputs. our method draws its strength from making normal-\nization a part of the model architecture and performing the\nnormalization for each training mini-batch. batch nor-\nmalization allows us to use much higher learning rates and\nbe less careful", "deepnet: scaling transformers to 1,000 layers\n\nhongyu wang\u2217 shuming ma\u2217 li dong shaohan huang dongdong zhang furu wei\u2020\n\nmicrosoft research\n\nhttps://github.com/microsoft/unilm\n\n2\n2\n0\n2\n\n \nr\na\n\nm\n1\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n1\nv\n5\n5\n5\n0\n0\n\n.\n\n3\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nin this paper, we propose a simple yet effective method to stabilize extremely\ndeep transformers. speci\ufb01cally, we introduce a new normalization function\n(deepnorm) to modify the residual connection in transformer, accompanying\nwith theoretically derived initialization. in-depth theoretical analysis shows that\nmodel updates can be bounded in a stable way. the proposed method combines the\nbest of two worlds, i.e., good performance of post-ln and stable training of pre-ln,\nmaking deepnorm a preferred alternative. we successfully scale transformers up\nto 1,000 layers (i.e., 2,500 attention and feed-forward network sublayers) without\ndif\ufb01culty, which is one order of magnitude deeper than previous deep transform-\ners. remark", "further\nannual\nreviews\nclick here for quick links to \nannual reviews content online, \nincluding:\n\u2022 other articles in this volume\n\u2022 top cited articles\n\u2022 top downloaded articles\n\u2022 our comprehensive search\n\ncerebellum-like structures\nand their implications for\ncerebellar function\ncurtis c. bell,1 victor han,2\nand nathaniel b. sawtell1\n1neurological sciences institute, oregon health and science university,\nbeaverton, oregon 97006; email: bellc@ohsu.edu, sawtelln@ohsu.edu\n2oregon regional primate center, oregon health and science university,\nbeaverton, oregon 97006; email: hanv@ohsu.edu\n\nannu. rev. neurosci. 2008. 31:1\u201324\n\nfirst published online as a review in advance on\nfebruary 14, 2008\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev.neuro.30.051606.094225\ncopyright c(cid:2) 2008 by annual reviews.\nall rights reserved\n\n0147-006x/08/0721-0001$20.00\n\nkey words\nforward model, synaptic plasticity, electric \ufb01sh, cerebellum\n\nabstract\n", "letter\n\ndoi:10.1038/nature19818\n\ncortico-fugal output from visual cortex promotes \nplasticity of innate motor behaviour\n\nbao-hua liu1,2, andrew d. huberman3 & massimo scanziani1,2,4\n\nthe mammalian visual cortex massively innervates the brainstem, \na phylogenetically older structure, via cortico-fugal axonal \nprojections1. many cortico-fugal projections target brainstem \nnuclei that mediate innate motor behaviours, but the function \nof these projections remains poorly understood1\u20134. a prime \nexample of such behaviours is the optokinetic reflex (okr), an \ninnate eye movement mediated by the brainstem accessory optic \nsystem3,5,6, that stabilizes images on the retina as the animal \nmoves through the environment and is thus crucial for vision5. \nthe okr is plastic, allowing the amplitude of this reflex to be \nadaptively adjusted relative to other oculomotor reflexes and \nthereby ensuring image stability throughout life7\u201311. although the \nplasticity of the okr is thought to involve subcorti", "a r t i c l e s\n\nlearning the value of information in an uncertain world\n\ntimothy e j behrens1,2, mark w woolrich1, mark e walton2 & matthew f s rushworth1,2\n\nour decisions are guided by outcomes that are associated with decisions made in the past. however, the amount of in\ufb02uence\neach past outcome has on our next decision remains unclear. to ensure optimal decision-making, the weight given to decision\noutcomes should re\ufb02ect their salience in predicting future outcomes, and this salience should be modulated by the volatility of\nthe reward environment. we show that human subjects assess volatility in an optimal manner and adjust decision-making\naccordingly. this optimal estimate of volatility is re\ufb02ected in the fmri signal in the anterior cingulate cortex (acc) when each\ntrial outcome is observed. when a new piece of information is witnessed, activity levels re\ufb02ect its salience for predicting future\noutcomes. furthermore, variations in this acc signal across the population predict variat", "2017 ieee/rsj international conference on intelligent robots and systems (iros)\nseptember 24\u201328, 2017, vancouver, bc, canada\n\n978-1-5386-2682-5/17/$31.00 \u00a92017 ieee\n\n2371\n\nauthorized licensed use limited to: university of washington libraries. downloaded on september 29,2023 at 07:03:10 utc from ieee xplore.  restrictions apply. \n\ndeepreinforcementlearningwithsuccessorfeaturesfornavigationacrosssimilarenvironmentsjingweizhangjosttobiasspringenbergjoschkaboedeckerwolframburgardabstract\u2014inthispaperweconsidertheproblemofrobotnavigationinsimplemaze-likeenvironmentswheretherobothastorelyonitsonboardsensorstoperformthenav-igationtask.inparticular,weareinterestedinsolutionstothisproblemthatdonotrequirelocalization,mappingorplanning.additionally,werequirethatoursolutioncanquicklyadapttonewsituations(e.g.,changingnavigationgoalsandenvironments).tomeetthesecriteriaweframethisproblemasasequenceofrelatedreinforcementlearningtasks.weproposeasuccessor-feature-baseddeepreinforcementlearningalgorithmt", "0\n2\n0\n2\n\n \nr\np\na\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n0\n2\n7\n7\n0\n\n.\n\n2\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nlocal propagation in\n\nconstraint-based neural networks\n\ngiuseppe marra\u2217, matteo tiezzi\u2020, stefano melacci\u2020, alessandro betti\u2020, marco maggini\u2020, marco gori\u2020\n\n\u2217dept. of information engineering\n\nuniversity of florence\n\n{g.marra,alessandro.betti}@unifi.it\n\nflorence, italy\n\n\u2020dept. of information engineering and mathematical sciences\n\nuniversity of siena\n\n{mtiezzi,mela,maggini,marco}@diism.unisi.it\n\nsiena, italy\n\nabstract\u2014in this paper we study a constraint-based repre-\nsentation of neural network architectures. we cast the learning\nproblem in the lagrangian framework and we investigate a\nsimple optimization procedure that is well suited to ful\ufb01l the\nso-called architectural constraints, learning from the available\nsupervisions. the computational structure of the proposed local\npropagation (lp) algorithm is based on the search for saddle\npoints in the adjoint space composed of weights, neural outputs,\nand la", "richards, b. a., lillicrap, t. p., beaudoin, p., bengio, y., bogacz, r.,\nchristensen, a., clopath, c., costa, r. p., de berker, a., ganguli, s.,\ngillon, c. j., hafner, d., kepecs, a., kriegeskorte, n., latham, p.,\nlindsay, g. w., miller, k. d., naud, r., pack, c. c., ... kording, k. p.\n(2019). a deep learning framework for neuroscience. nature\nneuroscience, 22(11), 1761-1770. https://doi.org/10.1038/s41593-\n019-0520-2\n\npeer reviewed version\n\nlink to published version (if available):\n10.1038/s41593-019-0520-2\n\nlink to publication record in explore bristol research\npdf-document\n\nthis is the author accepted manuscript (aam). the final published version (version of record) is available online\nvia springer nature at https://www.nature.com/articles/s41593-019-0520-2. please refer to any applicable terms\nof use of the publisher.\n\nuniversity of bristol - explore bristol research\ngeneral rights\nthis document is made available in accordance with publisher policies. please cite only the\npublished", "j neurophysiol 101: 1813\u20131822, 2009.\nfirst published january 28, 2009; doi:10.1152/jn.91050.2008.\n\ntime course of attentional modulation in the frontal eye field during\ncurve tracing\n\np. s. khayat,1,3 a. pooresmaeili,1 and p. r. roelfsema1,2\n1department of vision and cognition, netherlands institute for neuroscience, institute of the royal netherlands academy of arts\nand sciences; 2department of integrative neurophysiology, centre for neurogenomics and cognitive research, vrije universiteit,\namsterdam, the netherlands; and 3department of physiology, mcgill university, montreal, quebec, canada\n\nsubmitted 19 september 2008; accepted in \ufb01nal form 23 january 2009\n\nkhayat ps, pooresmaeili a, roelfsema pr. time course of atten-\ntional modulation in the frontal eye \ufb01eld during curve tracing. j\nneurophysiol 101: 1813\u20131822, 2009. first published january 28,\n2009; doi:10.1152/jn.91050.2008. neurons in the frontal eye \ufb01elds\n(fefs) register incoming visual information and select visual stimuli\ntha", "n\ne\nu\nr\na\nl\n \nn\ne\nt\nw\no\nr\nk\ns\n,\n \nv\no\nl\n.\n \n6\n,\n \np\np\n.\n \n8\n0\n7\n-\n8\n2\n \ni\n,\n \n1\n9\n9\n3\n \n0\n8\n9\n3\n-\n6\n0\n8\n0\n/\n9\n3\n \n$\n6\n.\n0\n0\n \n+\n \n.\n0\n0\n \np\nr\ni\nn\nt\ne\nd\n \ni\nn\n \nt\nh\ne\n \nu\ns\na\n.\n \na\nl\nl\n \nr\ni\ng\nh\nt\ns\n \nr\ne\ns\ne\nr\nv\ne\nd\n.\n \nc\no\np\ny\nr\ni\ng\nh\nt\n \n\u00a9\n \n1\n9\n9\n3\n \np\ne\nr\ng\na\nm\no\nn\n \np\nr\ne\ns\ns\n \nl\nt\nd\n.\n \no\nr\ni\ng\ni\nn\na\nl\n \nc\no\nn\nt\nr\ni\nb\nu\nt\ni\no\nn\n \na\n \nm\ne\nt\nh\no\nd\n \nf\no\nr\n \ni\nm\np\nr\no\nv\ni\nn\ng\n \nt\nh\ne\n \nr\ne\na\nl\n-\nt\ni\nm\ne\n \nr\ne\nc\nu\nr\nr\ne\nn\nt\n \nl\ne\na\nr\nn\ni\nn\ng\n \na\nl\ng\no\nr\ni\nt\nh\nm\n \nt\nh\ni\ne\nr\nr\ny\n \nc\na\nt\nf\no\nl\ni\ns\n \nk\na\nt\nh\no\nl\ni\ne\nk\ne\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ne\ni\nt\n \nl\ne\nu\nv\ne\nn\n,\n \nb\ne\nl\ng\ni\nu\nm\n \n(\nr\ne\nc\ne\ni\nv\ne\nd\n \n1\n3\n \na\np\nr\ni\nl\n \n1\n9\n9\n2\n;\n \nr\ne\nv\ni\ns\ne\nd\n \na\nn\nd\n \na\nc\nc\ne\np\nt\ne\nd\n \n1\n3\n \nj\na\nn\nu\na\nr\ny\n \n1\n9\n9\n3\n \n)\n \na\nb\ns\nt\nr\na\nc\nt\n-\n-\nw\ni\nl\nl\ni\na\nm\ns\n \na\nn\nd\n \nz\ni\np\ns\ne\nr\n \n(\n1\n9\n8\n9\n)\n \np\nr\no\np\no\ns\ne\nd\n \nt\nw\no\n \na\nn\na\nl\no\ng\nl\nt\ne\n \nl\ne\na\nr\nn\ni\nn\ng\n \na\nl\ng\no\nr\ni\nt\nh\nm\ns\n \nf\no\nr\n \nf\nu\nl\nl\ny\n \nr\ne\nc\nu\nr\nr\ne\nn\nt\n \nn\ne\nt\nw\no\nr\nk\ns\n.\n \nt\nh\ne\n \nf\ni\nr\ns\nt\n \nm\ne\nt\nh\no\nd\n \ni\ns\n \na\nn\n \ne\nx\na\nc\nt\n \ng\nr\na\nd\ni\n", "8\n1\n0\n2\n\n \nc\ne\nd\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n2\n4\n8\n0\n1\n\n.\n\n5\n0\n8\n1\n:\nv\ni\nx\nr\na\n\napproximating real-time recurrent learning with\n\nrandom kronecker factors\n\nasier mujika \u2217\n\ndepartment of computer science\n\neth z\u00fcrich, switzerland\nasierm@inf.ethz.ch\n\nflorian meier\n\ndepartment of computer science\n\neth z\u00fcrich, switzerland\nmeierflo@inf.ethz.ch\n\nangelika steger\n\ndepartment of computer science\n\neth z\u00fcrich, switzerland\nsteger@inf.ethz.ch\n\nabstract\n\ndespite all the impressive advances of recurrent neural networks, sequential data is\nstill in need of better modelling. truncated backpropagation through time (tbptt),\nthe learning algorithm most widely used in practice, suffers from the truncation bias,\nwhich drastically limits its ability to learn long-term dependencies.the real-time\nrecurrent learning algorithm (rtrl) addresses this issue, but its high computa-\ntional requirements make it infeasible in practice. the unbiased online recurrent\noptimization algorithm (uoro) approximates rtrl wit", "letters\n\nvol 453 | 19 june 2008 | doi:10.1038/nature06910\n\nneural substrates of vocalization feedback\nmonitoring in primate auditory cortex\nsteven j. eliades1 & xiaoqin wang1\n\nvocal communication involves both speaking and hearing, often\ntaking place concurrently. vocal production, including human\nspeech and animal vocalization, poses a number of unique chal-\nlenges for the auditory system. it is important for the auditory\nsystem to monitor external sounds continuously from the acoustic\nenvironment during speaking despite the potential for sensory\nmasking by self-generated sounds1. it is also essential for the\nauditory system to monitor feedback of one\u2019s own voice. this\nself-monitoring may play a part in distinguishing between self-\ngenerated or externally generated2,3 auditory inputs and in detect-\ning errors in our vocal production4. previous work in humans5\u201310\nand other animals11\u201313 has demonstrated that the auditory cortex is\nlargely suppressed during speaking or vocalizing. despit", "axiomatic attribution for deep networks\n\nmukund sundararajan * 1 ankur taly * 1 qiqi yan * 1\n\nabstract\n\nwe study the problem of attributing the pre-\ndiction of a deep network to its input features,\na problem previously studied by several other\nworks. we identify two fundamental axioms\u2014\nsensitivity and implementation invariance that\nattribution methods ought to satisfy. we show\nthat they are not satis\ufb01ed by most known attri-\nbution methods, which we consider to be a fun-\ndamental weakness of those methods. we use\nthe axioms to guide the design of a new attri-\nbution method called integrated gradients. our\nmethod requires no modi\ufb01cation to the original\nnetwork and is extremely simple to implement;\nit just needs a few calls to the standard gradi-\nent operator. we apply this method to a couple\nof image models, a couple of text models and a\nchemistry model, demonstrating its ability to de-\nbug networks, to extract rules from a network,\nand to enable users to engage with models better.\n\n1. m", "h i g h l i g h t s\n\nc e l l b i o lo g y   o f  t h e   n e u r o n\n\ndirecting neurofilament traffic\n\nneurofilaments are components of the\nneuronal cytoskeleton that are important\nboth for the maturation of axons and the\nmaintenance of axonal integrity. the\nindividual subunits that make up the\nneurofilaments are transported from the cell\nbody to the axon, where they bundle together\nto form filaments. so how does the neuron\nensure that the neurofilaments are assembled\nin the correct compartment of the cell? as\nshea and colleagues report in the journal of\ncell science, phosphorylation, and in\nparticular the activity of the serine/threonine\nkinase cdk5 (cyclin-dependent protein\nkinase 5), seems to be the key.\n\nthe authors investigated the effects of\n\nenhancing or inhibiting the activity of cdk5 in\ncultured chick dorsal root ganglion neurons.\nthey found that if they overexpressed \n\ncdk5 along with its activator p35 \u2014 a\nmanipulation that leads to increased\nphosphorylation of the carboxyl t", "5\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n8\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n1\n5\n2\n2\n0\n\n.\n\n2\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nfrom pixels to torques: policy learning with deep dynamical models\n\nniklas wahlstr\u00a8om\ndivision of automatic control, link\u00a8oping university, link\u00a8oping, sweden\nthomas b. sch\u00a8on\ndepartment of information technology, uppsala university, sweden\nmarc peter deisenroth\ndepartment of computing, imperial college london, united kingdom\n\nnikwa@isy.liu.se\n\nthomas.schon@it.uu.se\n\nm.deisenroth@imperial.ac.uk\n\nabstract\n\ndata-ef\ufb01cient learning in continuous state-action\nspaces using very high-dimensional observations\nremains a key challenge in developing fully\nautonomous systems.\nin this paper, we con-\nsider one instance of this challenge,\nthe pix-\nels to torques problem, where an agent must\nlearn a closed-loop control policy from pixel in-\nformation only. we introduce a data-ef\ufb01cient,\nmodel-based reinforcement learning algorithm\nthat learns such a closed-loop policy directly\nfrom pixel information. the key in"]}