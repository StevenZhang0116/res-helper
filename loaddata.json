{"path": ["../paper/1-s2.0-S0959438800001537-main.pdf", "../paper/PIIS0896627309002062.pdf", "../paper/2012.05208.pdf", "../paper/carlson16.pdf", "../paper/PhysRevLett.59.2229.pdf", "../paper/make-03-00032-v2.pdf", "../paper/1-s2.0-S0896627315010405-main.pdf", "../paper/2107.04084.pdf", "../paper/*nrn3136.pdf", "../paper/fnhum-06-00359.pdf", "../paper/s10827-009-0179-x.pdf", "../paper/1703.00810.pdf", "../paper/nature12983.pdf", "../paper/1-s2.0-S0896627310007579-main.pdf", "../paper/2030801.pdf", "../paper/s41467-020-17236-y.pdf", "../paper/6-3-406.pdf", "../paper/nature11911.pdf", "../paper/1710.10044.pdf", "../paper/2111.06377.pdf", "../paper/nn.4486.pdf", "../paper/nature14855.pdf", "../paper/science.279.5355.1351.pdf", "../paper/Parallel_Reinforcement_Learning_with_Linear_Functi.pdf", "../paper/1908.01867.pdf", "../paper/nature07140.pdf", "../paper/SussilloArXive16.pdf", "../paper/2112.full.pdf", "../paper/1804.00222.pdf", "../paper/johansen-et-al-2014-hebbian-and-neuromodulatory-mechanisms-interact-to-trigger-associative-memory-formation.pdf", "../paper/wan13.pdf", "../paper/1702.08591.pdf", "../paper/2102.05815.pdf", "../paper/graybiel-2008-habits-rituals-and-the-evaluative-brain.pdf", "../paper/nrn2286.pdf", "../paper/1811.03600.pdf", "../paper/1803.01206.pdf", "../paper/1-s2.0-S0896627317307791-mainext.pdf", "../paper/1611.01232.pdf", "../paper/srep00417.pdf", "../paper/1301.3583.pdf", "../paper/1602.03253.pdf", "../paper/1-s2.0-S009286741730990X-main.pdf", "../paper/nature13294.pdf", "../paper/nature10776.pdf", "../paper/1-s2.0-S0896627321007285-main.pdf", "../paper/2101.03288.pdf", "../paper/agarwal14a.pdf", "../paper/deconvolutionalnetworks.pdf", "../paper/1602.02830.pdf", "../paper/1936.full.pdf", "../paper/41593_2010_BFnn2501_MOESM20_ESM.pdf", "../paper/1406.2661.pdf", "../paper/1-s2.0-S0896627319300108-main.pdf", "../paper/fncir-09-00085.pdf", "../paper/science.pdf", "../paper/annurev-neuro-120320-082744.pdf", "../paper/2111.08005.pdf", "../paper/1602.05179.pdf", "../paper/guo22d.pdf", "../paper/1905.00414.pdf", "../paper/nn.4242.pdf", "../paper/0700.pdf", "../paper/Eur J of Neuroscience - 2020 - Rubin.pdf", "../paper/1-s2.0-S016622361100110X-main.pdf", "../paper/fnsyn-02-00029.pdf", "../paper/2108.01368.pdf", "../paper/PIIS0896627317304142.pdf", "../paper/1-s2.0-S0092867419311705-main.pdf", "../paper/NIPS-2011-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent-Paper.pdf", "../paper/NeurIPS-2022-biologically-plausible-backpropagation-through-arbitrary-timespans-via-local-neuromodulators-Supplemental-Conference.pdf", "../paper/440396v1.full.pdf", "../paper/nokland19a.pdf", "../paper/1705.08292.pdf", "../paper/1801.04062.pdf", "../paper/PhysRevX.4.021039.pdf", "../paper/1-s2.0-S089662731500625X-main.pdf", "../paper/guideTR.pdf", "../paper/annurev-vision-082114-035447.pdf", "../paper/1905.12616.pdf", "../paper/annurev-neuro-092619-094115.pdf", "../paper/NIPS-2017-cortical-microcircuits-as-gated-recurrent-neural-networks-Paper.pdf", "../paper/1809.02386.pdf", "../paper/nature23020.pdf", "../paper/mishkin-iclr2016.pdf", "../paper/1810.01075.pdf", "../paper/NIPS-2017-attention-is-all-you-need-Paper.pdf", "../paper/1-s2.0-S0896627312008215-main.pdf", "../paper/smith2003.pdf", "../paper/s41467-023-40141-z.pdf", "../paper/annurev-neuro-072116-031109.pdf", "../paper/fncom-08-00038.pdf", "../paper/wacongne-et-al-2011-evidence-for-a-hierarchy-of-predictions-and-prediction-errors-in-human-cortex.pdf", "../paper/1211.5063.pdf", "../paper/1705.02436.pdf", "../paper/2106.02736.pdf", "../paper/nips_deep_workshop_2010.pdf", "../paper/1802.01933.pdf", "../paper/nature04676.pdf", "../paper/1706.05806.pdf", "../paper/1-s2.0-S1053811910000698-main.pdf", "../paper/s41467-018-05873-3.pdf", "../paper/1611.01353.pdf", "../paper/document.pdf", "../paper/elife-13665-v2.pdf", "../paper/2203.00573.pdf", "../paper/Rahnev&Denison(2018)BBS.pdf", "../paper/cunningham15a.pdf", "../paper/nn.2590.pdf", "../paper/elife-66551-v1.pdf", "../paper/2006.07232.pdf", "../paper/1207.0057.pdf", "../paper/nn.4650.pdf", "../paper/1506.02025.pdf", "../paper/TR_Dimensionality_Reduction_Review_2009.pdf", "../paper/Zavatone-Veth_2022_J._Stat._Mech._2022_114008.pdf", "../paper/WRAP-constraining-levels-justification-Sanborn-2017.pdf", "../paper/NIPS-1999-policy-gradient-methods-for-reinforcement-learning-with-function-approximation-Paper.pdf", "../paper/nrn1607.pdf", "../paper/rstb.2019.0761.pdf", "../paper/1603.05642.pdf", "../paper/1902.04760.pdf", "../paper/cortes_vapnik95.pdf", "../paper/gk7812.pdf", "../paper/NIPS-2017-dynamic-routing-between-capsules-Paper.pdf", "../paper/rstb.2008.0306.pdf", "../paper/bromberg-martin-et-al-2010-a-pallidus-habenula-dopamine-pathway-signals-inferred-stimulus-values.pdf", "../paper/science.273.5283.1868.pdf", "../paper/CABN.8.4.429.pdf", "../paper/nihms530595.pdf", "../paper/ardid.jn2007.pdf", "../paper/1802.09766.pdf", "../paper/Cowles-MarkovChainMonte-1996.pdf", "../paper/PIIS0896627302008206.pdf", "../paper/ullman-et-al-2016-atoms-of-recognition-in-human-and-computer-vision.pdf", "../paper/ncomms11393.pdf", "../paper/3685.full.pdf", "../paper/2022.03.28.485868v2.full.pdf", "../paper/rieecml05.pdf", "../paper/brainsci-09-00300.pdf", "../paper/NIPS-2016-matrix-completion-has-no-spurious-local-minimum-Paper.pdf", "../paper/1403.6382.pdf", "../paper/2022.05.09.491042v3.full.pdf", "../paper/smdae_techreport.pdf", "../paper/dawcourvday08.pdf", "../paper/murdoch-et-al-2019-definitions-methods-and-applications-in-interpretable-machine-learning.pdf", "../paper/nn.2396.pdf", "../paper/PIIS0896627312007039.pdf", "../paper/file121231231.pdf", "../paper/*1-s2.0-0893608094901090-main.pdf", "../paper/1711.02391.pdf", "../paper/nn.3711.pdf", "../paper/s00422-012-0512-8.pdf", "../paper/nature03721.pdf", "../paper/nn.4042.pdf", "../paper/1812.05905.pdf", "../paper/nn.4056.pdf", "../paper/1-s2.0-S0896627316001021-main.pdf", "../paper/1602.05179v2.pdf", "../paper/s41586-023-06377-x.pdf", "../paper/PehlevanSenguptaChklovskii18.pdf", "../paper/BHR2014.pdf", "../paper/*ijcv06-mvu.pdf", "../paper/PIIS089662731400734X.pdf", "../paper/2020102120200969969.pdf", "../paper/Gerstner96.pdf", "../paper/s41586-021-03819-2.pdf", "../paper/1-s2.0-S0959438818302009-main.pdf", "../paper/nature13282.pdf", "../paper/physrev.00023.2014.pdf", "../paper/954_ffjord_free_form_continuous_dy.pdf", "../paper/1-s2.0-016622369390081V-main.pdf", "../paper/file (7).pdf", "../paper/A_model_of_saliency-based_visual_attention_for_rapid_scene_analysis.pdf", "../paper/7255.full.pdf", "../paper/1312.6026.pdf", "../paper/Analysis_of_the_back-propagation_algorithm_with_momentum.pdf", "../paper/835_towards_deep_learning_models_r.pdf", "../paper/PIIS0960982221010526.pdf", "../paper/1-s2.0-S0896627311009305-main.pdf", "../paper/1804.11271.pdf", "../paper/srep32672.pdf", "../paper/1-s2.0-S1053811914005199-main.pdf", "../paper/science.aad3647.pdf", "../paper/1-s2.0-S0896627318309577-main.pdf", "../paper/GreBouSmoSch05.pdf", "../paper/PIIS0896627309005479.pdf", "../paper/Paper 29.pdf", "../paper/pnas.1903070116.pdf", "../paper/1-s2.0-S027826261530035X-main.pdf", "../paper/s41583-021-00473-5.pdf", "../paper/1809.03702.pdf", "../paper/science.1236425.pdf", "../paper/s41583-018-0049-5.pdf", "../paper/jn.01095.2002.pdf", "../paper/nn.3658.pdf", "../paper/PhysRevLett.97.048104.pdf", "../paper/NIPS-2016-composing-graphical-models-with-neural-networks-for-structured-representations-and-fast-inference-Paper.pdf", "../paper/PIIS089662730300761X.pdf", "../paper/science.aav7893.pdf", "../paper/pnas.93.23.13339.pdf", "../paper/1812.07040.pdf", "../paper/1-s2.0-S0896627301005426-main.pdf", "../paper/2201.08025.pdf", "../paper/1306.1091.pdf", "../paper/9659.full.pdf", "../paper/s41928-022-00913-9.pdf", "../paper/sprekeler-naud-2018-sparse-bursts-optimize-information-transmission-in-a-multiplexed-neural-code.pdf", "../paper/gershman-daw-2017-reinforcement-learning-and-episodic-memory-in-humans-and-animals-an-integrative-framework.pdf", "../paper/nature03687.pdf", "../paper/elife-18073-v2.pdf", "../paper/2201.13415.pdf", "../paper/doe-nc.pdf", "../paper/NeurIPS-2020-the-interplay-between-randomness-and-structure-during-learning-in-rnns-Paper.pdf", "../paper/nn1100_1178.pdf", "../paper/annurev-neuro-071714-034002.pdf", "../paper/NN00new.pdf", "../paper/annurev-neuro-080317-061948.pdf", "../paper/1607.06450.pdf", "../paper/1003.0358.pdf", "../paper/maron19a.pdf", "../paper/1-s2.0-S0896627319300534-main.pdf", "../paper/*pnas.1403112111.pdf", "../paper/nn.3106.pdf", "../paper/1319.full.pdf", "../paper/1611.09913.pdf", "../paper/nature04671.pdf", "../paper/PIIS0896627304007123.pdf", "../paper/*nature12160.pdf", "../paper/PIIS0896627301003014.pdf", "../paper/neco.2010.05-09-1010.pdf", "../paper/cogscibm.pdf", "../paper/nature18933.pdf", "../paper/1810.06721.pdf", "../paper/pnas.1320116110.pdf", "../paper/Murphy (Miller), Balanced Amplification_ A New Mechanism of Selective Amplification of Neural Activity Patterns.pdf", "../paper/1610.03483.pdf", "../paper/1809.08848.pdf", "../paper/s41586-019-0919-7.pdf", "../paper/acs.chemrev.6b00163.pdf", "../paper/1-s2.0-S1364661309002617-main.pdf", "../paper/s42256-019-0048-x.pdf", "../paper/1-s2.0-S0893608001000685-main.pdf", "../paper/PhysRevLett.69.3717.pdf", "../paper/1411.1792.pdf", "../paper/2451.full.pdf", "../paper/NIPS-2006-greedy-layer-wise-training-of-deep-networks-Paper.pdf", "../paper/1995-williams.pdf", "../paper/1806.05451.pdf", "../paper/1512.04860.pdf", "../paper/s41586-023-06221-2.pdf", "../paper/PIIS0896627303001697.pdf", "../paper/1901.08584.pdf", "../paper/NIPS-2013-annealing-between-distributions-by-averaging-moments-Paper.pdf", "../paper/file (6).pdf", "../paper/9587.full.pdf", "../paper/annurev-neuro-070815-013851.pdf", "../paper/1-s2.0-S0896627306004715-main.pdf", "../paper/1506.00019.pdf", "../paper/1-s2.0-S1364661320301066-main.pdf", "../paper/1-s2.0-S0959438821000933-main.pdf", "../paper/1608.05859.pdf", "../paper/nature12600.pdf", "../paper/3397_single_layers_of_attention_suf.pdf", "../paper/Mignacco_2021_Mach._Learn.__Sci._Technol._2_035029.pdf", "../paper/1-s2.0-S0959438813000330-main.pdf", "../paper/18999.full.pdf", "../paper/nature11321.pdf", "../paper/PhysRevX.8.041029.pdf", "../paper/Bidirectional_Long_Short-Term_Memory_Networks_for_Predicting_the_Subcellular_Localization_of_Eukaryotic_Proteins.pdf", "../paper/1-s2.0-S0896627317304713-main.pdf", "../paper/1301.3584.pdf", "../paper/1511.05222.pdf", "../paper/1-s2.0-S0959438814000373-main.pdf", "../paper/2006.15222.pdf", "../paper/1807.01251.pdf", "../paper/41467_2016_BFncomms13276_MOESM954_ESM.pdf", "../paper/Simard.pdf", "../paper/1-s2.0-S0079742108605368-main.pdf", "../paper/JN_Kloosterman_Chen14.pdf", "../paper/1-s2.0-S0959438813002158-main.pdf", "../paper/nn.4244.pdf", "../paper/nature08499.pdf", "../paper/science.1225266.pdf", "../paper/1612.02734.pdf", "../paper/Cook_Software_Validation.pdf", "../paper/nn2066.pdf", "../paper/nn.3477.pdf", "../paper/Cook-ValidationSoftwareBayesian-2006.pdf", "../paper/1-s2.0-S0959438821001276-main.pdf", "../paper/nn1209.pdf", "../paper/1805.10451.pdf", "../paper/nn.3311.pdf", "../paper/spruston-remy-2007-dendritic-spikes-induce-single-burst-long-term-potentiation.pdf", "../paper/jozefowicz15.pdf", "../paper/383076a0.pdf", "../paper/*NIPS-2003-locality-preserving-projections-Paper.pdf", "../paper/nature11057.pdf", "../paper/1412.0233.pdf", "../paper/s41586-020-2907-3.pdf", "../paper/Probability_of_error_for_optimal_codes_in_a_Gaussian_channel.pdf", "../paper/1-s2.0-S0092867407012056-main.pdf", "../paper/nature25457.pdf", "../paper/1-s2.0-S0896627312008471-main.pdf", "../paper/science.1195870.pdf", "../paper/schulman15.pdf", "../paper/nature11527.pdf", "../paper/NeurIPS-2018-slayer-spike-layer-error-reassignment-in-time-Paper.pdf", "../paper/Model-Free_reinforcement_learning_with_continuous_action_in_practice.pdf", "../paper/s41586-021-03771-1.pdf", "../paper/1903.06070.pdf", "../paper/2102.09351.pdf", "../paper/1611.02648.pdf", "../paper/ncomms4675.pdf", "../paper/pnas.1109359109.pdf", "../paper/1910.01619.pdf", "../paper/A_Comprehensive_Review_of_Stability_Analysis_of_Continuous-Time_Recurrent_Neural_Networks.pdf", "../paper/1-s2.0-S0165017307001774-main.pdf", "../paper/balduzzi17b.pdf", "../paper/1511.07543.pdf", "../paper/2022.11.14.516537v1.full.pdf", "../paper/1-s2.0-036402139090002E-main.pdf", "../paper/Baldi_Hornik-89.pdf", "../paper/*1312.6120.pdf", "../paper/ncomms13804.pdf", "../paper/1709.04396.pdf", "../paper/514_unbiased_online_recurrent_opti.pdf", "../paper/science.1169405.pdf", "../paper/pnas.1820226116.pdf", "../paper/Myung-2003.pdf", "../paper/1-s2.0-S0896627303002551-main.pdf", "../paper/nn0900_895.pdf", "../paper/2002.04745.pdf", "../paper/jn.2000.83.1.588.pdf", "../paper/1-s2.0-S1074742704000735-main.pdf", "../paper/Formal_Theory_of_Creativity_Fun_and_Intrinsic_Motivation_19902010.pdf", "../paper/1811.03567.pdf", "../paper/1-s2.0-S0092867411001279-main.pdf", "../paper/RumelhartBackprop.pdf", "../paper/s41593-019-0417-0.pdf", "../paper/1-s2.0-S089662730900083X-main.pdf", "../paper/ma18a.pdf", "../paper/35016072.pdf", "../paper/418939v2.full.pdf", "../paper/1-s2.0-089360809090019H-main.pdf", "../paper/2105.14602.pdf", "../paper/1708.02182.pdf", "../paper/annurev.neuro.29.051605.113038.pdf", "../paper/1-s2.0-S0896627319301230-main.pdf", "../paper/NeurIPS-2019-untangling-in-invariant-speech-recognition-Paper.pdf", "../paper/elife-16534-v1.pdf", "../paper/PIIS0896627317304634.pdf", "../paper/fnins-11-00324.pdf", "../paper/science.aaa4056.pdf", "../paper/2481.full.pdf", "../paper/pnas.2014196118.pdf", "../paper/1407.7906.pdf", "../paper/1-s2.0-S0896627309000038-main.pdf", "../paper/*PIIS0896627312008173.pdf", "../paper/1-s2.0-S0896627309001287-mainext.pdf", "../paper/1610.01644.pdf", "../paper/NeurIPS-2021-neural-population-geometry-reveals-the-role-of-stochasticity-in-robust-perception-Paper.pdf", "../paper/41467_2021_21696_MOESM1_ESM.pdf", "../paper/nature20101.pdf", "../paper/elife-66039-v4.pdf", "../paper/2305.08746.pdf", "../paper/nature14273.pdf", "../paper/ncfast.pdf", "../paper/2209.07484.pdf", "../paper/science.274.5286.427.pdf", "../paper/s41583-023-00756-z.pdf", "../paper/1512.06293.pdf", "../paper/1-s2.0-S0959438821000027-main.pdf", "../paper/nn.3064.pdf", "../paper/1607.04331.pdf", "../paper/1312.6034.pdf", "../paper/18-188.pdf", "../paper/1-s2.0-S2352154616301371-main.pdf", "../paper/s41592-022-01675-0.pdf", "../paper/1-s2.0-S1053811922004797-main.pdf", "../paper/s41593-017-0028-6.pdf", "../paper/nrn.2016.150.pdf", "../paper/1611.01578.pdf", "../paper/1711.07205.pdf", "../paper/1909.05858.pdf", "../paper/Input_feature_selection_by_mutual_information_based_on_Parzen_window.pdf", "../paper/Hippocampus - 2015 - Buzs ki - Hippocampal sharp wave\u2010ripple  A cognitive biomarker for episodic memory and planning.pdf", "../paper/nn.4247.pdf", "../paper/s41593-018-0095-3.pdf", "../paper/s42256-022-00556-7.pdf", "../paper/neco.2008.11-07-654.pdf", "../paper/1411.1784.pdf", "../paper/1-s2.0-S0959438818301065-main.pdf", "../paper/ben-yishai-et-al-1995-theory-of-orientation-tuning-in-visual-cortex.pdf", "../paper/pnas.201821594.pdf", "../paper/Chap6_PDP86.pdf", "../paper/1-s2.0-S009286741930220X-main.pdf", "../paper/1-s2.0-S0896627318304185-main.pdf", "../paper/1409.3215.pdf", "../paper/download.pdf", "../paper/1606.07129.pdf", "../paper/1-s2.0-S2211124718303735-main.pdf", "../paper/2022.08.15.503870v1.full.pdf", "../paper/nn1222.pdf", "../paper/nature07150.pdf", "../paper/NIPS-1999-actor-critic-algorithms-Paper.pdf", "../paper/1709.01953.pdf", "../paper/jones-love-BBS.pdf", "../paper/1-s2.0-S0896627316309576-main.pdf", "../paper/1981-20731-001.pdf", "../paper/1901.09049.pdf", "../paper/PIIS0960982213009202.pdf", "../paper/pin.pdf", "../paper/1703.10622.pdf", "../paper/9353.full.pdf", "../paper/1707.02921.pdf", "../paper/NeurIPS_time_permutation_invariant_representation.pdf", "../paper/*Lafon06.pdf", "../paper/ravanbakhsh17a.pdf", "../paper/nonconvergence.pdf", "../paper/19-562.pdf", "../paper/Multi-column_deep_neural_networks_for_image_classification.pdf", "../paper/s41586-019-1346-5.pdf", "../paper/089976698300017115.pdf", "../paper/2020.06.15.148114v2.full.pdf", "../paper/science.1104171.pdf", "../paper/nature10835.pdf", "../paper/pnas.1316181111.pdf", "../paper/1702.08608.pdf", "../paper/PIIS0896627319304945.pdf", "../paper/2010.00525.pdf", "../paper/1611.00712.pdf", "../paper/Lakshminarayan_Chinta_Venkateswararao_PhDThesis.pdf", "../paper/s41586-021-03652-7.pdf", "../paper/1-s2.0-S1471489216301333-main.pdf", "../paper/s41467-021-21696-1.pdf", "../paper/pnas.1820296116.pdf", "../paper/CHL90.pdf", "../paper/1911.00890.pdf", "../paper/2006.12878.pdf", "../paper/2020.11.02.365072v1.full.pdf", "../paper/1-s2.0-S0143417913000784-mainext.pdf", "../paper/1-s2.0-S1053811913005065-main.pdf", "../paper/srivastava14a.pdf", "../paper/pnas.201403112si.pdf", "../paper/mmc2.pdf", "../paper/NeurIPS-2022-biologically-plausible-backpropagation-through-arbitrary-timespans-via-local-neuromodulators-Paper-Conference.pdf", "../paper/jphysiol00723-0461.pdf", "../paper/q_learn.pdf", "../paper/NIPS-1993-credit-assignment-through-time-alternatives-to-backpropagation-Paper.pdf", "../paper/1506.06579.pdf", "../paper/1503.03585.pdf", "../paper/1605.07146.pdf", "../paper/1806.01242.pdf", "../paper/564476v2.full.pdf", "../paper/nature18942.pdf", "../paper/science.1159775.pdf", "../paper/11573.full.pdf", "../paper/1-s2.0-S0896627311001966-main.pdf", "../paper/s41593-019-0460-x.pdf", "../paper/pnas.0508601103.pdf", "../paper/tr00-004.pdf", "../paper/nbt1406.pdf", "../paper/1410.3916.pdf", "../paper/2005.11362.pdf", "../paper/The_use_of_fast_Fourier_transform_for_the_estimation_of_power_spectra_A_method_based_on_time_averaging_over_short_modified_periodograms.pdf", "../paper/s41593-021-00857-x.pdf", "../paper/18531.full.pdf", "../paper/*s41583-020-0277-3.pdf", "../paper/file copy 12.pdf", "../paper/1703.00522.pdf", "../paper/s11263-008-0178-9.pdf", "../paper/1-s2.0-S0896627314000191-main.pdf", "../paper/baity-jesi18a.pdf", "../paper/iconip98_pre.pdf", "../paper/1602.03032.pdf", "../paper/1-s2.0-S0092867415009733-main.pdf", "../paper/8492_no_free_lunch_from_deep_learni.pdf", "../paper/1-s2.0-S1364661304002153-main.pdf", "../paper/1-s2.0-S0079612305490111-main.pdf", "../paper/annurev-neuro-062111-150509.pdf", "../paper/elife-43299-v3.pdf", "../paper/1-s2.0-S016622361100124X-main.pdf", "../paper/PIIS0092867415011964.pdf", "../paper/nn.2889.pdf", "../paper/1905.04610.pdf", "../paper/larochelle11a.pdf", "../paper/1905.11786.pdf", "../paper/1511.09468.pdf", "../paper/1609.04836.pdf", "../paper/nn.3981.pdf", "../paper/1-s2.0-S0896627314003602-main.pdf", "../paper/out.pdf", "../paper/1906.00443.pdf", "../paper/nature13235.pdf", "../paper/12477.full.pdf", "../paper/NeurIPS-2020-supervised-contrastive-learning-Paper.pdf", "../paper/s41467-017-01827-3.pdf", "../paper/PIIS0896627312009920.pdf", "../paper/2103.14662.pdf", "../paper/1707.04926.pdf", "../paper/1703.01365.pdf", "../paper/nips02-localglobal-in-press.pdf", "../paper/s41586-019-1816-9.pdf", "../paper/file (1).pdf", "../paper/nrn.2016.53.pdf", "../paper/1-s2.0-S0959438817300612-main.pdf", "../paper/1911.02116.pdf", "../paper/atanasov_etal_iclr_2022.pdf", "../paper/elife-20899-v2.pdf", "../paper/file1111.pdf", "../paper/nature01341.pdf", "../paper/CollinsFrank2013PsychReview.pdf", "../paper/P10-1040.pdf", "../paper/1507.06527.pdf", "../paper/yeung-et-al-2004-synaptic-homeostasis-and-input-selectivity-follow-from-a-calcium-dependent-plasticity-model.pdf", "../paper/node_perturbation.pdf", "../paper/elife-48198-v1.pdf", "../paper/gal17a.pdf", "../paper/PIIS0896627316305116.pdf", "../paper/A Learning Algorithm for Continually Running Fully recurrent neural networks.pdf", "../paper/Convolutional adaptive denoising autoencoders for hierarchical feature extraction.pdf", "../paper/9565.full.pdf", "../paper/1-s2.0-S030645221730547X-main.pdf", "../paper/9424.full.pdf", "../paper/file copy 13.pdf", "../paper/nn.4433.pdf", "../paper/file copy 4.pdf", "../paper/1411.4077.pdf", "../paper/jn.1998.79.2.1017.pdf", "../paper/1207.4708.pdf", "../paper/nature08577.pdf", "../paper/s00415-011-6299-z.pdf", "../paper/file2222.pdf", "../paper/bhs348.pdf", "../paper/11597.full.pdf", "../paper/Box-SamplingBayesInference-1980.pdf", "../paper/PIIS0896627300806339.pdf", "../paper/1606.03498.pdf", "../paper/A Computational Model of Birdsong Learning by Auditory Experience and Auditory Feedback 1998-2926.pdf", "../paper/1-s2.0-S2405896321005462-main.pdf", "../paper/gutmann12a.pdf", "../paper/1-s2.0-S0893608002002289-main.pdf", "../paper/BF03194840.pdf", "../paper/13402.full.pdf", "../paper/elife-36275-v1.pdf", "../paper/nihms766520.pdf", "../paper/1-s2.0-S0092867414002906-main.pdf", "../paper/s41593-021-00873-x.pdf", "../paper/PIIS0896627312008586.pdf", "../paper/4052.full.pdf", "../paper/s41467-023-37180-x.pdf", "../paper/10356_a_path_towards_autonomous_mach.pdf", "../paper/mmc3.pdf", "../paper/s42254-021-00314-5.pdf", "../paper/alain14a.pdf", "../paper/1-s2.0-S0896627321005018-main.pdf", "../paper/1-s2.0-S0166432809005099-main.pdf", "../paper/NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "../paper/1-s2.0-S0896627313005576-main.pdf", "../paper/1611.02163.pdf", "../paper/1812.11118.pdf", "../paper/tramer20a.pdf", "../paper/s41593-019-0555-4.pdf", "../paper/1610.04161.pdf", "../paper/nn1905.pdf", "../paper/Han_Direct_Feedback_Alignment_Based_Convolutional_Neural_Network_Training_for_Low-Power_ICCVW_2019_paper.pdf", "../paper/1503.02101.pdf", "../paper/1511.02476.pdf", "../paper/1-s2.0-S0893608017302903-main.pdf", "../paper/1504.00941.pdf", "../paper/SMPBSSW-09.pdf", "../paper/petreska-etal-2011-nips-preprint.pdf", "../paper/Suh_2011_Science.pdf", "../paper/1994_Koch_Meinhardt_Pattern_Formation.pdf", "../paper/2006.10739.pdf", "../paper/1-s2.0-S0896627319300716-main.pdf", "../paper/gk8001.pdf", "../paper/nature17643.pdf", "../paper/nature11554.pdf", "../paper/jeb188912.pdf", "../paper/2206.11228.pdf", "../paper/annurev-neuro-100219-105424.pdf", "../paper/35020214.pdf", "../paper/zenke17a.pdf", "../paper/PIIS2211124714008638.pdf", "../paper/1-s2.0-S0960982213011299-main.pdf", "../paper/1-s2.0-S0166223604003352-main.pdf", "../paper/2308.16898.pdf", "../paper/1906.02691.pdf", "../paper/nature00854.pdf", "../paper/file.pdf", "../paper/elife-09685-v1.pdf", "../paper/pnas.79.8.2554.pdf", "../paper/1804.08150.pdf", "../paper/file copy 11.pdf", "../paper/1810.04805.pdf", "../paper/1-s2.0-S1074742798938436-main.pdf", "../paper/1-s2.0-S1053811909007186-main.pdf", "../paper/lu-et-al-2014-spike-timing-dependent-bdnf-secretion-and-synaptic-plasticity.pdf", "../paper/Maximum likelihood estimation of cascade point-process neural encoding models.pdf", "../paper/2105.13120.pdf", "../paper/file copy 6.pdf", "../paper/annurev.neuro.23.1.649.pdf", "../paper/070704277.pdf", "../paper/s41467-017-01109-y.pdf", "../paper/NeurIPS-2020-a-biologically-plausible-neural-network-for-slow-feature-analysis-Paper.pdf", "../paper/1-s2.0-S0896627312003844-main.pdf", "../paper/jn.00429.2011.pdf", "../paper/1809.04184.pdf", "../paper/1-s2.0-S0893608000000265-main.pdf", "../paper/2305.20050.pdf", "../paper/s41593-018-0147-8.pdf", "../paper/1-s2.0-S0959438817300910-am.pdf", "../paper/nn1859.pdf", "../paper/NeurIPS-2020-learning-identifiable-and-interpretable-latent-models-of-high-dimensional-neural-activity-using-pi-vae-Paper.pdf", "../paper/fncom-15-640235.pdf", "../paper/1705.08026.pdf", "../paper/1177728716.pdf", "../paper/2006.13436.pdf", "../paper/NIPS-2002-distance-metric-learning-with-application-to-clustering-with-side-information-Paper.pdf", "../paper/NeurIPS-2020-complex-dynamics-in-simple-neural-networks-understanding-gradient-flow-in-phase-retrieval-Paper.pdf", "../paper/1712.09913.pdf", "../paper/1802.08195.pdf", "../paper/nn1643.pdf", "../paper/1908.01580.pdf", "../paper/1-s2.0-S0092867419303915-main.pdf", "../paper/nature11347.pdf", "../paper/PIIS0896627318300072.pdf", "../paper/portilla99-reprint.pdf", "../paper/icml-2008-denoising-autoencoders.pdf", "../paper/fileasdad12.pdf", "../paper/nn.2479.pdf", "../paper/1705.08741.pdf", "../paper/neco_a_01193.pdf", "../paper/pnas01060-0383.pdf", "../paper/williams92simple.pdf", "../paper/1-s2.0-S0092867420312289-main.pdf", "../paper/nrn3476.pdf", "../paper/nature10918.pdf", "../paper/1609.01596.pdf", "../paper/science.1117593.pdf", "../paper/BF00961879.pdf", "../paper/1904.09751.pdf", "../paper/10099428.pdf", "../paper/2022.12.30.522267v1.full.pdf", "../paper/1-s2.0-S0896627318308328-main.pdf", "../paper/1-s2.0-S0896627312008021-main.pdf", "../paper/1-s2.0-S0010027716300907-main.pdf", "../paper/PhysRevLett.116.038701.pdf", "../paper/2301.08028.pdf", "../paper/journal.pone.0252345.pdf", "../paper/arjovsky16.pdf", "../paper/nn1253.pdf", "../paper/file copy 7.pdf", "../paper/TD_Policy_Eval_04.pdf", "../paper/8368.full.pdf", "../paper/s41467-020-14578-5.pdf", "../paper/srep28073.pdf", "../paper/file copy 10.pdf", "../paper/GershmanBlei2012.pdf", "../paper/2111.09794.pdf", "../paper/nature15257.pdf", "../paper/science.290.5500.2323.pdf", "../paper/nature05078.pdf", "../paper/nn.3405.pdf", "../paper/fncom-13-00018.pdf", "../paper/annurev.neuro.28.061604.135722.pdf", "../paper/1756-6606-6-10.pdf", "../paper/mazumder10a.pdf", "../paper/1-s2.0-S0896627301005244-main.pdf", "../paper/paper4.pdf", "../paper/nature08010.pdf", "../paper/s41593-018-0321-z.pdf", "../paper/1-s2.0-S0959438823001058-main.pdf", "../paper/s41562-017-0180-8.pdf", "../paper/PIIS0896627301005827.pdf", "../paper/1-s2.0-S0896627312009592-main.pdf", "../paper/nature10844.pdf", "../paper/nn.4197.pdf", "../paper/1-s2.0-S0893608002000564-main.pdf", "../paper/neural-network-approximation.pdf", "../paper/1-s2.0-S0959438811002236-main.pdf", "../paper/1-s2.0-S0896627315004134-main.pdf", "../paper/1611.05141.pdf", "../paper/2010.12632.pdf", "../paper/annrev.pdf", "../paper/7817.full.pdf", "../paper/science.7770778.pdf", "../paper/PIIS0896627311001036.pdf", "../paper/1810.02054.pdf", "../paper/pnas.1420068112.pdf", "../paper/9050.full.pdf", "../paper/s41592-020-01018-x.pdf", "../paper/9803008.pdf", "../paper/PhysRevLett.128.180201-accepted.pdf", "../paper/1812.06488.pdf", "../paper/elife-42870-v2.pdf", "../paper/mmc4.pdf", "../paper/1312.5602.pdf", "../paper/nn.4385.pdf", "../paper/fnins-12-00774.pdf", "../paper/1810.01993.pdf", "../paper/2021.12.21.473757.full.pdf", "../paper/1911.10688.pdf", "../paper/1906.04554.pdf", "../paper/elife-08998-v3.pdf", "../paper/file copy 14.pdf", "../paper/13522.full.pdf", "../paper/file copy 3.pdf", "../paper/1511.04599.pdf", "../paper/1112.6209.pdf", "../paper/1805.10369.pdf", "../paper/1-s2.0-S0166223612002032-main.pdf", "../paper/24_eligibility_traces_provide_a_d.pdf", "../paper/d93b.pdf", "../paper/He_Deep_Residual_Learning_CVPR_2016_paper.pdf", "../paper/1-s2.0-S0304394017302975-main.pdf", "../paper/ijcv04.pdf", "../paper/1-s2.0-S0301008296000421-main.pdf", "../paper/10.21105.joss.01003.pdf", "../paper/0899766054615699.pdf", "../paper/1-s2.0-S0149763416301336-main.pdf", "../paper/seo-et-al-2008-cortical-mechanisms-for-reinforcement-learning-in-competitive-games.pdf", "../paper/glimcher-2011-understanding-dopamine-and-reinforcement-learning-the-dopamine-reward-prediction-error-hypothesis.pdf", "../paper/nrn1932.pdf", "../paper/PIIS0896627322000058.pdf", "../paper/15497.full.pdf", "../paper/backprop.pdf", "../paper/1-s2.0-S0022249615000759-main.pdf", "../paper/1-s2.0-S089360800200045X-main.pdf", "../paper/1-s2.0-S266638992200160X-main.pdf", "../paper/nrn2022.pdf", "../paper/2009.05359.pdf", "../paper/2023_Farrell_current_opinion.pdf", "../paper/nrn.2016.40.pdf", "../paper/s41422-020-00448-8.pdf", "../paper/Santiago_Cadena_Diverse_feature_visualizations_ECCV_2018_paper.pdf", "../paper/1801.00062.pdf", "../paper/nrn2787.pdf", "../paper/1406.3269.pdf", "../paper/*nn.3450.pdf", "../paper/1707.06887.pdf", "../paper/RationalUseOfCognitiveResources.pdf", "../paper/2302.00487.pdf", "../paper/WIRES Cognitive Science - 2011 - Huang - Predictive coding.pdf", "../paper/neco.1990.2.4.490.pdf", "../paper/nrn2963.pdf", "../paper/nature04766.pdf", "../paper/file (12).pdf", "../paper/nn.3776.pdf", "../paper/1805.08651.pdf", "../paper/1603.01912.pdf", "../paper/1907.05600.pdf", "../paper/1603.09382.pdf", "../paper/1-s2.0-S0950705120302896-main.pdf", "../paper/1-s2.0-S0896627302009674-main.pdf", "../paper/farries-fairhall-2007-reinforcement-learning-with-modulated-spike-timing-dependent-synaptic-plasticity.pdf", "../paper/PIIS0092867421010606.pdf", "../paper/jn.00371.2011.pdf", "../paper/s41593-019-0414-3.pdf", "../paper/13326.full.pdf", "../paper/s10107-015-0893-2.pdf", "../paper/nature04968.pdf", "../paper/1-s2.0-S0092867421003731-main.pdf", "../paper/Gershman12.pdf", "../paper/annurev.neuro.23.1.473.pdf", "../paper/science.1155564.pdf", "../paper/NIPS-2003-learning-curves-for-stochastic-gradient-descent-in-linear-feedforward-networks-Paper.pdf", "../paper/MartensProvost_Explaining.pdf", "../paper/2006.07123.pdf", "../paper/Wilson.nn03.batch.pdf", "../paper/kunin20a.pdf", "../paper/1-s2.0-0167278983902981-main.pdf", "../paper/nature03236.pdf", "../paper/annurev.neuro.24.1.167.pdf", "../paper/1605.08104.pdf", "../paper/2010.11931.pdf", "../paper/1701.01724.pdf", "../paper/1910.01526.pdf", "../paper/930_bayesian_deep_convolutional_ne.pdf", "../paper/sharpee-et-al-2013-trade-off-between-curvature-tuning-and-position-invariance-in-visual-area-v4.pdf", "../paper/statistical-learning-of-temporal-community-structure-in-the-hippocampus.pdf", "../paper/ims.pdf", "../paper/file copy 2.pdf", "../paper/PIIS0896627318302502.pdf", "../paper/science.aal4835.pdf", "../paper/NIPS-2011-variational-learning-for-recurrent-spiking-networks-Paper.pdf", "../paper/2008.08186.pdf", "../paper/elife-47463-v2.pdf", "../paper/1712.00310.pdf", "../paper/241_deep_variational_information_b.pdf", "../paper/214262v2.full.pdf", "../paper/Azabou2023.pdf", "../paper/neco_a_00883.pdf", "../paper/PIIS089662730080700X.pdf", "../paper/1803.09574.pdf", "../paper/J Exper Analysis Behavior - 2013 - Lau - DYNAMIC RESPONSE\u2010BY\u2010RESPONSE MODELS OF MATCHING BEHAVIOR IN RHESUS MONKEYS.pdf", "../paper/PIIS0896627320307054.pdf", "../paper/Backpropagation_through_time_what_it_does_and_how_to_do_it.pdf", "../paper/mmc5.pdf", "../paper/Deep_HessianFree.pdf", "../paper/NIPS-2017-neural-system-identification-for-large-populations-separating-what-and-where-Paper.pdf", "../paper/pnas.1506407112.pdf", "../paper/s41467-019-11786-6.pdf", "../paper/1703.03400.pdf", "../paper/science.1179850.pdf", "../paper/1-s2.0-S106352030300023X-main.pdf", "../paper/Cold Spring Harb Perspect Biol-2012-Lu\u0308scher-a005710.pdf", "../paper/1-s2.0-S016622360001657X-main.pdf", "../paper/s41586-021-04223-6.pdf", "../paper/338947v3.full.pdf", "../paper/annurev-conmatphys-031119-050745.pdf", "../paper/elife-55592-v1.pdf", "../paper/sriperumbudur10a.pdf", "../paper/2110.10470.pdf", "../paper/NIPS-2016-memory-efficient-backpropagation-through-time-Paper.pdf", "../paper/12366.full.pdf", "../paper/1812.07965.pdf", "../paper/BF00114731.pdf", "../paper/PIIS0896627304005768.pdf", "../paper/PIIS0896627318304367.pdf", "../paper/Attention_as_an_Organ_System.pdf", "../paper/Liu_Modprop_2022.pdf", "../paper/NIPS-2014-expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-continuous-or-discrete-weights-Paper.pdf", "../paper/NIPS-2011-ica-with-reconstruction-cost-for-efficient-overcomplete-feature-learning-Paper.pdf", "../paper/1-s2.0-S0896627300810727-main.pdf", "../paper/fncom-07-00194.pdf", "../paper/18686.pdf", "../paper/1810.06966.pdf", "../paper/1712.08969.pdf", "../paper/NIPS-2001-on-spectral-clustering-analysis-and-an-algorithm-Paper.pdf", "../paper/neco.1995.7.6.1129.pdf", "../paper/bhl152.pdf", "../paper/2101.09258.pdf", "../paper/NIPS-2013-spectral-methods-for-neural-characterization-using-generalized-quadratic-models-Paper.pdf", "../paper/1301559.pdf", "../paper/TGbbs.pdf", "../paper/1909.13719.pdf", "../paper/science.1210280.pdf", "../paper/PIIS0896627309002372.pdf", "../paper/fncom-11-00048.pdf", "../paper/s41583-020-0275-5.pdf", "../paper/pnas.1031596100.pdf", "../paper/nn.2640.pdf", "../paper/1906.00889.pdf", "../paper/nrn1931.pdf", "../paper/zeilerECCV2014.pdf", "../paper/1206.3881.pdf", "../paper/1-s2.0-S0896627314001123-main.pdf", "../paper/1-s2.0-S0896627315007655-main.pdf", "../paper/s41583-023-00740-7.pdf", "../paper/carter19a.pdf", "../paper/1024.full.pdf", "../paper/NIPS-2016-linear-dynamical-neural-population-models-through-nonlinear-embeddings-Paper.pdf", "../paper/Neuroscience-Inspired_Online_Unsupervised_Learning_Algorithms_Artificial_Neural_Networks.pdf", "../paper/1904.05391.pdf", "../paper/1701.00160.pdf", "../paper/rstb.2013.0288.pdf", "../paper/1608.02164.pdf", "../paper/annurev-neuro-070815-013831.pdf", "../paper/s41551-020-0591-0.pdf", "../paper/1-s2.0-S0896627312003777-main.pdf", "../paper/jn.00803.2004.pdf", "../paper/pnas.2102157118.pdf", "../paper/AFST_2000_6_9_2_245_0.pdf", "../paper/aivodji19a.pdf", "../paper/s41586-021-04329-x.pdf", "../paper/9217-Article Text-12745-1-2-20201228.pdf", "../paper/nature12112.pdf", "../paper/1-s2.0-S0896627305008925-main.pdf", "../paper/psych.pdf", "../paper/Rho_Neural_Residual_Flow_Fields_for_Efficient_Video_Representations_ACCV_2022_paper.pdf", "../paper/1701.06538.pdf", "../paper/annurev-neuro-070815-013824.pdf", "../paper/NeurIPS-2020-dynamical-mean-field-theory-for-stochastic-gradient-descent-in-gaussian-mixture-classification-Paper.pdf", "../paper/cadieu-et-al-2007-a-model-of-v4-shape-selectivity-and-invariance.pdf", "../paper/weinberger09a.pdf", "../paper/neco.2007.19.2.442.pdf", "../paper/NIPS-2017-resurrecting-the-sigmoid-in-deep-learning-through-dynamical-isometry-theory-and-practice-Paper.pdf", "../paper/1669.full.pdf", "../paper/nature07709.pdf", "../paper/nn.3563.pdf", "../paper/empca.pdf", "../paper/13457.full.pdf", "../paper/pnas.0903214106.pdf", "../paper/annurev-neuro-090919-022842.pdf", "../paper/nrn2578.pdf", "../paper/10464.full.pdf", "../paper/WuNC06.pdf", "../paper/s1064827502419154.pdf", "../paper/1177011136.pdf", "../paper/1-s2.0-S0959438818301569-main.pdf", "../paper/1502.03509.pdf", "../paper/387278a0.pdf", "../paper/2002.09405.pdf", "../paper/1471-2202-9-S1-O13.pdf", "../paper/1-s2.0-S0168010212001800-main.pdf", "../paper/bengio14.pdf", "../paper/nature19325.pdf", "../paper/science.290.5500.2319.pdf", "../paper/1406.1078.pdf", "../paper/1-s2.0-S0896627305001170-main.pdf", "../paper/1507.04296.pdf", "../paper/OReillyFrank06_pbwm.pdf", "../paper/1207.0580.pdf", "../paper/12176.full.pdf", "../paper/science.1150769.pdf", "../paper/pnas.2111821118.pdf", "../paper/PIIS0896627318308560.pdf", "../paper/pnas.0505220103.pdf", "../paper/mmc6.pdf", "../paper/090771806.pdf", "../paper/s41583-023-00693-x.pdf", "../paper/shrikumar17a.pdf", "../paper/s41467-018-06560-z.pdf", "../paper/1-s2.0-0165017395000143-main.pdf", "../paper/NeurIPS-2018-gradient-descent-for-spiking-neural-networks-Paper.pdf", "../paper/1803.07770.pdf", "../paper/neco.2006.18.7.1577.pdf", "../paper/1089.full.pdf", "../paper/1604.00289.pdf", "../paper/nn.4191.pdf", "../paper/nn.3826.pdf", "../paper/taylor16.pdf", "../paper/Dasgupta17.pdf", "../paper/fncom-09-00149.pdf", "../paper/1509.06461.pdf", "../paper/1-s2.0-B9780444626042000228-main.pdf", "../paper/ncomms16091.pdf", "../paper/2107.13586.pdf", "../paper/nihms-1754234.pdf", "../paper/jocn_a_00365.pdf", "../paper/2023.07.18.549575v2.full.pdf", "../paper/2112.00980.pdf", "../paper/yang21f.pdf", "../paper/NeurIPS-2020-gaussian-gated-linear-networks-Paper.pdf", "../paper/1704.05796.pdf", "../paper/Valente2022nc_LDSvsRNN.pdf", "../paper/089976601750541787.pdf", "../paper/1412.3555.pdf", "../paper/218_towards_nonlinear_disentanglem.pdf", "../paper/16601.full.pdf", "../paper/2106.09620.pdf", "../paper/1811.04918.pdf", "../paper/2022.05.17.492325.full.pdf", "../paper/9673.full.pdf", "../paper/jn.1999.82.5.2676.pdf", "../paper/1-s2.0-S009286742031165X-main.pdf", "../paper/1601.00670.pdf", "../paper/1511.06342.pdf", "../paper/1412.7525.pdf", "../paper/*852_on_the_information_bottleneck_.pdf", "../paper/NIPS-2016-exponential-expressivity-in-deep-neural-networks-through-transient-chaos-Paper.pdf", "../paper/418939v1.full.pdf", "../paper/1-s2.0-S0364021387800253-main.pdf", "../paper/[Volume 1] David E. Rumelhart, James L. McClelland, PDP Research Group - Parallel Distributed Processing_ Explorations in the Microstructure of Cognition_ Foundations (1986, The MIT Press) - libgen.li.pdf", "../paper/1801.01944.pdf", "../paper/mmc2 (2).pdf", "../paper/The Journal of Physiology - 2013 - Grillner.pdf", "../paper/*s41586-021-04268-7.pdf", "../paper/Mainen-ReliabilitySpikeTiming-1995.pdf", "../paper/21m1446769.pdf", "../paper/annurev-neuro-061010-113641.pdf", "../paper/32.full.pdf", "../paper/webb20a.pdf", "../paper/1705.07874.pdf", "../paper/2210.02157.pdf", "../paper/12-BEJSP17.pdf", "../paper/7376.full.pdf", "../paper/buzsa\u0301ki-et-al-2007-hippocampal-place-cell-assemblies-are-speed-controlled-oscillators.pdf", "../paper/file (3).pdf", "../paper/1611.02779.pdf", "../paper/NeurIPS-2020-simulating-a-primary-visual-cortex-at-the-front-of-cnns-improves-robustness-to-image-perturbations-Paper.pdf", "../paper/083824v2.full.pdf", "../paper/1-s2.0-S0896627314001524-main.pdf", "../paper/Okorokova_2020_J._Neural_Eng._17_046035.pdf", "../paper/1509.02971.pdf", "../paper/1308.3432.pdf", "../paper/1-s2.0-S0896627314002517-main.pdf", "../paper/nn.2466.pdf", "../paper/elife-23763-v4.pdf", "../paper/jn.90941.2008.pdf", "../paper/2005.14165.pdf", "../paper/1606.05340.pdf", "../paper/Embedded_Data_Representations.pdf", "../paper/445715a.pdf", "../paper/woods-et-al-2002-shape-perception-reduces-activity-in-human-primary-visual-cortex.pdf", "../paper/1-s2.0-S0896627317301034-main.pdf", "../paper/file copy.pdf", "../paper/1412.6558.pdf", "../paper/2006.04182.pdf", "../paper/nrn2762.pdf", "../paper/Meyer-TwoMomentDecisionModels-1987.pdf", "../paper/1-s2.0-S0893608005800036-main.pdf", "../paper/NIPS-2016-deep-learning-models-of-the-retinal-response-to-natural-scenes-Paper.pdf", "../paper/deep_Qlearning_report.pdf", "../paper/759567.pdf", "../paper/The_Role_of_Constraints_in_Hebbian_Learning.pdf", "../paper/WaiJor08_FTML.pdf", "../paper/nature11601.pdf", "../paper/Nature Neuroscience_13_3_2010.pdf", "../paper/22.pdf", "../paper/1810.11393.pdf", "../paper/PhysRevX.11.021064.pdf", "../paper/1767_fantastic_generalization_measu.pdf", "../paper/dt.pdf", "../paper/3236386.3241340.pdf", "../paper/1505.04597.pdf", "../paper/barcodes.pdf", "../paper/nrn1055.pdf", "../paper/1-s2.0-0165027084900074-main.pdf", "../paper/2103.11511.pdf", "../paper/1704.08847.pdf", "../paper/s41593-021-01007-z.pdf", "../paper/s41467-022-34938-7.pdf", "../paper/fulltext.pdf", "../paper/978-1-4419-8853-9.pdf", "../paper/nn.3433.pdf", "../paper/1961189.1961200.pdf", "../paper/fnins-08-00349.pdf", "../paper/2303.04129.pdf", "../paper/nature07842.pdf", "../paper/murray-et-al-2016-stable-population-coding-for-working-memory-coexists-with-heterogeneous-neural-dynamics-in-prefrontal.pdf", "../paper/1-s2.0-S0165027016000418-main.pdf", "../paper/nature07467.pdf", "../paper/1807.03748.pdf", "../paper/pnas.1105933108.pdf", "../paper/1801.04381.pdf", "../paper/1606.04671.pdf", "../paper/s41593-018-0314-y.pdf", "../paper/12717.full.pdf", "../paper/1-s2.0-S0896627311005435-main.pdf", "../paper/PIIS0896627300000301.pdf", "../paper/2203.09517.pdf", "../paper/**16m1080173.pdf", "../paper/fnana-11-00071.pdf", "../paper/science.1249098.pdf", "../paper/ncomms6768.pdf", "../paper/1508.01983.pdf", "../paper/1611.07004.pdf", "../paper/1-s2.0-S0896627321007790-main (1).pdf", "../paper/nn0400_391.pdf", "../paper/7546.full.pdf", "../paper/annurev-neuro-071714-033919.pdf", "../paper/s41593-019-0480-6.pdf", "../paper/s41583-019-0133-5.pdf", "../paper/NeurIPS-2020-a-simple-normative-network-approximates-local-non-hebbian-learning-in-the-cortex-Paper.pdf", "../paper/NC110201.pdf", "../paper/BF00992696.pdf", "../paper/1-s2.0-S0896627319310955-main.pdf", "../paper/NIPS-2016-adaptive-optimal-training-of-animal-behavior-Paper.pdf", "../paper/1608.04644.pdf", "../paper/file (1) copy 2.pdf", "../paper/1210.4856.pdf", "../paper/1-s2.0-S089662731931044X-main.pdf", "../paper/0899766042321814.pdf", "../paper/1705.11146.pdf", "../paper/ewrl2011_submission_11.pdf", "../paper/linderman17a.pdf", "../paper/pnas.2201968119.pdf", "../paper/Tsividis14.pdf", "../paper/1-s2.0-S0896627312001729-main.pdf", "../paper/1806.08734.pdf", "../paper/NIPS-2011-inferring-spike-timing-dependent-plasticity-from-spike-train-data-Paper.pdf", "../paper/NIPS-2012-neurally-plausible-reinforcement-learning-of-working-memory-tasks-Paper.pdf", "../paper/2010.08262.pdf", "../paper/Eur J of Neuroscience - 2003 - Mongillo.pdf", "../paper/elife-21886-v2.pdf", "../paper/1412.6980.pdf", "../paper/cvpr03a.pdf", "../paper/1203.1513.pdf", "../paper/s43588-021-00030-1.pdf", "../paper/1508.04582.pdf", "../paper/jn.00095.2007.pdf", "../paper/121_pub_IEEE.pdf", "../paper/Self-Supervised_Visual_Feature_Learning_With_Deep_Neural_Networks_A_Survey.pdf", "../paper/1-s2.0-S0896627313011276-main.pdf", "../paper/pnas.2013663117.pdf", "../paper/2004.05154.pdf", "../paper/1801.05894.pdf", "../paper/Thesis-augmented.pdf", "../paper/A_direct_adaptive_method_for_faster_backpropagation_learning_the_RPROP_algorithm.pdf", "../paper/1412.6572.pdf", "../paper/NeurIPS-2018-which-neural-net-architectures-give-rise-to-exploding-and-vanishing-gradients-Paper.pdf", "../paper/fnins-15-633674.pdf", "../paper/girard-et-al-2001-feedforward-and-feedback-connections-between-areas-v1-and-v2-of-the-monkey-have-similar-rapid.pdf", "../paper/1503.00680.pdf", "../paper/1-s2.0-S0166223608000180-main.pdf", "../paper/fncom-04-00024.pdf", "../paper/elife-10989-v2.pdf", "../paper/1712.09926.pdf", "../paper/1502.01852.pdf", "../paper/s11263-014-0788-3.pdf", "../paper/s41593-019-0392-5.pdf", "../paper/1-s2.0-S136466130800137X-main.pdf", "../paper/1-s2.0-S1364661318301669-main.pdf", "../paper/nature14236.pdf", "../paper/nn736.pdf", "../paper/file (11).pdf", "../paper/Deep_Neural_Network_Approximation_Theory.pdf", "../paper/15747.full.pdf", "../paper/1-s2.0-S089662730300535X-main.pdf", "../paper/Cold Spring Harb Perspect Biol-2012-Mayford-a005751.pdf", "../paper/fncir-12-00053.pdf", "../paper/8455.full.pdf", "../paper/1308.0850.pdf", "../paper/1904.12191.pdf", "../paper/s41583-021-00502-3.pdf", "../paper/overview13.pdf", "../paper/1-s2.0-S0042698908004380-main.pdf", "../paper/elife-03476-v2.pdf", "../paper/1806.07572.pdf", "../paper/ncomms13276.pdf", "../paper/2021.11.08.467806.full.pdf", "../paper/35861.pdf", "../paper/pricai10.pdf", "../paper/RC61.full.pdf", "../paper/nn0201_184.pdf", "../paper/science.aab4113.pdf", "../paper/s10827-021-00780-x.pdf", "../paper/ncomms14531.pdf", "../paper/2106.12612.pdf", "../paper/12978.full.pdf", "../paper/nrn.2017.111.pdf", "../paper/wan21a.pdf", "../paper/ncomms12815.pdf", "../paper/s41593-018-0135-z.pdf", "../paper/2302.00111.pdf", "../paper/lewi-nc08.pdf", "../paper/pnas.0808113105.pdf", "../paper/083857v3.full.pdf", "../paper/henaff16.pdf", "../paper/pascanu13.pdf", "../paper/wang20k.pdf", "../paper/1510.06096.pdf", "../paper/455_icmlpaper.pdf", "../paper/Yagishita.DA.2014.Science.pdf", "../paper/41593_2017_BFnn4650_MOESM1_ESM.pdf", "../paper/s41593-021-00980-9.pdf", "../paper/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf", "../paper/1-s2.0-S0896627302010929-main.pdf", "../paper/2111.00034.pdf", "../paper/1802.03426.pdf", "../paper/1702.06463.pdf", "../paper/nrn1888.pdf", "../paper/2102.09672.pdf", "../paper/PIIS009286742031254X.pdf", "../paper/nn.4613.pdf", "../paper/1806.01261.pdf", "../paper/elife-10094-v2.pdf", "../paper/free_energy.pdf", "../paper/1002.full.pdf", "../paper/1-s2.0-S002200009791504X-main.pdf", "../paper/2007.00810.pdf", "../paper/Eur J of Neuroscience - 2018 - Shindou.pdf", "../paper/friston-et-al-2009-predictive-coding-under-the-free-energy-principle.pdf", "../paper/PhysRevX.5.021028.pdf", "../paper/1703.00887.pdf", "../paper/blei03a.pdf", "../paper/2101.03419.pdf", "../paper/Sutton-Precup-Singh-AIJ99.pdf", "../paper/1609.08144.pdf", "../paper/1-s2.0-S0165380698000169-main.pdf", "../paper/10295-Article Text-13823-1-2-20201228.pdf", "../paper/1312.6114.pdf", "../paper/s41593-018-0152-y.pdf", "../paper/greensmith04a.pdf", "../paper/nrn3241.pdf", "../paper/nn.4617.pdf", "../paper/NIPS-2016-optimal-architectures-in-a-solvable-model-of-deep-networks-Paper.pdf", "../paper/35044563.pdf", "../paper/1806.05759.pdf", "../paper/NeurIPS-2018-assessing-the-scalability-of-biologically-motivated-deep-learning-algorithms-and-architectures-Paper.pdf", "../paper/ncomms12554.pdf", "../paper/science.aav9436.pdf", "../paper/1807.04587.pdf", "../paper/NeurIPS-2019-limitations-of-lazy-training-of-two-layers-neural-network-Paper.pdf", "../paper/1-s2.0-S0042698911001544-main.pdf", "../paper/neco_a_01571.pdf", "../paper/1-s2.0-S2352154615001229-main.pdf", "../paper/2201.11903.pdf", "../paper/nn.4401.pdf", "../paper/BdSLT.pdf", "../paper/783_deep_neural_networks_as_gaussi.pdf", "../paper/neco.2007.19.3.706.pdf", "../paper/1-s2.0-S0893608014002135-main.pdf", "../paper/1-s2.0-S0896627314006461-main.pdf", "../paper/1606.04460.pdf", "../paper/1-s2.0-S0896627315008260-main.pdf", "../paper/1410.5401.pdf", "../paper/3870.full.pdf", "../paper/41586_2023_6031_MOESM1_ESM.pdf", "../paper/1711.05136.pdf", "../paper/nrn.2016.9.pdf", "../paper/1-s2.0-S0028393213000675-main.pdf", "../paper/nrn.2018.6.pdf", "../paper/BF00198477.pdf", "../paper/elife-27756-v2.pdf", "../paper/1511.06581.pdf", "../paper/1-s2.0-S0896627312008197-main.pdf", "../paper/Markram-1997.pdf", "../paper/hm95.pdf", "../paper/214262.full.pdf", "../paper/1605.08454.pdf", "../paper/agivision2011.pdf", "../paper/drew-abbott-2006-extending-the-effects-of-spike-timing-dependent-plasticity-to-behavioral-timescales.pdf", "../paper/2006.11239.pdf", "../paper/1-s2.0-S0896627318303878-main.pdf", "../paper/jjnh91.pdf", "../paper/pdp8.pdf", "../paper/nature14540.pdf", "../paper/nmeth.4399.pdf", "../paper/file (10).pdf", "../paper/nn.3024.pdf", "../paper/1703.07914.pdf", "../paper/1612.05596.pdf", "../paper/1503.00690.pdf", "../paper/nature03015.pdf", "../paper/s41583-022-00620-6.pdf", "../paper/neco_a_01160.pdf", "../paper/16494.full.pdf", "../paper/1611.05763.pdf", "../paper/1-s2.0-S1074742714001543-main.pdf", "../paper/2000-15248-005.pdf", "../paper/neco.1992.4.3.415.pdf", "../paper/089976603321780272.pdf", "../paper/Independent_rate_and_temporal_coding_in_hippocampa.pdf", "../paper/bishop-gtm-ncomp-98.pdf", "../paper/*1-s2.0-S0959438818300990-main.pdf", "../paper/bengio03a.pdf", "../paper/s41586-019-1767-1.pdf", "../paper/opt_comp.pdf", "../paper/323533a0.pdf", "../paper/NeurIPS-2018-understanding-batch-normalization-Paper.pdf", "../paper/PhysRevX.8.031003.pdf", "../paper/9475.full.pdf", "../paper/fnins-12-00608.pdf", "../paper/carreira-perpinan14.pdf", "../paper/*manifold-perception.pdf", "../paper/41593_2019_460_MOESM1_ESM.pdf", "../paper/NIPS-2008-gaussian-process-factor-analysis-for-low-dimensional-single-trial-analysis-of-neural-population-activity-Paper.pdf", "../paper/1811.02017.pdf", "../paper/PIIS1364661319300129.pdf", "../paper/1-s2.0-S0028390806002401-main.pdf", "../paper/neco.1996.8.5.895.pdf", "../paper/PIIS0960982215002067.pdf", "../paper/Eur J of Neuroscience - 2020 - Rubin - The credit assignment problem in cortico\u2010basal ganglia\u2010thalamic networks  A review .pdf", "../paper/Barto1983.pdf", "../paper/1804.02464.pdf", "../paper/shouval-et-al-2002-a-unified-model-of-nmda-receptor-dependent-bidirectional-synaptic-plasticity.pdf", "../paper/s42979-021-00815-1.pdf", "../paper/1412.6614.pdf", "../paper/41583_2020_277_MOESM1_ESM.pdf", "../paper/1-s2.0-S1364661315001023-main.pdf", "../paper/s41586-020-2802-y.pdf", "../paper/1609.09059.pdf", "../paper/neco.1997.9.7.1493.pdf", "../paper/nature16961.pdf", "../paper/hyvarinen05a.pdf", "../paper/*1902.06162.pdf", "../paper/neco05.pdf", "../paper/jn.00024.2007.pdf", "../paper/1602.05908.pdf", "../paper/*NIPS-2002-stochastic-neighbor-embedding-Paper.pdf", "../paper/1-s2.0-S0896627310002321-main.pdf", "../paper/jn.00454.2022.pdf", "../paper/nn.2501.pdf", "../paper/PIIS0896627315010375.pdf", "../paper/1406.2989.pdf", "../paper/torcs.pdf", "../paper/1-s2.0-0885064X88900210-main.pdf", "../paper/pnas.2018422118.pdf", "../paper/elife-22901-v1.pdf", "../paper/pnas.201611835si.pdf", "../paper/1611.00035.pdf", "../paper/1-s2.0-S0896627318304744-main.pdf", "../paper/yang21c.pdf", "../paper/2103.00020.pdf", "../paper/1905.07088.pdf", "../paper/1706.10295.pdf", "../paper/1-s2.0-S1364661307001593-main.pdf", "../paper/nrn1327.pdf", "../paper/1502.04681.pdf", "../paper/NeurIPS-2019-a-unified-theory-for-the-origin-of-grid-cells-through-the-lens-of-pattern-formation-Paper.pdf", "../paper/1-1-1.pdf", "../paper/1803.10760.pdf", "../paper/1703.01988.pdf", "../paper/2210.03310.pdf", "../paper/croce20b.pdf", "../paper/1610.06545.pdf", "../paper/*s41586-018-0102-6.pdf", "../paper/bergstra12a.pdf", "../paper/2006.10246.pdf", "../paper/PIIS0092867408012981.pdf", "../paper/s41467-019-08931-6.pdf", "../paper/file copy 9.pdf", "../paper/1803.01686.pdf", "../paper/nature14153.pdf", "../paper/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf", "../paper/s13040-017-0155-3.pdf", "../paper/lleintro.pdf", "../paper/bordelon20a.pdf", "../paper/1-s2.0-S0896627309002876-main.pdf", "../paper/NIPS-2014-inferring-synaptic-conductances-from-spike-trains-with-a-biophysically-inspired-point-process-model-Paper.pdf", "../paper/s42256-020-0216-z.pdf", "../paper/NeurIPS-2019-intrinsic-dimension-of-data-representations-in-deep-neural-networks-Paper.pdf", "../paper/rstb.2016.0260.pdf", "../paper/nrn2558.pdf", "../paper/negweights2008.penult.pdf", "../paper/1311.2901.pdf", "../paper/Ghahramani 2015 Nature.pdf", "../paper/fncir-07-00037.pdf", "../paper/12368.full.pdf", "../paper/1505.04544.pdf", "../paper/1205.4839.pdf", "../paper/1905.11946.pdf", "../paper/nocedal80.pdf", "../paper/1305.6663.pdf", "../paper/kass1995.pdf", "../paper/1-s2.0-S0893608003002466-main.pdf", "../paper/science.274.5293.1724.pdf", "../paper/s41593-019-0381-8.pdf", "../paper/1-s2.0-S0301008213000129-main.pdf", "../paper/385533a0.pdf", "../paper/nature01614.pdf", "../paper/Voxel-Based Morphometry.pdf", "../paper/science.1255514.pdf", "../paper/337129a0.pdf", "../paper/elife-49547-v2.pdf", "../paper/NIPS-2011-empirical-models-of-spiking-in-neural-populations-Paper.pdf", "../paper/1-s2.0-S0896627316307231-mainext.pdf", "../paper/elife-04250-v1.pdf", "../paper/1-s2.0-S0149763418303609-main.pdf", "../paper/file (2).pdf", "../paper/PIIS0896627318309930.pdf", "../paper/1-s2.0-S0896627308008362-main.pdf", "../paper/1909.11304.pdf", "../paper/allen-zhu19a.pdf", "../paper/2012.04030.pdf", "../paper/Spectral_Clustering_on_Multiple_Manifolds.pdf", "../paper/deng2014large.pdf", "../paper/wang-2010-neurophysiological-and-computational-principles-of-cortical-rhythms-in-cognition.pdf", "../paper/nn.2648.pdf", "../paper/273.full_.pdf", "../paper/s41586-019-1261-9.pdf", "../paper/jn.01171.2003.pdf", "../paper/s43586-020-00001-2.pdf", "../paper/NeurIPS-2020-learning-to-learn-with-feedback-and-local-plasticity-Paper.pdf", "../paper/annurev-statistics-022513-115657.pdf", "../paper/NIPS-2013-distributed-representations-of-words-and-phrases-and-their-compositionality-Paper.pdf", "../paper/PhysRevLett.126.180604-accepted.pdf", "../paper/OReilly96_generec_nc.pdf", "../paper/P-2237-30457224.pdf", "../paper/1902.06720.pdf", "../paper/braun2022exact.pdf", "../paper/nrn.2015.26.pdf", "../paper/file copy 8.pdf", "../paper/370140a0.pdf", "../paper/1-s2.0-S1364661316300432-main.pdf", "../paper/jn.1998.80.1.1.pdf", "../paper/neco.2007.19.6.1468.pdf", "../paper/1806.07366.pdf", "../paper/nn.4403.pdf", "../paper/FFA13.pdf", "../paper/1801.10130.pdf", "../paper/1511.06434.pdf", "../paper/NeurIPS-2018-manifold-tiling-localized-receptive-fields-are-optimal-in-similarity-preserving-neural-networks-Paper.pdf", "../paper/1-s2.0-S0896627315003645-main.pdf", "../paper/fnins-10-00106.pdf", "../paper/elife-21492-v2.pdf", "../paper/Using_goal_driven_deep_learning_models_t.pdf", "../paper/nn.2264.pdf", "../paper/1508.00330.pdf", "../paper/1909.08053.pdf", "../paper/s41593-018-0309-8.pdf", "../paper/2310.13018.pdf", "../paper/1412.4564.pdf", "../paper/NeurIPS-2018-learning-a-latent-manifold-of-odor-representations-from-neural-responses-in-piriform-cortex-Paper.pdf", "../paper/1-s2.0-S0022249608001181-main.pdf", "../paper/Deep_Neural_Networks_for_Acoustic_Modeling_in_Speech_Recognition_The_Shared_Views_of_Four_Research_Groups.pdf", "../paper/1-s2.0-S002224961100071X-main.pdf", "../paper/nn.3807.pdf", "../paper/1507.07580.pdf", "../paper/1811.10766.pdf", "../paper/67258.pdf", "../paper/1-s2.0-S009286741730538X-main.pdf", "../paper/PIIS2667237521001600.pdf", "../paper/PVF.pdf", "../paper/PhysRevLett.65.1683.pdf", "../paper/nrn1427.pdf", "../paper/10005.full.pdf", "../paper/1502.04156.pdf", "../paper/1-s2.0-S0959438817300454-main.pdf", "../paper/s41593-019-0517-x.pdf", "../paper/s42256-022-00498-0.pdf", "../paper/Adaptive_control_of_Markov_chains_with_average_cost.pdf", "../paper/GrosmarkBuzsaki_Science_2016_Combined.pdf", "../paper/s41467-017-02717-4.pdf", "../paper/2206.00364.pdf", "../paper/1605.07110.pdf", "../paper/ENEURO.0427-20.2020.full.pdf", "../paper/89_M_JPA.pdf", "../paper/1711.05769.pdf", "../paper/4693.full.pdf", "../paper/41467_2020_14578_MOESM1_ESM.pdf", "../paper/science.aaa5542.pdf", "../paper/s41593-019-0520-2.pdf", "../paper/A_generalized_iterative_LQG_method_for_locally-optimal_feedback_control_of_constrained_nonlinear_stochastic_systems.pdf", "../paper/file (1) copy.pdf", "../paper/2011.08088.pdf", "../paper/sdm97.pdf", "../paper/1602.07389.pdf", "../paper/seijen14.pdf", "../paper/ruvolo13.pdf", "../paper/bengio12a.pdf", "../paper/s10994-012-5278-7.pdf", "../paper/b101281.pdf", "../paper/2023.07.09.548255.full.pdf", "../paper/2008.02217.pdf", "../paper/canatara_phd_dissertation.pdf", "../paper/1702.05043.pdf", "../paper/science.1188224.pdf", "../paper/NIPS-1999-spike-based-learning-rules-and-stabilization-of-persistent-neural-activity-Paper.pdf", "../paper/neco.1989.1.2.270.pdf", "../paper/1176325622.pdf", "../paper/1503.05571.pdf", "../paper/PIIS0960982221016821.pdf", "../paper/122344.122377.pdf", "../paper/Back_propagation_through_adjoints_for_the_identification_of_nonlinear_dynamic_systems_using_recurrent_neural_models.pdf", "../paper/science.1172377.pdf", "../paper/IEEE_transactions_on_pattern_analysis_20(12).pdf", "../paper/1710.02298.pdf", "../paper/nature06445.pdf", "../paper/350.full.pdf", "../paper/PIIS0896627301001787.pdf", "../paper/1-s2.0-S095943880900124X-main.pdf", "../paper/he-embedding2.pdf", "../paper/1511.02543.pdf", "../paper/nn.2388.pdf", "../paper/2002.05709.pdf", "../paper/1-s2.0-S0896627309006953-main.pdf", "../paper/file (9).pdf", "../paper/duchi11a.pdf", "../paper/nn.2439.pdf", "../paper/NeurIPS-2021-credit-assignment-through-broadcasting-a-global-error-vector-Paper.pdf", "../paper/085886v1.full.pdf", "../paper/Saxe.13.HierCat.pdf", "../paper/1409.0473.pdf", "../paper/Kalman60.pdf", "../paper/nn.4061.pdf", "../paper/J of Comparative Neurology - 1 July 1989 - Rockland.pdf", "../paper/krotov-hopfield-2019-unsupervised-learning-by-competing-hidden-units.pdf", "../paper/s41586-020-2350-5.pdf", "../paper/Unified Segmentation.pdf", "../paper/1606.05336.pdf", "../paper/s41586-021-03950-0.pdf", "../paper/Ras04.pdf", "../paper/agarwal-etal.pdf", "../paper/nn.4049.pdf", "../paper/s10107-006-0706-8.pdf", "../paper/GoodmanEtAl2015-Chapter.pdf", "../paper/bernstein18a.pdf", "../paper/1-s2.0-S0896627314011350-main.pdf", "../paper/1411.5908.pdf", "../paper/annurev-neuro-072116-031538.pdf", "../paper/p087c.pdf", "../paper/NIPS-2014-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization-Paper.pdf", "../paper/BayesOptLoop.pdf", "../paper/NeurIPS-2019-bipartite-expander-hopfield-networks-as-self-decoding-high-capacity-error-correcting-codes-Paper.pdf", "../paper/9870.full.pdf", "../paper/1511.05946.pdf", "../paper/s41593-019-0377-4.pdf", "../paper/1511.05952.pdf", "../paper/elife-51005-v3.pdf", "../paper/pnas.81.10.3088.pdf", "../paper/1-s2.0-S0959438821001227-main.pdf", "../paper/NIPS-2009-measuring-invariances-in-deep-networks-Paper.pdf", "../paper/Moser-Annreview+2008.pdf", "../paper/1802.03268.pdf", "../paper/1-s2.0-S0306452214008707-main.pdf", "../paper/jn.00097.2009.pdf", "../paper/annualrev_10.pdf", "../paper/1806.01822.pdf", "../paper/nature09159.pdf", "../paper/elife-47889-v2.pdf", "../paper/nn1560.pdf", "../paper/LakeEtAl2015Science.pdf", "../paper/1412.6856.pdf", "../paper/NeurIPS-2020-can-the-brain-do-backpropagation-exact-implementation-of-backpropagation-in-predictive-coding-networks-Paper.pdf", "../paper/camastra2016.pdf", "../paper/NIPS-2014-a-framework-for-studying-synaptic-plasticity-with-neural-spike-train-data-Paper.pdf", "../paper/Batty, Multilayer recurrent network models of primate retinal ganglion cll responses.pdf", "../paper/1605.06743.pdf", "../paper/NeurIPS-2020-neural-path-features-and-neural-path-kernel-understanding-the-role-of-gates-in-deep-learning-Paper.pdf", "../paper/science.3749885.pdf", "../paper/s41593-022-01088-4.pdf", "../paper/1-s2.0-S0959438818300485-main.pdf", "../paper/*1-s2.0-S0959438816000118-main.pdf", "../paper/s41586-023-06415-8.pdf", "../paper/2011.13456.pdf", "../paper/1-s2.0-S0896627317304178-main.pdf", "../paper/1512.00567.pdf", "../paper/NeurIPS-2019-adversarial-robustness-through-local-linearization-Paper.pdf", "../paper/978-3-642-35289-8.pdf", "../paper/1702.08360.pdf", "../paper/Gerstner(2015)_three_factor_EM.pdf", "../paper/2006.02427.pdf", "../paper/s41583-022-00642-0.pdf", "../paper/fnhum-08-00825.pdf", "../paper/1-s2.0-S0896627320303664-main.pdf", "../paper/PIIS0896627310002874.pdf", "../paper/PhysRevLett.61.259.pdf", "../paper/Plumbley02-nnica_accepted_notice.pdf", "../paper/nature03689.pdf", "../paper/neco_a_00409.pdf", "../paper/nn.2599.pdf", "../paper/ranzato-cvpr-07.pdf", "../paper/Behavioral_time_scale_synaptic_plasticity_underlie.pdf", "../paper/1-s2.0-S0959438815000768-main.pdf", "../paper/1802.06070.pdf", "../paper/BleiLafferty2009.pdf", "../paper/1-s2.0-S0896627315004766-main.pdf", "../paper/1-s2.0-S0959438814001044-main.pdf", "../paper/2106.02073.pdf", "../paper/nature14446.pdf", "../paper/1312.6199.pdf", "../paper/BTT.pdf", "../paper/A_1013776130161.pdf", "../paper/nn.2957.pdf", "../paper/564476.full.pdf", "../paper/1411.6191.pdf", "../paper/1-s2.0-S089662731730185X-main.pdf", "../paper/2002.04010.pdf", "../paper/1-s2.0-S030645220200026X-main.pdf", "../paper/1684.full.pdf", "../paper/PIIS0896627318307414.pdf", "../paper/zimmermann21a.pdf", "../paper/s41583-020-00395-8.pdf", "../paper/nature13665.pdf", "../paper/2210.08340.pdf", "../paper/2310.02207.pdf", "../paper/Representation_Learning_A_Review_and_New_Perspectives.pdf", "../paper/1-s2.0-S0896627317306955-main.pdf", "../paper/1605.08803.pdf", "../paper/fncom-11-00024.pdf", "../paper/*nature12742.pdf", "../paper/nn.3532.pdf", "../paper/1511.06295.pdf", "../paper/pnas.0802631105.pdf", "../paper/A_1011204814320.pdf", "../paper/PhysRevResearch.3.013176.pdf", "../paper/elife-25784-v1.pdf", "../paper/pnas.1506855112.pdf", "../paper/file (5).pdf", "../paper/*vandermaaten08a.pdf", "../paper/Martin-Brualla_NeRF_in_the_Wild_Neural_Radiance_Fields_for_Unconstrained_Photo_CVPR_2021_paper.pdf", "../paper/2011.12428.pdf", "../paper/nn.3917.pdf", "../paper/1806.00900.pdf", "../paper/691_kernel_rnn_learning_kernl_.pdf", "../paper/lecun1.pdf", "../paper/fnhum-05-00039.pdf", "../paper/2202.08384.pdf", "../paper/1-s2.0-S0896627310009384-main.pdf", "../paper/1811.03804.pdf", "../paper/5314.full.pdf", "../paper/nn.4062.pdf", "../paper/glorot10a.pdf", "../paper/pnas.1604850113.pdf", "../paper/nn1826.pdf", "../paper/1909.08156.pdf", "../paper/1803.00885.pdf", "../paper/8672-Article Text-12200-1-2-20201228.pdf", "../paper/1-s2.0-S0896627321010357-main.pdf", "../paper/hyva\u0308rinen-2013-independent-component-analysis-recent-advances.pdf", "../paper/1401.4082.pdf", "../paper/1802.07569.pdf", "../paper/2007.02686.pdf", "../paper/castellani-et-al-2001-a-biophysical-model-of-bidirectional-synaptic-plasticity-dependence-on-ampa-and-nmda-receptors.pdf", "../paper/PIIS0960982222001166.pdf", "../paper/nn.2163.pdf", "../paper/nature11649.pdf", "../paper/2023.01.16.523429.full.pdf", "../paper/science.1166466.pdf", "../paper/MACnc92b.pdf", "../paper/neco_a_00998.pdf", "../paper/BF02289451.pdf", "../paper/1910.02054.pdf", "../paper/s41583-019-0197-2.pdf", "../paper/s10994-015-5528-6.pdf", "../paper/1312.6211.pdf", "../paper/1910.07104.pdf", "../paper/1-s2.0-S0959438817300429-main.pdf", "../paper/koh17a.pdf", "../paper/5609.full.pdf", "../paper/nrn.2017.85.pdf", "../paper/1-s2.0-S0896627318300655-main.pdf", "../paper/physrev.00030.2005.pdf", "../paper/nature13664.pdf", "../paper/Deep_learning_and_the_information_bottleneck_principle.pdf", "../paper/NeurIPS-2021-relative-flatness-and-generalization-Supplemental.pdf", "../paper/Deep Learning with Dynamic Spiking Neurons and FixedFeedback Weights.pdf", "../paper/s41467-022-34452-w.pdf", "../paper/2435.full.pdf", "../paper/s41593-019-0550-9.pdf", "../paper/2104.14294.pdf", "../paper/1904.11955.pdf", "../paper/2310.06114.pdf", "../paper/PIIS0896627316310406.pdf", "../paper/1-s2.0-S0959438820300817-main.pdf", "../paper/nature06725.pdf", "../paper/nn.3137.pdf", "../paper/6266.full.pdf", "../paper/NIPS-2016-bayesian-latent-structure-discovery-from-multi-neuron-recordings-Paper.pdf", "../paper/1-s2.0-S0924809905800150-main.pdf", "../paper/nn.3645.pdf", "../paper/NIPS-2016-probing-the-compositionality-of-intuitive-functions-Paper.pdf", "../paper/pnas.1905544116.pdf", "../paper/978-3-319-12637-1.pdf", "../paper/1-s2.0-S0893608019300784-main.pdf", "../paper/nn.3862.pdf", "../paper/s41593-020-0671-1.pdf", "../paper/osdi22-zheng-lianmin.pdf", "../paper/1510.05067.pdf", "../paper/nn0199_79.pdf", "../paper/1-s2.0-S0010945217303258-main.pdf", "../paper/mumford-carlsson et al.pdf", "../paper/jcb_200806149.pdf", "../paper/2203.10036.pdf", "../paper/3697.full.pdf", "../paper/089976602760407955.pdf", "../paper/1606.03813.pdf", "../paper/neco.1994.6.2.296.pdf", "../paper/1-s2.0-S0893608020303117-main.pdf", "../paper/jn.00364.2007.pdf", "../paper/NeurIPS-2018-how-to-start-training-the-effect-of-initialization-and-architecture-Paper.pdf", "../paper/1907.08549.pdf", "../paper/s41539-019-0048-y.pdf", "../paper/1-s2.0-S0959438816302641-main.pdf", "../paper/2010.14765.pdf", "../paper/1-s2.0-S0042698997001697-main.pdf", "../paper/3351.full.pdf", "../paper/PIIS1550413118305151.pdf", "../paper/41586_2012_BFnature11129_MOESM225_ESM.pdf", "../paper/*saul03a.pdf", "../paper/s41593-020-00733-0.pdf", "../paper/1711.00165.pdf", "../paper/blundell15.pdf", "../paper/1-s2.0-S2352154618302092-main.pdf", "../paper/24_line_attractor_dynamics_in_rec.pdf", "../paper/2006.09011.pdf", "../paper/2210.06591.pdf", "../paper/2105.10446.pdf", "../paper/nrn3785.pdf", "../paper/s41586-023-06031-6.pdf", "../paper/elife-17086-v1.pdf", "../paper/annurev.neuro.22.1.241.pdf", "../paper/s41598-018-22160-9.pdf", "../paper/1-s2.0-S1053811913006599-main.pdf", "../paper/35087601.pdf", "../paper/elife-05558-v2.pdf", "../paper/s41586-018-0654-5.pdf", "../paper/Magnetic Resonance in Med - 2005 - Buxton - Dynamics of blood flow and oxygenation changes during brain activation  The.pdf", "../paper/1703.03906.pdf", "../paper/s41593-019-0364-9.pdf", "../paper/ncomms13239.pdf", "../paper/WISPmanhattenrule2015.pdf", "../paper/science.aah6066.pdf", "../paper/journal.pcbi.1010255.pdf", "../paper/1-s2.0-S0042698997001211-main.pdf", "../paper/hebbdot.pdf", "../paper/asru_2013.pdf", "../paper/s41467-020-19788-5.pdf", "../paper/ncomms13749.pdf", "../paper/PIIS0896627317305093.pdf", "../paper/das-et-al-2006-computational-prediction-of-methylation-status-in-human-genomic-sequences.pdf", "../paper/nature14251.pdf", "../paper/mniha16.pdf", "../paper/1635.full.pdf", "../paper/whalen-et-al-2020-delta-oscillations-are-a-robust-biomarker-of-dopamine-depletion-severity-and-motor-dysfunction-in.pdf", "../paper/1-s2.0-S0896627313011276-mmc1.pdf", "../paper/Surrogate_Gradient_Learning_in_Spiking_Neural_Networks_Bringing_the_Power_of_Gradient-Based_Optimization_to_Spiking_Neural_Networks.pdf", "../paper/1634.full.pdf", "../paper/jn.01311.2006.pdf", "../paper/*science.aao0284.pdf", "../paper/s41592-019-0598-1.pdf", "../paper/*pnas.2015509117.pdf", "../paper/s42256-021-00430-y.pdf", "../paper/file (4).pdf", "../paper/2111.11215.pdf", "../paper/1-s2.0-S0959438815001889-main.pdf", "../paper/1910.02509.pdf", "../paper/NeurIPS-2018-long-short-term-memory-and-learning-to-learn-in-networks-of-spiking-neurons-Paper.pdf", "../paper/NeurIPS-2021-reverse-engineering-learned-optimizers-reveals-known-and-novel-mechanisms-Paper.pdf", "../paper/2010.02502.pdf", "../paper/neco.2006.18.6.1318.pdf", "../paper/nn963.pdf", "../paper/1-s2.0-S0042698901000736-main.pdf", "../paper/PhysRevE.69.066138.pdf", "../paper/blais-et-al-1999-the-role-of-presynaptic-activity-in-monocular-deprivation-comparison-of-homosynaptic-and.pdf", "../paper/8812.full.pdf", "../paper/1506.02438.pdf", "../paper/1806.06144.pdf", "../paper/bhx339.pdf", "../paper/1390156.1390177.pdf", "../paper/hyvarinen19a.pdf", "../paper/nature02618.pdf", "../paper/GershmanDaw17.pdf", "../paper/2561_score_based_generative_modelin.pdf", "../paper/1803.03635.pdf", "../paper/pnas.1802705116.pdf", "../paper/coalson-et-al-2018-the-impact-of-traditional-neuroimaging-methods-on-the-spatial-localization-of-cortical-areas.pdf", "../paper/2984875.pdf", "../paper/5066.full.pdf", "../paper/Eur J of Neuroscience - 2012 - Collins - How much of reinforcement learning is working memory  not reinforcement learning .pdf", "../paper/nn.4339.pdf", "../paper/NIPS-2007-kernel-measures-of-conditional-dependence-Paper.pdf", "../paper/bhaa023.pdf", "../paper/10532_stochastic_solutions_for_linea.pdf", "../paper/Hippocampus - 2007 - Burgess - An oscillatory interference model of grid cell firing.pdf", "../paper/Manifold_clustering.pdf", "../paper/PIIS1364661319300610.pdf", "../paper/pnas.1611835114.pdf", "../paper/PIIS0896627313000937.pdf", "../paper/jn.00697.2004.pdf", "../paper/41593_2018_147_MOESM1_ESM.pdf", "../paper/2911.pdf", "../paper/1910.03561.pdf", "../paper/Normalized_cuts_and_image_segmentation.pdf", "../paper/1300015.pdf", "../paper/Predicting_spike_timing_of_neocortical_pyramidal_neurons.pdf", "../paper/2108.00131.pdf", "../paper/fncom-10-00094.pdf", "../paper/annurev-neuro-070918-050421.pdf", "../paper/s41592-018-0109-9.pdf", "../paper/NIPS-2007-the-tradeoffs-of-large-scale-learning-Paper.pdf", "../paper/science.1123513.pdf", "../paper/1310.5438.pdf", "../paper/2004.08013.pdf", "../paper/1-s2.0-S1053811914009094-main.pdf", "../paper/PINN_RPK_2019_1.pdf", "../paper/1-s2.0-S1053811913005053-main.pdf", "../paper/fncom-09-00120.pdf", "../paper/applsci-12-10228-v2.pdf", "../paper/436.full.pdf", "../paper/1-s2.0-S0896627307006265-mainext.pdf", "../paper/s41467-021-23103-1.pdf", "../paper/*Laplacian.pdf", "../paper/nn.3865.pdf", "../paper/friston-2013-life-as-we-know-it.pdf", "../paper/variational-intro.pdf", "../paper/1676_adversarial_score_matching_and.pdf", "../paper/NIPS-2016-the-forget-me-not-process-Paper.pdf", "../paper/3646.full.pdf", "../paper/089976699300016827.pdf", "../paper/erhan10a.pdf", "../paper/1-s2.0-S0896627311001255-main.pdf", "../paper/2003.08934.pdf", "../paper/Variational Inference  A Review for Statisticians.pdf", "../paper/6028.full.pdf", "../paper/annurev-neuro-080317-061956.pdf", "../paper/1-s2.0-S1053811913003108-main.pdf", "../paper/science.298.5594.824.pdf", "../paper/2209.14988.pdf", "../paper/2022.03.17.484712v1.full.pdf", "../paper/Human Brain Mapping - 2021 - Gao - Nonlinear manifold learning in functional magnetic resonance imaging uncovers a.pdf", "../paper/1609.05158.pdf", "../paper/1808.03357.pdf", "../paper/duvenaud13.pdf", "../paper/1904.00687.pdf", "../paper/s43588-022-00390-2.pdf", "../paper/science.1238406.pdf", "../paper/1-s2.0-S095943880600122X-main.pdf", "../paper/salakhutdinov09a.pdf", "../paper/1-s2.0-S0031320313000678-main.pdf", "../paper/nrn3962.pdf", "../paper/1-s2.0-S0896627305003624-main.pdf", "../paper/1804.06893.pdf", "../paper/1608.08782.pdf", "../paper/cshperspect-SYP-a005710.pdf", "../paper/1-s2.0-S089662731200092X-main.pdf", "../paper/1912.02164.pdf", "../paper/1-s2.0-S095943881500183X-main.pdf", "../paper/nature24636.pdf", "../paper/nrn.2017.149.pdf", "../paper/nn.2831.pdf", "../paper/pnas.1319438111.pdf", "../paper/1602.04915.pdf", "../paper/0004057.pdf", "../paper/s41586-020-03171-x.pdf", "../paper/netn_a_00018.pdf", "../paper/Scalarized_multi-objective_reinforcement_learning_Novel_design_techniques.pdf", "../paper/s41583-022-00570-z.pdf", "../paper/2205.05198.pdf", "../paper/1703.08475.pdf", "../paper/Response_acquisition_by_humans_with_delayed_reinfo.pdf", "../paper/1990-27437-001.pdf", "../paper/wangf16.pdf", "../paper/1906.04688.pdf", "../paper/2022.12.07.519455v1.full.pdf", "../paper/1-s2.0-S0959438814000166-main.pdf", "../paper/NeurIPS-2018-neural-tangent-kernel-convergence-and-generalization-in-neural-networks-Paper.pdf", "../paper/PIIS2211124718317960.pdf", "../paper/1907.02649.pdf", "../paper/pnas.2111821118.sapp.pdf", "../paper/2455_unbiased_contrastive_divergenc.pdf", "../paper/1806.09077.pdf", "../paper/file (8).pdf", "../paper/s41586-022-04915-7.pdf", "../paper/1705.03122.pdf", "../paper/jov-8-8-11.pdf", "../paper/1907.06374.pdf", "../paper/1606.08165.pdf", "../paper/0912.3832.pdf", "../paper/science.1254126.pdf", "../paper/2106.13031.pdf", "../paper/1604.06057.pdf", "../paper/NeurIPS-2021-credit-assignment-through-broadcasting-a-global-error-vector-Supplemental.pdf", "../paper/7476.full.pdf", "../paper/1911.04252.pdf", "../paper/1-s2.0-S1364661313001277-main.pdf", "../paper/2102.12627.pdf", "../paper/ito-2001-cerebellar-long-term-depression-characterization-signal-transduction-and-functional-roles.pdf", "../paper/Gradient-based_learning_applied_to_document_recognition.pdf", "../paper/NeurIPS-2018-how-does-batch-normalization-help-optimization-Paper.pdf", "../paper/1811.01768.pdf", "../paper/BF00337259.pdf", "../paper/8360.full.pdf", "../paper/PIIS1364661312001957.pdf", "../paper/nature11129.pdf", "../paper/2006.10350.pdf", "../paper/fnsyn-02-00025.pdf", "../paper/1-s2.0-S0896627308009434-main.pdf", "../paper/neco_a_00949.pdf", "../paper/1705.10412.pdf", "../paper/1-s2.0-S0896627311010051-main.pdf", "../paper/chen20j.pdf", "../paper/2307.06324.pdf", "../paper/*1608.05343.pdf", "../paper/21243-Article Text-25256-1-2-20220628.pdf", "../paper/2010.03409.pdf", "../paper/16-505.pdf", "../paper/annurev.physiol.64.092501.114547.pdf", "../paper/fnint-07-00025.pdf", "../paper/vincent10a.pdf", "../paper/1-s2.0-S0896627318305439-main.pdf", "../paper/jn.90833.2008.pdf", "../paper/aan3846_bittner_sm.pdf", "../paper/science.1211095.pdf", "../paper/2108.01210.pdf", "../paper/nn.3496.pdf", "../paper/nihms-1010474.pdf", "../paper/joachims_98a.pdf", "../paper/1705.07224.pdf", "../paper/bcm.pdf", "../paper/nn.3643.pdf", "../paper/41467_2020_17236_MOESM1_ESM.pdf", "../paper/1651_implementing_inductive_bias_fo.pdf", "../paper/koutnik2014gecco.pdf", "../paper/41586_2013_BFnature12160_MOESM50_ESM.pdf", "../paper/sutskever13.pdf", "../paper/1502.03167.pdf", "../paper/annurev.neuro.30.051606.094225.pdf", "../paper/nature19818.pdf", "../paper/nn1954.pdf", "../paper/Deep_reinforcement_learning_with_successor_features_for_navigation_across_similar_environments.pdf", "../paper/A_deep_learning_framework_for_neuroscience_vFinal_RC.pdf", "../paper/jn.91050.2008.pdf", "../paper/1805.10842.pdf", "../paper/nature06910.pdf", "../paper/3305890.3306024.pdf", "../paper/nrn1379.pdf"], "abstract": ["732\n\ncomplementary roles of basal ganglia and cerebellum in\nlearning and motor control\nkenji doya\n\nthe classical notion that the basal ganglia and the cerebellum\nare dedicated to motor control has been challenged by the\naccumulation of evidence revealing their involvement in non-\nmotor, cognitive functions. from a computational viewpoint, it\nhas been suggested that the cerebellum, the basal ganglia,\nand the cerebral cortex are specialized for different types of\nlearning: namely, supervised learn", "neuron\n\narticle\n\npathway interactions and synaptic plasticity in the\ndendritic tuft regions of ca1 pyramidal neurons\n\nhiroto takahashi1 and jeffrey c. magee1,*\n1howard hughes medical institute, janelia farm research campus, 19700 helix drive, ashburn, va 20147, usa\n*correspondence: mageej@janelia.hhmi.org\ndoi 10.1016/j.neuron.2009.03.007\n\nsummary\n\ninput comparison is thought\nto occur in many\nneuronal circuits, including the hippocampus, where\nfunctionally important\ninteractions between the\nschaf", "on the binding problem in arti\ufb01cial neural networks\n\nklaus gre\ufb00\u2217\ngoogle research, brain team\ntucholskystra\u00dfe 2, 10116 berlin, germany\nsjoerd van steenkiste\nj\u00fcrgen schmidhuber\nistituto dalle molle di studi sull\u2019intelligenza arti\ufb01ciale (idsia)\nuniversit\u00e0 della svizzera italiana (usi)\nscuola universitaria professionale della svizzera italiana (supsi)\nvia la santa 1, 6962 viganello, switzerland\n\nklausg@google.com\n\nsjoerd@idsia.ch\n\njuergen@idsia.ch\n\nabstract\n\ncontemporary neural networks still fall s", "partition functions from\n\nrao-blackwellized tempered sampling\n\ndavid e. carlson\u22171,2\npatrick stinson\u22172\nari pakman\u22171,2\nliam paninski1,2\n1 department of statistics\n2 grossman center for the statistics of mind\ncolumbia university, new york, ny, 10027\n\nabstract\n\npartition functions of probability distributions\nare important quantities for model evaluation and\ncomparisons. we present a new method to com-\npute partition functions of complex and multi-\nmodal distributions. such distributions are of-\nten", "volume 59, number 19\n\nphysical review letters\n\n9 november 1987\n\ngeneralization\n\nof back-propagation\n\nto recurrent neural networks\n\napplied physics laboratory,\n\nfernando j. pineda\njohns hopkins university, laurel, maryland 20707\n(received 10 june 1987)\n\nan adaptive neural network with asymmetric\n\nhopfield network with graded neurons\nhinton,\nto the master/slave\n\nand williams\n\nto modify adaptively\n\nconnections\nand uses a recurrent\n\nthis network is related to the\nof the 6 rule of rumelhart,\nthe syna", "article\nclassi\ufb01cation of explainable arti\ufb01cial intelligence methods\nthrough their output formats\n\ngiulia vilone *,\u2020\n\nand luca longo \u2020\n\napplied intelligence research centre, technological university dublin, d08 x622 dublin, ireland;\nluca.longo@tudublin.ie\n* correspondence: giulia.vilone@tudublin.ie\n\u2020 these authors contributed equally to this work.\n\nabstract: machine and deep learning have proven their utility to generate data-driven models with\nhigh accuracy and precision. however, their non-line", "article\n\npulvinar-cortex interactions in vision and attention\n\nhighlights\nd neuronal properties and attentional modulation are similar in\n\npulvinar and v4\n\nauthors\n\nhuihui zhou, robert john schafer,\nrobert desimone\n\nd v4 leads pulvinar in gamma synchrony during attentive\n\nstimulus processing\n\ncorrespondence\nhh.zhou@siat.ac.cn\n\nd pulvinar deactivation reduces both sensory response and\n\nattentional effect in v4\n\nd following pulvinar deactivation, cortex appears to go to an\n\ninactive state\n\nin brie", "interpreting neural computations by examining intrinsic and embedding dimensionality of\nneural activity\n\nmehrdad jazayeri1, srdjan ostojic2\n\nsummary\n\nthe ongoing exponential rise in recording capacity calls for new approaches for analysing and\ninterpreting neural data. effective dimensionality has emerged as an important property of neural\nactivity across populations of neurons, yet different studies rely on different definitions and\ninterpretations of this quantity. here we focus on intrinsic a", "r e v i e w s\n\nnormalization as a canonical neural \ncomputation\n\nmatteo carandini1 and david j.\u00a0heeger2\n\nabstract | there is increasing evidence that the brain relies on a set of canonical neural \ncomputations, repeating them across brain regions and modalities to apply similar \noperations to different problems. a promising candidate for such a computation is \nnormalization, in which the responses of neurons are divided by a common factor that \ntypically includes the summed activity of a pool of", "a model-based approach to trial-by-trial p300 amplitude\n\ufb02uctuations\n\noriginal research article\npublished: 08 february 2013\ndoi: 10.3389/fnhum.2012.00359\n\nantonio kolossa1,tim fingscheidt 1*, karl wessel 2,3 and bruno kopp 2,4\n\n1 institute for communications technology, technische universit\u00e4t braunschweig, braunschweig, germany\n2 cognitive neurology, technische universit\u00e4t braunschweig, braunschweig, germany\n3 department of neurology, braunschweig hospital, braunschweig, germany\n4 department of n", "j comput neurosci (2010) 29:107\u2013126\ndoi 10.1007/s10827-009-0179-x\n\na new look at state-space models for neural data\nliam paninski \u00b7 yashar ahmadian \u00b7\ndaniel gil ferreira \u00b7 shinsuke koyama \u00b7\nkamiar rahnama rad \u00b7 michael vidne \u00b7\njoshua vogelstein \u00b7 wei wu\n\nreceived: 22 december 2008 / revised: 6 july 2009 / accepted: 16 july 2009 / published online: 1 august 2009\n\u00a9 springer science + business media, llc 2009\n\nabstract state space methods have proven indispens-\nable in neural data analysis. however", "7\n1\n0\n2\n\n \nr\np\na\n9\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n1\n8\n0\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nopening the black box of deep neural networks via information\n\nopening the black box of deep neural networks\n\nvia information\n\nravid schwartz-ziv\nedmond and lilly safra center for brain sciences\nthe hebrew university of jerusalem\njerusalem, 91904, israel\nnaftali tishby\u2217\nschool of engineering and computer science\nand edmond and lilly safra center for brain sciences\nthe hebrew university of jerusalem\njerusalem,", "review\ninterneuron cell types \nare fit to function\n\nadam kepecs1 & gordon fishell2\n\ndoi:10.1038/nature12983\n\nunderstanding brain circuits begins with an appreciation of their component parts \u2014 the cells. although gabaergic interneu-\nrons are a minority population within the brain, they are crucial for the control of inhibition. determining the diversity of these \ninterneurons has been a central goal of neurobiologists, but this amazing cell type has so far defied a generalized classification \nsy", "neuron\n\nviewpoint\n\ncortical preparatory activity: representation\nof movement or first cog in a dynamical machine?\n\nmark m. churchland,1,* john p. cunningham,1,3 matthew t. kaufman,2 stephen i. ryu,1,4 and krishna v. shenoy1,2,5\n1department of electrical engineering\n2neurosciences program\nstanford university, stanford, ca 94305, usa\n3department of engineering, university of cambridge, cambridge cb2 1pz, uk\n4department of neurosurgery, palo alto medical foundation, palo alto, ca 94301, usa\n5depart", "a framework for equilibrium equations \nauthor(s): gilbert strang \nsource: siam review, jun., 1988, vol. 30, no. 2 (jun., 1988), pp. 283-297  \npublished by: society for industrial and applied mathematics \n\nstable url: https://www.jstor.org/stable/2030801\n\njstor is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide \nrange of content in a trusted digital archive. we use information technology and tools to increase productivity and \nfacilitat", "article\n\nhttps://doi.org/10.1038/s41467-020-17236-y\n\nopen\n\na solution to the learning dilemma for recurrent\nnetworks of spiking neurons\nguillaume bellec1,2, franz scherr\nrobert legenstein\n\n1, elias hajek1, darjan salaj\n\n1,2, anand subramoney\n\n1 & wolfgang maass\n\n1\u2709\n\n1,\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\nrecurrently connected networks of spiking neurons underlie the astounding information\nprocessing capabilities of the brain. yet in spite of extensive research, how they can learn\nthrough synaptic pl", "functional  significance  of long-term\npotentiation for  sequence  learning and\nprediction\n\nl. f. abbott and  kenneth  i. blum\n\ncenter for complex  systems, brandeis university, waltham,\nmassachusetts 02254\n\npopulation  coding, where  neurons with  broad and  overlapping firing\nrate  tuning  curves  collectively  encode  information  about a stimulus,\nis  a  common feature  of  sensory  systems. we  use  decoding methods\nand  measured  properties  of  nmda-mediated  ltp  induction  to study\nthe ", "article\n\ndoi:10.1038/nature11911\n\nfunctional organization of human\nsensorimotor cortex for speech articulation\n\nkristofer e. bouchard1,2, nima mesgarani1,2, keith johnson3 & edward f. chang1,2,4\n\nspeaking is one of the most complex actions that we perform, but nearly all of us learn to do it effortlessly. production of\nfluent speech requires the precise, coordinated movement of multiple articulators (for example, the lips, jaw, tongue and\nlarynx) over rapid time scales. here we used high-resolut", "distributional reinforcement learning with quantile regression\n\nwill dabney\n\ndeepmind\n\nmark rowland\n\nuniversity of cambridge\u2217\n\nmarc g. bellemare\n\ngoogle brain\n\nr\u00b4emi munos\n\ndeepmind\n\n7\n1\n0\n2\n\n \nt\nc\no\n7\n2\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\nv\n4\n4\n0\n0\n1\n\n.\n\n0\n1\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nin reinforcement learning an agent interacts with the environ-\nment by taking actions and observing the next state and re-\nward. when sampled probabilistically, these state transitions,\nrewards, and actions can all indu", "masked autoencoders are scalable vision learners\n\nkaiming he\u2217,\u2020 xinlei chen\u2217 saining xie yanghao li piotr doll\u00b4ar ross girshick\n\n\u2217equal technical contribution\n\n\u2020project lead\n\nfacebook ai research (fair)\n\n1\n2\n0\n2\n \nc\ne\nd\n9\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n3\nv\n7\n7\n3\n6\n0\n\n.\n\n1\n1\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nthis paper shows that masked autoencoders (mae) are\nscalable self-supervised learners for computer vision. our\nmae approach is simple: we mask random patches of the\ninput image and reconstruct the mi", "a r t i c l e s\n\ninhibitory suppression of heterogeneously tuned \nexcitation enhances spatial coding in ca1 place cells\nchristine grienberger1,2, aaron d milstein1,2, katie c bittner1, sandro romani1 & jeffrey c magee1\nplace cells in the ca1 region of the hippocampus express location-specific firing despite receiving a steady barrage of \nheterogeneously tuned excitatory inputs that should compromise output dynamic range and timing. we examined the role \nof synaptic inhibition in countering the d", "letter\narithmetic and local circuitry underlying dopamine\nprediction errors\n\ndoi:10.1038/nature14855\n\nneir eshel1, michael bukwich1, vinod rao1, vivian hemmelder1, ju tian1 & naoshige uchida1\n\ndopamine neurons are thought to facilitate learning by comparing\nactual and expected reward1,2. despite two decades of investiga-\ntion, little is known about how this comparison is made. to deter-\nmine how dopamine neurons calculate prediction error, we\ncombined optogenetic manipulations with extracellular", "this analysis did not include medial and orbital frontal\nregions; neither of these regions showed content-\nspecific sustained activity.\n\n24. in the right hemisphere, the differences between the\nspatial extents of sustained activation for face and\nspatial working memory were 6.1 and 3.3 cm3 and\n0.31 and 0.23% signal change in the middle frontal\ncortex, and 1.8 and 1.8 cm3 and 0.39 and 0.25% in\nthe inferior frontal cortex (medians across subjects; p\n. 0.1 for all comparisons).\n\n25. others (2, 4, 2", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/221615826\n\nparallel reinforcement learning with linear function approximation.\n\nconference paper \u00b7 may 2007\n\ndoi: 10.1145/1329125.1329179\u00a0\u00b7\u00a0source: dblp\n\ncitations\n12\n\n2 authors, including:\n\ndaniel kudenko\nforschungszentrum l3s\n\n202 publications\u00a0\u00a0\u00a03,431 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n646\n\nall content following this page was uploaded by daniel kudenko on 29 may 2014.\n\nthe user has reque", "9\n1\n0\n2\n\n \n\np\ne\ns\n6\n\n \n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n7\n6\n8\n1\n0\n\n.\n\n8\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nneuroscience-inspired online unsupervised learning algorithms\n\ncengiz pehlevan\u22171 and dmitri b. chklovskii\u20202,3\n\n1john a. paulson school of engineering and applied sciences, harvard university\n\n2flatiron institute, simons foundation\n\n3neuroscience institute, nyu medical center\n\nabstract\n\nalthough the currently popular deep learning networks achieve unprecedented performance\n\non some tasks, the human brain", "vol 454 | 21 august 2008 | doi:10.1038/nature07140\n\nletters\n\nspatio-temporal correlations and visual signalling in a\ncomplete neuronal population\njonathan w. pillow1, jonathon shlens2, liam paninski3, alexander sher4, alan m. litke4, e. j. chichilnisky2\n& eero p. simoncelli5\n\nthe\n\nindicate\n\nexistence of\n\nstatistical dependencies in the responses of sensory neurons gov-\nern both the amount of stimulus information conveyed and the\nmeans by which downstream neurons can extract it. although a\nvariet", "6\n1\n0\n2\n\n \n\ng\nu\na\n2\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n5\n1\n3\n6\n0\n\n.\n\n8\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nlfads - latent factor analysis via dynamical\n\nsystems\n\ndavid sussillo\u2217\ngoogle, inc.\n\nsussillo@google.com\n\nl.f. abbott\n\ndepartment of neuroscience\n\ncolumbia university\n\nlfabbott@columbia.edu\n\nrafal jozefowicz\u2020\n\ngoogle, inc.\n\nrafal@openai.com\n\nchethan pandarinath\u2021\ndepartment of neurosurgery\n\nstanford university\n\nchethan@gatech.edu\n\nabstract\n\nneuroscience is experiencing a data revolution in which many hundreds", "the \n\njournal \n\nof  neuroscience, \n\nmarch \n\n15,  1996, \n\n16(6):2112-2126 \n\nrepresentation \ndynamics  of  the  head-direction \n\nof  spatial  orientation \n\nby  the  intrinsic \n\ncell  ensemble:  a  theory \n\nkechen  zhang \ndepartment \n\nof  cognitive  science,  university \n\nof  california  at  san  diego,  la  jolla,  california  92093-05 \n\n15 \n\nin \n\nfound \n\nin  the \n\nlandmarks \n\n(hd)  cells \n\ninformation \n\nrepresented \n\nlimbic  system \n\nthe  instantaneous \n\nrats  represent \nhead  direction \nin  the ", "9\n1\n0\n2\n\n \n\nb\ne\nf\n6\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n2\n2\n2\n0\n0\n\n.\n\n4\n0\n8\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2019\n\nmeta-learning update rules for unsuper-\nvised representation learning\n\nluke metz\ngoogle brain\nlmetz@google.com\n\nniru maheswaranathan\ngoogle brain\nnirum@google.com\n\nbrian cheung\nuniversity of california, berkeley\nbcheung@berkeley.edu\n\njascha sohl-dickstein\ngoogle brain\njaschasd@google.com\n\nabstract\n\na major goal of unsupervised learning is to discover data represe", "hebbian and neuromodulatory mechanisms interact to\ntrigger associative memory formation\n\njoshua p. johansena,b,c,1,2, lorenzo diaz-mataixc,1, hiroki hamanakaa, takaaki ozawaa, edgar ycua, jenny koivumaaa,\nashwani kumara, mian houc, karl deisserothd,e, edward s. boydenf, and joseph e. ledouxc,g,2\n\nalaboratory for neural circuitry of memory, riken brain science institute, wako, saitama 351-0198, japan; bdepartment of life sciences, graduate school\nof arts and sciences, university of tokyo, tokyo 1", "regularization of neural networks using dropconnect\n\nli wan\nmatthew zeiler\nsixin zhang\nyann lecun\nrob fergus\ndept. of computer science, courant institute of mathematical science, new york university\n\nwanli@cs.nyu.edu\nzeiler@cs.nyu.edu\nzsx@cs.nyu.edu\nyann@cs.nyu.edu\nfergus@cs.nyu.edu\n\nabstract\n\nwe introduce dropconnect, a generalization\nof dropout (hinton et al., 2012), for regular-\nizing large fully-connected layers within neu-\nral networks. when training with dropout,\na randomly selected subset", "8\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n6\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n1\n9\n5\n8\n0\n\n.\n\n2\n0\n7\n1\n:\nv\ni\nx\nr\na\n\ntheshatteredgradientsproblem:ifresnetsaretheanswer,thenwhatisthequestion?davidbalduzzi1marcusfrean1lennoxleary1jplewis12kurtwan-duoma1brianmcwilliams3abstractalong-standingobstacletoprogressindeeplearningistheproblemofvanishingandex-plodinggradients.although,theproblemhaslargelybeenovercomeviacarefullyconstructedinitializationsandbatchnormalization,archi-tecturesincorporatingskip-connectionssuchashighwayandresn", "1\n2\n0\n2\n\n \n\nb\ne\nf\n1\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n5\n1\n8\n5\n0\n\n.\n\n2\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nrepresentation matters: o\ufb04ine pretraining\n\nfor sequential decision making\n\nmengjiao yang\n\no\ufb01r nachum\n\nsherryy@google.com\n\nofirnachum@google.com\n\ngoogle research\n\ngoogle research\n\nabstract\n\nthe recent success of supervised learning methods on ever larger o\ufb04ine datasets has spurred interest in\nthe reinforcement learning (rl) \ufb01eld to investigate whether the same paradigms can be translated to rl\nalgorithms. thi", "further\nannual\nreviews\nclick here for quick links to \nannual reviews content online, \nincluding:\n\u2022 other articles in this volume\n\u2022 top cited articles\n\u2022 top downloaded articles\n\u2022 our comprehensive search\n\nhabits, rituals, and the\nevaluative brain\nann m. graybiel\ndepartment of brain and cognitive science and the mcgovern institute for brain research,\nmassachusetts institute of technology, cambridge, massachusetts 02139;\nemail: graybiel@mit.edu\n\nannu. rev. neurosci. 2008. 31:359\u201387\n\nthe annual revi", "r e v i e w s\n\npyramidal neurons: dendritic structure \nand synaptic integration\n\nnelson spruston\nabstract | pyramidal neurons are characterized by their distinct apical and basal dendritic \ntrees and the pyramidal shape of their soma. they are found in several regions of the cns and, \nalthough the reasons for their abundance remain unclear, functional studies \u2014 especially of \nca1 hippocampal and layer v neocortical pyramidal neurons \u2014 have offered insights into the \nfunctions of their unique cel", "9\n1\n0\n2\n\n \nl\nu\nj\n \n\n9\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n0\n6\n3\n0\n\n.\n\n1\n1\n8\n1\n:\nv\ni\nx\nr\na\n\njournal of machine learning research 20 (2019) 1-49\n\nsubmitted 11/18; published 7/19\n\nmeasuring the e\ufb00ects of data parallelism\n\non neural network training\n\nchristopher j. shallue\u2217\njaehoon lee\u2217 \u2020\njoseph antognini\u2020\njascha sohl-dickstein\n\nroy frostig\n\ngeorge e. dahl\n\ngoogle brain\n1600 amphiteatre parkway\nmountain view, ca, 94043, usa\n\neditor: rob fergus\n\nshallue@google.com\n\njaehlee@google.com\n\njoe.antognini@gmai", "on the power of over-parametrization in\n\nneural networks with quadratic activation\n\nsimon s. du 1 jason d. lee 2\n\n8\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n4\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n0\n2\n1\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe provide new theoretical insights on why over-\nparametrization is effective in learning neural\nnetworks. for a k hidden node shallow net-\nwork with quadratic activation and n training\n\ndata points, we show as long as k \u2265 \u221a2n, over-\n\nparametrization enables local search algorithms\nto \ufb01", "i an update to this article is included at the end\n\na sensorimotor circuit in mouse cortex for visual\nflow predictions\n\narticle\n\nhighlights\nd mouse a24b/m2 sends a dense topographically organized\n\ninput to v1\n\nd motor-related signals from a24b/m2 drive motor and\n\nmismatch signals in v1\n\nd training to navigate a left-right inverted world reverses a24b/\n\nm2 visuomotor coding\n\nd stimulation of a24b/m2 axons in v1 in navigating mice elicits\n\nturning behavior\n\nauthors\n\nmarcus leinweber, daniel r. war", "7\n1\n0\n2\n\n \nr\np\na\n4\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n2\n3\n2\n1\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\ndeep information propagation\n\nsamuel s. schoenholz\u2217\ngoogle brain\n\njustin gilmer\u2217\ngoogle brain\n\nsurya ganguli\nstanford university\n\njascha sohl-dickstein\ngoogle brain\n\nabstract\n\nwe study the behavior of untrained neural networks whose weights and biases are\nrandomly distributed using mean \ufb01eld theory. we show the existence of depth\nscales that naturally limit the ", "subject areas:\nplasticity\nsynaptic transmission\nreceptors\nsensory systems\n\nreceived\n10 april 2012\n\naccepted\n9 may 2012\n\npublished\n23 may 2012\n\ncorrespondence and\nrequests for materials\nshould be addressed to\nm.t. (mariomtv@\nhotmail.com) or g.k.\n(kohr@mpimf-\nheidelberg.mpg.de)\n\nnoradrenergic \u2018tone\u2019 determines\ndichotomous control of cortical\nspike-timing-dependent plasticity\n\nhumberto salgado2, georg ko\u00a8hr1 & mario trevin\u02dco1\n\n1department of molecular neurobiology, max planck institute for medical ", "big neural networks waste capacity\n\nyann n. dauphin & yoshua bengio\n\nd\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle\n\nuniversit\u00b4e de montr\u00b4eal, montr\u00b4eal, qc, canada\n\n3\n1\n0\n2\n\n \nr\na\n\n \n\nm\n4\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n3\n8\n5\n3\n\n.\n\n1\n0\n3\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nthis article exposes the failure of some big neural networks to leverage added\ncapacity to reduce under\ufb01tting. past research suggest diminishing returns when\nincreasing the size of neural networks. our experiments on imagene", "a kernelized stein discrepancy for goodness-of-\ufb01t tests\n\nqiang liu\ncomputer science, dartmouth college, nh, 03755\njason d. lee\nmichael jordan\ndepartment of electrical engineering and computer science university of california, berkeley, ca 94709\n\njasondlee88@eecs.berkeley.edu\njordan@cs.berkeley.edu\n\nqliu@cs.dartmouth.edu\n\n6\n1\n0\n2\n\n \nl\nu\nj\n \n\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n3\n5\n2\n3\n0\n\n.\n\n2\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe derive a new discrepancy statistic for mea-\nsuring differences between two", "transcriptional architecture of synaptic\ncommunication delineates gabaergic neuron\nidentity\n\narticle\n\ngraphical abstract\n\nauthors\nanirban paul, megan crow,\nricardo raudales, miao he, jesse gillis,\nz. josh huang\n\ncorrespondence\nhuangj@cshl.edu\n\nin brief\ngabaergic neuron types are\ndistinguished by a transcriptional\narchitecture that encodes their synaptic\ncommunication patterns.\n\nhighlights\nd single-cell transcriptome analysis of phenotype\n\ncharacterized gabaergic neurons\n\nd computation screen ide", "letter\nengineering a memory with ltd and ltp\n\nsadegh nabavi1*, rocky fox1*, christophe d. proulx1, john y. lin2, roger y. tsien2,3 & roberto malinow1\n\ndoi:10.1038/nature13294\n\nit has been proposed that memories are encoded by modification of\nsynaptic strengths through cellular mechanisms such as long-term\npotentiation (ltp) and long-term depression (ltd)1. however, the\ncausal link between these synaptic processes and memory has been\ndifficult to demonstrate2. here we show that fear conditioning3", "article\n\ndoi:10.1038/nature10776\n\nconditional modulation of spike-timing-\ndependent plasticity forolfactory learning\n\nstijn cassenaer1,2 & gilles laurent1,3\n\nmushroom bodies are a well-known site for associative learning in insects. yet the precise mechanisms that underlie\nplasticity there and ensure their specificity remain elusive. in locusts, the synapses between the intrinsic mushroom\nbody neurons and their postsynaptic targets obey a hebbian spike-timing-dependent plasticity (stdp) rule. al", "olfactory landmarks and path integration converge\nto form a cognitive spatial map\n\narticle\n\nhighlights\nd localized odor cues can serve as landmarks to guide virtual\n\nnavigation in the dark\n\nd evolution of the ca1 spatial map re\ufb02ects iterative recognition\n\nof odor landmarks\n\nd path integration imposes spatial meaning on odor cues to\n\nestablish them as landmarks\n\nd a model reveals how odors and path integration interact to\n\nextend spatial maps\n\nauthors\n\nwalter fischler-ruiz, david g. clark,\nnarend", "1\n2\n0\n2\n\n \n\nb\ne\nf\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n8\n8\n2\n3\n0\n\n.\n\n1\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nhow to train your energy-based models\n\nyang song\nstanford university\ndiederik p. kingma\ngoogle research\n\nyangsong@cs.stanford.edu\n\ndpkingma@google.com\n\nabstract\n\nenergy-based models (ebms), also known as non-normalized probabilistic models, specify\nprobability density or mass functions up to an unknown normalizing constant. unlike\nmost other probabilistic models, ebms do not place a restriction on the tracta", "jmlr: workshop and conference proceedings vol 35:1\u201315, 2014\n\nlearning sparsely used overcomplete dictionaries\n\nalekh agarwal\nmicrosoft research, new york ny usa\nanimashree anandkumar\ndept of eecs, uc irvine, irvine, ca usa\nprateek jain\nmicrosoft research, bangalore, india\npraneeth netrapalli\ndept of ece, ut austin, austin, tx usa\n\nrashish tandon\ndept of cs, ut austin, austin, tx usa\n\nalekha@microsoft.com\n\na.anandkumar@uci.edu\n\nprajain@microsoft.com\n\npraneethn@utexas.edu\n\nrashish@cs.utexas.edu\n\na", "deconvolutional networks\n\nmatthew d. zeiler, dilip krishnan, graham w. taylor and rob fergus\ndept. of computer science, courant institute, new york university\n\n{zeiler,dilip,gwtaylor,fergus}@cs.nyu.edu\n\nabstract\n\nbuilding robust low and mid-level image representa-\ntions, beyond edge primitives, is a long-standing goal in\nvision. many existing feature detectors spatially pool edge\ninformation which destroys cues such as edge intersections,\nparallelism and symmetry. we present a learning frame-\nwo", "6\n1\n0\n2\n\n \nr\na\n\n \n\nm\n7\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n3\n8\n2\n0\n\n.\n\n2\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nbinarized neural networks: training neural networks with weights and\n\nactivations constrained to +1 or \u22121\n\nmatthieu courbariaux*1\nitay hubara*2\ndaniel soudry3\nran el-yaniv2\nyoshua bengio1,4\n1universit\u00b4e de montr\u00b4eal\n2technion - israel institute of technology\n3columbia university\n4cifar senior fellow\n*indicates equal contribution. ordering determined by coin \ufb02ip.\n\nmatthieu.courbariaux@gmail.com\nitayhubara@gma", "the  journal \n\nof  neuroscience, \n\nmarch \n\n1,  1996, \n\n76(5):1936-1947 \n\na  framework \npredictive  hebbian  learning \n\nfor  mesencephalic \n\ndopamine  systems  based  on \n\npeter  dayan,* \n\np.  read  montague,\u2019 \nl division  of  neuroscience, \ncognitive  science,  cambridge,  massachusetts \nfor  biological  studies,  la  jolla,  california  92037,  and  4the  department \nla  jolla,  california  92093 \n\nbaylor  college  of  medicine,  houston, \n\nand  terrence \n\nj.  sejnowskw \n\n02139,  3the  howard  ", "supplementary materials \n\n \n \n\n \n \n \n\ncortical phenomenon \n\n \n\nstimulus onset quenches neural variability: a widespread  \n\nchurchland mm**, yu bm**, cunningham jp, sugrue lp, cohen mr, corrado gs, newsome \n\nwt, clark am, hosseini p, scott bb, bradley dc, smith ma, kohn a, movshon ja, \n\narmstrong km, moore t, chang sw, snyder lh, lisberger sg, priebe nj, finn im, ferster d, \n\nryu si, santhanam g, sahani m, and shenoy kv \n\n \n\n \n\n \n\nnature neuroscience: doi:10.1038/nn.2501\f", "4\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n1\n6\n6\n2\n\n.\n\n6\n0\n4\n1\n:\nv\ni\nx\nr\na\n\ngenerative adversarial nets\n\nian j. goodfellow, jean pouget-abadie\u2217, mehdi mirza, bing xu, david warde-farley,\n\nsherjil ozair\u2020, aaron courville, yoshua bengio\u2021\n\nd\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle\n\nuniversit\u00b4e de montr\u00b4eal\nmontr\u00b4eal, qc h3c 3j7\n\nabstract\n\nwe propose a new framework for estimating generative models via an adversar-\nial process, in which we simultaneously train two mo", "article\n\nvasoactive intestinal polypeptide-expressing\ninterneurons in the hippocampus support goal-\noriented spatial learning\n\nhighlights\nd ca2+ imaging indicates bimodal activity dynamics of vip\n\ninterneurons in vivo\n\nd activity of vip interneurons is modulated by task and learning\n\ndemands\n\nd vip-mediated disinhibition supports spatially guided reward\n\nlearning\n\nauthors\n\ngergely farkas turi, wen-ke li,\nspyridon chavlis, ...,\nboris valery zemelman,\npanayiota poirazi, attila losonczy\n\ncorrespond", "review\npublished: 19 january 2016\ndoi: 10.3389/fncir.2015.00085\n\nneuromodulated\nspike-timing-dependent plasticity,\nand theory of three-factor learning\nrules\n\nnicolas fr\u00e9maux and wulfram gerstner *\n\nschool of computer science and brain mind institute, school of life sciences, \u00e9cole polytechnique f\u00e9d\u00e9rale de lausanne,\nlausanne, switzerland\n\nclassical hebbian learning puts the emphasis on joint pre- and postsynaptic activity,\nbut neglects the potential role of neuromodulators. since neuromodulators", "reports\n\nfig. 3. theory, presented as the experiment (see\nfig. 1). the shg source is the magnetic compo-\nnent of the lorentz force on metal electrons in\nthe srrs.\n\nthe setup for measuring the shg is described\nin the supporting online material (22). we expect\nthat the shg strongly depends on the resonance\nthat is excited. obviously, the incident polariza-\ntion and the detuning of the laser wavelength\nfrom the resonance are of particular interest. one\npossibility for controlling the detuning is to", "annual review of neuroscience\nthe geometry of information\ncoding in correlated neural\npopulations\n\nrava azeredo da silveira1,2,3,4 and fred rieke5\n1department of physics, ecole normale sup\u00e9rieure, 75005 paris, france; email: rava@ens.fr\n2laboratoire de physique de l\u2019ens, universit\u00e9 paris sciences & lettres (psl), cnrs,\nsorbonne universit\u00e9, universit\u00e9 de paris, 75006 paris, france\n3institute of molecular and clinical ophthalmology basel, 4031 basel, switzerland\n4faculty of science, university of ", "2\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n6\n1\n\n \n \n]\n\nv\n\ni\n.\ns\ns\ne\ne\n[\n \n \n\n2\nv\n5\n0\n0\n8\n0\n\n.\n\n1\n1\n1\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2022\n\nsolving inverse problems in medical imaging\nwith score-based generative models\n\nyang song\u02da, liyue shen\u02da, lei xing & stefano ermon\nstanford university\n{yangsong@cs,liyues@,lei@,ermon@cs}.stanford.edu\n\nabstract\n\nreconstructing medical images from partial measurements is an important inverse\nproblem in computed tomography (ct) and magnetic resonance imaging (mr", "7\n1\n0\n2\n\n \nr\na\n\n \n\nm\n8\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n9\n7\n1\n5\n0\n\n.\n\n2\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nequilibrium propagation: bridging the gap between energy-based\n\nmodels and backpropagation\n\nbenjamin scellier and yoshua bengio*\n\nuniversit\u00e9 de montr\u00e9al, montreal institute for learning algorithms\n\nmarch 30, 2017\n\nabstract\n\nwe introduce equilibrium propagation, a learning framework for energy-based models. it involves only one kind of\nneural computation, performed in both the \ufb01rst phase (when the prediction ", "adversarially trained neural representations may already be as robust as\n\ncorresponding biological neural representations\n\nchong guo 1 michael j. lee 1 2 3 guillaume leclerc 4 joel dapello 1 2 5 yug rao 6 aleksander madry 4 7\n\njames j. dicarlo 1 2 3\n\nabstract\n\nvisual systems of primates are the gold standard\nof robust perception. there is thus a general be-\nlief that mimicking the neural representations that\nunderlie those systems will yield artificial visual\nsystems that are adversarially robus", "similarity of neural network representations revisited\n\nsimon kornblith 1 mohammad norouzi 1 honglak lee 1 geoffrey hinton 1\n\n9\n1\n0\n2\n\n \nl\nu\nj\n \n\n9\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n4\n1\n4\n0\n0\n\n.\n\n5\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nrecent work has sought to understand the behav-\nior of neural networks by comparing representa-\ntions between layers and between different trained\nmodels. we examine methods for comparing neu-\nral network representations based on canonical\ncorrelation analysis (cca). we show ", "f o c u s   o n   n e u r a l   c o m p u tat i o n  a n d   t h e o r y  \n\nr e v i e w\n\nthe mechanics of state-dependent neural \ncorrelations\n\nbrent doiron1,2, ashok litwin-kumar1\u20133, robert rosenbaum1,2,4,5, gabriel k ocker1,2,6 & kre\u0161imir josi\u01077,8\n\nsimultaneous recordings from large neural populations are becoming increasingly common. an important feature of population \nactivity is the trial-to-trial correlated fluctuation of spike train outputs from recorded neuron pairs. similar to the firin", "proceedings of the twenty-sixth international joint conference on arti\ufb01cial intelligence (ijcai-17)\n\n4949\n\nvalueiterationnetworksavivtamar1,yiwu1,garrettthomas1,sergeylevine1,pieterabbeel1;21ucberkeley,2openaifavivt,jxwuyi,gwthomasg@berkeley.edu,svlevine@eecs.berkeley.edu,pabbeel@cs.berkeley.eduabstractweintroducethevalueiterationnetwork(vin):afullydifferentiableneuralnetworkwitha\u2018planningmodule\u2019embeddedwithin.vinscanlearntoplan,andaresuitableforpredictingoutcomesthatinvolveplanning-basedreasoni", "\f", "review\n\nspecial  issue:  hippocampus  and  memory\n\na  neohebbian  framework  for  episodic\nmemory;  role  of  dopamine-dependent\nlate  ltp\njohn  lisman1,  anthony  a.  grace2 and  emrah  duzel3,4,5\n\n1 department  of  biology  and  volen  center  for  complex  systems,  brandeis  university,  waltham,  ma  02454-9110,  usa\n2 departments  of  neuroscience,  psychiatry  and  psychology,  university  of  pittsburgh,  pittsburgh,  pa  15260,  usa\n3 institute  of  cognitive  neuroscience,  university ", "synaptic neuroscience\ndendritic synapse location and neocortical spike-timing-\ndependent plasticity\n\nreview article\npublished: 21 july 2010\ndoi: 10.3389/fnsyn.2010.00029\n\nrobert c. froemke1*, johannes j. letzkus 2, bj\u00f6rn m. kampa 3, giao b. hang4,5 and greg j. stuart 6\n\n1  departments of otolaryngology and physiology/neuroscience, molecular neurobiology program, the helen and martin kimmel center for biology and medicine, \n\nskirball institute of biomolecular medicine, new york university school ", "robust compressed sensing mri with deep\n\ngenerative priors\n\najil jalal\u2217\n\nece, ut austin\n\najiljalal@utexas.edu\n\nmarius arvinte*\nece, ut austin\n\narvinte@utexas.edu\n\ngiannis daras\ncs, ut austin\n\ngiannisdaras@utexas.edu\n\n1\n2\n0\n2\n\n \nc\ne\nd\n6\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n8\n6\n3\n1\n0\n\n.\n\n8\n0\n1\n2\n:\nv\ni\nx\nr\na\n\neric price\n\ncs, ut austin\n\necprice@cs.utexas.edu\n\nalexandros g. dimakis\n\nece, ut austin\n\ndimakis@austin.utexas.edu\n\njonathan i. tamir\nece, ut austin\n\njtamir@utexas.edu\n\nabstract\n\nthe csgm framework", "neuron\n\nperspective\n\nsymmetry breaking in space-time hierarchies\nshapes brain dynamics and behavior\n\najay s. pillai1,2 and viktor k. jirsa3,*\n1department of neurology and developmental medicine, kennedy krieger institute, baltimore, md 21205, usa\n2department of neurology, johns hopkins university school of medicine, baltimore, md 21205, usa\n3institut de neurosciences des syste` mes, inserm, aix-marseille universite\u00b4 , 13005 marseille, france\n*correspondence: viktor.jirsa@univ-amu.fr\nhttp://dx.do", "continual learning in a multi-layer network of an\nelectric fish\n\narticle\n\ngraphical abstract\n\nauthors\nsalomon z. muller, abigail n. zadina,\nl.f. abbott, nathaniel b. sawtell\n\ncorrespondence\nns2635@columbia.edu\n\nin brief\nusing a cerebellum-like structure in an\nelectric \ufb01sh as a model system for\ninvestigating mechanisms of learning in\nmulti-layer networks, muller et al.\nobserved that functional\ncompartmentalization within individual\nneurons allows synaptic plasticity at an\nintermediate processing ", "hogwild!: a lock-free approach to parallelizing\n\nstochastic gradient descent\n\nfeng niu\n\nleonn@cs.wisc.edu\n\nbenjamin recht\n\nbrecht@cs.wisc.edu\n\nchristopher r\u00b4e\n\nchrisre@cs.wisc.edu\n\nstephen j. wright\n\nswright@cs.wisc.edu\n\ncomputer sciences department\nuniversity of wisconsin-madison\n\nmadison, wi 53706\n\nabstract\n\nstochastic gradient descent (sgd) is a popular algorithm that can achieve state-\nof-the-art performance on a variety of machine learning tasks. several researchers\nhave recently proposed s", "a methods\n\na.1 network model\n\nwe consider a discrete-time implementation of a rate-based recurrent neural network (rnn) similar\nto the form in [56]. we denote the observable states, i.e. \ufb01ring rates, as zt at time t, and the\ncorresponding internal states as st. the dynamics of those states are governed by\n\nsj,t+1 = \u03b7 sj,t + (1 \u2212 \u03b7)\n\nwjl zl,t +\n\nw in\n\njm xm,t+1\n\n\uf8eb\uf8ed(cid:88)\n\nl(cid:54)=j\n\n(cid:88)\n\np\n\n\uf8f6\uf8f8\n\nzj,t = relu (sj,t),\n\n(s1)\nwhere \u03b7 = e\u2212dt/\u03c4m denotes the leak factor for simulation time step d", "biorxiv preprint \nthe copyright holder for this preprint (which was not\ncertified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available under \n\nthis version posted october 10, 2018. \n\nhttps://doi.org/10.1101/440396\n; \n\ndoi: \n\na\n\ncc-by-nc-nd 4.0 international license\n.\n\nfunctional clustering of dendritic activity during decision-making \n\n \nkerlin a12, mohar b12, flickinger d1, maclennan bj1, davis c1, spruston n1, svobo", "training neural networks with local error signals\n\narild n\u00f8kland * 1 lars h. eidnes * 2\n\nabstract\n\nsupervised training of neural networks for classi-\n\ufb01cation is typically performed with a global loss\nfunction. the loss function provides a gradient\nfor the output layer, and this gradient is back-\npropagated to hidden layers to dictate an update\ndirection for the weights. an alternative approach\nis to train the network with layer-wise loss func-\ntions. in this paper we demonstrate, for the \ufb01rst\nti", "8\n1\n0\n2\n\n \n\ny\na\nm\n2\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n2\n9\n2\n8\n0\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nthe marginal value of adaptive gradient methods\n\nin machine learning\n\nashia c. wilson(cid:93), rebecca roelofs(cid:93), mitchell stern(cid:93), nathan srebro\u2020, and benjamin recht(cid:93)\n{ashia,roelofs,mitchell}@berkeley.edu, nati@ttic.edu, brecht@berkeley.edu\n\n(cid:93)university of california, berkeley\n\n\u2020toyota technological institute at chicago\n\nabstract\n\nadaptive optimization methods, which perform lo", "1\n2\n0\n2\n\n \n\ng\nu\na\n4\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n2\n6\n0\n4\n0\n\n.\n\n1\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nmutual information neural estimation\n\nmohamed ishmael belghazi 1 aristide baratin 1 2 sai rajeswar 1 sherjil ozair 1 yoshua bengio 1 3 4\n\naaron courville 1 3 r devon hjelm 1 4\n\nabstract\n\nwe argue that the estimation of mutual informa-\ntion between high dimensional continuous ran-\ndom variables can be achieved by gradient descent\nover neural networks. we present a mutual infor-\nmation neural estimator (mine) ", "physical review x 4, 021039 (2014)\n\nbalanced networks of spiking neurons with spatially dependent recurrent connections\n\nrobert rosenbaum and brent doiron\n\ndepartment of mathematics, university of pittsburgh, pittsburgh, pennsylvania 15260, usa\n\nand center for the neural basis of cognition, pittsburgh, pennsylvania 15213, usa\n\n(received 27 august 2013; revised manuscript received 14 february 2014; published 28 may 2014)\n\nnetworks of model neurons with balanced recurrent excitation and inhibition", "article\n\nprecision of inhibition: dendritic inhibition by\nindividual gabaergic synapses on hippocampal\npyramidal cells is con\ufb01ned in space and time\n\nhighlights\nd new paradigm to measure inhibition by individual gabaergic\n\nsynapses\n\nauthors\n\nfiona e. mu\u00a8 llner, corette j. wierenga,\ntobias bonhoeffer\n\nd a realistic model for dendritic ca2+ inhibition\n\nd ca2+ transients from back-propagating aps are inhibited with\n\nlarge dynamic range\n\nd ca2+ is inhibited with micrometer/millisecond precision in\n\nb", "department of computer science\nuniversity of toronto\nhttp://learning.cs.toronto.edu\n\n6 king\u2019s college rd, toronto\nm5s 3g4, canada\nfax: +1 416 978 1455\n\ncopyright c(cid:13) geo\ufb00rey hinton 2010.\n\naugust 2, 2010\n\nutml tr 2010\u2013003\n\na practical guide to training\nrestricted boltzmann machines\n\nversion 1\n\ndepartment of computer science, university of toronto\n\ngeo\ufb00rey hinton\n\n\f", "vs01ch17-kriegeskorte\n\nari\n\n4 november 2015\n\n10:24\n\ndeep neural networks:\na new framework for\nmodeling biological vision\nand brain information\nprocessing\nnikolaus kriegeskorte\nmedical research council cognition and brain sciences unit, university of cambridge,\ncambridge cb2 7ef, united kingdom; email: nikolaus.kriegeskorte@mrc-cbu.cam.ac.uk\n\nkeywords\nbiological vision, computer vision, object recognition, neural network,\ndeep learning, arti\ufb01cial intelligence, computational neuroscience\n\nabstract", "0\n2\n0\n2\n \nc\ne\nd\n1\n1\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n3\nv\n6\n1\n6\n2\n1\n\n.\n\n5\n0\n9\n1\n:\nv\ni\nx\nr\na\n\ndefending against neural fake news\n\nrowan zellers\u2660, ari holtzman\u2660, hannah rashkin\u2660, yonatan bisk\u2660\n\nali farhadi\u2660\u2665, franziska roesner\u2660, yejin choi\u2660\u2665\n\n\u2660paul g. allen school of computer science & engineering, university of washington\n\n\u2665allen institute for arti\ufb01cial intelligence\nhttps://rowanzellers.com/grover\n\nabstract\n\nrecent progress in natural language generation has raised dual-use concerns. while\napplications ", "ne43ch12_vyas\n\narjats.cls\n\njune 23, 2020\n\n11:56\n\nannual review of neuroscience\ncomputation through neural\npopulation dynamics\n\nsaurabh vyas,1,3 matthew d. golub,2,3\ndavid sussillo,2,3,4 and krishna v. shenoy1,2,3,5\n1department of bioengineering, stanford university, stanford, california 94305, usa;\nemail: smvyas@stanford.edu\n2department of electrical engineering, stanford university, stanford, california 94305, usa\n3wu tsai neurosciences institute, stanford university, stanford, california 94305", "cortical microcircuits as\n\ngated-recurrent neural networks\n\nrui ponte costa\u2217\n\nyannis m. assael\u2217\n\ncentre for neural circuits and behaviour\n\ndept. of computer science\n\ndept. of physiology, anatomy and genetics\n\nuniversity of oxford, oxford, uk\n\nuniversity of oxford, oxford, uk\n\nrui.costa@cncb.ox.ac.uk\n\nand deepmind, london, uk\n\nyannis.assael@cs.ox.ac.uk\n\nbrendan shillingford\u2217\n\ndept. of computer science\n\nuniversity of oxford, oxford, uk\n\nand deepmind, london, uk\n\nbrendan.shillingford@cs.ox.ac.uk\n\nn", "9\n1\n0\n2\n\n \n\nn\na\nj\n \n8\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n6\n8\n3\n2\n0\n\n.\n\n9\n0\n8\n1\n:\nv\ni\nx\nr\na\n\na geometrical analysis of global stability in trained feedback\nnetworks\n\nfrancesca mastrogiuseppe 1,2, srdjan ostojic 1\n\n1 laboratoire de neurosciences cognitives et computationelles, inserm u960 and\n2 laboratoire de physique statistique, cnrs umr 8550\n\u00e9cole normale sup\u00e9rieure - psl research university, paris, france\n\nrecurrent neural networks have been extensively studied in the context of neuroscienc", "letter\n\ndoi:10.1038/nature23020\n\ndistinct timescales of population coding across \ncortex\n\ncaroline a. runyan1*, eugenio piasini2*, stefano panzeri2 & christopher d. harvey1\n\nthe  cortex  represents  information  across  widely  varying \ntimescales1\u20135. for instance, sensory cortex encodes stimuli that \nfluctuate over few tens of milliseconds6,7, whereas in association \ncortex behavioural choices can require the maintenance of \ninformation over seconds8,9. however, it remains poorly understood \nwh", "published as a conference paper at iclr 2016\n\nall you need is a good init\n\ndmytro mishkin, jiri matas\n\ncenter for machine perception\nczech technical university in prague\nczech republic {mishkdmy,matas}@cmp.felk.cvut.cz\n\nabstract\n\nlayer-sequential unit-variance (lsuv) initialization \u2013 a simple method for weight\ninitialization for deep net learning \u2013 is proposed. the method consists of the two\nsteps. first, pre-initialize weights of each convolution or inner-product layer with\northonormal matrices", "implicit self-regularization in deep neural networks: evidence\n\nfrom random matrix theory and implications for learning\n\ncharles h. martin\u2217\n\nmichael w. mahoney\u2020\n\nabstract\n\nrandom matrix theory (rmt) is applied to analyze the weight matrices of deep neural\nnetworks (dnns), including both production quality, pre-trained models such as alexnet\nand inception, and smaller models trained from scratch, such as lenet5 and a miniature-\nalexnet. empirical and theoretical results clearly indicate that the ", "attention is all you need\n\nashish vaswani\u2217\ngoogle brain\n\navaswani@google.com\n\nnoam shazeer\u2217\ngoogle brain\n\nnoam@google.com\n\nniki parmar\u2217\ngoogle research\n\nnikip@google.com\n\njakob uszkoreit\u2217\ngoogle research\nusz@google.com\n\nllion jones\u2217\ngoogle research\n\nllion@google.com\n\naidan n. gomez\u2217 \u2020\nuniversity of toronto\n\naidan@cs.toronto.edu\n\n\u0142ukasz kaiser\u2217\ngoogle brain\n\nlukaszkaiser@google.com\n\nillia polosukhin\u2217 \u2021\n\nillia.polosukhin@gmail.com\n\nabstract\n\nthe dominant sequence transduction models are based on c", "neuron\n\nreview\n\nserotonin in the modulation\nof neural plasticity and networks:\nimplications for neurodevelopmental disorders\n\nklaus-peter lesch1,2,* and jonas waider1\n1division of molecular psychiatry, laboratory of translational neuroscience, adhd clinical research network, department of psychiatry,\npsychosomatics and psychotherapy, university of wu\u00a8 rzburg, 97080 wu\u00a8 rzburg, germany\n2department of neuroscience, school of mental health and neuroscience, maastricht university, 6211 lk maastricht", "article\n\ncommunicated by roger brockett\n\nestimating a state-space model from point process\nobservations\n\nanne c. smith\nasmith@neurostat.mgh.harvard.edu\nneuroscience statistics research laboratory, department of anesthesia and critical\ncare, massachusetts general hospital, boston, ma 02114, u.s.a.\n\nemery n. brown\nbrown@neurostat.mgh.harvard.edu\nneuroscience statistics research laboratory, department of anesthesia and critical\ncare, massachusetts general hospital, boston, ma 02114, u.s.a., and div", "article\n\nhttps://doi.org/10.1038/s41467-023-40141-z\n\nexperimental validation of the free-energy\nprinciple with in vitro neural networks\n\nreceived: 12 october 2022\n\naccepted: 13 july 2023\n\ncheck for updates\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\ntakuya isomura 1\n\n, kiyoshi kotani2, yasuhiko jimbo3 & karl j. friston 4,5\n\nempirical applications of the free-energy principle are not straightforward\nbecause they entail a commitment to a particular process theory, especially a", "ne40ch17-uchida\n\nari\n\n4 july 2017\n\n17:51\n\nneural circuitry of reward\nprediction error\nmitsuko watabe-uchida,1,\u2217 neir eshel,1,2,\u2217\nand naoshige uchida1\n1department of molecular and cellular biology, center for brain science, harvard university,\ncambridge, massachusetts 02138; email: mitsuko@mcb.harvard.edu, uchida@mcb.harvard.edu\n2department of psychiatry and behavioral sciences, stanford university school of medicine,\nstanford, california 94305; email: neshel@stanford.edu\n\nannu. rev. neurosci. 20", "original research article\npublished: 04 april 2014\ndoi: 10.3389/fncom.2014.00038\n\nstochastic variational learning in recurrent spiking\nnetworks\ndanilo jimenez rezende 1,2* and wulfram gerstner 1,2\n\n1 laboratory of cognitive neuroscience, school of life sciences, brain mind institute, ecole polytechnique federale de lausanne, lausanne, vaud, switzerland\n2 laboratory of computational neuroscience, school of computer and communication sciences, ecole polytechnique federale de lausanne, lausanne, va", "evidence for a hierarchy of predictions and prediction\nerrors in human cortex\n\ncatherine wacongnea,b,c,1,2, etienne labyta,b,c,1, virginie van wassenhovea,b,c, tristan bekinschteind, lionel naccachee,f,\nand stanislas dehaenea,b,c,g,2\n\nacognitive neuroimaging unit, institut national de la sant\u00e9 et de la recherche m\u00e9dicale, u992, f-91191 gif/yvette, france; bneurospin center, institute of\nbioimaging commissariat \u00e0 l\u2019energie atomique, f-91191 gif/yvette, france; cuniversit\u00e9 paris 11, orsay, france;", "on the di\ufb03culty of training recurrent neural networks\n\n3\n1\n0\n2\n\n \n\nb\ne\nf\n6\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n3\n6\n0\n5\n\n.\n\n1\n1\n2\n1\n:\nv\ni\nx\nr\na\n\nrazvan pascanu\nuniversite de montreal\n\ntomas mikolov\nbrno university\n\nyoshua bengio\nuniversite de montreal\n\nabstract\n\nthere are two widely known issues with prop-\nerly training recurrent neural networks, the\nvanishing and the exploding gradient prob-\nlems detailed in bengio et al. (1994).\nin\nthis paper we attempt to improve the under-\nstanding of the under", "9\n1\n0\n2\n\n \n\nv\no\nn\n0\n3\n\n \n\n \n \n]\nt\ni\n.\ns\nc\n[\n \n \n\n9\nv\n6\n3\n4\n2\n0\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nnonlinear information bottleneck\n\nartemy kolchinsky,1, \u2217 brendan d. tracey,1, 2 and david h. wolpert1, 3, 4\n\n1santa fe institute, 1399 hyde park road, santa fe, nm, 87501, usa\n\n2dept aeronautics & astronautics, massachusetts institute of technology, cambridge, ma 02139, usa\n\n3complexity science hub, vienna, austria\n\n4arizona state university, tempe, az 85287, usa\n\ninformation bottleneck (ib) is a technique fo", "published as a conference paper at iclr 2022\n\nexposing the\nbehind masked\nmetropolis\u2013hastings\n\nimplicit energy networks\nvia\n\nlanguage models\n\n2\n2\n0\n2\n\n \nr\na\n\n \n\nm\n5\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n3\n7\n2\n0\n\n.\n\n6\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nkartik goyal1, chris dyer2, taylor berg-kirkpatrick3\n1carnegie mellon university, 2deepmind, 3uc san diego\nkartikgo@ttic.edu, cdyer@google.com, tberg@eng.ucsd.edu\n\nabstract\n\nwhile recent work has shown that scores from models trained by the ubiquitous\nmasked language mo", "important gains from supervised fine-tuning of\n\ndeep architectures on large labeled sets\n\npascal lamblin\n\nuniversity of montreal\n\nlamblinp@iro.umontreal.ca\n\nyoshua bengio\n\nuniversity of montreal\n\nyoshua.bengio@umontreal.ca\n\nabstract\n\nthe declared goal of many investigations of deep learning algorithms is to ex-\nploit unsupervised learning algorithms to discover useful representations of the\ndata. but how useful are the representations discovered by the current learning\nalgorithms? this is often ", "a survey of methods\n\nfor explaining black box models\n\nriccardo guidotti1,2, anna monreale1, salvatore ruggieri1, franco turini1,\n\ndino pedreschi1, fosca giannotti2\n\n1 university of pisa, {name.surname}@di.unipi.it\n2 isti-cnr, pisa, {name.surname}@isti.cnr.it\n\nabstract. in the last years many accurate decision support systems\nhave been constructed as black boxes, that is as systems that hide their\ninternal logic to the user. this lack of explanation constitutes both a\npractical and an ethical iss", "vol 441|11 may 2006|doi:10.1038/nature04676\n\nletters\n\nneurons in the orbitofrontal cortex encode\neconomic value\ncamillo padoa-schioppa1 & john a. assad1\n\neconomic choice is the behaviour observed when individuals\nselect one among many available options. there is no intrinsically\n\u2018correct\u2019 answer: economic choice depends on subjective prefer-\nences. this behaviour is traditionally the object of economic\nanalysis1 and is also of primary interest in psychology2. however,\nthe underlying mental proce", "svcca: singular vector canonical correlation\n\nanalysis for deep learning dynamics and\n\ninterpretability\n\n7\n1\n0\n2\n\n \n\nv\no\nn\n8\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n6\n0\n8\n5\n0\n\n.\n\n6\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nmaithra raghu,1,2 justin gilmer,1 jason yosinski,3 & jascha sohl-dickstein1\n\n1google brain 2cornell university 3uber ai labs\n\nmaithrar gmail com, gilmer google com, yosinski uber com, jaschasd google com\n\nabstract\n\nwe propose a new technique, singular vector canonical correlation analysis\n(svcca), a too", "neuroimage 52 (2010) 833\u2013847\n\ncontents lists available at sciencedirect\n\nneuroimage\n\nj o u r n a l h o m e p a g e : w w w. e l s e v i e r. c o m / l o c a t e / y n i m g\n\nattractor concretion as a mechanism for the formation of context representations\nmattia rigotti a,b, daniel ben dayan rubin a,b, sara e. morrison a, c. daniel salzman a,c,d,e,f,g, stefano fusi a,b,\u204e\na department of neuroscience, columbia university college of physicians and surgeons, new york, ny 10032-2695, usa\nb institute ", "article\n\ndoi: 10.1038/s41467-018-05873-3\n\nopen\n\nreconciling persistent and dynamic hypotheses\nof working memory coding in prefrontal cortex\n1,4,5 & steven w. kennerley\n\n1, john p. towers1, joni d. wallis2,3, laurence t. hunt\n\nsean e. cavanagh\n\n1,2,3\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\ncompeting accounts propose that working memory (wm) is subserved either by persistent\nactivity in single neurons or by dynamic (time-varying) activity across a neural population.\nhere, we compare these hypotheses acros", "information dropout: learning optimal\n\nrepresentations through noisy computation\n\n1\n\nalessandro achille and stefano soatto\n\ndepartment of computer science\nuniversity of california, los angeles\n\n405 hilgard ave, los angeles, 90095, ca, usa\n\nemail: {achille,soatto}@cs.ucla.edu\n\n7\n1\n0\n2\n\n \n\nb\ne\nf\n2\n1\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n3\n5\n3\n1\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014the cross-entropy loss commonly used in deep learning is\nclosely related to the de\ufb01ning properties of optimal representat", "`\n\nstatistical parametric maps in functional\n\nimaging: a general linear approach\n\nfriston kj1, holmes ap2, worsley kj3, poline j-p1 frith cd1 and frackowiak rsj1\n\n1.  the wellcome dept. of cognitive neurology, institute of neurology, queen square\n\nwc1n 3bg and the mrc cyclotron unit, hammersmith hospital, w12 ohs, uk\n\n2.  dept. of statistics, glasgow university, glasgow g12 8qq uk\n\n3. dept of mathematics and statistics, mcgill university, montreal h3a 2k6\n\ncanada\n\naddress for correspondence\n\nkar", "short report\n\nmidbrain dopamine neurons compute\ninferred and cached value prediction\nerrors in a common framework\nbrian f sadacca1*, joshua l jones1, geoffrey schoenbaum1,2,3*\n\n1intramural research program of the national institute on drug abuse, national\ninstitutes of health, bethesda, united states; 2department of anatomy and\nneurobiology, university of maryland school of medicine, baltimore, united states;\n3department of neuroscience, johns hopkins school of medicine, baltimore, united\nstates", "2\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n6\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n3\n7\n5\n0\n0\n\n.\n\n3\n0\n2\n2\n:\nv\ni\nx\nr\na\n\ncontrasting random and learned features in deep bayesian linear regression\n\njacob a. zavatone-veth,1, 2, \u2217 william l. tong,3, \u2020 and cengiz pehlevan3, 2, \u2021\n1department of physics, harvard university, cambridge, massachusetts 02138, usa\n2center for brain science, harvard university, cambridge, massachusetts 02138, usa\n\n3john a. paulson school of engineering and applied sciences,\n\nharvard university, cambridge,", "behavioral and brain sciences\n\nsuboptimality in perceptual decision making\n\ncambridge.org/bbs\n\ndobromir rahneva and rachel n. denisonb\n\ntarget article\n\nauthors d. rahnev and r. n. denison\ncontributed equally to this work.\n\ncite this article: rahnev d, denison rn. (2018)\nsuboptimality in perceptual decision making.\nbehavioral and brain sciences 41, e223: 1\u201366.\ndoi:10.1017/s0140525x18000936\n\ntarget article accepted: 9 february 2018\ntarget article manuscript online: 27 february\n2018\ncommentaries ac", "journal of machine learning research 16 (2015) 2859-2900\n\nsubmitted 5/14; revised 3/15; published 12/15\n\nlinear dimensionality reduction:\n\nsurvey, insights, and generalizations\n\njohn p. cunningham\ndepartment of statistics\ncolumbia university\nnew york city, usa\n\nzoubin ghahramani\ndepartment of engineering\nuniversity of cambridge\ncambridge, uk\n\neditor: gert lanckriet\n\njpc2181@columbia.edu\n\nzoubin@eng.cam.ac.uk\n\nabstract\n\nlinear dimensionality reduction methods are a cornerstone of analyzing high d", "a r t i c l e s\n\ntemporal context calibrates interval timing\nmehrdad jazayeri1,2 & michael n shadlen2\n\nwe use our sense of time to identify temporal relationships between events and to anticipate actions. the degree to which we \ncan exploit temporal contingencies depends on the variability of our measurements of time. we asked humans to reproduce time \nintervals drawn from different underlying distributions. as expected, production times were more variable for longer intervals. \nhowever, product", "tools and resources\n\ninterpreting wide-band neural activity\nusing convolutional neural networks\nmarkus frey1,2*, sander tanni3, catherine perrodin4, alice o\u2019leary3,\nmatthias nau1,2, jack kelly5, andrea banino6, daniel bendor4, julie lefort3,\nchristian f doeller1,2,7\u2020, caswell barry3\u2020*\n\n1kavli institute for systems neuroscience, centre for neural computation, the egil\nand pauline braathen and fred kavli centre for cortical microcircuits, ntnu,\nnorwegian university of science and technology, trond", "0\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n2\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n3\n2\n7\n0\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\na practical sparse approximation for\n\nreal time recurrent learning\n\njacob menick\u2217\n\ndeepmind\n\nerich elsen\u2217\ndeepmind\n\nutku evci\ngoogle\n\nuniversity college london\n\nsimon osindero\n\nkaren simonyan\n\ndeepmind\n{jmenick, eriche, evcu, osindero, simonyan, gravesa}@google.com\n\ndeepmind\n\nalex graves\ndeepmind\n\nabstract\n\ncurrent methods for training recurrent neural networks are based on backpropagation through time,\nwhic", "2\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n3\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n7\n5\n0\n0\n\n.\n\n7\n0\n2\n1\n:\nv\ni\nx\nr\na\n\nimplicit density estimation by local moment\n\nmatching to sample from auto-encoders\n\nyoshua bengio, guillaume alain, and salah rifai\n\ndepartment of computer science and operations research\n\nuniversity of montreal\n\nmontreal, h3c 3j7\n\njuly 3, 2012\n\nabstract\n\nrecent work suggests that some auto-encoder variants do a good job of cap-\nturing the local manifold structure of the unknown data generating density. this\np", ".\n\nd\ne\nv\nr\ne\ns\ne\nr\n \ns\nt\nh\ng\ni\nr\n \nl\nl\n\na\n\n \n.\n\ne\nr\nu\nt\na\nn\n \nr\ne\ng\nn\ni\nr\np\ns\n\n \nf\no\n \nt\nr\na\np\n\n \n,\n.\n\nc\nn\n\ni\n \n,\n\na\nc\n\n \n\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n8\n1\n0\n2\n \n\u00a9\n\n \n\na r t i c l e s\n\nthe hippocampus as a predictive map\n  & samuel j gershman4 \nkimberly l stachenfeld1,2, matthew m botvinick1,3 \n\n \n\na cognitive map has long been the dominant metaphor for hippocampal function, embracing the idea that place cells encode a \ngeometric representation of space. however, evidence for predictive coding, reward ", "6\n1\n0\n2\n\n \n\nb\ne\nf\n4\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n3\nv\n5\n2\n0\n2\n0\n\n.\n\n6\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nspatial transformer networks\n\nmax jaderberg\n\nkaren simonyan\n\nandrew zisserman\n\nkoray kavukcuoglu\n\ngoogle deepmind, london, uk\n\n{jaderberg,simonyan,zisserman,korayk}@google.com\n\nabstract\n\nconvolutional neural networks de\ufb01ne an exceptionally powerful class of models,\nbut are still limited by the lack of ability to be spatially invariant to the input data\nin a computationally and parameter ef\ufb01cient manner. in thi", "tilburg centre for creative computing\ntilburg university\nhttp://www.uvt.nl/ticc\n\np.o. box 90153\n5000 le tilburg, the netherlands\nemail: ticc@uvt.nl\n\ncopyright c(cid:13) laurens van der maaten, eric postma, and jaap van den herik 2009.\n\noctober 26, 2009\n\nticc tr 2009\u2013005\n\ndimensionality reduction: a comparative\n\nreview\n\nlaurens van der maaten\n\neric postma\n\njaap van den herik\n\nticc, tilburg university\n\nabstract\n\nin recent years, a variety of nonlinear dimensionality reduction techniques have been\n", "journal of statistical mechanics:theory and experiment     paperasymptotics of representation learning in finitebayesian neural networks*to cite this article: jacob a zavatone-veth et al j. stat. mech. (2022) 114008 view the article online for updates and enhancements.you may also likethe axial anomaly and consistency of thefinite-width light-cone local duality sum rulefor the form factor of the transition * *  0minghai li, ze-kun guo and jueping liu-quantum transport through the edgestates of z", " \n \n \n \noriginal citation: \nsanborn, adam n. and silva, ricardo. (2013) constraining bridges between levels of analysis : \na computational justification for locally bayesian learning. journal of mathematical \npsychology, volume 57 (number 3-4). pp. 94-106. \n \npermanent wrap url: \nhttp://wrap.warwick.ac.uk/57365  \n \ncopyright and reuse: \nthe warwick research archive portal (wrap) makes this work by researchers of the \nuniversity of warwick available open access under the following conditions.  co", "policy  gradient  methods for \n\nreinforcement  learning with function \n\napproximation \n\nrichard s.  sutton, david mcallester, satinder singh, yishay mansour \n\nat&t labs - research,  180 park avenue,  florham park,  nj 07932 \n\nabstract \n\nfunction  approximation  is  essential  to  reinforcement  learning,  but \nthe standard approach of approximating a  value function and deter \nmining  a  policy  from  it  has so  far  proven theoretically  intractable. \nin this paper we explore an alternative ap", "r e v i e w s\n\nthe organization of recent\nand remote memories\n\npaul w. frankland*\u2021 and bruno bontempi\u00a7\n\nabstract | a fundamental question in memory research is how our brains can form enduring\nmemories. in humans, memories of everyday life depend initially on the medial temporal lobe\nsystem, including the hippocampus. as these memories mature, they are thought to become\nincreasingly dependent on other brain regions such as the cortex. little is understood about how\nnew memories in the hippocampu", "the chemical brain hypothesis for\nthe origin of nervous systems\n\nroyalsocietypublishing.org/journal/rstb\n\ng\u00e1sp\u00e1r j\u00e9kely\n\nresearch\n\ncite this article: j\u00e9kely g. 2021 the chemical\nbrain hypothesis for the origin of nervous\nsystems. phil. trans. r. soc. b 376: 20190761.\nhttps://doi.org/10.1098/rstb.2019.0761\n\naccepted: 25 november 2020\n\none contribution of 10 to a theme issue \u2018basal\ncognition: multicellularity, neurons and the\ncognitive lens\u2019.\n\nsubject areas:\nevolution, neuroscience, theoretical bi", "6\n1\n0\n2\n\n \n\ny\na\nm\n0\n2\n\n \n\n \n \n]\n\n.\n\nc\no\nh\nt\na\nm\n\n[\n \n \n\n3\nv\n2\n4\n6\n5\n0\n\n.\n\n3\n0\n6\n1\n:\nv\ni\nx\nr\na\n\noptimalblack-boxreductionsbetweenoptimizationobjectiveszeyuanallen-zhuzeyuan@csail.mit.eduprincetonuniversityeladhazanehazan@cs.princeton.eduprincetonuniversity\ufb01rstcirculatedonfebruary5,2016\u2217abstractthediverseworldofmachinelearningapplicationshasgivenrisetoaplethoraofalgorithmsandoptimizationmethods,\ufb01nelytunedtothespeci\ufb01cregressionorclassi\ufb01cationtaskathand.wereducethecomplexityofalgorithmdesignformachi", "scaling limits of wide neural networks with weight sharing:\n\ngaussian process behavior, gradient independence, and neural tangent\n\nkernel derivation\n\n0\n2\n0\n2\n\n \nr\np\na\n4\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n0\n6\n7\n4\n0\n\n.\n\n2\n0\n9\n1\n:\nv\ni\nx\nr\na\n\ngreg yang 1\n\nabstract\n\nseveral recent trends in machine learning theory and practice, from the design of state-of-the-art gaussian process\nto the convergence analysis of deep neural nets (dnns) under stochastic gradient descent (sgd), have found\nit fruitful to stud", "machine  learning,  20, 273-297 (1995)\n\u00a9  1995 kluwer academic  publishers, boston.  manufactured in the  netherlands.\n\nsupport-vector  networks\n\ncorinna cortes \nvladimir vapnik \nat&t bell labs., holmdel, nj 07733, usa\n\neditor: lorenza saitta\n\ncorinna@neural.att.com\nvlad@neural.att.com\n\nabstract.  the support-vector  network is a new learning  machine for two-group  classification  problems.  the\nmachine  conceptually  implements  the  following  idea:  input  vectors  are  non-linearly  mapped ", "ann. n.y. acad. sci. issn 0077-8923\n\nannals of the new york academy of sciences\nspecial issue: the year in cognitive neuroscience\nreview\n\nbeyond the feedforward sweep: feedback computations in\nthe visual cortex\n\ngabriel kreiman1\n1children\u2019s hospital, harvard medical school and center for brains, minds, and machines, boston, massachusetts. 2cognitive\nlinguistic and psychological sciences, carney institute for brain science, brown university, providence, rhode island\n\nand thomas serre2\n\naddresses ", "dynamic routing between capsules\n\nsara sabour\n\nnicholas frosst\n\ngeoffrey e. hinton\n\ngoogle brain\n\ntoronto\n\n{sasabour, frosst, geoffhinton}@google.com\n\nabstract\n\na capsule is a group of neurons whose activity vector represents the instantiation\nparameters of a speci\ufb01c type of entity such as an object or an object part. we use\nthe length of the activity vector to represent the probability that the entity exists and\nits orientation to represent the instantiation parameters. active capsules at one l", "phil. trans. r. soc. b (2009) 364, 1183\u20131191\ndoi:10.1098/rstb.2008.0306\n\nthe neurobiology of memory based predictions\n\nhoward eichenbaum* and norbert j. fortin\n\ncenter for memory and brain, boston university, 2 cummington street, boston, ma 02215, usa\n\nrecent \ufb01ndings indicate that, in humans, the hippocampal memory system is involved in the capacity\nto imagine the future as well as remember the past. other studies have suggested that animals may\nalso have the capacity to recall the past and plan", "j neurophysiol 104: 1068 \u20131076, 2010.\nfirst published june 10, 2010; doi:10.1152/jn.00158.2010.\n\na pallidus-habenula-dopamine pathway signals inferred stimulus values\n\nethan s. bromberg-martin,1 masayuki matsumoto,1,2 simon hong,1 and okihide hikosaka1\n1laboratory of sensorimotor research, national eye institute, national institutes of health, bethesda, maryland; and 2primate research\ninstitute, kyoto university, inuyama, aichi, japan\n\nsubmitted 5 february 2010; accepted in \ufb01nal form 9 june 2010", "dynamics of ongoing activity:\n\nexplanation of the large variability\n\nin evoked cortical responses\n\namos arieli, alexander sterkin, amiram grinvald, ad aertsen*\n\nevoked activity in the mammalian cortex and the resulting behavioral responses exhibit\na large variability to repeated presentations of the same stimulus. this study examined\nwhether the variability can be attributed to ongoing activity. ongoing and evoked spa-\ntiotemporal activity patterns in the cat visual cortex were measured with rea", "cognitive, affective, & behavioral neuroscience\n2008, 8 (4), 429-453\ndoi:10.3758/cabn.8.4.429\n\nconnections between cn omputational\n\nand neurobiological perspectives\n\non decision making\n\ndecision theory, reinforcement learning,\n\nand the brain\n\npeter dayan\n\nuniversity college london, london, england\n\nand\n\nnathaniel d. daw\n\nnew york university, new york, new york\n\ndecision making is a core competence for animals and humans acting and surviving in environments they \nonly partially comprehend, gainin", "a\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\nt\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nhhs public access\nauthor manuscript\nnature. author manuscript; available in pmc 2014 august 05.\n\npublished in final edited form as:\n\nnature. 2013 november 7; 503(7474): 78\u201384. doi:10.1038/nature12742.\n\ncontext-dependent computation by recurrent dynamics in \nprefrontal cortex\n\nvalerio mante1,+,*, david sussillo2,+, krishna v. shenoy2,3, and will", "8486 \u2022 the journal of neuroscience, august 8, 2007 \u2022 27(32):8486 \u2013 8495\n\nbehavioral/systems/cognitive\n\nan integrated microcircuit model of attentional processing\nin the neocortex\n\nsalva ardid,1,2 xiao-jing wang,3 and albert compte1,2\n1instituto de neurociencias de alicante, universidad miguel herna\u00b4ndez\u2013consejo superior de investigaciones cient\u0131\u00b4ficas, 03550 sant joan d\u2019alacant, spain,\n2institut d\u2019investigacions biome`diques august pi i sunyer, 08036 barcelona, spain, and 3department of neurobio", "learning representations for neural\n\nnetwork-based classi\ufb01cation using the\n\ninformation bottleneck principle\n\n1\n\n9\n1\n0\n2\n\n \nr\np\na\n1\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n6\nv\n6\n6\n7\n9\n0\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nrana ali amjad, student member, ieee, and bernhard c. geiger, senior member, ieee\n\nabstract\u2014in this theory paper, we investigate training deep neural networks (dnns) for classi\ufb01cation via minimizing the information\nbottleneck (ib) functional. we show that the resulting optimization problem suffers ", "markov chain monte carlo convergence diagnostics: a comparative review \nauthor(s): mary kathryn cowles and bradley p. carlin \nsource: journal of the american statistical association, jun., 1996, vol. 91, no. 434 \n(jun., 1996), pp. 883-904\npublished by: taylor & francis, ltd. on behalf of the american statistical association \n\n \n\nstable url: https://www.jstor.org/stable/2291683\n \nreferences \nlinked references are available on jstor for this article: \nhttps://www.jstor.org/stable/2291683?seq=1&cid", "neuron, vol. 35, 773\u2013782, august 15, 2002, copyright \uf8e92002 by cell press\n\ngain modulation from background synaptic input\n\nfrances s. chance,1,3 l.f. abbott,2\nand alex d. reyes1\n1center for neural science\nnew york university\nnew york, new york 10003\n2 volen center for complex systems and\ndepartment of biology\nbrandeis university\nwaltham, massachusetts 02454\n\nsummary\n\ngain modulation is a prominent feature of neuronal\nactivity recorded in behaving animals, but the mecha-\nnism by which it occurs is", "atoms of recognition in human and computer vision\n\nshimon ullmana,b,1,2, liav assifa,1, ethan fetayaa, and daniel hararia,c,1\n\nadepartment of computer science and applied mathematics, weizmann institute of science, rehovot 7610001, israel; bdepartment of brain and cognitive\nsciences, massachusetts institute of technology, cambridge, ma 02139; and cmcgovern institute for brain research, cambridge, ma 02139\n\nedited by michael e. goldberg, columbia university college of physicians, new york, ny, an", "article\n\nreceived 7 jul 2015 | accepted 21 mar 2016 | published 26 apr 2016\n\nopen\nneural substrates of cognitive biases during\nprobabilistic inference\nalireza soltani1, peyman khorsand1, clara guo1, shiva farashahi1 & janet liu1\n\ndoi: 10.1038/ncomms11393\n\ndecision making often requires simultaneously learning about and combining evidence from\nvarious sources of information. however, when making inferences from these sources,\nhumans show systematic biases that are often attributed to heuristics o", "the journal of neuroscience, march 25, 2009 \u2022 29(12):3685\u20133694 \u2022 3685\n\nbehavioral/systems/cognitive\n\ncorrelated connectivity and the distribution of firing rates\nin the neocortex\n\nalexei a. koulakov, toma\u00b4s\u02c7 hroma\u00b4dka, and anthony m. zador\ncold spring harbor laboratory, cold spring harbor, new york 11724\n\ntwo recent experimental observations pose a challenge to many cortical models. first, the activity in the auditory cortex is sparse, and\nfiring rates can be described by a lognormal distributio", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/2022.03.28.485868\n; \n\nthis version posted july 1, 2023. \n\nthe copyright holder for this preprint (which\n\nwas not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\navailable under a\n\ncc-by-nc 4.0 international license\n.\n\nwhat can 1.8 billion regressions tell us about the pressures shaping\n\nhigh-level visual representation in brains and machines?\n\ncolin conwell1\u2217, j", "neural fitted q iteration - first experiences\nwith a data e\ufb03cient neural reinforcement\n\nlearning method\n\nmartin riedmiller\n\nneuroinformatics group,\n\nuniversity of onsabr\u00a8uck, 49078 osnabr\u00a8uck\n\nabstract. this paper introduces nfq, an algorithm for e\ufb03cient and ef-\nfective training of a q-value function represented by a multi-layer percep-\ntron. based on the principle of storing and reusing transition experiences,\na model-free, neural network based reinforcement learning algorithm is\nproposed. the ", "review\nneuromodulators and long-term synaptic\nplasticity in learning and memory:\na steered-glutamatergic perspective\n\namjad h. bazzari * and h. rheinallt parri\n\nschool of life and health sciences, aston university, birmingham b4 7et, uk; parrihr@aston.ac.uk\n* correspondence: bazzaria@aston.ac.uk; tel.: +44-(0)1212044186\n\nreceived: 7 october 2019; accepted: 29 october 2019; published: 31 october 2019\n\nabstract: the molecular pathways underlying the induction and maintenance of long-term synaptic\n", "matrix completion has no spurious local minimum\n\nrong ge\n\nduke university\n\n308 research drive, nc 27708\nrongge@cs.duke.edu.\n\njason d. lee\n\nuniversity of southern california\n3670 trousdale pkwy, ca 90089\n\njasonlee@marshall.usc.edu.\n\ntengyu ma\n\nprinceton university\n\n35 olden street, nj 08540\n\ntengyu@cs.princeton.edu.\n\nabstract\n\nmatrix completion is a basic machine learning problem that has wide applica-\ntions, especially in collaborative \ufb01ltering and recommender systems. simple\nnon-convex optimiza", "cnn features off-the-shelf: an astounding baseline for recognition\n\nali sharif razavian hossein azizpour\n\njosephine sullivan stefan carlsson\n\ncvap, kth (royal institute of technology)\n\n{razavian,azizpour,sullivan,stefanc}@csc.kth.se\n\nstockholm, sweden\n\n4\n1\n0\n2\n\n \n\ny\na\nm\n2\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n3\nv\n2\n8\n3\n6\n\n.\n\n3\n0\n4\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nrecent results indicate that the generic descriptors ex-\ntracted from the convolutional neural networks are very\npowerful. this paper adds to the moun", "anne.churchland@\n\n*forcorrespondence:\ninternationalbrainlab.org (akc); liam.\n(lp); nicholas.steinmetz@\ninternationalbrainlab.org (nas)\n\npaninski@internationalbrainlab.org\n\nbiorxiv preprint \n\nhttps://doi.org/10.1101/2022.05.09.491042\n; \n\ndoi: \nwas not certified by peer review) is the author/funder. all rights reserved. no reuse allowed without permission. \n\nthis version posted may 12, 2022. \n\nthe copyright holder for this preprint (which\n\nreproducibilityofin-vivo\n1 electrophysiologicalmeasurement", "1\n\na connection between score matching\n\nand denoising autoencoders\n\npascal vincent\n\nvincentp@iro.umontreal.ca\n\ndept. iro, universit\u00e9 de montr\u00e9al,\n\ncp 6128, succ. centre-ville, montr\u00e9al (qc) h3c 3j7, canada.\n\nd\u00e9partement d\u2019informatique et de recherche op\u00e9rationnelle\n\ntechnical report 1358\n\ndecember 2010\n\nthis is a preprint version of a note that has been\naccepted for publication in neural computation.\n\nkeywords: autoencoder, energy based models, score matching, denoising, density\n\nestimation.\n\nab", "19-charter&oaksford-chap19  11/5/07  11:22 am  page 427\n\nchapter 19\nsemi-rational models of\nconditioning: the case of trial\norder\nnathaniel d. daw, aaron c. courville, and\npeter dayan\njune 15, 2007\n\n1 introduction\nbayesian treatments of animal conditioning start from a generative model that speci-\nfies precisely a set of assumptions about the structure of the learning task. optimal\nrules for learning are direct mathematical consequences of these assumptions. in\nterms of marr\u2019s (1982) levels of a", "de\ufb01nitions, methods, and applications in interpretable\nmachine learning\n\nw. james murdocha,1, chandan singhb,1, karl kumbiera,2, reza abbasi-aslb,c,d,2, and bin yua,b,3\n\nastatistics department, university of california, berkeley, ca 94720; belectrical engineering and computer science department, university of california,\nberkeley, ca 94720; cdepartment of neurology, university of california, san francisco, ca 94158; and dallen institute for brain science, seattle, wa 98109\n\ncontributed by bin yu", "a r t i c l e s\n\nfragmentation of grid cell maps in a \nmulticompartment environment\n\ndori derdikman1, jonathan r whitlock1, albert tsao1, marianne fyhn1,2, torkel hafting1,2,  \nmay-britt moser1 & edvard i moser1\n\nto determine whether entorhinal spatial representations are continuous or fragmented, we recorded neural activity in grid cells \nwhile rats ran through a stack of interconnected, zig-zagged compartments of equal shape and orientation (a hairpin maze). the \ndistribution of spatial firing", "neuron\n\nreview\n\nthe spike-timing dependence of plasticity\n\ndaniel e. feldman1,*\n1department of molecular and cell biology, and helen wills neuroscience institute, university of california, berkeley, berkeley,\nca 94720-3200, usa\n*correspondence: dfeldman@berkeley.edu\nhttp://dx.doi.org/10.1016/j.neuron.2012.08.001\n\nin spike-timing-dependent plasticity (stdp), the order and precise temporal interval between presynaptic and\npostsynaptic spikes determine the sign and magnitude of long-term potentiati", "forward and backward inference in spatial cognition\n\nwill d. penny1*, peter zeidman1, neil burgess2\n\n1 wellcome trust centre for neuroimaging, university college, london, london, united kingdom, 2 institute for cognitive neuroscience, university college, london,\nlondon, united kingdom\n\nabstract\n\nthis paper shows that the various computations underlying spatial cognition can be implemented using statistical inference\nin a single probabilistic model. inference is implemented using a common set of ", "p\ne\nr\ng\na\nm\no\nn\n \nn\ne\nu\nr\na\nl\n \nn\ne\nt\nw\no\nr\nk\ns\n,\n \nv\no\nl\n.\n \n7\n,\n \nn\no\n.\n \n3\n,\n \np\np\n.\n \n5\n0\n7\n-\n5\n2\n2\n,\n \n1\n9\n9\n4\n \nc\no\np\ny\nr\ni\ng\nh\nt\n \n\u00a9\n \n1\n9\n9\n4\n \ne\nl\ns\ne\nv\ni\ne\nr\n \ns\nc\ni\ne\nn\nc\ne\n \nl\nt\nd\n \np\nr\ni\nn\nt\ne\nd\n \ni\nn\n \nt\nh\ne\n \nu\ns\na\n.\n \na\nl\nl\n \nr\ni\ng\nh\nt\ns\n \nr\ne\ns\ne\nr\nv\ne\nd\n \n0\n8\n9\n3\n-\n6\n0\n8\n0\n/\n9\n4\n \n$\n6\n.\n0\n0\n \n+\n \n.\n0\n0\n \nc\no\nn\nt\nr\ni\nb\nu\nt\ne\nd\n \na\nr\nt\ni\nc\nl\ne\n \nt\no\np\no\nl\no\ng\ny\n \nr\ne\np\nr\ne\ns\ne\nn\nt\ni\nn\ng\n \nn\ne\nt\nw\no\nr\nk\ns\n \nt\nh\no\nm\na\ns\n \nm\na\nr\nt\ni\nn\ne\nt\nz\n \n1\n'\n2\n \na\nn\nd\n \nk\n_\nl\na\nu\ns\n \ns\nc\nh\nu\nl\n", "7\n1\n0\n2\n\n \n\nv\no\nn\n7\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n1\n9\n3\n2\n0\n\n.\n\n1\n1\n7\n1\n:\nv\ni\nx\nr\na\n\n95\n\na tutorial on canonical correlation methods\n\nviivi uurtio, aalto university\njo \u02dcao m. monteiro, university college london\njaz kandola, imperial college london\njohn shawe-taylor, university college london\ndelmiro fernandez-reyes, university college london\njuho rousu, aalto university\n\ncanonical correlation analysis is a family of multivariate statistical methods for the analysis of paired sets\nof variables.", "a r t i c l e s\n\npartitioning neuronal variability\nrobbe l t goris1, j anthony movshon1 & eero p simoncelli1,2\n\nresponses of sensory neurons differ across repeated measurements. this variability is usually treated as stochasticity  \narising within neurons or neural circuits. however, some portion of the variability arises from fluctuations in excitability due  \nto factors that are not purely sensory, such as arousal, attention and adaptation. to isolate these fluctuations, we developed  \na model", "biol cybern (2012) 106:523\u2013541\ndoi 10.1007/s00422-012-0512-8\n\nprospects\n\nactive inference and agency: optimal control without\ncost functions\nkarl friston \u00b7 spyridon samothrakis \u00b7\nread montague\n\nreceived: 1 february 2012 / accepted: 16 july 2012 / published online: 3 august 2012\n\u00a9 the author(s) 2012. this article is published with open access at springerlink.com\n\nabstract this paper describes a variational free-energy for-\nmulation of (partially observable) markov decision problems\nin decision ma", "vol 436|11 august 2005|doi:10.1038/nature03721\n\narticles\n\nmicrostructure of a spatial map in the\nentorhinal cortex\n\ntorkel hafting1*, marianne fyhn1*, sturla molden1\u2020, may-britt moser1 & edvard i. moser1\n\nthe ability to \ufb01nd one\u2019s way depends on neural algorithms that integrate information about place, distance and\ndirection, but the implementation of these operations in cortical microcircuits is poorly understood. here we show that\nthe dorsocaudal medial entorhinal cortex (dmec) contains a direc", "a r t i c l e s\n\na neural network that finds a naturalistic solution for \nthe production of muscle activity\ndavid sussillo1, mark m churchland2, matthew t kaufman1,4 & krishna v shenoy1,3\nit remains an open question how neural responses in motor cortex relate to movement. we explored the hypothesis that motor \ncortex reflects dynamics appropriate for generating temporally patterned outgoing commands. to formalize this hypothesis, we \ntrained recurrent neural networks to reproduce the muscle acti", "9\n1\n0\n2\n\n \n\nn\na\nj\n \n\n9\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n0\n9\n5\n0\n\n.\n\n2\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nsoft actor-critic algorithms and applications\n\ntuomas haarnoja\u2217\u2020\u2021\n\naurick zhou\u2217\u2020\n\nkristian hartikainen\u2217\u2020\n\ngeorge tucker\u2021\n\nsehoon ha\u2021\n\njie tan\u2021\n\nvikash kumar\u2021\n\nhenry zhu\u2020\n\nabhishek gupta\u2020\n\npieter abbeel\u2020\n\nsergey levine\u2020\u2021\n\nabstract\n\nmodel-free deep reinforcement learning (rl) algorithms have been successfully\napplied to a range of challenging sequential decision making and control tasks.\nhowever, these methods", "a r t i c l e s\n\nthe medial entorhinal cortex is necessary for temporal \norganization of hippocampal neuronal activity\nmagdalene i schlesiger1,2,6, christopher c cannova1,6, brittney l boublil1, jena b hales3, emily a mankin1, \nmark p brandon1, jill k leutgeb1, christian leibold4 & stefan leutgeb1,5\n\nthe superficial layers of the medial entorhinal cortex (mec) are a major input to the hippocampus. the high proportion \nof spatially modulated cells, including grid cells and border cells, in these ", "article\n\nrecurrent network models of sequence generation\nand memory\n\nhighlights\nd sequences emerge in random networks by modifying a small\n\nfraction of their connections\n\nauthors\n\nkanaka rajan, christopher d. harvey,\ndavid w. tank\n\nd analysis reveals new circuit mechanism for input-dependent\n\nsequence propagation\n\nd sequential activation may provide a dynamic mechanism for\n\nshort-term memory\n\ncorrespondence\nkrajan@princeton.edu (k.r.),\nharvey@hms.harvard.edu (c.d.h.),\ndwtank@princeton.edu (d.w.t", "6\n1\n0\n2\n\n \n\nb\ne\nf\n4\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n9\n7\n1\n5\n0\n\n.\n\n2\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ntowards a biologically plausible backprop\n\nbenjamin scellier and yoshua bengio\u2217\n\nuniversit\u00e9 de montr\u00e9al, montreal institute for learning algorithms\n\nseptember 1, 2021\n\nabstract\n\nthis work follows bengio and fischer (2015) in which theoretical foundations were laid to show how iterative in-\nference can backpropagate error signals. neurons move their activations towards con\ufb01gurations corresponding to lower\nene", "a high-performance speech neuroprosthesis\n\nhttps://doi.org/10.1038/s41586-023-06377-x\nreceived: 21 january 2023\naccepted: 27 june 2023\npublished online: 23 august 2023\nopen access\n\n check for updates\n\nfrancis r. willett1,15\u2009\u2709, erin m. kunz2,3,15, chaofei fan4,15, donald t. avansino1, guy h. wilson5, \neun young choi6, foram kamdar6, matthew f. glasser7,8, leigh r. hochberg9,10,11, \nshaul druckmann12, krishna v. shenoy1,2,3,12,13,14 & jaimie m. henderson3,6\n\nspeech brain\u2013computer interfaces (bcis)", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/315570715\n\nwhy do similarity matching objectives lead to\nhebbian/anti-hebbian networks?\n\narticle\u00a0\u00a0in\u00a0\u00a0neural computation \u00b7 january 2018\n\ndoi: 10.1162/neco_a_01018\n\ncitations\n62\n\n3 authors:\n\nreads\n1,009\n\ncengiz pehlevan\nharvard university\n\nanirvan m sengupta\nrutgers, the state university of new jersey\n\n103 publications\u00a0\u00a0\u00a01,238 citations\u00a0\u00a0\u00a0\n\n147 publications\u00a0\u00a0\u00a05,427 citations\u00a0\u00a0\u00a0\n\nsee profi", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nsingle  neuron  dynamics  and  computation\nnicolas  brunel1,*,  vincent  hakim2 and  magnus  je  richardson3\n\nat  the  single  neuron  level,  information  processing  involves  the\ntransformation  of  input  spike  trains  into  an  appropriate  output\nspike  train.  building  upon  the  classical  view  of  a  neuron  as  a\nthreshold  device,  models  have  been  developed  in  recent  years\nthat  take  into  account  the  diverse  e", "international journal of computer vision 70(1), 77\u201390, 2006\nc(cid:2) 2006 springer science + business media, llc. manufactured in the netherlands.\ndoi: 10.1007/s11263-005-4939-z\n\nunsupervised learning of image manifolds by semide\ufb01nite programming\n\ndepartment of computer and information science, university of pennsylvania, philadelphia, pa 19104-6389\n\nkilian q. weinberger and lawrence k. saul\n\nkilianw@cis.upenn.edu\nlsaul@cis.upenn.edu\n\nreceived april 5, 2005; revised august 12, 2005; accepted sep", "neuron\n\narticle\n\noverriding phasic dopamine signals\nredirects action selection\nduring risk/reward decision making\n\ncolin m. stopper,1 maric t.l. tse,1 david r. montes,1 candice r. wiedman,1 and stan b. floresco1,*\n1department of psychology and brain research centre, university of british columbia, vancouver, bc v6t 1z4, canada\n*correspondence: \ufb02oresco@psych.ubc.ca\nhttp://dx.doi.org/10.1016/j.neuron.2014.08.033\n\nsummary\n\nphasic increases and decreases in dopamine (da)\ntransmission encode reward p", "research\n\nneuroscience\n\ndendritic action potentials and computation in\nhuman layer 2/3 cortical neurons\n\nalbert gidon1, timothy adam zolnik1, pawel fidzinski2,3, felix bolduan4, athanasia papoutsi5,\npanayiota poirazi5, martin holtkamp2, imre vida3,4, matthew evan larkum1,3*\n\nthe active electrical properties of dendrites shape neuronal input and output and are fundamental to\nbrain function. however, our knowledge of active dendrites has been almost entirely acquired from\nstudies of rodents. in th", "\f", "highly accurate protein structure prediction \nwith alphafold\n\nhttps://doi.org/10.1038/s41586-021-03819-2\nreceived: 11 may 2021\naccepted: 12 july 2021\npublished online: 15 july 2021\nopen access\n\n check for updates\n\njohn jumper1,4\u2009\u2709, richard evans1,4, alexander pritzel1,4, tim green1,4, michael figurnov1,4, \nolaf ronneberger1,4, kathryn tunyasuvunakool1,4, russ bates1,4, augustin \u017e\u00eddek1,4, \nanna potapenko1,4, alex bridgland1,4, clemens meyer1,4, simon a. a. kohl1,4, \nandrew j. ballard1,4, andrew c", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nbackpropagation  through  time  and  the  brain\ntimothy  p  lillicrap1,2,3 and  adam  santoro1,3\n\nit  has  long  been  speculated  that  the  backpropagation-of-error\nalgorithm  (backprop)  may  be  a  model  of  how  the  brain  learns.\nbackpropagation-through-time  (bptt)  is  the  canonical\ntemporal-analogue  to  backprop  used  to  assign  credit  in\nrecurrent  neural  networks  in  machine  learning,  but  there\u2019s  even\nle", "letter\npurkinje-cell plasticity and cerebellar motor\nlearning are graded by complex-spike duration\n\ndoi:10.1038/nature13282\n\nyan yang1 & stephen g. lisberger1,2\n\nbehavioural learning is mediated by cellular plasticity, such as changes\nin the strength of synapses at specific sites in neural circuits. the the-\nory of cerebellar motor learning1\u20133 relies on movement errors signalled\nby climbing-fibre inputs to cause long-term depression of synapses\nfrom parallel fibres to purkinje cells4,5. however,", "physiol rev 95: 853\u2013951, 2015\npublished june 24, 2015; doi:10.1152/physrev.00023.2014\n\nneuronal reward and decision signals:\nfrom theories to data\nwolfram schultz\n\ndepartment of physiology, development and neuroscience, university of cambridge, cambridge, united\nkingdom\n\nl schultz w. neuronal reward and decision signals: from theories to data. physiol rev 95:\n\n853\u2013951, 2015. published june 24, 2015; doi:10.1152/physrev.00023.2014.\u2014re-\nwards are crucial objects that induce learning, approach beha", "published as a conference paper at iclr 2019\n\nffjord: free-form continuous dynamics for\nscalable reversible generative models\n\nwill grathwohl\u2217\u2020\u2021 , ricky t. q. chen\u2217\u2020, jesse bettencourt\u2020, ilya sutskever\u2021 , david duvenaud\u2020\n\nabstract\n\nreversible generative models map points from a simple distribution to a complex\ndistribution through an easily invertible neural network. likelihood-based training\nof these models requires restricting their architectures to allow cheap computation\nof jacobian determin", "l\no\nn\ng\n-\nt\ne\nr\nm\n \nd\ne\np\nr\ne\ns\na\no\nn\n \no\nf\n \ne\nx\nd\nt\na\nt\no\nr\ny\n \ns\ny\nn\na\np\nt\ni\nc\n \nt\nr\na\nn\ns\nm\ni\ns\ns\ni\no\nn\n \na\nn\nd\n \ni\nt\ns\n \nr\ne\nl\na\nt\ni\no\nn\ns\nh\ni\np\n \nt\no\n \nl\no\nn\ng\n-\nt\ne\nr\nm\n \np\no\nt\ne\nn\n#\na\nt\ni\no\nn\n \na\nl\na\ni\nn\n \na\nr\nt\no\nl\na\n \na\nn\nd\n \nw\no\nl\nf\n \ns\ni\nn\ng\ne\nr\n \na\nl\na\ni\nn\n \na\nr\nt\no\nl\na\n \na\nn\nd\n \nw\no\nf\nf\n \ns\ni\nn\ng\ne\nr\n \na\nr\ne\n \na\nt\n \nt\nh\ne\n \nm\na\nx\n-\np\nl\na\nn\nc\nk\n \ni\nn\ns\nt\ni\nt\nu\nt\n \nf\ni\nj\nr\n \nh\ni\nm\nf\no\nr\ns\nc\nh\nu\nn\ng\n,\n \nd\ne\nu\nt\ns\nc\nh\no\nr\nd\ne\nn\n \ns\nt\nr\na\ns\ns\ne\n \n4\n6\n,\n \nd\n-\n6\n0\n5\n2\n8\n \nf\nr\na\nn\nk\nf\nu\nr\n", "why is real-world visual object recognition\nhard?\nnicolas pinto1,2[\n\n, david d. cox1,2,3[\n\n, james j. dicarlo1,2*\n\n1 mcgovern institute for brain research, massachusetts institute of technology, cambridge, massachusetts, united states of america, 2 department of brain and cognitive\nsciences, massachusetts institute of technology, cambridge, massachusetts, united states of america, 3 the rowland institute at harvard, cambridge, massachusetts, united\nstates of america\n\nprogress in understanding th", "1254\n\nieee transactions on pattern analysis and machine intelligence,  vol.  20,  no.  11,  november  1998\n\nshort papers\n\na model of saliency-based visual attention\n\nfor rapid scene analysis\n\nlaurent itti, christof koch, and ernst niebur\n\nabstract\u2014a visual attention system, inspired by the behavior and the\nneuronal architecture of the early primate visual system, is presented.\nmultiscale image features are combined into a single topographical\nsaliency map. a dynamical neural network then selects", "the journal of neuroscience, august 15, 2018 \u2022 38(33):7255\u20137269 \u2022 7255\n\nbehavioral/cognitive\n\nlarge-scale, high-resolution comparison of the core visual\nobject recognition behavior of humans, monkeys, and\nstate-of-the-art deep artificial neural networks\n\nx rishi rajalingham,* x elias b. issa,* x pouya bashivan, x kohitij kar, kailyn schmidt, and x james j. dicarlo\nmcgovern institute for brain research and department of brain and cognitive sciences, massachusetts institute of technology, cambridg", "4\n1\n0\n2\n\n \nr\np\na\n4\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n5\nv\n6\n2\n0\n6\n\n.\n\n2\n1\n3\n1\n:\nv\ni\nx\nr\na\n\nhow to construct deep recurrent neural networks\n\nrazvan pascanu1, caglar gulcehre1, kyunghyun cho2, and yoshua bengio1\n\n1d\u00b4epartement d\u2019informatique et de recherche op\u00b4erationelle, universit\u00b4e de montr\u00b4eal,\n\n{pascanur, gulcehrc}@iro.umontreal.ca, yoshua.bengio@umontreal.ca\n\n2department of information and computer science, aalto university school of science,\n\nkyunghyun.cho@aalto.fi\n\nabstract\n\nin this paper, we ex", "ieee transactions on  neural  networks. vol.  5.  no.  3.  may  l y y 4  \n\nsos \n\nanalysis  of  the  back-propagation \n\nalgorithm  with  momentum \n\nv.  v.  phansalkar  and  p.  s. sastrq \n\npattems  are presented  frequently so  that  ( 5 )  is essentially  equivalent \nto  the  algorithm \n\n/ l j , ( / t  + 1) = u ( , ( / ) )   - ~ l ( h e / i i l / < , ) ( l / ( t i ) )  \n\n+ i l ( l l # / ( t t )  - t o / ( t t  - 1 ) ) .  \n\n(6) \n\nabsfracr-  in  this  letter,  the  hack-propagation  algorithm  wit", "published as a conference paper at iclr 2018\n\ntowards deep learning models resistant to\nadversarial attacks\n\naleksander m \u02dbadry, aleksandar makelov, ludwig schmidt, dimitris tsipras, adrian vladu\u2217\ndepartment of electrical engineering and computer science\nmassachusetts institute of technology\ncambridge, ma 02139, usa\n{madry,amakelov,ludwigs,tsipras,avladu}@mit.edu\n\nabstract\n\nrecent work has demonstrated that neural networks are vulnerable to adversarial\nexamples, i.e., inputs that are almost indi", "representational drift in the mouse visual cortex\n\ngraphical abstract\n\nauthors\n\narticle\n\ndaniel deitch, alon rubin, yaniv ziv\n\ncorrespondence\nyaniv.ziv@weizmann.ac.il\n\nin brief\ndeitch et al. \ufb01nd continuous changes in\nneuronal responses to the same stimuli\n(representational drift) over minutes to\ndays across multiple visual areas, cortical\nlayers, and cell types. despite these\nchanges in the coding of individual cells,\nthe structure of the relationships between\npopulation activity patterns remain", "perspective\n\nneuron\n\nwhat is optimal about motor control?\n\nkarl friston1,*\n1the wellcome trust centre for neuroimaging, institute of neurology, 12 queen square, london wc1n 3bg, uk\n*correspondence: k.friston@ucl.ac.uk\ndoi 10.1016/j.neuron.2011.10.018\n\nthis article poses a controversial question: is optimal control theory useful for understanding motor behavior\nor is it a misdirection? this question is becoming acute as people start to con\ufb02ate internal models in motor\ncontrol and perception (poep", "8\n1\n0\n2\n\n \n\ng\nu\na\n6\n1\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n1\n7\n2\n1\n1\n\n.\n\n4\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ngaussian process behaviour in wide deep neural networks\n\nalexander g. de g. matthews\ndepartment of engineering\ntrumpington street\nuniversity of cambridge, uk\n\nmark rowland\ndepartment of pure mathematics and mathematical statistics\nwilberforce road\nuniversity of cambridge, uk\n\njiri hron\ndepartment of engineering\ntrumpington street\nuniversity of cambridge, uk\n\nrichard e. turner\ndepartment of engineering\ntr", "open\n\nreceived: 19 august 2015\naccepted: 11 august 2016\npublished: 07 september 2016\n\ndeep networks can resemble \nhuman feed-forward vision in \ninvariant object recognition\n\nsaeed reza kheradpisheh1,2, masoud ghodrati3,4, mohammad ganjtabesh1 & \ntimoth\u00e9e masquelier2,5,6,7\n\ndeep convolutional neural networks (dcnns) have attracted much attention recently, and have shown \nto be able to recognize thousands of object categories in natural image databases. their architecture \nis somewhat similar to t", "neuroimage 99 (2014) 509\u2013524\n\ncontents lists available at sciencedirect\n\nneuroimage\n\nj ou r n a l h o m e p a ge : w ww . e l s e v i e r . c o m / l oc a te / y ni mg\n\ncorrespondences between retinotopic areas and myelin maps in human\nvisual cortex\nrouhollah o. abdollahi a,1, hauke kolster a,1, matthew f. glasser b,1, emma c. robinson c, timothy s. coalson b,\ndonna dierker b, mark jenkinson c, david c. van essen b,2, guy a. orban a,d,\u204e,2\na laboratorium voor neuro-en psychofysiologie, ku leuven,", "22. j. k. polansky et al., eur. j. immunol. 38, 1654\u20131663\n\n(2008).\n\n23. d. e. chatterton, d. n. nguyen, s. b. bering, p. t. sangild, int.\n\nj. biochem. cell biol. 45, 1730\u20131747 (2013).\n\n24. s. z. josefowicz et al., nature 482, 395\u2013399 (2012).\n25. g. longo, i. berti, a. w. burks, b. krauss, e. barbi, lancet 382,\n\n1656\u20131664 (2013).\n\n26. j. cahenzli, y. k\u00f6ller, m. wyss, m. b. geuking, k. d. mccoy,\n\ncell host microbe 14, 559\u2013570 (2013).\n\n27. k. d. mccoy et al., immunity 24, 329\u2013339 (2006).\n\nacknowled", "article\n\nhigher-order thalamocortical inputs gate synaptic\nlong-term potentiation via disinhibition\n\ngraphical abstract\n\nauthors\n\nleena e. williams, anthony holtmaat\n\ncorrespondence\nanthony.holtmaat@unige.ch\n\nin brief\nusing ex vivo patch-clamp recordings,\noptogenetics, and chemogenetics,\nwilliams and holtmaat dissect the circuits\nunderlying sensory-driven ltp in the\ncortex. this reveals a circuit motif in\nwhich higher-order thalamocortical input\ngates plasticity of intracortical synapses\nvia vip", "measuring statistical dependence with\n\nhilbert-schmidt norms\n\narthur gretton1, olivier bousquet2, alex smola3, and bernhard sch\u00a8olkopf1\n\n1 mpi for biological cybernetics, spemannstr. 38, 72076 t\u00a8ubingen, germany\n\n{arthur, bernhard.schoelkopf}@tuebingen.mpg.de\n2 pertinence, 32, rue des je\u02c6uneurs, 75002 paris, france\n\n3 national ict australia, north road, canberra 0200 act, australia\n\nolivier.bousquet@pertinence.com\n\nalex.smola@nicta.com.au\n\nabstract. we propose an independence criterion based on ", "neuron\n\narticle\n\ngenerating coherent patterns of activity\nfrom chaotic neural networks\n\ndavid sussillo1,* and l.f. abbott1,*\n1department of neuroscience, department of physiology and cellular biophysics, columbia university college of physicians and surgeons,\nnew york, ny 10032-2695, usa\n*correspondence: sussillo@neurotheory.columbia.edu (d.s.), lfa2103@columbia.edu (l.f.a.)\ndoi 10.1016/j.neuron.2009.07.018\n\nsummary\n\nneural circuits display complex activity patterns both\nspontaneously and when r", "regularized auto-encoders estimate local statistics\n\nguillaume alain, yoshua bengio and salah rifai\n\ndepartment of computer science and operations research\n\nuniversity of montreal\n\nmontreal, h3c 3j7, quebec, canada\n\nabstract\n\nwhat do auto-encoders learn about the underlying data generating distribution?\nrecent work suggests that some auto-encoder variants do a good job of capturing\nthe local manifold structure of the unknown data generating density. this paper\nclari\ufb01es some of these previous int", "s\nc\ni\nt\ns\ni\nt\na\nt\ns\n\nreconciling modern machine-learning practice and\nthe classical bias\u2013variance trade-off\n\nmikhail belkina,b,1, daniel hsuc, siyuan maa, and soumik mandala\n\nadepartment of computer science and engineering, the ohio state university, columbus, oh 43210; bdepartment of statistics, the ohio state university,\ncolumbus, oh 43210; and ccomputer science department and data science institute, columbia university, new york, ny 10027\n\nedited by peter j. bickel, university of california, ", "brain and cognition 112 (2017) 92\u201397\n\ncontents lists available at sciencedirect\n\nbrain and cognition\n\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / b & c\n\nspecial invited review\na review of predictive coding algorithms\n\nm.w. spratling\n\n\u21d1\n\nking\u2019s college london, department of informatics, london, uk\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 21 may 2015\nrevised 9 november 2015\naccepted 13 november 2015\navailable online 19 january 2016\n\nke", "biological constraints on neural \nnetwork models of cognitive function\n\nfriedemann\u00a0pulverm\u00fcller \nmalte\u00a0r.\u00a0henningsen-schomers \n\n , rosario\u00a0tomasello \n\n , \n\n  and thomas\u00a0wennekers \n\n \n\nabstract | neural network models are potential tools for improving our under\u00ad\nstanding of complex brain functions. to address this goal, these models need to be \nneurobiologically realistic. however, although neural networks have advanced \ndramatically in recent years and even achieve human\u00adlike performance on com\u00ad", "8\n1\n0\n2\n\n \n\np\ne\ns\n1\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n0\n7\n3\n0\n\n.\n\n9\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nsparse attentive backtracking: temporal credit\n\nassignment through reminding\n\nnan rosemary ke1, anirudh goyal1, olexa bilaniuk1, jonathan binas1,\n\nmichael c. mozer5, chris pal1,6, yoshua bengio1\u2020\n\n1 mila, universit\u00e9 de montr\u00e9al\n\n5 university of colorado, boulder\n6 mila, polytechnique montr\u00e9al\n\n\u2020cifar senior fellow.\n\nabstract\n\nlearning long-term dependencies in extended temporal sequences requires credit\nassi", "deep cortical layers are activated\ndirectly by thalamus\n\nchristine m. constantinople and randy m. bruno*\n\nthe thalamocortical (tc) projection to layer 4 (l4) is thought to be the main route by which sensory\norgans communicate with cortex. sensory information is believed to then propagate through\nthe cortical column along the l4\u2192l2/3\u2192l5/6 pathway. here, we show that sensory-evoked responses\nof l5/6 neurons in rats derive instead from direct tc synapses. many l5/6 neurons exhibited\nsensory-evoked ", "can neocortical feedback alter the \nsign of plasticity?\n\nblake\u00a0a.\u00a0richards \n\n  and timothy\u00a0p.\u00a0lillicrap\n\nit can turn synaptic plasticity in a neuron on \nor off but cannot alter the sign of synaptic \nplasticity (for example, whether synapses \npotentiate or depress). instead, the term rpe \ndetermi nes the sign of plasticity. however, \nin our opinion it may be important for fbj \nto determine whether neurons potentiate or \ndepress their\u00a0synapses.\n\nroelfsema and holtmaat state that the \nweight change", "j neurophysiol 90: 415\u2013 430, 2003.\nfirst published february 26, 2003; 10.1152/jn.01095.2002.\n\nwhat determines the frequency of fast network oscillations with irregular\nneural discharges? i. synaptic dynamics and excitation-inhibition balance\n\nnicolas brunel1 and xiao-jing wang2\n1centre national de la recherche scienti\ufb01que-neurophysique et physiologie du syste`me moteur-universite\u00b4 paris rene\u00b4 descartes, 75270\nparis cedex 06, france; and 2volen center, brandeis university, waltham, massachusetts ", "a r t i c l e s\n\ntwo types of asynchronous activity in networks of \nexcitatory and inhibitory spiking neurons\nsrdjan ostojic\nasynchronous activity in balanced networks of excitatory and inhibitory neurons is believed to constitute the primary medium \nfor the propagation and transformation of information in the neocortex. here we show that an unstructured, sparsely connected \nnetwork of model spiking neurons can display two fundamentally different types of asynchronous activity that imply vastly ", "prl 97, 048104 (2006)\n\np h y s i c a l r e v i e w l e t t e r s\n\nweek ending\n28 july 2006\n\ngradient learning in spiking neural networks by dynamic perturbation of conductances\n\n1kavli institute for theoretical physics, university of california, santa barbara, california 93106, usa\n\n2howard hughes medical institute and department of brain and cognitive sciences, massachusetts institute of technology,\n\nila r. fiete1 and h. sebastian seung2\n\ncambridge, massachusetts 02139, usa\n\n(received 19 januar", "composing graphical models with neural networks\nfor structured representations and fast inference\n\nmatthew james johnson\n\nharvard university\n\nmattjj@seas.harvard.edu\n\ndavid duvenaud\nharvard university\n\ndduvenaud@seas.harvard.edu\n\nalexander b. wiltschko\nharvard university, twitter\n\nawiltsch@fas.harvard.edu\n\nsandeep r. datta\n\nharvard medical school\n\nsrdatta@hms.harvard.edu\n\nryan p. adams\n\nharvard university, twitter\nrpa@seas.harvard.edu\n\nabstract\n\nwe propose a general modeling and inference framew", "neuron, vol. 40, 1063\u20131073, december 18, 2003, copyright \uf8e92003 by cell press\n\nlearning in spiking neural\nnetworks by reinforcement of\nstochastic synaptic transmission\n\nviewpoint\n\nh. sebastian seung\nhoward hughes medical institute and\nbrain and cognitive sciences department\nmassachusetts institute of technology\ncambridge, massachusetts 02139\n\nsummary\n\nit is well-known that chemical synaptic transmission\nis an unreliable process, but the function of such unre-\nliability remains unclear. here i con", "research\n\nresearch article summary \u25e5\n\nneuroscience\n\nspontaneous behaviors drive\nmultidimensional, brainwide activity\n\ncarsen stringer*\u2020, marius pachitariu*\u2020, nicholas steinmetz, charu bai reddy,\nmatteo carandini\u2021, kenneth d. harris\u2020\u2021\n\nintroduction: in the absence of sensory\ninputs, the brain produces structured patterns\nof activity, which can be as large as or larger\nthan sensory-driven activity. ongoing activity\nexists even in primary sensory cortices and\nhas been hypothesized to reflect recapi", "proc. natl. acad. sci. usa\nvol. 93, pp. 13339\u201313344, november 1996\nneurobiology\n\nhow the brain keeps the eyes still\nh. s. seung\nbell laboratories, lucent technologies, murray hill, nj 07974\n\ncommunicated by john j. hopfield, california institute of technology, pasadena, ca, june 10, 1996 (received for review january 7, 1996)\n\nabstract\nthe brain can hold the eyes still because it\nstores a memory of eye position. the brain\u2019s memory of\nhorizontal eye position appears to be represented by persistent", " \n \n \n\ndeep learning incorporating  \nbiologically-inspired neural dynamics \n\nstanis\u0142aw wo\u017aniak1,*, angeliki pantazi1, thomas bohnstingl1,2, and evangelos eleftheriou1 \n1 ibm research \u2013 zurich, r\u00fcschlikon, switzerland \n2 institute of theoretical computer science, graz university of technology, graz, austria\n\n  \n\n \n\nneural networks have become the key technology of artificial intelligence and have contributed to breakthroughs in several \nmachine  learning  tasks,  primarily  owing  to  advances  i", "neuron, vol. 32, 1149\u20131164, december 20, 2001, copyright \uf8e92001 by cell press\n\nrate, timing, and cooperativity jointly determine\ncortical synaptic plasticity\n\nper jesper sjo\u00a8 stro\u00a8 m, gina g. turrigiano,\nand sacha b. nelson1\nbrandeis university\ndepartment of biology\nvolen center for complex systems\nmailstop 008 415 south street\nwaltham, massachusetts 02454\n\nsummary\n\ncortical long-term plasticity depends on firing rate,\nspike timing, and cooperativity among inputs, but how\nthese factors interact d", "low-pass filtering sgd for recovering flat optima in the deep\n\nlearning optimization landscape\n\ndevansh bisla\ndb3484@nyu.edu\n\njing wang\n\njw5665@nyu.edu\n\nanna choromanska\n\nac5455@nyu.edu\n\n2\n2\n0\n2\n\n \n\nb\ne\nf\n4\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n2\n0\n8\n0\n\n.\n\n1\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nin this paper, we study the sharpness of a deep\nlearning (dl) loss landscape around local min-\nima in order to reveal systematic mechanisms\nunderlying the generalization abilities of dl\nmodels. our analysis is perfor", "deep generative stochastic networks trainable by backprop\n\nyoshua bengio\u2217\n\u00b4eric thibodeau-laufer\nguillaume alain\nd\u00b4epartement d\u2019informatique et recherche op\u00b4erationnelle, universit\u00b4e de montr\u00b4eal,\u2217& canadian inst. for advanced research\njason yosinski\ndepartment of computer science, cornell university\n\nfind.us@on.the.web\n\nabstract\n\nwe introduce a novel training principle for prob-\nabilistic models that is an alternative to max-\nimum likelihood.\nthe proposed generative\nstochastic networks (gsn) fr", "the journal of neuroscience, july 21, 2010 \u2022 30(29):9659 \u20139669 \u2022 9659\n\nbehavioral/systems/cognitive\n\ndecoding complete reach and grasp actions from local\nprimary motor cortex populations\n\ncarlos e. vargas-irwin,1 gregory shakhnarovich,2,3 payman yadollahpour,2,3 john m. k. mislow,1,4 michael j. black,2\nand john p. donoghue1,5\ndepartments of 1neuroscience and 2computer science, brown university, providence, rhode island 02912, 3toyota technological institute, chicago,\nillinois 60637, 4department ", "flexible brain\u2013computer interfaces\n\nhttps://doi.org/10.1038/s41928-022-00913-9\n\nreceived: 6 october 2022\n\naccepted: 7 december 2022\n\npublished online: 2 february 2023\n\n check for updates\n\nxin tang\u2009\n\n \u20091,2, hao shen\u2009\n\n \u20091,2, siyuan zhao\u2009\n\n \u20091, na li1 & jia liu\u2009\n\n \u20091 \n\nbrain\u2013computer interfaces\u2014which allow direct communication between the \nbrain and external computers\u2014have potential applications in neuroscience, \nmedicine and virtual reality. current approaches are, however, based on \nconventional", "sparse bursts optimize information transmission\nin a multiplexed neural code\n\nrichard nauda,b,1 and henning sprekelerc,d\n\nauniversity of ottawa brain and mind research institute, department of cellular and molecular medicine, university of ottawa, ottawa, on k1h 8m5,\ncanada; bdepartment of physics, university of ottawa, ottawa, on k1n 6n5, canada; cbernstein center for computational neuroscience berlin, 10115\nberlin, germany; and dmodelling of cognitive processes, institute of software engineeri", "ps68ch05-gershman\n\nari\n\n4 november 2016\n\n10:31\n\nreinforcement learning and\nepisodic memory in humans\nand animals: an integrative\nframework\nsamuel j. gershman1 and nathaniel d. daw2\n1department of psychology and center for brain science, harvard university, cambridge,\nmassachusetts 02138; email: gershman@fas.harvard.edu\n2princeton neuroscience institute and department of psychology, princeton university,\nprinceton, new jersey 08544\n\nannu. rev. psychol. 2017. 68:101\u201328\n\nfirst published online as a", "letters\n\nvol 435|23 june 2005|doi:10.1038/nature03687\n\ninvariant visual representation by single neurons in\nthe human brain\nr. quian quiroga1,2\u2020, l. reddy1, g. kreiman3, c. koch1 & i. fried2,4\n\nit takes a fraction of a second to recognize a person or an object\neven when seen under strikingly different conditions. how such a\nrobust, high-level representation is achieved by neurons in the\nhuman brain is still unclear1\u20136. in monkeys, neurons in the upper\nstages of the ventral visual pathway respond", "research article\n\nadaptive learning and decision-making\nunder uncertainty by metaplastic\nsynapses guided by a surprise detection\nsystem\nkiyohito iigaya1,2,3*\n\n1gatsby computational neuroscience unit, university college london, london,\nunited kingdom; 2center for theoretical neuroscience, college of physicians and\nsurgeons, columbia university , new york, united states; 3department of physics,\ncolumbia university, new york, united states\n\nabstract recent experiments have shown that animals and hu", "towards scaling difference target propagation by learning backprop targets\n\nmaxence ernoult 1 fabrice normandin * 2 abhinav moudgil * 2 3 sean spinney 2 4 eugene belilovsky 2 3\n\nirina rish 2 4 blake richards 2 5 6 yoshua bengio 2 4\n\n2\n2\n0\n2\n\n \n\nn\na\nj\n \n\n1\n3\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n5\n1\n4\n3\n1\n\n.\n\n1\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nthe development of biologically-plausible learn-\ning algorithms is important for understanding\nlearning in the brain, but most of them fail to\nscale-up to real-world tas", "letter\n\ncommunicated by naftali tishby\n\nasymptotic theory of information-theoretic experimental\ndesign\n\nliam paninski\nliam@gatsby.ucl.ac.uk\ngatsby computational neuroscience unit,\nuniversity college london, london, wc1n 3ar, u.k.\n\nwe discuss an idea for collecting data in a relatively ef\ufb01cient manner. our\npoint of view is bayesian and information-theoretic: on any given trial,\nwe want to adaptively choose the input in such a way that the mutual in-\nformation between the (unknown) state of the sy", "the interplay between randomness and structure\n\nduring learning in rnns\n\nfriedrich schuessler\n\ntechnion\n\nschuessler@campus.technion.ac.il\n\nfrancesca mastrogiuseppe\n\ngatsby unit, ucl\n\nf.mastrogiuseppe@ucl.ac.uk\n\nalexis dubreuil\n\nens paris\n\nsrdjan ostojic\n\nens paris\n\nalexis.dubreuil@gmail.com\n\nsrdjan.ostojic@ens.fr\n\nomri barak\n\ntechnion\n\nomri.barak@gmail.com\n\nabstract\n\nrecurrent neural networks (rnns) trained on low-dimensional tasks have been\nwidely used to model functional biological networks. h", "m\no\nc\n\n.\n\ne\nr\nu\nt\na\nn\n\n.\ni\n\nc\ns\no\nr\nu\ne\nn\n\n/\n/\n:\np\nt\nt\nh\n \n\u2022\n \n.\n\nc\nn\n\ni\n \n\n \n\na\nc\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n0\n0\n0\n2\n\u00a9\n\n \n\n \n\nreview\n\n\u00a9 2000 nature america inc. \u2022 http://neurosci.nature.com\n\nsynaptic plasticity: taming the beast\n\nl. f. abbott and sacha b. nelson\n\ndepartment of biology and volen center, brandeis university, waltham, massachusetts 02454-9110, usa\n\ncorrespondence should be addressed to l.f.a. (abbott@brandeis.edu)\n\nsynaptic plasticity provides the basis for most models of learning, mem", "ne38ch10-froemke\n\nari\n\n21 may 2015\n\n10:44\n\nplasticity of cortical\nexcitatory-inhibitory balance\n\nrobert c. froemke1,2\n1skirball institute for biomolecular medicine, neuroscience institute, and departments\nof otolaryngology, neuroscience, and physiology, new york university school of medicine,\nnew york, ny 10016; email: robert.froemke@med.nyu.edu\n2center for neural science, new york university, new york, ny 10003\n\nannu. rev. neurosci. 2015. 38:195\u2013219\n\nfirst published online as a review in advanc", "independent component analysis:\n\nalgorithms and applications\n\naapo hyv\u00e4rinen and erkki oja\n\nneural networks research centre\nhelsinki university of technology\n\np.o. box 5400, fin-02015 hut, finland\nneural networks, 13(4-5):411-430, 2000\n\nabstract\n\na fundamental problem in neural network research, as well as in many other disciplines, is \ufb01nding a suitable\nrepresentation of multivariate data, i.e. random vectors. for reasons of computational and conceptual simplicity,\nthe representation is often so", "ne41ch12_medina\n\nari\n\n24 may 2018\n\n7:30\n\nannu. rev. neurosci. 2018. 41:233\u201353\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nhttps://doi.org/10.1146/annurev-neuro-080317-\n061948\ncopyright c(cid:2) 2018 by annual reviews.\nall rights reserved\n\nannual review of neuroscience\ncomputational principles of\nsupervised learning in the\ncerebellum\njennifer l. raymond1 and javier f. medina2\n1department of neurobiology, stanford university school of medicine, stanford,\ncalifornia 943", "6\n1\n0\n2\n\n \nl\nu\nj\n \n\n1\n2\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n0\n5\n4\n6\n0\n\n.\n\n7\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nlayer normalization\n\njimmy lei ba\n\nuniversity of toronto\n\njamie ryan kiros\nuniversity of toronto\n\njimmy@psi.toronto.edu\n\nrkiros@cs.toronto.edu\n\ngeoffrey e. hinton\nuniversity of toronto\n\nand google inc.\n\nhinton@cs.toronto.edu\n\nabstract\n\ntraining state-of-the-art, deep neural networks is computationally expensive. one\nway to reduce the training time is to normalize the activities of the neurons. a\nrecently ", "1\n\n0\n1\n0\n2\n\n \nr\na\n\nm\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n8\n5\n3\n0\n\n.\n\n3\n0\n0\n1\n:\nv\ni\nx\nr\na\n\ndeep big simple neural nets excel on hand-\n\nwritten digit recognition\n\ndan claudiu cires\u00b8an1, 2,\nueli meier1, 2,\nluca maria gambardella1, 2,\nj\u00a8urgen schmidhuber1, 2\n1idsia, galleria 2, 6928 manno-lugano, switzerland.\n2university of lugano & supsi, switzerland.\n\nkeywords: nn (neural network) , mlp (multilayer perceptron), gpu (graphics\n\nprocessing unit), training set deformations, mnist 1, bp (back-propagation)", "on the universality of invariant networks\n\nhaggai maron 1 ethan fetaya 2 3 nimrod segol 1 yaron lipman 1\n\nabstract\n\nconstraining linear layers in neural networks to\nrespect symmetry transformations from a group\ng is a common design principle for invariant net-\nworks that has found many applications in ma-\nchine learning. in this paper, we consider a fun-\ndamental question that has received little atten-\ntion to date: can these networks approximate any\n(continuous) invariant function? we tackle t", "cortical areas interact through a communication\nsubspace\n\narticle\n\nhighlights\nd visual cortical areas interact through a communication\n\nsubspace (cs)\n\nd the cs de\ufb01nes which activity patterns in a source area relate\n\nto downstream activity\n\nd the largest activity patterns in a source area are not matched\n\nto the cs\n\nd the cs allows for selective and \ufb02exible routing of population\n\nsignals between areas\n\nauthors\n\njoa\u02dc o d. semedo, amin zandvakili,\nchristian k. machens, byron m. yu,\nadam kohn\n\ncorre", "y\nr\na\nt\nn\ne\nm\nm\no\nc\n\ne\ne\ns\n\ne\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\n\nperformance-optimized hierarchical models predict\nneural responses in higher visual cortex\n\ndaniel l. k. yaminsa,1, ha honga,b,1, charles f. cadieua, ethan a. solomona, darren seiberta, and james j. dicarloa,2\n\nadepartment of brain and cognitive sciences and mcgovern institute for brain research, massachusetts institute of technology, cambridge, ma 02139;\nand bharvard-mit division of health sciences and technology, institute for medical engine", "a r t i c l e s\n\ngating and control of primary visual cortex by pulvinar\ngopathy purushothaman1, roan marion2, keji li3 & vivien a casagrande1,3\n\nthe primary visual cortex (v1) receives its driving input from the eyes via the lateral geniculate nucleus (lgn) of the thalamus. \nthe lateral pulvinar nucleus of the thalamus also projects to v1, but this input is not well understood. we manipulated lateral \npulvinar neural activity in prosimian primates and assessed the effect on supra-granular layer", "the journal of neuroscience, january 21, 2015 \u2022 35(3):1319 \u20131334 \u2022 1319\n\ndevelopment/plasticity/repair\n\nsynaptic consolidation: from synapses to behavioral\nmodeling\n\nx lorric ziegler, x friedemann zenke, x david b. kastner, and x wulfram gerstner\nschool of computer and communication sciences and school of life sciences, brain mind institute, ecole polytechnique fe\u00b4de\u00b4rale de lausanne,\n1015 lausanne epfl, switzerland\n\nsynaptic plasticity, a key process for memory formation, manifests itself acros", "7\n1\n0\n2\n\n \nr\na\n\nm\n3\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n3\n1\n9\n9\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\ncapacity and trainability in recurrent\nneural networks\n\njasmine collins\u2217, jascha sohl-dickstein & david sussillo\ngoogle brain\ngoogle inc.\nmountain view, ca 94043, usa\n{jlcollins, jaschasd, sussillo}@google.com\n\nabstract\n\ntwo potential bottlenecks on the expressiveness of recurrent neural networks\n(rnns) are their ability to store information about the task in t", "letters\n\nvol 440|20 april 2006|doi:10.1038/nature04671\n\nsynaptic scaling mediated by glial tnf-a\ndavid stellwagen1 & robert c. malenka1\n\ntwo general forms of synaptic plasticity that operate on different\ntimescales are thought to contribute to the activity-dependent\nre\ufb01nement of neural circuitry during development: (1) long-term\npotentiation (ltp) and long-term depression (ltd), which\ninvolve rapid adjustments in the strengths of individual synapses\nin response to speci\ufb01c patterns of correlated ", "neuron, vol. 44, 691\u2013700, november 18, 2004, copyright \uf8e92004 by cell press\n\nbidirectional parallel fiber plasticity\nin the cerebellum under climbing fiber control\n\nmichiel coesmans, john t. weber,\nchris i. de zeeuw, and christian hansel*\ndepartment of neuroscience\nerasmus university medical center\n3000 dr rotterdam\nthe netherlands\n\nsummary\n\ncerebellar parallel fiber (pf)-purkinje cell (pc) syn-\napses can undergo postsynaptically expressed long-\nterm depression (ltd) or long-term potentiation (lt", "article\n\ndoi:10.1038/nature12160\n\nthe importance of mixed selectivity in\ncomplex cognitive tasks\n\nmattia rigotti1,2,3, omri barak1{, melissa r. warden4,5, xiao-jing wang2,6, nathaniel d. daw2,3, earl k. miller4 & stefano fusi1\n\nsingle-neuron activity in the prefrontal cortex (pfc) is tuned to mixtures of multiple task-related aspects. such mixed\nselectivity is highly heterogeneous, seemingly disordered and therefore difficult to interpret. we analysed the neural\nactivity recorded in monkeys duri", "neuron, vol. 30, 593\u2013607, may, 2001, copyright \u00aa 2001 by cell press\n\nneuronal correlates of motor performance and\nmotor learning in the primary motor cortex\nof monkeys adapting to an external force field\n\nchiang-shan ray li,1,3 camillo padoa-schioppa,1\nand emilio bizzi1,2\n1department of brain and cognitive sciences\nmassachusetts institute of technology\ncambridge, massachusetts 02139\n\nsummary\n\nthe primary motor cortex (m1) is known to control\nmotor performance. recent findings have also impli-\nca", "letter\n\ncommunicated by bruno averbeck\n\nlearning spike-based population codes by reward\nand population feedback\n\njohannes friedrich\nfriedrich@pyl.unibe.ch\nrobert urbanczik\nurbanczik@pyl.unibe.ch\nwalter senn\nsenn@pyl.unibe.ch\ndepartment of physiology, university of bern, ch-3012 bern, switzerland\n\nwe investigate a recently proposed model for decision learning in a pop-\nulation of spiking neurons where synaptic plasticity is modulated by a\npopulation signal in addition to reward feedback. for the ", "cognitive \n\nscience \n\n9,  147-169 \n\n(1985) \n\na  learning algorithm  for \n\nboltzmann machines* \n\ndavid  h.  ackley \n\ngeoffrey \ncomputer  science department \n\ne.  hinton \n\ncarnegie-mellon  university \n\nterrence \n\nj.  sejnowski \n\nbiophysics  department \n\nthe johns  hopkins  university \n\ncomputotionol \n\nof  massively \n\nparallel \n\nnetworks \n\nof  simple \n\nthe \nelements \nconnections \nfraction \nlem \nnetworks \nbut \nsearch \nthere \npreexisting \nstraints \nmethod, \neral \nknowledge \nexamples \nthot \ntivity \n\no", "article\na multi-modal parcellation of human \ncerebral cortex\n\nmatthew f. glasser1, timothy s. coalson1*, emma c. robinson2,3*, carl d. hacker4*, john harwell1, essa yacoub5, \nkamil ugurbil5, jesper andersson2, christian f. beckmann6,7, mark jenkinson2, stephen m. smith2 & david c. van essen1\n\ndoi:10.1038/nature18933\n\nunderstanding the amazingly complex human cerebral cortex requires a map (or parcellation) of its major subdivisions, \nknown as cortical areas. making an accurate areal map has been", "8\n1\n0\n2\n \nc\ne\nd\n1\n2\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n2\nv\n1\n2\n7\n6\n0\n\n.\n\n0\n1\n8\n1\n:\nv\ni\nx\nr\na\n\noptimizing agent behavior over long time scales by\ntransporting value\nchia-chun hung1\u2217\u2020, timothy lillicrap1\u2217\u2020, josh abramson1\u2217, yan wu1, mehdi mirza1, federico\ncarnevale1, arun ahuja1, greg wayne1\u2217\u2020.\n1deepmind, 5 new street square, london ec4a 3tw, uk.\n\u2217these authors contributed equally to this work.\n\u2020to whom correspondence should be addressed.\n\nhumans spend a remarkable fraction of waking life engaged in acts", "a theory for how sensorimotor skills are learned and\nretained in noisy and nonstationary neural circuits\n\nrobert ajemiana,1, alessandro d\u2019ausiliob,c, helene moormand,e, and emilio bizzia,d,1\n\namcgovern institute for brain research and ddepartment of brain and cognitive sciences, massachusetts institute of technology, cambridge, ma 02139;\nbdepartment of psychology, university of rome \u201cla sapienza,\u201d 00185 rome, italy; cinter-university centre for research on cognitive processing in natural\nand art", "neuron\n\narticle\n\nbalanced ampli\ufb01cation: a new mechanism\nof selective ampli\ufb01cation\nof neural activity patterns\n\nbrendan k. murphy1,2 and kenneth d. miller2,*\n1graduate group in biophysics, university of california, san francisco, san francisco, ca 94122, usa\n2center for theoretical neuroscience, sloan program in theoretical neuroscience, department of neuroscience, columbia university\ncollege of physicians and surgeons, new york, ny 10032, usa\n*correspondence: ken@neurotheory.columbia.edu\ndoi 10.", "learning in implicit generative models\n\nshakir mohamed 1 balaji lakshminarayanan 1\n\n7\n1\n0\n2\n\n \n\nb\ne\nf\n7\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n4\nv\n3\n8\n4\n3\n0\n\n.\n\n0\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\ngenerative adversarial networks (gans) provide\nan algorithmic framework for constructing gen-\nerative models with several appealing proper-\nties:\nthey do not require a likelihood function\nto be speci\ufb01ed, only a generating procedure; they\nprovide samples that are sharp and compelling;\nand they allow us to harness", "dynamical isometry is achieved in residual networks in a universal way for any activation\n\nfunction\n\nwojciech tarnowski,1, \u2217 piotr warcho\u0142,1, \u2020 stanis\u0142aw jastrz\u02dbebski,2, \u2021 jacek tabor,2, \u00a7 and maciej a. nowak3, \u00b6\n\n1m. smoluchowski institute of physics, jagiellonian university, pl\u201330\u2013348 krak\u00f3w, poland\n2faculty of mathematics and computer science, jagiellonian university, krak\u00f3w, poland\n\n3m. smoluchowski institute of physics and mark kac complex systems research center, jagiellonian university, p", "article\ndiscrete attractor dynamics underlies \npersistent activity in the frontal cortex\n\nhidehiko k. inagaki1, lorenzo fontolan1, sandro romani1* & karel svoboda1*\n\nhttps://doi.org/10.1038/s41586-019-0919-7\n\nshort-term memories link events separated in time, such as past sensation and future actions. short-term memories are \ncorrelated with slow neural dynamics, including selective persistent activity, which can be maintained over seconds. in a \ndelayed response task that requires short-term me", "this is an open access article published under an acs authorchoice license, which permits\ncopying and redistribution of the article or any adaptations for non-commercial purposes.\n\nreview\n\npubs.acs.org/cr\n\ncoarse-grained protein models and their applications\nsebastian kmiecik,\u2020 dominik gront,\u2020 michal kolinski,\u2021 lukasz wieteska,\u2020,\u00a7 aleksandra elzbieta dawid,\u2020\nand andrzej kolinski*,\u2020\n\u2020\nfaculty of chemistry, university of warsaw, pasteura 1, 02-093 warsaw, poland\n\u2021\nbioinformatics laboratory, mossak", "review\n\nperceptual learning, motor learning and automaticity\n\nperceptual learning rules based on\nreinforcers and attention\npieter r. roelfsema1,2, arjen van ooyen2 and takeo watanabe3\n\n1 department of vision & cognition, netherlands institute for neurosciences, an institute of the royal netherlands academy of arts\nand sciences (knaw), meibergdreef 47, 1105 ba, amsterdam, the netherlands\n2 department of integrative neurophysiology, centre for neurogenomics and cognitive research, vu university, d", "stop explaining black box machine learning \nmodels for high stakes decisions and use \ninterpretable models instead\n\ncynthia rudin\u200a\n\n\u200a\n\nblack box machine learning models are currently being used for high-stakes decision making throughout society, causing prob-\nlems in healthcare, criminal justice and other domains. some people hope that creating methods for explaining these black box \nmodels will alleviate some of the problems, but trying to explain black box models, rather than creating models t", "pergamon\n\nneural networks 14 (2001) 941\u00b1953\n\n2001 special issue\n\nneural\n\nnetworks\n\nwww.elsevier.com/locate/neunet\n\nfrom arti\u00aecial neural networks to spiking neuron populations\n\nand back again\n\nmarc de kamps*, frank van der velde\n\nunit of experimental and theoretical psyhology, leiden university, wassenaarsewag 52, 2333 ak leiden, the netherlands\n\nreceived 10 october 2000; revised 4 april 2001; accepted 4 april 2001\n\nabstract\n\nin this paper, we investigate the relation between arti\u00aecial neural ne", "volume 69, number 26\n\nph ysical revi ew letters\n\n28 december 1992\n\nsuppressing chaos in neural networks by noise\n\nl. molgedey, j. schuchhardt,\n\nand h. g. schuster\n\ninstitut\n\nfur theoretische physik, olshausenstrasse\n\n$0, d 290-0 kiel i, germany\n\n(received 20 july 1992)\n\nwe study discrete parallel dynamics of. a fully connected network of nonlinear\n\ncouplings under\n\nrandom asymmetric\n\nvia long-range\nmean-field equations, which become exact in the thermodynamical\nand the maximal lyapunov exponent ", "preprintrelease.fullcitation:yosinskij,clunej,bengioy,andlipsonh.howtransferablearefeaturesindeepneuralnetworks?inadvancesinneuralinformationprocessingsystems27(nips\u201914),nipsfoundation,2014.howtransferablearefeaturesindeepneuralnetworks?jasonyosinski,1jeffclune,2yoshuabengio,3andhodlipson41dept.computerscience,cornelluniversity2dept.computerscience,universityofwyoming3dept.computerscience&operationsresearch,universityofmontreal4dept.mechanical&aerospaceengineering,cornelluniversityabstractmanyde", "the journal of neuroscience, april 1, 2000, 20(7):2451\u20132458\n\nsynaptic activity modulates the induction of bidirectional synaptic\nchanges in adult mouse hippocampus\n\nanaclet ngezahayo,1 melitta schachner,1,2 and alain artola1\n1department of neurobiology, swiss federal institute of technology zu\u00a8 rich, ho\u00a8 nggerberg, ch-8093 zu\u00a8 rich, switzerland,\nand 2zentrum fu\u00a8 r molekulare neurobiologie, universita\u00a8 t hamburg, d-20246 hamburg, germany\n\nactivity-dependent synaptic plasticity is critical for lea", "greedy layer-wise training of deep networks\n\nyoshua bengio, pascal lamblin, dan popovici, hugo larochelle\n\nfbengioy,lamblinp,popovicd,larochehg@iro.umontreal.ca\n\nuniversit\u00b7e de montr\u00b7eal\n\nmontr\u00b7eal, qu\u00b7ebec\n\nabstract\n\ncomplexity theory of circuits strongly suggests that deep architectures can be much\nmore ef(cid:2)cient (sometimes exponentially) than shallow architectures, in terms of\ncomputational elements required to represent some functions. deep multi-layer\nneural networks have many levels o", "gradient-based  learning  algorithms  for  recurrent \n\nnetworks  and  their  computational  complexity \n\nronald  j.  williams \n\ncollege  of  computer  science \n\nnortheastern  university \n\nboston,  ma  02115 \n\nand \n\ndavid  zipser \n\ndepartment  of  cognitive  science \nuniversity  of  california,  san  diego \n\nla  jolla,  ca  92093 \n\nappears  in  y.  chauvin  &  d.  e.  rumelhart  (eds.) \n\nback-propagation:  theory,  architectures  and  applications. \n\nhillsdale,  nj:  erlbaum.  1995. \n\n1 \n\nintrodu", "the committee machine: computational to statistical gaps\n\nin learning a two-layers neural network\n\nbenjamin aubin(cid:63)\u2020, antoine maillard\u2020, jean barbier\u2666,\n\nflorent krzakala\u2020, nicolas macris\u2297 and lenka zdeborov\u00e1(cid:63)\n\nabstract\n\nheuristic tools from statistical physics have been used in the past to locate the phase transitions and\ncompute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural\nnetworks. in this contribution, we provide a rigorous ", "increasing the action gap:\n\nnew operators for reinforcement learning\n\nmarc g. bellemare and georg ostrovski and arthur guez\n\nphilip s. thomas\u2217 and r\u00b4emi munos\n\n{bellemare,ostrovski,aguez,munos}@google.com; philipt@cs.cmu.edu\n\ngoogle deepmind\n\n5\n1\n0\n2\n \nc\ne\nd\n5\n1\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\nv\n0\n6\n8\n4\n0\n\n.\n\n2\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nthis paper introduces new optimality-preserving oper-\nators on q-functions. we \ufb01rst describe an operator for\ntabular representations, the consistent bellman ope", "scientific discovery in the age of artificial \nintelligence\n\nhttps://doi.org/10.1038/s41586-023-06221-2\nreceived: 30 march 2022\naccepted: 16 may 2023\npublished online: 2 august 2023\n\n check for updates\n\nhanchen wang1,2,37,38,39, tianfan fu3,39, yuanqi du4,39, wenhao gao5, kexin huang6, \n ziming liu7, payal chandak8, shengchao liu9,10, peter van katwyk11,12, andreea deac9,10, \nanima anandkumar2,13, karianne bergen11,12, carla p. gomes4, shirley ho14,15,16,17, \npushmeet kohli18, joan lasenby1, jur", "neuron, vol. 28, 329\u2013337, april 24, 2003, copyright \uf8e92003 by cell press\n\ntemporal difference models and\nreward-related learning in the human brain\n\njohn p. o\u2019doherty,1,* peter dayan,2 karl friston,1\nhugo critchley,1 and raymond j. dolan1\n1wellcome department of imaging neuroscience\ninstitute of neurology\n2 gatsby computational neuroscience unit\nuniversity college london\nlondon wc1n 3bg\nunited kingdom\n\nsummary\n\ntemporal difference learning has been proposed as a\nmodel for pavlovian conditioning, ", "9\n1\n0\n2\n\n \n\ny\na\nm\n7\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n4\n8\n5\n8\n0\n\n.\n\n1\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nfine-grained analysis of optimization and generalization for\n\noverparameterized two-layer neural networks\n\nsanjeev arora\u2217\n\nsimon s. du\u2020 wei hu\u2021\n\nzhiyuan li\u00a7\n\nruosong wang\u00b6\n\nabstract\n\nrecent works have cast some light on the mystery of why deep nets \ufb01t any data and generalize despite\nbeing very overparametrized. this paper analyzes training and generalization for a simple 2-layer relu\nnet with random initiali", "annealing between distributions by\n\naveraging moments\n\nroger grosse\n\nchris j. maddison\n\nruslan salakhutdinov\n\ncomp. sci. & ai lab\n\ndept. of computer science\n\ndepts. of statistics and comp. sci.,\n\nmit\n\ncambridge, ma 02139\n\nuniversity of toronto\ntoronto, on m5s 3g4\n\nuniversity of toronto\n\ntoronto, on m5s 3g4, canada\n\nabstract\n\nmany powerful monte carlo techniques for estimating partition functions, such\nas annealed importance sampling (ais), are based on sampling from a sequence\nof intermediate di", "deep supervised, but not unsupervised, models may\nexplain it cortical representation\n\nseyed-mahdi khaligh-razavi*, nikolaus kriegeskorte*\n\nmedical research council, cognition and brain sciences unit, cambridge, united kingdom\n\nabstract\n\ninferior temporal (it) cortex in human and nonhuman primates serves visual object recognition. computational object-\nvision models, although continually improving, do not yet reach human performance. it is unclear to what extent the\ninternal representations of co", "the journal of neuroscience, november 1, 1999, 19(21):9587\u20139603\n\nsynaptic basis of cortical persistent activity: the importance of\nnmda receptors to working memory\n\nxiao-jing wang\nvolen center for complex systems and department of physics, brandeis university, waltham,\nmassachusetts 02454-9110\n\ndelay-period activity of prefrontal cortical cells, the neural hall-\nmark of working memory, is generally assumed to be sustained\nby reverberating synaptic excitation in the prefrontal cortical\ncircuit. p", "ne39ch12-kohn\n\nari\n\n11 june 2016\n\n9:13\n\ncorrelations and neuronal\npopulation information\n\nadam kohn,1,2 ruben coen-cagli,3\ningmar kanitscheider,3,4,5 and alexandre pouget3,6,7\n1dominick purpura department of neuroscience, albert einstein college of medicine, bronx,\nnew york 10461; email: adam.kohn@einstein.yu.edu\n2department of ophthalmology and visual sciences, albert einstein college of medicine,\nbronx, new york 10461\n3department of basic neuroscience, university of geneva, ch-1211 geneva, swi", "neuron 51, 227\u2013238, july 20, 2006 \u00aa2006 elsevier inc. doi 10.1016/j.neuron.2006.06.017\n\na cooperative switch determines the\nsign of synaptic plasticity in distal\ndendrites of neocortical pyramidal neurons\n\nper jesper sjo\u00a8 stro\u00a8 m1,2,* and michael ha\u00a8 usser1,2\n1 wolfson institute for biomedical research\n2 department of physiology\nuniversity college london\nlondon wc1e 6bt\nunited kingdom\n\nsummary\n\npyramidal neurons in the cerebral cortex span multi-\nple cortical layers. how the excitable properties", "5\n1\n0\n2\n\n \nt\nc\no\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n9\n1\n0\n0\n0\n\n.\n\n6\n0\n5\n1\n:\nv\ni\nx\nr\na\n\na critical review of recurrent neural networks\n\nfor sequence learning\n\nzachary c. lipton\n\njohn berkowitz\n\nzlipton@cs.ucsd.edu\n\njaberkow@physics.ucsd.edu\n\ncharles elkan\n\nelkan@cs.ucsd.edu\n\njune 5th, 2015\n\nabstract\n\ncountless learning tasks require dealing with sequential data. image\ncaptioning, speech synthesis, and music generation all require that a model\nproduce outputs that are sequences. in other domains,", "trends in cognitive sciences\n\nreview\nwhat are memories for? the hippocampus\nbridges past experience with future decisions\n\nnatalie biderman,1,2,4 akram bakkour,1,2,4 and daphna shohamy1,3,*\n\nmany decisions require \ufb02exible reasoning that depends on inference, generaliza-\ntion, and deliberation. here, we review emerging \ufb01ndings indicating that the\nhippocampus, known for its role in long-term memory, contributes to these \ufb02exible\naspects of value-based decision-making. this work offers new insights ", "available online at www.sciencedirect.com\n\nsciencedirect\n\ncurrent opinion in\n\nneurobiology\n\ninterpreting neural computations by examining\nintrinsic and embedding dimensionality of neural\nactivity\nmehrdad jazayeri1 and srdjan ostojic2\n\nabstract\nthe ongoing exponential rise in recording capacity calls for\nnew approaches for analysing and interpreting neural data.\neffective dimensionality has emerged as an important property\nof neural activity across populations of neurons, yet different\nstudies re", "using the output embedding to improve language models\n\no\ufb01r press and lior wolf\nschool of computer science\ntel-aviv university, israel\n\n{ofir.press,wolf}@cs.tau.ac.il\n\n7\n1\n0\n2\n\n \n\nb\ne\nf\n1\n2\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n3\nv\n9\n5\n8\n5\n0\n\n.\n\n8\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe study the topmost weight matrix of\nneural network language models. we\nshow that this matrix constitutes a valid\nword embedding. when training language\nmodels, we recommend tying the input\nembedding and this output embedding.\nwe anal", "letter\ndendritic spikes enhance stimulus selectivity in\ncortical neurons in vivo\n\ndoi:10.1038/nature12600\n\nspencer l. smith1,2, ikuko t. smith1,2, tiago branco1,3 & michael ha\u00a8usser1\n\nneuronal dendrites are electrically excitable: they can generate\nregenerative events such as dendritic spikes in response to suffi-\nciently strong synaptic input1\u20133. although such events have been\nobserved in many neuronal types4\u20139, it is not well understood how\nactive dendrites contribute to the tuning of neuronal", "under review as a conference paper at iclr 2021\n\nsingle layers of attention suffice to predict\nprotein contacts\n\nanonymous authors\npaper under double-blind review\n\nabstract\n\nthe established approach to unsupervised protein contact prediction estimates co-\nevolving positions using undirected graphical models. this approach trains a potts\nmodel on a multiple sequence alignment, then predicts that the edges with high-\nest weight correspond to contacts in the 3d structure. on the other hand, in-\ncre", "machine learning: science and technologypaper \u2022 open accessstochasticity helps to navigate rough landscapes:comparing gradient-descent-based algorithms inthe phase retrieval problemto cite this article: francesca mignacco et al 2021 mach. learn.: sci. technol. 2 035029 view the article online for updates and enhancements.you may also likegd-1: the relic of an old metal-poorglobular clusterguang-wei li,  , brian yanny et al.-high-resolution spectroscopy of the gd-1stellar stream localizes the per", "available  online  at  www.sciencedirect.com\n\nneuroscience  of  affect:  brain  mechanisms  of  pleasure  and\ndispleasure\nkent  c  berridge1 and  morten  l  kringelbach2,3\n\naffective  neuroscience  aims  to  understand  how  affect\n(pleasure  or  displeasure)  is  created  by  brains.  progress  is  aided\nby  recognizing  that  affect  has  both  objective  and  subjective\nfeatures.  those  dual  aspects  re\ufb02ect  that  affective  reactions  are\ngenerated  by  neural  mechanisms,  selected  in  e", "the journal of neuroscience, november 27, 2013 \u2022 33(48):18999 \u201319011 \u2022 18999\n\nsystems/circuits\n\noptimizing working memory with heterogeneity of\nrecurrent cortical excitation\n\nzachary p. kilpatrick,1 bard ermentrout,2,3 and brent doiron2,3\n1department of mathematics, university of houston, houston, texas 77204, 2department of mathematics, university of pittsburgh, pittsburgh,\npennsylvania 15260, and 3center for the neural basis of cognition, pittsburgh, pennsylvania 15213\n\na neural correlate of p", "letter\nactivity in motor\u2013sensory projections reveals\ndistributed coding in somatosensation\n\ndoi:10.1038/nature11321\n\nleopoldo petreanu1{, diego a. gutnisky1, daniel huber1{, ning-long xu1, dan h. o\u2019connor1, lin tian1{, loren looger1\n& karel svoboda1\n\ncortical-feedback projections to primary sensory areas terminate\nmost heavily in layer 1 (l1) of the neocortex 1,2, where they make\nsynapses with tuft dendrites of pyramidal neurons. l1 input is\nthought to provide \u2018contextual\u2019 information3, but the ", "physical review x 8, 041029 (2018)\n\noptimal sequence memory in driven random networks\n\njannis schuecker,1,2,* sven goedeke,1,3,* and moritz helias1,4\n\n1institute of neuroscience and medicine (inm-6) and institute for advanced simulation (ias-6)\n\nand jara brain institute i, j\u00fclich research centre, 52428 j\u00fclich, germany\n\n2fraunhofer center for machine learning and fraunhofer iais, 53757 sankt augustin, germany\n\n3neural network dynamics and computation, institute of genetics, university of bonn,\n\n4", "ieee/acm transactions on computational biology and bioinformatics, vol. 4, no. 3,\n\njuly-september 2007\n\n441\n\nbidirectional long short-term memory\nnetworks for predicting the subcellular\n\nlocalization of eukaryotic proteins\n\ntrias thireou and martin reczko\n\nabstract\u2014an algorithm called bidirectional long short-term memory networks (blstm) for processing sequential data is\nintroduced. this supervised learning method trains a special recurrent neural network to use very long-range symmetric sequenc", "distinct feedforward and feedback effects of\nmicrostimulation in visual cortex reveal neural\nmechanisms of texture segregation\n\narticle\n\nhighlights\nd microstimulation of visual cortex evokes local excitation\n\nfollowed by inhibition\n\nd microstimulation of v1 causes feedforward excitation and\n\ninhibition in v4\n\nd microstimulation of v4 only causes feedback-based\n\nreductions in v1 \ufb01ring rates\n\nd when v4 is suppressed by microstimulation, v1 \ufb01gure-\n\nground segregation is reduced\n\nauthors\n\np. christi", "revisiting natural gradient for deep networks\n\nrazvan pascanu\n\nuniversit\u00b4e de montr\u00b4eal\n\nmontr\u00b4eal qc h3c 3j7 canada\nr.pascanu@gmail.com\n\nyoshua bengio\n\nuniversit\u00b4e de montr\u00b4eal\n\nmontr\u00b4eal qc h3c 3j7 canada\n\nyoshua.bengio@umontreal.ca\n\n4\n1\n0\n2\n\n \n\nb\ne\nf\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n7\nv\n4\n8\n5\n3\n\n.\n\n1\n0\n3\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe evaluate natural gradient descent, an algorithm originally proposed in amari\n(1997), for learning deep models. the contributions of this paper are as follows.\nwe sh", "local dynamics in trained recurrent neural networks\n\nfaculty of medicine, technion\u2013israel institute of technology, haifa 32000, israel and\n\nnetwork biology research labratories, technion - israel institute of technology, haifa 32000, israel\n\nalexander rivkind\u2217 and omri barak\u2020\n\n6\n1\n0\n2\n\n \nl\nu\nj\n \n3\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n5\nv\n2\n2\n2\n5\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nlearning a task induces connectivity changes in neural circuits, thereby changing their dynamics.\nto elucidate task related neur", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\ncomputational  neuroscience:  beyond  the  local  circuit\nhaim  sompolinsky1,2\n\ncomputational  neuroscience  has  focused  largely  on  the\ndynamics  and  function  of  local  circuits  of  neuronal  populations\ndedicated  to  a  common  task,  such  as  processing  a  common\nsensory  input,  storing  its  features  in  working  memory,  choosing\nbetween  a  set  of  options  dictated  by  controlled  experimental\nsettings  or  generat", "published as a conference paper at iclr 2021\n\nbertology meets biology: interpreting\nattention in protein language models\n\nali madani1\n\nlav r. varshney1,2\n\njesse vig1\nrichard socher1\n1salesforce research, 2university of illinois at urbana-champaign\n{jvig,amadani,cxiong,rsocher,nazneen.rajani}@salesforce.com\nvarshney@illinois.edu\n\nnazneen fatema rajani1\n\ncaiming xiong1\n\n1\n2\n0\n2\n\n \nr\na\n\n \n\nm\n8\n2\n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n3\nv\n2\n2\n2\n5\n1\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\ntransformer architectures have ", "9\n1\n0\n2\n\n \n\nv\no\nn\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n6\nv\n1\n5\n2\n1\n0\n\n.\n\n7\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ntraining behavior of deep neural network in\n\nfrequency domain(cid:63)\n\nzhi-qin john xu(cid:63)(cid:63)1, yaoyu zhang2, and yanyang xiao2\n\n1 school of mathematical sciences and institute of natural sciences, shanghai jiao\n\ntong university, shanghai, china,\n\nxuzhiqin@sjtu.edu.cn,\n\n2 school of mathematics, institute for advanced study, princeton, nj 08540, usa,\n\n3 the brain cognition and brain disease institute, ", "supplementary figure 1: schematic of reinforcement learning in a multilayer network. the\nforward path is computed by the blue neurons at top. input enters at the leftmost input neurons,\nx, and is transformed to hidden activity by, h = \u03c6(w0x), where \u03c6(\u00b7) is the transfer function.\noutput via the rightmost neurons, y, is computed from the hidden neurons as, y = \u03c6(wh).\nan error, e, between a desired outcome and actual outcome is computed downstream of the\noutput neurons, and summarized by the scalar", "best practices for convolutional neural networks  \n\napplied to visual document analysis \n\npatrice y. simard, dave steinkraus, john c. platt \n\nmicrosoft research, one microsoft way, redmond wa 98052 \n\n{patrice,v-davste,jplatt}@microsoft.com \n\n \n\n \n \n\nabstract \n\n \n    neural  networks  are  a  powerful  technology  for \nclassification  of  visual  inputs  arising  from  documents. \nhowever, there is a confusing plethora of different neural \nnetwork  methods  that  are  used  in  the  literature  a", "catastrophic interference in connectionist \nnetworks: the sequential learning problem \n\nmichael  mccloskey \n\nneal j .  cohen \n\ni.  introduction \n\nconnectionist networks in  which  information  is  stored  in  weights  on \nconnections between simple processing units have attracted considerable \ninterest  in  cognitive  science (e.g., rumelhart,  mcclelland,  & the  pdp \nresearch  group,  1986;  mcclelland,  rumelhart,  & the  pdp  research \ngroup, 1986). much of the interest centers around two ch", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/257310494\n\nbayesian decoding using unsorted spikes in the rat hippocampus.\n\narticle\u00a0\u00a0in\u00a0\u00a0journal of neurophysiology \u00b7 october 2013\n\ndoi: 10.1152/jn.01046.2012\u00a0\u00b7\u00a0source: pubmed\n\ncitations\n88\n\n4 authors, including:\n\nreads\n446\n\nfabian kloosterman\nneuro-electronics research flanders\n\n63 publications\u00a0\u00a0\u00a02,548 citations\u00a0\u00a0\u00a0\n\nstuart p layton\nmassachusetts institute of technology\n\n6 publications\u00a0\u00a0", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nworking  models  of  working  memory\nomri  barak1 and  misha  tsodyks2\n\nworking  memory  is  a  system  that  maintains  and  manipulates\ninformation  for  several  seconds  during  the  planning  and\nexecution  of  many  cognitive  tasks.  traditionally,  it  was  believed\nthat  the  neuronal  underpinning  of  working  memory  is  stationary\npersistent  \ufb01ring  of  selective  neuronal  populations.  recent\nadvances  introduced  new  i", "p e r s p e c t i v e  \n\nf o c u s   o n   n e u r a l   c o m p u tat i o n  a n d   t h e o r y\n\nusing goal-driven deep learning models to understand \nsensory cortex\ndaniel l k yamins1,2 & james j dicarlo1,2\nfueled by innovation in the computer vision and artificial \nintelligence communities, recent developments in \ncomputational neuroscience have used goal-driven hierarchical \nconvolutional neural networks (hcnns) to make strides in \nmodeling neural single-unit and population responses in hig", "vol 461 | 15 october 2009 | doi:10.1038/nature08499\n\narticles\n\nintracellular dynamics of hippocampal\nplace cells during virtual navigation\n\nchristopher d. harvey1,2,3, forrest collman1,2,3, daniel a. dombeck1,2,3 & david w. tank1,2,3\n\nhippocampal place cells encode spatial information in rate and temporal codes. to examine the mechanisms underlying\nhippocampal coding, here we measured the intracellular dynamics of place cells by combining in vivo whole-cell recordings\nwith a virtual-reality syst", "reports\n\na large-scale model of the\nfunctioning brain\nchris eliasmith,* terrence c. stewart, xuan choo, trevor bekolay, travis dewolf,\nyichuan tang, daniel rasmussen\na central challenge for cognitive and systems neuroscience is to relate the incredibly complex behavior\nof animals to the equally complex activity of their brains. recently described, large-scale neural models\nhave not bridged this gap between neural activity and biological function. in this work, we present a\n2.5-million-neuron mod", "7\n1\n0\n2\n\n \nc\ne\nd\n2\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n4\n3\n7\n2\n0\n\n.\n\n2\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nlearning in the machine:\n\nrandom backpropagation and the deep\n\nlearning channel\n\npierre baldi1,\u2217, peter sadowski1, and zhiqin lu2\n\nabstract\n\nabstract: random backpropagation (rbp) is a variant of the\nbackpropagation algorithm for training neural networks, where the\ntranspose of the forward matrices are replaced by \ufb01xed random ma-\ntrices in the calculation of the weight updates. it is remarkable both\nbecause o", "validationofsoftwareforbayesianmodelsusingposteriorquantilessamanthar.cook,andrewgelman,anddonaldb.rubinthisarticlepresentsasimulation-basedmethoddesignedtoestablishthecomputa-tionalcorrectnessofsoftwaredevelopedto\ufb01taspeci\ufb01cbayesianmodel,capitalizingonpropertiesofbayesianposteriordistributions.weillustratethevalidationtechniquewithtwoexamples.thevalidationmethodisshownto\ufb01nderrorsinsoftwarewhentheyexistand,moreover,thevalidationoutputcanbeinformativeaboutthenatureandlocationofsucherrors.wealsocom", "d e c i s i o n m a k i n g\n\nr e v i e w\n\nchoice, uncertainty and value in prefrontal and\ncingulate cortex\n\nmatthew f s rushworth & timothy e j behrens\n\nreinforcement learning models that focus on the striatum and dopamine can predict the choices of animals and people.\nrepresentations of reward expectation and of reward prediction errors that are pertinent to decision making, however, are not\ncon\ufb01ned to these regions but are also found in prefrontal and cingulate cortex. moreover, decisions are ", "a r t i c l e s\n\nextended practice of a motor skill is associated with \nreduced metabolic activity in m1\nnathalie picard1,2, yoshiya matsuzaka5 & peter l strick1\u20134\nhow does long-term training and the development of motor skills modify the activity of the primary motor cortex (m1)? to address \nthis issue, we trained monkeys for ~1\u20136 years to perform visually guided and internally generated sequences of reaching \nmovements. then, we used [14c]2-deoxyglucose (2dg) uptake and single-neuron recording", "validation of software for bayesian models using posterior quantiles \nauthor(s): samantha r. cook, andrew gelman and donald b. rubin \nsource: journal of computational and graphical statistics, sep., 2006, vol. 15, no. 3 \n(sep., 2006), pp. 675-692\npublished by: taylor & francis, ltd. on behalf of the american statistical association, \ninstitute of mathematical statistics, and interface foundation of america\n\n \n\n \n\nstable url: https://www.jstor.org/stable/27594203\n \nreferences \nlinked references a", "available online at www.sciencedirect.com\n\nsciencedirect\n\ncurrent opinion in\n\nneurobiology\n\ntowards the next generation of recurrent network\nmodels for cognitive neuroscience\nguangyu robert yang1,2,3 and manuel molano-maz\u00f3n4\n\nabstract\nrecurrent neural networks (rnns) trained with machine\nlearning techniques on cognitive tasks have become a widely\naccepted tool for neuroscientists. in this short opinion piece,\nwe discuss fundamental challenges faced by the early work of\nthis approach and recent s", "e\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\ne\nr\nu\nt\na\nn\nm\no\nc\n.\ne\nr\nu\nt\na\nn\nw\nw\nw\n\n/\n\n.\n\n/\n/\n:\np\nt\nt\nh\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n4\n0\n0\n2\n\u00a9\n\n \n\na r t i c l e s\n\nprefrontal cortex and decision making in a mixed-\nstrategy game\n\ndominic j barraclough, michelle l conroy & daeyeol lee\n\nin a multi-agent environment, where the outcomes of one\u2019s actions change dynamically because they are related to the behavior\nof other beings, it becomes difficult to make an optimal decision about how to act.", "geometric understanding of deep learning\n\nna lei \u2217\n\nzhongxuan luo \u2020\n\nshing-tung yau \u2021\n\ndavid xianfeng gu \u00a7\n\nabstract\n\ndeep learning is the mainstream technique for many machine learning tasks, including image recog-\nnition, machine translation, speech recognition, and so on. it has outperformed conventional methods in\nvarious \ufb01elds and achieved great successes. unfortunately, the understanding on how it works remains\nunclear. it has the central importance to lay down the theoretic foundation for", "a r t i c l e s\n\ngrid cells require excitatory drive from the hippocampus\ntora bonnevie1, benjamin dunn1, marianne fyhn1,3, torkel hafting1,3, dori derdikman1,3, john l kubie2, \nyasser roudi1, edvard i moser1 & may-britt moser1\nto determine how hippocampal backprojections influence spatially periodic firing in grid cells, we recorded neural activity in \nthe medial entorhinal cortex (mec) of rats after temporary inactivation of the hippocampus. we report two major changes in \nentorhinal grid cell", "dendritic spikes induce single-burst\nlong-term potentiation\n\nstefan remy and nelson spruston*\n\ndepartment of neurobiology and physiology, northwestern university, evanston, il 60208\n\ncommunicated by bert sakmann, max planck institute for medical research, heidelberg, germany, september 5, 2007 (received for review may 2, 2007)\n\nthe hippocampus is essential for episodic memory, which requires\nsingle-trial\nlearning. although long-term potentiation (ltp) of\nsynaptic strength is a candidate mechanis", "an empirical exploration of recurrent network architectures\n\nrafal jozefowicz\ngoogle inc.\nwojciech zaremba\nnew york university, facebook1\nilya sutskever\ngoogle inc.\n\nrafalj@google.com\n\nwoj.zaremba@gmail.com\n\nilyasu@google.com\n\nabstract\n\nthe recurrent neural network (rnn) is an ex-\ntremely powerful sequence model that is often\ndif\ufb01cult to train. the long short-term memory\n(lstm) is a speci\ufb01c rnn architecture whose\ndesign makes it much easier to train. while\nwildly successful in practice, the lstm", "letters to nature \n\na neuronal learning rule for \nsub-millisecond temporal \ncoding \n\nwulfram gerstner*:j:, richard kempter*, \nj.  leo van  hemmen* & hermann wagnert+ \n\n* physik-department and t fakultat fur chemie und  biologie, \ntechnische  universitat;  monchen, d-85747 garching bei  monchen, \ngermany \n\na \n\nb  o '---'---------~-'---\n\na:: ~ \n\na  paradox  that  exists  in  auditory  and  electrosensory  neural \nsystems1'2  is  that  they  encode  behaviourally relevant  signals  in \nthe range of", "locality preserving projections\n\nxiaofei he\n\ndepartment of computer science\n\nthe university of chicago\n\nchicago, il 60637\n\npartha niyogi\n\ndepartment of computer science\n\nthe university of chicago\n\nchicago, il 60637\n\nxiaofei@cs.uchicago.edu\n\nniyogi@cs.uchicago.edu\n\nabstract\n\nmany problems in information processing involve some form of dimen-\nsionality reduction. in this paper, we introduce locality preserving pro-\njections (lpp). these are linear projective maps that arise by solving a\nvariationa", "article\n\ndoi:10.1038/nature11057\n\nbrain-wide neuronal dynamics during\nmotor adaptation in zebrafish\n\nmisha b. ahrens1,2, jennifer m. li1, michael b. orger3, drew n. robson1, alexander f. schier1, florian engert1 & ruben portugues1\n\na fundamental question in neuroscience is how entire neural circuits generate behaviour and adapt it to changes in\nsensory feedback. here we use two-photon calcium imaging to record the activity of large populations of neurons at the\ncellular level, throughout the bra", "the loss surfaces of multilayer networks\n\n5\n1\n0\n2\n\n \n\nn\na\nj\n \n\n1\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n3\n3\n2\n0\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\nanna choromanska\nachoroma@cims.nyu.edu\n\nmikael hena\ufb00\nmbh305@nyu.edu\n\nmichael mathieu\nmathieu@cs.nyu.edu\n\ng\u00b4erard ben arous\n\nyann lecun\n\nbenarous@cims.nyu.edu\n\nyann@cs.nyu.edu\n\ncourant institute of mathematical sciences\n\nnew york, ny, usa\n\nabstract\n\n1 introduction\n\nwe study the connection between the highly\nnon-convex loss function of a simple model of\nthe fully-connect", "phenotypic variation of transcriptomic cell \ntypes in mouse motor cortex\n\nhttps://doi.org/10.1038/s41586-020-2907-3\nreceived: 5 february 2020\naccepted: 16 october 2020\npublished online: 12 november 2020\nopen access\n\n check for updates\n\nfederico scala1,2,12, dmitry kobak3,12, matteo bernabucci1,2, yves bernaerts3,4,  \ncathryn ren\u00e9 cadwell5, jesus ramon castro1,2, leonard hartmanis6, xiaolong jiang1,2,7, \nsophie laturnus3, elanine miranda1,2, shalaka mulherkar2, zheng huan tan1,2, zizhen yao8, \nho", "the bell system\n\ntechnical journal\n\nvolume xxxviii\n\nmay1959\n\nnumber 3\n\ncopi/\",itl 1969, a\"\",n'can t.z.\"aon. and t.1<graph companl/\n\nprobability of error for optimal codes\n\nin a gaussian channel\n\nby claude e. shannon\n\n(manuscript received october 17, 1958)\n\na study is made of coding and decoding systems for a continuous channel\nwith an additive gaussian noise and subject to an average power limitation\nat the transmitter. upper and lower bounds are found for the error prob \nability in decoding wit", "emotion enhances learning via\nnorepinephrine regulation of\nampa-receptor traf\ufb01cking\n\nhailan hu,1 eleonore real,1 kogo takamiya,2 myoung-goo kang,2 joseph ledoux,3 richard l. huganir,2\nand roberto malinow1,*\n1cold spring harbor laboratory, cold spring harbor, ny 11724, usa\n2howard hughes medical institute, department of neuroscience, johns hopkins university school of medicine, baltimore,\nmd 21205, usa\n3new york university, new york, ny 10003, usa\n*correspondence: malinow@cshl.org\ndoi 10.1016/j.c", "letter\n\ndoi:10.1038/nature25457\n\ndopamine neuron activity before action initiation \ngates and invigorates future movements\n\njoaquim alves da silva1,2, fatuel tecuapetla1,3, vitor paix\u00e3o1 & rui m. costa1,2,4\n\ndeciding when and whether to move is critical for survival. loss of \ndopamine neurons (dans) of the substantia nigra pars compacta \n(snc) in patients with parkinson\u2019s disease causes deficits in \nmovement initiation and slowness of movement1. the role of dans \nin self-paced movement has mostl", "neuron\n\nreview\n\nneuropeptide transmission in brain circuits\n\nanthony n. van den pol1,*\n1department of neurosurgery, yale university, new haven, ct 06520, usa\n*correspondence: anthony.vandenpol@yale.edu\nhttp://dx.doi.org/10.1016/j.neuron.2012.09.014\n\nneuropeptides are found in many mammalian cns neurons where they play key roles in modulating neuronal\nactivity. in contrast to amino acid transmitter release at the synapse, neuropeptide release is not restricted to\nthe synaptic specialization, and ", "reports\n\nenhanced actin depolymerization at the mdia1-\nbound barbed end. this inhibition occurs in the\nsubmillimolar range of pi, which is two orders of\nmagnitude lower than the dissociation constant of\npi and g-actin (26). thus, binding of pi to f-actin\ninhibits profilin-induced depolymerization (fig. 4e).\nadp-g-actin (5 mm) elongated mdia1-bound\nf-actin faster in the presence of 20 mm pi than in\nits absence (fig. 3c). this effect of pi was more\nprominent in the presence of profilin than in its", "trust region policy optimization\n\njohn schulman\nsergey levine\nphilipp moritz\nmichael jordan\npieter abbeel\nuniversity of california, berkeley, department of electrical engineering and computer sciences\n\njoschu@eecs.berkeley.edu\nslevine@eecs.berkeley.edu\npcmoritz@eecs.berkeley.edu\njordan@cs.berkeley.edu\npabbeel@cs.berkeley.edu\n\nabstract\n\nin this article, we describe a method for optimiz-\ning control policies, with guaranteed monotonic\nimprovement. by making several approxima-\ntions to the theoreti", "article\n\ndoi:10.1038/nature11527\n\ninput-specific control of reward and\naversion in the ventral tegmental area\n\nstephan lammel1*, byung kook lim1*, chen ran1, kee wui huang1, michael j. betley1, kay m. tye2, karl deisseroth3\n& robert c. malenka1\n\nventral tegmental area (vta) dopamine neurons have important roles in adaptive and pathological brain functions\nrelated to reward and motivation. however, it is unknown whether subpopulations of vta dopamine neurons\nparticipate in distinct circuits that ", "slayer: spike layer error reassignment in time\n\nsumit bam shrestha\u2217\n\ntemasek laboratories @ nus\nnational university of singapore\n\nsingapore, 117411\n\ngarrick orchard\u2020\n\ntemasek laboratories @ nus\nnational university of singapore\n\nsingapore, 117411\n\ntslsbs@nus.edu.sg\n\ntslgmo@nus.edu.sg\n\nabstract\n\ncon\ufb01guring deep spiking neural networks (snns) is an exciting research avenue\nfor low power spike event based computation. however, the spike generation\nfunction is non-differentiable and therefore not dir", "2012 american control conference\nfairmont queen elizabeth, montr\u00e9al, canada\njune 27-june 29, 2012\n\n978-1-4577-1096-4/12/$26.00 \u00a92012 aacc\n\n2177\n\nauthorized licensed use limited to: university of washington libraries. downloaded on october 28,2023 at 19:13:50 utc from ieee xplore.  restrictions apply. \n\nmodel-freereinforcementlearningwithcontinuousactioninpracticethomasdegris,patrickm.pilarski,richards.suttonabstract\u2014reinforcementlearningmethodsareoftencon-sideredasapotentialsolutiontoenablearobo", "in silico saturation mutagenesis of cancer \ngenes\n\nhttps://doi.org/10.1038/s41586-021-03771-1\nreceived: 2 july 2020\naccepted: 25 june 2021\npublished online: 28 july 2021\n\n check for updates\n\nferran mui\u00f1os1,4\u2009\u2709, francisco mart\u00ednez-jim\u00e9nez1,4, oriol pich1, abel gonzalez-perez1,2\u2009\u2709 & \nnuria lopez-bigas1,2,3\u2009\u2709\n\ndespite the existence of good catalogues of cancer genes1,2, identifying the specific \nmutations of those genes that drive tumorigenesis across tumour types is still a \nlargely unsolved probl", "attention-based selective plasticity\n\nsoheil kolouri 1 nicholas ketz 1 xinyun zou 2 jeffrey krichmar 2 praveen pilly 1\n\n9\n1\n0\n2\n\n \nr\na\n\nm\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n0\n7\n0\n6\n0\n\n.\n\n3\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nneural networks,\n\ncatastrophic forgetting/interference is a critical\nproblem for lifelong learning machines, which\nimpedes the agents from maintaining their pre-\nviously learned knowledge while learning new\ntasks.\nin particular, suf-\nfer plenty from the catastrophic forgetting phe-\nn", "deep learning for image super-resolution \n\na comprehensive review of deep learning-\n\nbased single image super-resolution \n\nsyed muhammad arsalan bashir1, 2,*\uf02a, yi wang1, and mahrukh khan3 \n\n \n\n1 school of electronics and information, northwestern polytechnical university, xi\u2019an, shaanxi, china  \n2 space and upper atmosphere research commission, karachi, sindh, pakistan  \n3 department of computer science, national university of computer and emerging sciences, karachi, sindh, \npakistan \n \nabstract", "7\n1\n0\n2\n\n \n\nn\na\nj\n \n\n3\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n8\n4\n6\n2\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nunder review as a conference paper at iclr 2017\n\ndeep unsupervised clustering with gaussian\nmixture variational autoencoders\n\nnat dilokthanakul1,\u2217, pedro a. m. mediano1, marta garnelo1,\nmatthew c. h. lee1, hugh salimbeni1, kai arulkumaran2 & murray shanahan1\n1department of computing, 2department of bioengineering\nimperial college london\nlondon, uk\n\u2217n.dilokthanakul14@imperial.ac.uk\n\nabstract\n\nwe study a varia", "article\n\nreceived 24 sep 2013 | accepted 17 mar 2014 | published 28 apr 2014\n\ndoi: 10.1038/ncomms4675\n\nopen\n\nthe stimulus-evoked population response in visual\ncortex of awake monkey is a propagating wave\nlyle muller1,*, alexandre reynaud2,*, fre\u00b4de\u00b4ric chavane2 & alain destexhe1\n\npropagating waves occur in many excitable media and were recently found in neural systems\nfrom retina to neocortex. while propagating waves are clearly present under anaesthesia,\nwhether they also appear during awake an", "calcium-based plasticity model explains sensitivity of\nsynaptic changes to spike pattern, rate, and\ndendritic location\n\nmichael graupnera,b,1 and nicolas brunela\n\nalaboratory of neurophysics and physiology, unit\u00e9 mixte de recherche 8119, cnrs and universit\u00e9 paris descartes, 75270 paris cedex 06, france; and bcenter\nfor neural science, new york university, new york, ny 10003-6603\n\nedited by terrence j. sejnowski, salk institute for biological studies, la jolla, ca, and approved january 20, 2012 (", "0\n2\n0\n2\n\n \n\nb\ne\nf\n4\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n9\n1\n6\n1\n0\n\n.\n\n0\n1\n9\n1\n:\nv\ni\nx\nr\na\n\nbeyond linearization: on quadratic and higher-order\n\napproximation of wide neural networks\n\nyu bai\u2217\n\njason d. lee\u2020\n\nfebruary 18, 2020\n\nabstract\n\nrecent theoretical work has established connections between over-parametrized neural net-\nworks and linearized models governed by the neural tangent kernels (ntks). ntk theory\nleads to concrete convergence and generalization results, yet the empirical performance of", "ieee transactions on neural networks and learning systems, vol. 25, no. 7, july 2014\n\n1229\n\na comprehensive review of stability analysis of\n\ncontinuous-time recurrent neural networks\n\nhuaguang zhang, senior member, ieee, zhanshan wang, member, ieee, and derong liu, fellow, ieee\n\nabstract\u2014 stability problems of continuous-time recurrent\nneural networks have been extensively studied, and many papers\nhave been published in the literature. the purpose of this paper\nis to provide a comprehensive revi", "b r a i n r e s e a r c h r e v i e w s 5 7 ( 2 0 0 8 ) 1 2 5 \u2013 1 3 3\n\nava i l a b l e a t w w w. s c i e n c e d i r e c t . c o m\n\nw w w. e l s ev i e r. c o m / l o c a t e / b r a i n r e s r ev\n\nreview\n\ncombining modules for movement\n\ne. bizzia,\u204e, v.c.k. cheungb, a. d'avellac, p. saltiela, m. treschd\nadepartment of brain and cognitive sciences and mcgovern institute for brain research, massachusetts institute of technolgy 46-6189,\n77 massachusetts avenue, cambridge, ma 02139, usa\nbdivision ", "the shattered gradients problem:\n\nif resnets are the answer, then what is the question?\n\ndavid balduzzi 1 marcus frean 1 lennox leary 1 jp lewis 1 2 kurt wan-duo ma 1 brian mcwilliams 3\n\nabstract\n\na long-standing obstacle to progress in deep\nlearning is the problem of vanishing and ex-\nploding gradients. although, the problem has\nlargely been overcome via carefully constructed\ninitializations and batch normalization, archi-\ntectures incorporating skip-connections such as\nhighway and resnets perf", "6\n1\n0\n2\n\n \n\nb\ne\nf\n8\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n3\n4\n5\n7\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2016\n\nconvergent learning: do different neural\nnetworks learn the same representations?\n\nyixuan li1\u2217, jason yosinski1\u2217, jeff clune2, hod lipson3, & john hopcroft1\n1cornell university\n2university of wyoming\n3columbia university\n{yli,yosinski,jeh}@cs.cornell.edu\njeffclune@uwyo.edu, hod.lipson@columbia.edu\n\nabstract\n\nrecent successes in training large, deep neural network", "biorxiv preprint \nthe copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\nhttps://doi.org/10.1101/2022.11.14.516537\n; \n\nthis version posted november 15, 2022. \n\ndoi: \n\navailable under a\n\ncc-by 4.0 international license\n.\n\nwhen and why grid cells appear or not in trained path integrators\n\nben sorscher1, gabriel c. mel2, aran nayebi3, lisa giocomo4, daniel yamins5,6, ", "cognitive \n\nscience  14,  179-211 \n\n(1990) \n\nfinding structure in time \n\njeffreyl.elman \n\nuniversity \n\nof  california, \n\nsan  diego \n\ntime \n\nunderlies \n\ndescribed \n\ntime \nrepresent \nresent \nspatial \nfirst \nto  provide \nare \nterns \nreflect \nthus \ntions \nis  reported \nof  xor) \nlearn \nable \nto \nwith  memory \nbound \ncably \nture, \nwhich \ngeneralizations \nfor \n\nrepresenting \n\ninteresting \n\nmany \nin  connectionist \n\nhuman \n\nbehaviors. \n\nmodels \n\nis  very \n\nimportant. \n\ntime \nimplicitly \nrepresentation", "neural  networks,  vol.  2,  pp.  53-58,  1989 \nprinted in  the  usa.  all  rights  reserved. \n\n0893-6080/89 $3.00  +  .00 \ncopyright \u00a9  1989  pergamon press pic \n\noriginal  contribution \n\nneural networks and  principal  component analysis: \n\nlearning from  examples without  local minima \n\npierre baldi  and  kurt hornik * \n\nuniversity of california.  san diego \n\n(received  18  may  1988;  revised and accepted 16 august 1988) \n\nabstract-we  consider the problem of learning from  examples  in  lay", "4\n1\n0\n2\n\n \n\nb\ne\nf\n9\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n0\n2\n1\n6\n\n.\n\n2\n1\n3\n1\n:\nv\ni\nx\nr\na\n\nexact solutions to the nonlinear dynamics of learning in\n\ndeep linear neural networks\n\nandrew m. saxe (asaxe@stanford.edu)\n\ndepartment of electrical engineering\n\njames l. mcclelland (mcclelland@stanford.edu)\n\ndepartment of psychology\n\nsurya ganguli (sganguli@stanford.edu)\n\ndepartment of applied physics\n\nstanford university, stanford, ca 94305 usa\n\nabstract\n\ndespite the widespread practical success of deep learn", "article\n\nreceived 19 dec 2015 | accepted 2 nov 2016 | published 5 jan 2017\n\ndoi: 10.1038/ncomms13804\n\nopen\n\nlayer-speci\ufb01city in the effects of attention\nand working memory on activity in primary\nvisual cortex\ntimo van kerkoerle1,*, matthew w. self2,* & pieter r. roelfsema2,3,4\n\nneuronal activity in early visual cortex depends on attention shifts but the contribution to\nworking memory has remained unclear. here, we examine neuronal activity in the different\nlayers of the primary visual cortex (v1", "a tutorial on deep learning for music information\n\nretrieval\n\nkeunwoo choi\n\nkeunwoo.choi@qmul.ac.uk\n\ngy\u00f6rgy fazekas\n\ng.fazekas@qmul.ac.uk\n\nkyunghyun cho\n\nkyunghyun.cho@nyu.edu\n\nmark sandler\n\nmark.sandler@qmul.ac.uk\n\n8\n1\n0\n2\n\n \n\ny\na\nm\n3\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n6\n9\n3\n4\n0\n\n.\n\n9\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\nfollowing their success in computer vision and other ar-\neas, deep learning techniques have recently become widely\nadopted in music information retrieval (mir) research. how-\never, the maj", "published as a conference paper at iclr 2018\n\nunbiased online recurrent optimization\n\ncorentin tallec\nlaboratoire de recherche en informatique\nuniversit\u00e9 paris sud\ngif-sur-yvette, 91190, france\ncorentin.tallec@u-psud.fr\n\nyann ollivier\nlaboratoire de recherche en informatique\nuniversit\u00e9 paris sud\ngif-sur-yvette, 91190, france\nyann@yann-ollivier.org\n\nabstract\n\nthe novel unbiased online recurrent optimization (uoro) algorithm allows for\nonline learning of general recurrent computational graphs such", "research article\n\nrepresentation of confidence\nassociated with a decision by\nneurons in the parietal cortex\nroozbeh kiani and michael n. shadlen\n\nthe degree of confidence in a decision provides a graded and probabilistic assessment of\nexpected outcome. although neural mechanisms of perceptual decisions have been studied\nextensively in primates, little is known about the mechanisms underlying choice certainty. we have\nshown that the same neurons that represent formation of a decision encode certa", "a mathematical theory of semantic development in\ndeep neural networks\n\nandrew m. saxea,1, james l. mcclellandb, and surya gangulic,d\n\nadepartment of experimental psychology, university of oxford, oxford ox2 6gg, united kingdom; bdepartment of psychology, stanford university,\nstanford, ca 94305; cdepartment of applied physics, stanford university, stanford, ca 94305; and dgoogle brain, google, mountain view, ca 94043\n\nedited by terrence j. sejnowski, salk institute for biological studies, la joll", "journal of mathematical psychology 47 (2003) 90\u2013100\n\ntutorial\n\ntutorial on maximum likelihood estimation\n\nin jae myung*\n\ndepartment of psychology, ohio state university, 1885 neil avenue mall, columbus, oh 43210-1222, usa\n\nreceived 30 november 2001; revised 16 october 2002\n\nabstract\n\nin this paper, i provide a tutorial exposition on maximum likelihood estimation (mle). the intended audience of this tutorial are\nresearchers who practice mathematical modeling of cognition but are unfamiliar with t", "neuron, vol. 38, 473\u2013485, may 8, 2003, copyright \uf8e92003 by cell press\n\nrobust spatial working memory\nthrough homeostatic synaptic scaling\nin heterogeneous cortical networks\n\nalfonso renart, pengcheng song,\nand xiao-jing wang*\nvolen center for complex systems\nbrandeis university\nwaltham, massachusetts 02454\n\nsummary\n\nthe concept of bell-shaped persistent neural activity\nrepresents a cornerstone of the theory for the internal\nrepresentation of analog quantities, such as spatial\nlocation or head dir", "m\no\nc\n\n.\n\ne\nr\nu\nt\na\nn\n\n.\ni\n\nc\ns\no\nr\nu\ne\nn\n\n/\n/\n:\np\nt\nt\nh\n \n\u2022\n \n.\n\nc\nn\n\ni\n \n\n \n\na\nc\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n0\n0\n0\n2\n\u00a9\n\n \n\n \n\n\u00a9 2000 nature america inc. \u2022 http://neurosci.nature.com\n\narticles\n\nsomatic epsp amplitude is\nindependent of synapse location in\nhippocampal pyramidal neurons\n\njeffrey c. magee1 and erik p. cook2\n\n1 neuroscience center, louisiana state university medical center, 2020 gravier st., new orleans, louisiana 70112, usa\n2 howard hughes medical institute, baylor college of medicine, o", "on layer normalization in the transformer architecture\n\nruibin xiong\u2020* 1 2 yunchang yang* 3 di he 4 5 kai zheng 4 shuxin zheng 5 chen xing 6 huishuai zhang 5\n\nyanyan lan 1 2 liwei wang 4 3 tie-yan liu 5\n\nabstract\n\nthe transformer is widely used in natural lan-\nguage processing tasks. to train a transformer\nhowever, one usually needs a carefully designed\nlearning rate warm-up stage, which is shown to\nbe crucial to the \ufb01nal performance but will slow\ndown the optimization and bring more hyper-\npara", "fourier analysis of sinusoidally driven thalamocortical relay\nneurons and a minimal integrate-and-fire-or-burst model\n\ngregory d. smith,1,4 charles l. cox,3 s. murray sherman,3 and john rinzel1,2,4\n1center for neural science and 2courant institute of mathematical sciences, new york university, new york, new york\n10003; 3department of neurobiology, state university of new york, stony brook, new york 11794; and 4mathematical\nresearch branch, national institute of diabetes and digestive and kidney ", "neurobiology of learning and memory 82 (2004) 171\u2013177\n\nwww.elsevier.com/locate/ynlme\n\nminireview\n\nmemory systems of the brain: a brief history and current perspective\n\nlarry r. squire*\n\ndepartments of psychiatry, neurosciences, and psychology, university of california, san diego, la jolla, ca 92093, usa\n\nveterans a\ufb00airs healthcare system, san diego, ca 92161, usa\n\nreceived 23 april 2004; accepted 14 june 2004\n\navailable online 4 august 2004\n\nabstract\n\nthe idea that memory is composed of distinct", "230\n\nieee transactions on autonomous mental development, vol. 2, no. 3, september 2010\n\nformal theory of creativity, fun, and\n\nintrinsic motivation (1990\u20132010)\n\nj\u00fcrgen schmidhuber\n\nabstract\u2014the simple, but general formal theory of fun and\nintrinsic motivation and creativity (1990\u20132010) is based on the\nconcept of maximizing intrinsic reward for the active creation\nor discovery of novel, surprising patterns allowing for improved\nprediction or data compression. it generalizes the traditional\n\ufb01eld o", "8\n1\n0\n2\n\n \nc\ne\nd\n1\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n7\n6\n5\n3\n0\n\n.\n\n1\n1\n8\n1\n:\nv\ni\nx\nr\na\n\ncbmmmemono.92november21,2018biologically-plausiblelearningalgorithmscanscaletolargedatasetswillxiao1,honglinchen2,qianliliao2andtomasopoggio21departmentofmolecularandcellularbiology,harvarduniveristy2centerforbrains,minds,andmachines,mitabstractthebackpropagation(bp)algorithmisoftenthoughttobebiologicallyimplausibleinthebrain.oneofthemainreasonsisthatbprequiressymmetricweightmatricesinthefeedforwardandfeed-ba", "leading edge\n\nreview\n\nhallmarks of cancer: the next generation\n\ndouglas hanahan1,2,* and robert a. weinberg3,*\n1the swiss institute for experimental cancer research (isrec), school of life sciences, epfl, lausanne ch-1015, switzerland\n2the department of biochemistry & biophysics, ucsf, san francisco, ca 94158, usa\n3whitehead institute for biomedical research, ludwig/mit center for molecular oncology, and mit department of biology, cambridge,\nma 02142, usa\n*correspondence: dh@ep\ufb02.ch (d.h.), weinb", "(v)\n\ntwo\n\nlearning  internal  representations\n\nberror  propagation\n\ndavid  e. ruineihart,  geoffrey  e.  hint..,\n\nand  ronald  j. williams\n\n0 \n\n4 \n\nseptember  1985\n\nics  report  8506\n\ncognitive \n\nscience\n\niaq  i\n\ninstitute  for  cognitive  science\n\nuniversity  of california,  san  diego \n\n862 18 \n\nla jolla, california  92093\n120,\n\n\f", "classification of electrophysiological and \nmorphological neuron types in the mouse  \nvisual cortex\n\nnathan w. gouwens1,2, staci a. sorensen1,2, jim berg1,2, changkyu lee1, tim jarsky1, jonathan ting1, \nsusan m. sunkin1, david feng\u200a\n\u200a1, costas a. anastassiou1, eliza barkan1, kris bickley1, nicole blesie1, \nthomas braun1, krissy brouner1, agata budzillo1, shiella caldejon1, tamara casper1, dan castelli1, \npeter chong1, kirsten crichton1, christine cuhaciyan1, tanya l. daigle1, rachel dalley1, nic", "neuron\n\nreport\n\nrewards evoke learning of unconsciously\nprocessed visual stimuli in adult humans\n\naaron r. seitz,1,2,3,* dongho kim,1,3 and takeo watanabe1\n1department of psychology, boston university, 64 cummington street, boston, ma 02215, usa\n2department of psychology, university of california, riverside, 900 university avenue, riverside, ca 92521, usa\n3these authors contributed equally to this work\n*correspondence: aseitz@ucr.edu\ndoi 10.1016/j.neuron.2009.01.016\n\nsummary\n\nthe study of human ", "the power of interpolation: understanding the effectiveness of sgd in\n\nmodern over-parametrized learning\u2020\n\nsiyuan ma 1 raef bassily 1 mikhail belkin 1\n\nabstract\n\n1\n\nintroduction\n\nin this paper we aim to formally explain the phe-\nnomenon of fast convergence of stochastic gradi-\nent descent (sgd) observed in modern machine\nlearning. the key observation is that most mod-\nern learning architectures are over-parametrized\nand are trained to interpolate the data by driving\nthe empirical loss (classi\ufb01ca", "16. tunnicliffe, v., mcarthur, a. g. & mchugh, d. a biogeographical perspective of the deep-sea\n\nhydrothermal vent fauna. adv. mar. biol. 34, 353\u2013442 (1998).\n\n17. sibuet, m. & olu, k. biogeography, biodiversity and \ufb02uid dependence of deep-sea cold-seep\n\ncommunities at active and passive margins. deep-sea res. ii 45, 517\u2013567 (1998).\n\n18. heezen, b. c. & tharp, m. world ocean floor (map). united states navy, of\ufb01ce of naval research\n\n(washington, d.c., 1977).\n\n19. bouchet, p. & metivier, b. living ", "biorxiv preprint \n\nthe copyright holder for this preprint (which was\nnot certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available \n\nthis version posted september 20, 2018. \n\nhttps://doi.org/10.1101/418939\n; \n\ndoi: \n\nunder a\n\ncc-by-nc-nd 4.0 international license\n.\n\nprobing variability in a cognitive map using manifold inference from neural dynamics\n\nryan j. lowy,1, sam lewalleny,1,3, dmitriy aronov1,4, rhino nev", "n\ne\nu\nr\na\nl\n \nn\ne\nt\nw\no\nr\nk\ns\n,\n \nv\no\nl\n.\n \n3\n,\n \np\np\n.\n \n3\n6\n7\n-\n3\n7\n5\n,\n \n1\n9\n9\n0\n \n0\n8\n9\n3\n-\n6\n0\n8\n0\n/\n9\n0\n \n$\n3\n.\n0\n0\n \n+\n \n.\n0\n0\n \np\nr\ni\nn\nt\ne\nd\n \ni\nn\n \nt\nh\ne\n \nu\ns\na\n.\n \na\nl\nl\n \nr\ni\ng\nh\nt\ns\n \nr\ne\ns\ne\nr\nv\ne\nd\n.\n \nc\no\np\ny\nr\ni\ng\nh\nt\n \n~\n'\n:\n \n1\n9\n9\n0\n \np\ne\nr\ng\na\nm\no\nn\n \np\nr\ne\ns\ns\n \np\nl\nc\n \no\nr\ni\ng\ni\nn\na\nl\n \nc\no\nn\nt\nr\ni\nb\nu\nt\ni\no\nn\n \nt\nh\ne\n \no\np\nt\ni\nm\ni\ns\ne\nd\n \ni\nn\nt\ne\nr\nn\na\nl\n \nr\ne\np\nr\ne\ns\ne\nn\nt\na\nt\ni\no\nn\n \no\nf\n \nm\nu\nl\nt\ni\nl\na\ny\ne\nr\n \nc\nl\na\ns\ns\ni\nf\ni\ne\nr\n \nn\ne\nt\nw\no\nr\nk\ns\n \np\ne\nr\nf\no\nr\nm\ns\n \n", "1\n2\n0\n2\n\n \n\ny\na\nm\n0\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n0\n6\n4\n1\n\n.\n\n5\n0\n1\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2021\n\non the geometry of generalization and memo-\nrization in deep neural networks\n\ncory stephenson*,1, suchismita padhy*,1, abhinav ganesh1, yue hui2,\n\nhanlin tang1 and sueyeon chung+,3\n\n1intel labs, 2stanford university, 3columbia university,\n\n{cory.stephenson,suchismita.padhy,abhinav.ganesh,}@intel.com, yueh@stanford.edu,\n\nhanlin.tang@intel.com, sueyeon.chung@columb", "regularizing and optimizing lstm language models\n\nstephen merity 1 nitish shirish keskar 1 richard socher 1\n\n7\n1\n0\n2\n\n \n\ng\nu\na\n7\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n1\nv\n2\n8\n1\n2\n0\n\n.\n\n8\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nrecurrent neural networks (rnns), such as long\nshort-term memory networks (lstms), serve as\na fundamental building block for many sequence\nlearning tasks,\nincluding machine translation,\nlanguage modeling, and question answering. in\nthis paper, we consider the speci\ufb01c problem of\nword-level langu", "anrv314-ne30-21\n\nari\n\n21 may 2007\n\n13:44\n\nthe neural basis of\ndecision making\n\njoshua i. gold1 and michael n. shadlen2\n1department of neuroscience, university of pennsylvania, philadelphia,\npennsylvania 19104-6074; email: jigold@mail.med.upenn.edu\n2howard hughes medical institute and department of physiology and biophysics,\nuniversity of washington, seattle, washington 98195-7290;\nemail: shadlen@u.washington.edu\n\nannu. rev. neurosci. 2007. 30:535\u201374\n\nthe annual review of neuroscience is online a", "hippocampal contributions to model-based\nplanning and spatial memory\n\narticle\n\nhighlights\nd we tested planning and spatial memory in patients with\n\nhippocampal damage and controls\n\nd patients relied less on both model-based planning and\n\nallocentric spatial memory\n\nd the planning impairment was related to the amount of\n\ndamage to right hippocampus\n\nd planning and place memory covaried in controls but were\n\nless related in patients\n\nauthors\n\noliver m. vikbladh, michael r. meager,\njohn king, ..., ", "untangling in invariant speech recognition\n\ncory stephenson\n\nintel ai lab\n\njenelle feather\n\nmit\n\nsuchismita padhy\n\nintel ai lab\n\ncory.stephenson@intel.com\n\njfeather@mit.edu\n\nsuchismita.padhy@intel.com\n\noguz elibol\nintel ai lab\n\noguz.h.elibol@intel.com\n\nhanlin tang\nintel ai lab\n\nhanlin.tang@intel.com\n\nmit/ center for brains, minds, and machines\n\njosh mcdermott\n\njhm@mit.edu\n\nsueyeon chung\n\ncolumbia university/ mit\n\nsueyeon@mit.edu\n\nabstract\n\nencouraged by the success of deep neural networks on a v", "research article\n\nan event map of memory space in the\nhippocampus\nlorena deuker1,2*, jacob ls bellmund1,3, tobias navarro schro\u00a8 der1,3,\nchristian f doeller1,3*\n\n1donders institute for brain, cognition and behaviour, radboud university\nnijmegen, nijmegen, the netherlands; 2department of neuropsychology, institute\nof cognitive neuroscience, ruhr-university bochum, bochum, germany; 3kavli\ninstitute for systems neuroscience, centre for neural computation, egil and\npauline braathen and fred kavli ce", "neuron\n\nperspective\n\nneural manifolds for the control of movement\n\njuan a. gallego,1,2 matthew g. perich,3 lee e. miller,1,3,4 and sara a. solla1,5,*\n1department of physiology, northwestern university, chicago, il 60611, usa\n2neural and cognitive engineering group, centre for robotics and automation csic-upm, arganda del rey 28500, spain\n3department of biomedical engineering, northwestern university, evanston, il 60208, usa\n4department of physical medicine and rehabilitation, northwestern univer", "original research\npublished: 21 june 2017\ndoi: 10.3389/fnins.2017.00324\n\nevent-driven random\nback-propagation: enabling\nneuromorphic deep learning\nmachines\n\nemre o. neftci 1*, charles augustine 2, somnath paul 2 and georgios detorakis 1\n\n1 neuromorphic machine intelligence laboratory, department of cognitive sciences, university of california, irvine, irvine, ca,\nunited states, 2 circuit research lab, intel corporation, hilsboro, or, united states\n\nan ongoing challenge in neuromorphic computing ", "research | reports\n\nneuronal modeling\n\nsingle-trial spike trains in parietal\ncortex reveal discrete steps during\ndecision-making\n\nkenneth w. latimer,1,2 jacob l. yates,1,2 miriam l. r. meister,2,3\nalexander c. huk,1,2,4,5 jonathan w. pillow1,2,5,6*\n\nneurons in the macaque lateral intraparietal (lip) area exhibit firing rates that appear to\nramp upward or downward during decision-making. these ramps are commonly assumed\nto reflect the gradual accumulation of evidence toward a decision threshold. ", "the journal of neuroscience, february 16, 2011 \u2022 31(7):2481\u20132487 \u2022 2481\n\ndevelopment/plasticity/repair\n\ndopaminergic projections from midbrain to primary motor\ncortex mediate motor skill learning\n\njonas a. hosp,1,2* ana pekanovic,1,2* mengia s. rioult-pedotti,1,2,3 and andreas r. luft1,2,4\n1clinical neurorehabilitation, department of neurology, university of zurich, ch-8091 zurich, switzerland, 2rehabilitation initiative and technology\ncenter zurich, ch-8008 zurich, switzerland, 3department of n", "unsupervised neural network models of the ventral\nvisual stream\n\nchengxu zhuanga,1\ndaniel l. k. yaminsa,e,f\n\n, siming yanb\n\n, aran nayebic\n\n, martin schrimpfd\n\n, michael c. franka\n\n, james j. dicarlod\n\n, and\n\nadepartment of psychology, stanford university, stanford, ca 94305; bdepartment of computer science, the university of texas at austin, austin, tx\n78712; cneurosciences phd program, stanford university, stanford, ca 94305; dbrain and cognitive sciences, massachusetts institute of technology", "4\n1\n0\n2\n\n \n\np\ne\ns\n8\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n6\n0\n9\n7\n\n.\n\n7\n0\n4\n1\n:\nv\ni\nx\nr\na\n\nhow auto-encoders could provide credit\n\nassignment in deep networks via target propagation\n\nyoshua bengio\n\nuniversit\u00b4e de montr\u00b4eal\n\ncifar fellow\n\nabstract\n\nwe propose to exploit reconstruction as a layer-local training signal for deep learn-\ning. reconstructions can be propagated in a form of target propagation playing a\nrole similar to back-propagation but helping to reduce the reliance on derivatives\nin ord", "neuron\n\nreview\n\nthe normalization model of attention\n\njohn h. reynolds1,* and david j. heeger2\n1salk institute for biological studies, la jolla, ca 92037-1099, usa\n2department of psychology and center for neural science, new york university, new york, ny 10003, usa\n*correspondence: reynolds@salk.edu\ndoi 10.1016/j.neuron.2009.01.002\n\nattention has been found to have a wide variety of effects on the responses of neurons in visual cortex. we\ndescribe a model of attention that exhibits each of these", "neuron\n\noverview\n\nneuromodulation of neuronal circuits:\nback to the future\n\neve marder1,*\n1biology department and volen center, brandeis university, waltham, ma 02454, usa\n*correspondence: marder@brandeis.edu\nhttp://dx.doi.org/10.1016/j.neuron.2012.09.010\n\nall nervous systems are subject to neuromodulation. neuromodulators can be delivered as local hormones,\nas cotransmitters in projection neurons, and through the general circulation. because neuromodulators can\ntransform the intrinsic \ufb01ring pro", "i an update to this article is included at the end\n\nneuron\n\narticle\n\nbalanced ampli\ufb01cation: a new mechanism\nof selective ampli\ufb01cation\nof neural activity patterns\n\nbrendan k. murphy1,2 and kenneth d. miller2,*\n1graduate group in biophysics, university of california, san francisco, san francisco, ca 94122, usa\n2center for theoretical neuroscience, sloan program in theoretical neuroscience, department of neuroscience, columbia university\ncollege of physicians and surgeons, new york, ny 10032, usa\n*", "8\n1\n0\n2\n\n \n\nv\no\nn\n2\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n4\nv\n4\n4\n6\n1\n0\n\n.\n\n0\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nunderstanding intermediate layers\n\nusing linear classi\ufb01er probes\n\nguillaume alain\n\nmila, university of montreal\n\nguillaume.alain.umontreal@gmail.com\n\nyoshua bengio\n\nmila, university of montreal\n\nabstract\n\nneural network models have a reputation for being black boxes. we propose to\nmonitor the features at every layer of a model and measure how suitable they are\nfor classi\ufb01cation. we use linear classi\ufb01ers,", "neural population geometry reveals the role of\n\nstochasticity in robust perception\n\njoel dapello\u2217,1,2,3, jenelle feather\u2217,1,2,4, hang le\u2217,1, tiago marques1,2,4\n\ndavid d. cox5, josh h. mcdermott1,2,4,6, james j. dicarlo1,2,4, sueyeon chung1,7,8\n\n\u2217equal contribution, ordered alphabetically\n\n1department of brain and cognitive sciences, massachusetts institute of technology\n\n2mcgovern institute for brain research, massachusetts institute of technology\n\n3school of engineering and applied sciences, ha", "supplementary material: predictive learning as a network mechanism\n\nfor extracting low-dimensional latent space representations.\n\ncontents\n\n1 predictive learning and representations in the simple \u201ccard game\u201d example: further analysis\n\n1.1 learning neural representations in the card game example . . . . . . . . . . . . . . . . . . . . . .\n1.2 analysis of the regularity of representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n2 theoretical analysis of predictive learning and", "article\nhybrid computing using a neural \nnetwork with dynamic external memory\n\ndoi:10.1038/nature20101\n\nalex graves1*, greg wayne1*, malcolm reynolds1, tim harley1, ivo danihelka1, agnieszka grabska-barwi\u0144ska1, \nsergio g\u00f3mez colmenarejo1, edward grefenstette1, tiago ramalho1, john agapiou1, adri\u00e0 puigdom\u00e8nech badia1, \nkarl moritz hermann1, yori zwols1, georg ostrovski1, adam cain1, helen king1, christopher summerfield1, phil blunsom1, \nkoray kavukcuoglu1 & demis hassabis1\n\nartificial neural netw", "research article\n\na connectome of the drosophila central \ncomplex reveals network motifs suitable \nfor flexible navigation and context- \ndependent action\u00a0selection\nbrad k hulse*\u2020, hannah haberkern*\u2020, romain franconville*\u2020, \ndaniel turner- evans*\u2020, shin- ya takemura, tanya wolff, marcella noorman, \nmarisa dreher, chuntao dan, ruchi parekh, ann m hermundstad, gerald m rubin, \nvivek jayaraman*\n\njanelia research campus, howard hughes medical institute, ashburn, united states\n\nabstract flexible behav", "3\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n6\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n6\n4\n7\n8\n0\n\n.\n\n5\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nseeing is believing: brain-inspired modular\ntraining for mechanistic interpretability\n\nziming liu, eric gan & max tegmark\ndepartment of physics, institute for ai and fundamental interactions, mit\n{zmliu,ejgan,tegmark}@mit.edu\n\nabstract\n\nwe introduce brain-inspired modular training (bimt), a method for making neural\nnetworks more modular and interpretable. inspired by brains, bimt embeds neu-\nrons in a geometric", "letter\ndiverse coupling of neurons to populations in\nsensory cortex\n\ndoi:10.1038/nature14273\n\nmichael okun1,2,3, nicholas a. steinmetz1,2,3,4, lee cossell2,5, m. florencia iacaruso2,5, ho ko2{, pe\u00b4ter bartho\u00b4 6{, tirin moore4,\nsonja b. hofer2,5, thomas d. mrsic-flogel2,5, matteo carandini31 & kenneth d. harris1,2,61\n\na large population of neurons can, in principle, produce an astro-\nnomical number of distinct firing patterns. in cortex, however,\nthese patterns lie in a space of lower dimension1\u2013", "letter\n\ncommunicated by yann le cun\n\na fast learning algorithm for deep belief nets\n\ngeoffrey e. hinton\nhinton@cs.toronto.edu\nsimon osindero\nosindero@cs.toronto.edu\ndepartment of computer science, university of toronto, toronto, canada m5s 3g4\n\nyee-whye teh\ntehyw@comp.nus.edu.sg\ndepartment of computer science, national university of singapore,\nsingapore 117543\n\nwe show how to use \u201ccomplementary priors\u201d to eliminate the explaining-\naway effects that make inference dif\ufb01cult in densely connected be", "2\n2\n0\n2\n\n \n\np\ne\ns\n5\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n4\n8\n4\n7\n0\n\n.\n\n9\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nhydra attention:\n\nefficient attention with many heads\n\ndaniel bolya1,2\u22c6, cheng-yang fu2, xiaoliang dai2, peizhao zhang2, and judy\n\nhoffman1\n\n1 georgia tech\n\n{dbolya,judy}@gatech.edu\n\n{chengyangfu,xiaoliangdai,stzpz}@fb.com\n\n2 meta ai\n\nabstract. while transformers have begun to dominate many tasks in\nvision, applying them to large images is still computationally difficult. a\nlarge reason for this is that self", "mm...\nated with the largest activation, the move-\nment field, was determined by presenting a\nsaccade target at different locations. mon-\nkeys then performed a countermanding task\nthat manipulates the ability to inhibit a\nmovement at different stages of preparation\n(fig. 2) (9). we analyzed the growth of\nmovement-related activity of fef neurons\nduring trials with different reaction times to\ntest the merits of the two models (10). to\nrepresent the form of the accumulating sig-\nnal, we derived a pa", "https://doi.org/10.1038/s41583-023-00756-z\n\n check for updates\n\nhow deep is the brain? the shallow  \nbrain hypothesis\n\nmototaka suzuki\u2009\nabstract\n\n \u20091 \n\n, cyriel m. a. pennartz\u2009\n\n \u20091 & jaan aru\u2009\n\n \u20092 \n\ndeep learning and predictive coding architectures commonly assume \nthat inference in neural networks is hierarchical. however, largely \nneglected in deep learning and predictive coding architectures is the \nneurobiological evidence that all hierarchical cortical areas, higher \nor lower, project to ", "a mathematical theory of deep convolutional\n\nneural networks for feature extraction\n\nthomas wiatowski and helmut b\u00a8olcskei, fellow, ieee\n\n1\n\n7\n1\n0\n2\n\n \nt\nc\no\n4\n2\n\n \n\n \n \n]\nt\ni\n.\ns\nc\n[\n \n \n\n3\nv\n3\n9\n2\n6\n0\n\n.\n\n2\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014deep convolutional neural networks have led to\nbreakthrough results in numerous practical machine learning\ntasks such as classi\ufb01cation of images in the imagenet data\nset, control-policy-learning to play atari games or the board\ngame go, and image captioning. many", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nfrozen  algorithms:  how  the  brain\u2019s  wiring\nfacilitates  learning\ndhruva  v  raman  and  timothy  o\u2019leary\n\nsynapses  and  neural  connectivity  are  plastic  and  shaped  by\nexperience.  but  to  what  extent  does  connectivity  itself\nin\ufb02uence  the  ability  of  a  neural  circuit  to  learn?  insights  from\noptimization  theory  and  ai  shed  light  on  how  learning  can  be\nimplemented  in  neural  circuits.  though  a", "a r t i c l e s\n\ndecorrelation and efficient coding by retinal  \nganglion cells\nxaq pitkow1 & markus meister2\nan influential theory of visual processing asserts that retinal center-surround receptive fields remove spatial correlations in  \nthe visual world, producing ganglion cell spike trains that are less redundant than the corresponding image pixels. for bright, \nhigh-contrast images, this decorrelation would enhance coding efficiency in optic nerve fibers of limited capacity. we tested the \n", "6\n1\n0\n2\n\n \n\np\ne\ns\n0\n1\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n1\n3\n3\n4\n0\n\n.\n\n7\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nrandom projections of random manifolds\n\nsubhaneil lahiri,1 peiran gao,2 and surya ganguli1\n\n1department of applied physics, stanford university, stanford, ca 94305, usa\n\n2space exploration technologies co., hawthorne, ca 90250, usa\n\nabstract\n\ninteresting data often concentrate on low dimensional smooth manifolds inside a\nhigh dimensional ambient space. random projections are a simple, powerful tool for d", "4\n1\n0\n2\n\n \nr\np\na\n9\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n4\n3\n0\n6\n\n.\n\n2\n1\n3\n1\n:\nv\ni\nx\nr\na\n\ndeep inside convolutional networks: visualising\nimage classi\ufb01cation models and saliency maps\n\nkaren simonyan\n\nandrea vedaldi\n\nandrew zisserman\n\nvisual geometry group, university of oxford\n\n{karen,vedaldi,az}@robots.ox.ac.uk\n\nabstract\n\nthis paper addresses the visualisation of image classi\ufb01cation models, learnt us-\ning deep convolutional networks (convnets). we consider two visualisation\ntechniques, based on com", "journal of machine learning research 19 (2018) 1-57\n\nsubmitted 4/18; published 11/18\n\nthe implicit bias of gradient descent on separable data\n\ndaniel soudry\nelad hoffer\nmor shpigel nacson\ndepartment of electrical engineering,technion\nhaifa, 320003, israel\nsuriya gunasekar\nnathan srebro\ntoyota technological institute at chicago\nchicago, illinois 60637, usa\n\neditor: leon bottou\n\ndaniel.soudry@gmail.com\nelad.hoffer@gmail.com\nmor.shpigel@gmail.com\n\nsuriya@ttic.edu\nnati@ttic.edu\n\nabstract\n\nwe examine", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\ncomputational  models  as  statistical  tools\ndaniel  durstewitz,  georgia  koppe1 and  hazem  toutounji1\n\ntraditionally,  models  in  statistics  are  relatively  simple  \u2018general\npurpose\u2019  quantitative  inference  tools,  while  models  in\ncomputational  neuroscience  aim  more  at  mechanistically\nexplaining  speci\ufb01c  observations.  research  on  methods  for\ninferring  behavioral  and  neural  models  from  data,  however,  has\nsho", "a large-scale neural network training \nframework for generalized estimation of \nsingle-trial population dynamics\n\nhttps://doi.org/10.1038/s41592-022-01675-0\n\nreceived: 12 january 2021\n\naccepted: 14 october 2022\n\npublished online: 28 november 2022\n\n check for updates\n\nmohammad reza keshtkaran1,12, andrew r. sedler\u2009\nraeed h. chowdhury\u2009\nhansem sohn\u2009\nchethan pandarinath\u2009\n\n \u20097, mehrdad jazayeri7, lee e. miller\u2009\n\n \u20093,4, raghav tandon\u2009\n\n \u20091,2,11 \n\n \u20091,2,12, \n\n \u20093,8,9,10 & \n\n \u20091,2, diya basrai1,5, sarah", "neuroimage 258 (2022) 119360 \n\ncontents lists available at  sciencedirect \n\nneuroimage \n\njournal homepage:  www.elsevier.com/locate/neuroimage \n\nempirical transmit \ufb01eld bias correction of t1w/t2w myelin maps \n\nmatthew f. glasser \ngraham l. baum \ndavid c. van essen \n\na , b , \u2217 , timothy s. coalson \n\nf , joonas a. autio \n\nb , nicholas a. bock \n\nb , michael p. harms \n\nc , junqian xu \ng , edward j. auerbach \nd , douglas n. greve \ng \ni , takuya hayashi \n\nd , e , \nh , essa yacoub \n\nd , \n\na departments", "flexible timing by temporal scaling of cortical \nresponses\n\njing wang\u200a\n\n\u200a1,5, devika narain1,2,3,4, eghbal a. hosseini2,5 and mehrdad jazayeri\u200a\n\n\u200a1,2,5*\n\nmusicians can perform at different tempos, speakers can control the cadence of their speech, and children can flexibly vary \ntheir temporal expectations of events. to understand the neural basis of such flexibility, we recorded from the medial frontal \ncortex of nonhuman primates trained to produce different time intervals with different effect", "the neural and computational bases \nof semantic cognition\n\nmatthew a.\u00a0lambon ralph1, elizabeth jefferies2, karalyn patterson3,4  \nand timothy t.\u00a0rogers5\nabstract | semantic cognition refers to our ability to use, manipulate and generalize knowledge \nthat is acquired over the lifespan to support innumerable verbal and non-verbal behaviours. \nthis review summarizes key findings and issues arising from a decade of research into the \nneurocognitive and neurocomputational underpinnings of this abilit", "7\n1\n0\n2\n\n \n\nb\ne\nf\n5\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n8\n7\n5\n1\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nunder review as a conference paper at iclr 2017\n\nneural architecture search with\nreinforcement learning\n\nbarret zoph\u2217, quoc v. le\ngoogle brain\n{barretzoph,qvl}@google.com\n\nabstract\n\nneural networks are powerful and \ufb02exible models that work well for many dif\ufb01-\ncult learning tasks in image, speech and natural language understanding. despite\ntheir success, neural networks are still hard to design. in this paper", "8\n1\n0\n2\n\n \n\np\ne\ns\n \n0\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n5\nv\n5\n0\n2\n7\n0\n\n.\n\n1\n1\n7\n1\n:\nv\ni\nx\nr\na\n\ndecoding of neural data using cohomological feature extraction\n\nerik rybakkena, nils baasa, and benjamin dunnb\n\na{erik.rybakken, nils.baas}@ntnu.no, department of mathematical sciences, norwegian university of science and\n\nbbenjamin.dunn@ntnu.no, kavli institute for systems neuroscience, norwegian university of science and technology,\n\ntechnology, 7491 trondheim, norway\n\n7491 trondheim, norway\n\nseptemb", "9\n1\n0\n2\n\n \n\np\ne\ns\n0\n2\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n2\nv\n8\n5\n8\n5\n0\n\n.\n\n9\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nctrl: a conditional transformer language\nmodel for controllable generation\n\nnitish shirish keskar\u2217, bryan mccann\u2217, lav r. varshney, caiming xiong, richard socher\nsalesforce research\u2020\n\nabstract\n\nlarge-scale language models show promising text generation capabilities, but\nusers cannot easily control particular aspects of the generated text. we release\nctrl, a 1.63 billion-parameter conditional transformer langu", "ieee transactions on pattern analysis and machine intelligence, vol. 24, no. 12, december 2002\n\nx\n\ninput feature selection by mutual\n\ninformation based on parzen window\n\nnojun kwak, student member,\nieee computer society, and\nchong-ho choi, member, ieee\n\nabstract\u2014mutual information is a good indicator of relevance between variables,\nand have been used as a measure in several feature selection algorithms.\nhowever, calculating the mutual information is difficult, and the performance of a\nfeature se", "hippocampus 25:1073\u20131188 (2015)\n\nhippocampal sharp wave-ripple: a cognitive biomarker\n\nfor episodic memory and planning\n\ngy\u20acorgy buzs\u0013aki*\n\nabstract:\nsharp wave ripples (spw-rs) represent the most synchro-\nnous population pattern in the mammalian brain. their excitatory output\naffects a wide area of the cortex and several subcortical nuclei. spw-rs\noccur during \u201coff-line\u201d states of the brain, associated with consummatory\nbehaviors and non-rem sleep, and are in\ufb02uenced by numerous neurotrans-\nmitt", "a r t i c l e s\n\nexplicit information for category-orthogonal object \nproperties increases along the ventral stream\nha hong1\u20133,5, daniel l k yamins1,2,5, najib j majaj1,2,4 & james j dicarlo1,2\nextensive research has revealed that the ventral visual stream hierarchically builds a robust representation for supporting \nvisual object categorization tasks. we systematically explored the ability of multiple ventral visual areas to support a variety of \n\u2018category-orthogonal\u2019 object properties such as ", "learning by neural reassociation\n\ncorrected: publisher correction\n\nmatthew d. golub\u200a\nelizabeth c. tyler-kabara\u200a\n\n\u200a1,2,3, patrick t. sadtler2,4,5, emily r. oby2,4,5, kristin m. quick2,4,5, stephen i. ryu3,6, \n\u200a1,2,9,10*\n\n\u200a4,7,8, aaron p. batista2,4,5, steven m. chase\u200a\n\n\u200a2,9,10* and byron m. yu\u200a\n\nbehavior is driven by coordinated activity across a population of neurons. learning requires the brain to change the neural \npopulation activity produced to achieve a given behavioral goal. how does popul", "closed-form continuous-time neural \nnetworks\n\nhttps://doi.org/10.1038/s42256-022-00556-7\n\nreceived: 23 march 2022\n\naccepted: 5 october 2022\n\npublished online: 15 november 2022\n\n check for updates\n\n \u20091,5 \nramin hasani\u2009\nlucas liebenwein\u2009\ndaniela rus1\n\n, mathias lechner1,2,5, alexander amini1, \n \u20091, aaron ray1, max tschaikowski3, gerald teschl\u2009\n\n \u20094 & \n\ncontinuous-time neural networks are a class of machine learning systems \nthat can tackle representation learning on spatiotemporal decision-making ", "letter\n\ncommunicated by peter dayan\n\nstimulus representation and the timing of reward-prediction\nerrors in models of the dopamine system\n\nelliot a. ludvig\nelliot@cs.ualberta.ca\nrichard s. sutton\nsutton@cs.ualberta.ca\nuniversity of alberta, edmonton, alberta t6g 2e8, canada\n\ne. james kehoe\nj.kehoe@unsw.edu.au\nuniversity of new south wales, sydney 2052, new south wales, australia\n\nthe phasic \ufb01ring of dopamine neurons has been theorized to encode a\nreward-prediction error as formalized by the tempo", "4\n1\n0\n2\n\n \n\nv\no\nn\n6\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n4\n8\n7\n1\n\n.\n\n1\n1\n4\n1\n:\nv\ni\nx\nr\na\n\nconditional generative adversarial nets\n\nd\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle\n\nmehdi mirza\n\nuniversit\u00b4e de montr\u00b4eal\nmontr\u00b4eal, qc h3c 3j7\n\nmirzamom@iro.umontreal.ca\n\nsimon osindero\nflickr / yahoo inc.\n\nsan francisco, ca 94103\n\nosindero@yahoo-inc.com\n\nabstract\n\ngenerative adversarial nets [8] were recently introduced as a novel way to train\ngenerative models. in this work we introduce the ", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nbeyond  stdp  \u2014  towards  diverse  and  functionally\nrelevant  plasticity  rules\naparna  suvrathan\n\nsynaptic  plasticity,  induced  by  the  close  temporal  association\nof  two  neural  signals,  supports  associative  forms  of  learning.\nhowever,  the  millisecond  timescales  for  association  often  do\nnot  match  the  much  longer  delays  for  behaviorally  relevant\nsignals  that  supervise  learning.  in  particular,  i", "proc. natl. acad. sci. usa\nvol. 92, pp. 3844-3848, april 1995\nneurobiology\n\ntheory of orientation tuning in visual cortex\n\n(neural networks/cross-correlations/symmetry breaking)\n\nr. ben-yishai*, r. lev bar-or*, and h. sompolinskyt\n*racah institute of physics and center for neural computation, hebrew university, jerusalem 91904, israel; and tat&t bell laboratories,\nmurray hill, nj 07974\n\ncommunicated by pierre c. hohenberg, at&t bell laboratories, murray hill, nj, december 21, 1994 (received for ", "n\ne\nw\ns\n\nf\ne\na\nt\nu\nr\ne\n\nnews feature\n\nwhat are the limits of deep learning?\n\nthe much-ballyhooed artificial intelligence approach boasts impressive feats but still falls\nshort of human brainpower. researchers are determined to figure out what\u2019s missing.\n\nm. mitchell waldrop, science writer\n\nthere\u2019s no mistaking the image: it\u2019s a banana\u2014a big,\nripe, bright-yellow banana. yet the artificial intelligence\n(ai) identifies it as a toaster, even though it was trained\nwith the same powerful and oft-publ", "\f", "neuronal dynamics regulating brain and behavioral\nstate transitions\n\narticle\n\nauthors\naaron s. andalman, vanessa m. burns,\nmatthew lovett-barron, ..., marc levoy,\nkanaka rajan, karl deisseroth\n\ncorrespondence\ndeissero@stanford.edu\n\nin brief\nbrainwide imaging in zebra\ufb01sh and\nnetwork modeling reveal that switching\nfrom active to passive coping state arises\nfrom progressive activation of habenular\nneurons in response to behavioral\nchallenge.\n\ngraphical abstract\n\nbehavioral challenge causes passive ", "article\n\nflexible sensorimotor computations through rapid\nrecon\ufb01guration of cortical dynamics\n\ngraphical abstract\n\nauthors\n\nevan d. remington, devika narain,\neghbal a. hosseini, mehrdad jazayeri\n\ncorrespondence\nmjaz@mit.edu\n\nin brief\nremington et al. employ a dynamical\nsystems perspective to understand how\nthe brain \ufb02exibly controls timed\nmovements. results suggest that\nneurons in the frontal cortex form a\nrecurrent network whose behavior is\n\ufb02exibly controlled by inputs and initial\nconditions.\n\n", "4\n1\n0\n2\n \nc\ne\nd\n4\n1\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n3\nv\n5\n1\n2\n3\n\n.\n\n9\n0\n4\n1\n:\nv\ni\nx\nr\na\n\nsequence to sequence learning\n\nwith neural networks\n\nilya sutskever\n\ngoogle\n\noriol vinyals\n\ngoogle\n\nquoc v. le\n\ngoogle\n\nilyasu@google.com\n\nvinyals@google.com\n\nqvl@google.com\n\nabstract\n\ndeep neural networks (dnns) are powerful models that have achieved excel-\nlent performance on dif\ufb01cult learning tasks. although dnns work well whenever\nlarge labeled training sets are available, they cannot be used to map sequences", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/2703232\n\nfunction optimization using connectionist reinforcement learning algorithms\n\narticle\u00a0\u00a0in\u00a0\u00a0connection science \u00b7 september 1991\n\ndoi: 10.1080/09540099108946587\u00a0\u00b7\u00a0source: citeseer\n\ncitations\n278\n\n2 authors, including:\n\njing peng\nmontclair state university\n\n162 publications\u00a0\u00a0\u00a04,056 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n3,571\n\nall content following this page was uploaded by jing peng on 0", "6\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n2\n2\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n9\n2\n1\n7\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nexplainable restricted boltzmann machines for collaborative\n\nfiltering\n\nbehnoush abdollahi\ndept. of computer engineering & computer science, knowledge discovery and web mining lab, university of\nlouisville, louisville, ky 40222, usa\n\nb.abdollahi@louisville.edu\n\nolfa nasraoui\ndept. of computer engineering & computer science, knowledge discovery and web mining lab, university of\nlouisville, louisville, ky ", "report\n\nmanipulating hippocampal place cell activity by\nsingle-cell stimulation in freely moving mice\n\ngraphical abstract\n\nauthors\n\nmaria diamantaki, stefano coletta,\nkhaled nasr, ..., philipp berens,\npatricia preston-ferrer,\nandrea burgalossi\n\ncorrespondence\nandrea.burgalossi@cin.uni-tuebingen.de\n\nin brief\nplace cells can serve as a readout of\nhippocampal memory. diamantaki et al.\nshow that the activity of single place cells\ncan be rapidly modi\ufb01ed by single-cell\nstimulation in freely moving mic", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/2022.08.15.503870\n; \n\nthis version posted august 15, 2022. \n\nthe copyright holder for this preprint\n\n(which was not certified by peer review) is the author/funder. all rights reserved. no reuse allowed without permission. \n\nflexible multitask computation in recurrent networks utilizes shared dynamical motifs \n\nlaura driscoll1, krishna shenoy1,2-7, david sussillo1,5 \n\n1 department of electrical engineering, stanford university, stanford, ca, usa \n", "e\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\ne\nr\nu\nt\na\nn\nm\no\nc\n.\ne\nr\nu\nt\na\nn\nw\nw\nw\n\n/\n\n.\n\n/\n/\n:\np\nt\nt\nh\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n4\n0\n0\n2\n\u00a9\n\n \n\na r t i c l e s\n\nmodulation of spike timing by sensory deprivation\nduring induction of cortical map plasticity\n\ntansu celikel1,2, vanessa a szostak1 & daniel e feldman1\n\ndeprivation-induced plasticity of sensory cortical maps involves long-term potentiation (ltp) and depression (ltd) of cortical\nsynapses, but how sensory deprivation triggers ltp", "vol 454 | 14 august 2008 | doi:10.1038/nature07150\n\nletters\n\ninternal brain state regulates membrane potential\nsynchrony in barrel cortex of behaving mice\njames f. a. poulet1 & carl c. h. petersen1\n\nas\n\nextracellular\n\nrevealed by\n\nrecordings of\n\ninternal brain states form key determinants for sensory percep-\ntion, sensorimotor coordination and learning1,2. a prominent\nreflection of different brain states in the mammalian central nerv-\nous system is the presence of distinct patterns of cortical s", "actor-critic algorithms \n\nvijay  r.  konda \n\njohn  n.  tsitsiklis \n\nlaboratory for  information and decision  systems , \n\nmassachusetts institute of technology, \n\ncambridge,  ma,  02139. \n\nkonda@mit.edu,  jnt@mit.edu \n\nabstract \n\nwe  propose  and  analyze  a  class  of  actor-critic  algorithms  for \nsimulation-based  optimization  of  a  markov  decision  process  over \na  parameterized  family  of randomized  stationary  policies.  these \nare two-time-scale  algorithms in  which  the critic us", "7\n1\n0\n2\n\n \n\np\ne\ns\n8\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n3\n5\n9\n1\n0\n\n.\n\n9\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nimplicit regularization in deep learning\n\nby\n\nbehnam neyshabur\n\na thesis submitted\n\nin partial ful\ufb01llment of the requirements for\n\nthe degree of\n\ndoctor of philosophy in computer science\n\nat the\n\ntoyota technological institute at chicago\n\naugust, 2017\n\nthesis committee:\n\nnathan srebro (thesis advisor),\n\nyury makarychev,\n\nruslan salakhutdinov,\ngregory shakhnarovich\n\n\f", "behavioral and brain sciences (2011) 34, 169 \u2013231\ndoi:10.1017/s0140525x10003134\n\nbayesian fundamentalism\nor enlightenment? on the explanatory\nstatus and theoretical contributions of\nbayesian models of cognition\n\nmatt jones\ndepartment of psychology and neuroscience, university of colorado,\nboulder, co 80309\nmcj@colorado.edu\n\nhttp://matt.colorado.edu\n\nbradley c. love\ndepartment of psychology, university of texas, austin, tx 78712\nbrad_love@mail.utexas.edu\n\nhttp://love.psy.utexas.edu\n\nabstract: the", "neuron\n\nreview\n\na brief history of long-term potentiation\n\nroger a. nicoll1,*\n1department of cellular and molecular pharmacology, university of california at san francisco, san francisco, ca 94158, usa\n*correspondence: roger.nicoll@ucsf.edu\nhttp://dx.doi.org/10.1016/j.neuron.2016.12.015\n\nsince the discovery of long-term potentiation (ltp) in 1973, thousands of papers have been published on this\nintriguing phenomenon, which provides a compelling cellular model for learning and memory. although lt", "psychological review1981, vol. 88, no. 2, 135-170copyright 1981 by the american psychological association, inc.0033-295x/8i/8802-oi35$00.75toward a modern theory of adaptive networks:expectation and predictionrichard s. sutton and andrew g. bartocomputer and information science departmentuniversity of massachusetts\u2014amherstmany adaptive neural network theories are based on neuronlike adaptive elementsthat can behave as single unit analogs of associative conditioning. in this articlewe develop a s", "9\n1\n0\n2\n\n \n\nb\ne\nf\n1\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n9\n4\n0\n9\n0\n\n.\n\n1\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nbiologically inspired alternatives to backpropagation through\n\ntime for learning in recurrent neural nets\n\nguillaume bellec*, franz scherr*, elias hajek , darjan salaj , robert legenstein ,\n\nand wolfgang maass\n\ninstitute for theoretical computer science, graz university of technology,\n\n*first authors\n\naustria\n\nfebruary 22, 2019\n\nabstract\n\nthe way how recurrently connected networks of spiking neurons in the br", "magazine\nr709\n\nminute and 59 minute conditions and \nthen compared this average to the 5 \nminute condition; the impact of hub \nbaiting on long-term retention was \nlarger than in the 5 minute condition \n(t(7) = 2.80, p < 0.05). \n\nwe propose that hub placement \n\nprompted memory retrieval because \nrats expected a memory test after \nhub placement. our data suggest \nthat memory retrieval shortly after \nstudying promotes subsequent \nlong-term retention. importantly, the \nmemory test occurred early in t", "\u0000\u0002\u0001\u0004\u0003\u0006\u0005\u0007\u0001\t\b\u000b\n\f\u0005\u0007\u0001\u000e\r\u0010\u000f\u0012\u0011\u0013\u000f\u0014\u0003\u0016\u0015\u0018\u0017\n\n\u0017\u001a\u0015\u001c\u001b\u000b\u0005\u0007\u0001\u001e\u001d\u001f\b\n\n \"!$#&%('*)+#-,/.\u001e021\f!*3\n\n4$57698\f:\f;<6>=@?bac5>dfe+g(hjikamlonp4\ta\tnrq@stnvu\nwo5>df=@gvh(xy5z:[hke@\\ki\u0018etxyd\f]^h(5>gj_`6a;b5>:f6a5\ncm:\f;bd\u00065zg9ef;ghihje@\\k4le+g(et:`h(e\nu>monj;b:\fp^qremi\u0018e+?b?b5zpt5sace\u0016=tt\n4$etgvet:`h(e\fu*ik=7:f=tt^=\nv2wx_\"u>yms\nz&n{x|=@;b?r}\u001cg~=ttx\\\u007fe+g9t\u0081\u0080\u00826>ea\u0083\u0084h(e+g(e+:[hve\f\u0083\u00845\u0085t[]\n\nusv2=@g96982uzqtq@s\n\u0086y\u0087\u0010\u0088\u008a\u00897\u008b7\u008c\f\u008dt\u0089\n\n\u008e\t=>h\u000657ef;<=@:2;g:[\\\u007f5>g(5z:f6\u008a5j\u008ff5>pt;g:fe\u0091\u0090\t;gh(8\u0092=yd\fg(;getgjt`;\u0093efhvg(;g\u008f\f]\fh(;get:2\\\u007fetg\u001axye`t[5z?&d", "diving into the shallows: a computational perspective on\n\nlarge-scale shallow learning\n\nsiyuan ma, mikhail belkin\n\ndepartment of computer science and engineering\n\nthe ohio state university\n\n{masi,mbelkin}@cse.ohio-state.edu\n\n7\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n7\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n2\n2\n6\n0\n1\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\njune 20, 2017\n\nabstract\n\nremarkable recent success of deep neural networks has not been easy to analyze\ntheoretically. it has been particularly hard to disentangle relative signi\ufb01can", "the journal of neuroscience, may 29, 2013 \u2022 33(22):9353\u20139363 \u2022 9353\n\ndevelopment/plasticity/repair\n\ngabaergic circuits control spike-timing-dependent\nplasticity\n\nvincent paille,1,2* elodie fino,1,2* kai du,3,4 teresa morera-herreras,1,2 sylvie perez,1,2 jeanette hellgren kotaleski,3,4,5\nand laurent venance1,2\n1team dynamic and pathophysiology of neuronal networks, center for interdisciplinary research in biology, centre national de la recherche scientifique,\nunite\u00b4 mixte de recherche 7241/inserm", "enhanced deep residual networks for single image super-resolution\n\nbee lim\n\nsanghyun son\n\nheewon kim\n\nseungjun nah\n\nkyoung mu lee\n\ndepartment of ece, asri, seoul national university, 08826, seoul, korea\n\nforestrainee@gmail.com, thstkdgus35@snu.ac.kr, ghimhw@gmail.com\n\nseungjun.nah@gmail.com, kyoungmu@snu.ac.kr\n\n7\n1\n0\n2\n\n \nl\nu\nj\n \n\n0\n1\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n1\n2\n9\n2\n0\n\n.\n\n7\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nrecent research on super-resolution has progressed with\nthe development of deep convoluti", "learning time-invariant representations for\nindividual neurons from population dynamics\n\nlu mi1,2\u2217, trung le2\u2217\n\n, tianxing he2, eli shlizerman2, uygar s\u00fcmb\u00fcl1\n\n1 allen institute for brain science\n\n2 university of washington\n\n{lu.mi,uygars}@alleninstitute.org\n\n{tle45, shlizee}@uw.edu\ngoosehe@cs.washington.edu\n\nabstract\n\nneurons can display highly variable dynamics. while such variability presumably\nsupports the wide range of behaviors generated by the organism, their gene expres-\nsions are relati", "appl. comput. harmon. anal. 21 (2006) 5\u201330\n\nwww.elsevier.com/locate/acha\n\ndiffusion maps\n\nronald r. coifman \u2217, st\u00e9phane lafon 1\n\nmathematics department, yale university, new haven, ct 06520, usa\n\nreceived 29 october 2004; revised 19 march 2006; accepted 2 april 2006\n\navailable online 19 june 2006\n\ncommunicated by the editors\n\nabstract\n\nin this paper, we provide a framework based upon diffusion processes for \ufb01nding meaningful geometric descriptions of data sets.\nwe show that eigenfunctions of mar", "equivariance through parameter-sharing\n\nsiamak ravanbakhsh 1 jeff schneider 1 barnab\u00b4as p\u00b4oczos 1\n\nabstract\n\nwe propose to study equivariance in deep neu-\nral networks through parameter symmetries. in\nparticular, given a group (cid:71) that acts discretely\non the input and output of a standard neural net-\n\nwork layer \u03c6w\u2236 (cid:82)m \u2192 (cid:82)n , we show that \u03c6w\n\nis equivariant with respect to (cid:71)-action iff (cid:71) ex-\nplains the symmetries of the network parameters\nw. inspired by this obse", "\f", "journal of machine learning research 21 (2020) 1-34\n\nsubmitted 7/19; revised 3/20; published 6/20\n\na uni\ufb01ed framework of online learning algorithms for\n\ntraining recurrent neural networks\n\nowen marschall\ncenter for neural science\nnew york university\nnew york, ny 10003, usa\nkyunghyun cho\u2217\nnew york university\ncifar azrieli global scholar\n\ncristina savin\ncenter for neural science\ncenter for data science\nnew york university\n\neditor: yoshua bengio\n\noem214@nyu.edu\n\nkyunghyun.cho@nyu.edu\n\ncsavin@nyu.ed", "multi-column deep neural networks for image classi\ufb01cation\n\ndan cires\u00b8an, ueli meier and j\u00a8urgen schmidhuber\n\nidsia-usi-supsi\n\ngalleria 2, 6928 manno-lugano, switzerland\n\n{dan,ueli,juergen}@idsia.ch\n\nabstract\n\ntraditional methods of computer vision and machine\nlearning cannot match human performance on tasks such\nas the recognition of handwritten digits or traf\ufb01c signs. our\nbiologically plausible, wide and deep arti\ufb01cial neural net-\nwork architectures can. small (often minimal) receptive\n\ufb01elds of", "article\nhigh-dimensional geometry of \npopulation responses in visual cortex\n\ncarsen stringer1,2,6*, marius pachitariu1,3,6*, nicholas steinmetz3,5, matteo carandini4,7 & kenneth d. harris3,7*\n\nhttps://doi.org/10.1038/s41586-019-1346-5\n\na neuronal population encodes information most efficiently when its stimulus responses are high-dimensional and \nuncorrelated, and most robustly when they are lower-dimensional and correlated. here we analysed the dimensionality of \nthe encoding of natural images ", "letter\n\ncommunicated by alexandre pouget\n\nmutual information, fisher information, and population\ncoding\n\nnicolas brunel\njean-pierre nadal\nlaboratoire de physique statistique de i\u2019e.n.s.,\u2020 ecole normale sup\u00b4erieure, 75231\nparis cedex 05, france\n\nin the context of parameter estimation and model selection, it is only quite\nrecently that a direct link between the fisher information and information-\ntheoretic quantities has been exhibited. we give an interpretation of this\nlink within the standard fr", "biorxiv preprint \nthe copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is \n\nhttps://doi.org/10.1101/2020.06.15.148114\n; \n\nthis version posted june 17, 2020. \n\ndoi: \n\nmade available under a\n\ncc-by-nc-nd 4.0 international license\n.\n\nuntangling stability and gain modulation in cortical circuits with\n\nmultiple interneuron classes\n\nhannah bos1,2, anne-marie oswald2,3,4, and bre", "4. w. j. gehring, m. g. h. coles, d. e. meyer, e. donchin,\n\npsychophysiology 27, s34 (1990).\n\n20. d. boussaoud, s. p. wise, exp. brain res. 95, 28 (1993).\n21. m. m. botvinick, t. s. braver, d. m. barch, c. s. carter,\n\n5. j. hohnsbein, m. falkenstein, j. hoorman, j. psycho-\n\nj. c. cohen, psychol. rev. 108, 624 (2001).\n\nphysiol. 3, 32 (1989).\n\n6. p. s. bernstein, m. k. scheffers, m. g. h. coles, j. exp.\n\npsychol. hum. percept. perform. 21, 1312 (1995).\n\n22. a. d. jones, r. cho, l. e. nystrom, j. d", "article\n\ndoi:10.1038/nature10835\n\ngain control by layer six in cortical\ncircuits of vision\n\nshawn r. olsen1*, dante s. bortone1*, hillel adesnik1 & massimo scanziani1\n\nafter entering the cerebral cortex, sensory information spreads through six different horizontal neuronal layers that are\ninterconnected by vertical axonal projections. it is believed that through these projections layers can influence each\nother\u2019s response to sensory stimuli, but the specific role that each layer has in cortical ", "simultaneous selection by object-based attention in\nvisual and frontal cortex\n\narezoo pooresmaeilia,1, jasper poorta,2, and pieter r. roelfsemaa,b,c,3\n\nathe netherlands institute for neuroscience, royal netherlands academy of arts and sciences, 1105 ba, amsterdam, the netherlands; bdepartment of\nintegrative neurophysiology, centre for neurogenomics and cognitive research, vu university amsterdam, 1081 hv, amsterdam, the netherlands;\nand cpsychiatry department, academic medical centre, university", "7\n1\n0\n2\n\n \nr\na\n\nm\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n8\n0\n6\n8\n0\n\n.\n\n2\n0\n7\n1\n:\nv\ni\nx\nr\na\n\ntowards a rigorous science of interpretable machine learning\n\nfinale doshi-velez\u2217 and been kim\u2217\n\nfrom autonomous cars and adaptive email-\ufb01lters to predictive policing systems, machine learn-\ning (ml) systems are increasingly ubiquitous; they outperform humans on speci\ufb01c tasks [mnih\net al., 2013, silver et al., 2016, hamill, 2017] and often guide processes of human understanding\nand decisions [carton et al.", "neuron\n\nreview\n\nneuromodulation of spike-timing-dependent\nplasticity: past, present, and future\n\nzuzanna brzosko,1,2,3 susanna b. mierau,1,2 and ole paulsen1,*\n1department of physiology, development and neuroscience, university of cambridge, cambridge cb2 3eg, uk\n2these authors contributed equally\n3present address: sixfold bioscience ltd, translation and innovation hub, london w12 0bz, uk\n*correspondence: op210@cam.ac.uk\nhttps://doi.org/10.1016/j.neuron.2019.05.041\n\nspike-timing-dependent synapt", "1\n2\n0\n2\n\n \nr\na\n\nm\n \n6\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n4\nv\n5\n2\n5\n0\n0\n\n.\n\n0\n1\n0\n2\n:\nv\ni\nx\nr\na\n\na biologically plausible neural network for\n\nmulti-channel canonical correlation analysis\n\ndavid lipshutz\u22171, yanis bahroun\u22171, siavash golkar\u22171,\nanirvan m. sengupta1,2, and dmitri b. chklovskii1,3\n\n1center for computational neuroscience, flatiron institute\n2department of physics and astronomy, rutgers university\n\n3neuroscience institute, nyu medical center\n\nmarch 29, 2021\n\nabstract\n\ncortical pyramidal n", "7\n1\n0\n2\n\n \nr\na\n\nm\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n2\n1\n7\n0\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\nthe concrete distribution:\na continuous relaxation of\ndiscrete random variables\n\nchris j. maddison1,2, andriy mnih1, & yee whye teh1\n1deepmind, london, united kingdom\n2university of oxford, oxford, united kingdom\ncmaddis@stats.ox.ac.uk\n\nabstract\n\nthe reparameterization trick enables optimizing large scale stochastic computa-\ntion graphs via gradient descent. the es", " \n\n \n\nadaptive optimal-control algorithms \n\nfor brainlike networks \n\n \n\n \n\n \n\nby \n\n \n\n \n\n \n\n \n\nlakshminarayan chinta venkateswararao \n\n \n\n \n\n \n\n \n\na thesis submitted in conformity with the requirements \n\nfor the degree of doctor of philosophy \n\ngraduate department of physiology \n\nuniversity of toronto \n\n\u00a9 copyright lakshminarayan chinta venkateswararao 2010 \n\n \n\n \n\n\f", "geometry of abstract learned knowledge in \nthe hippocampus\n\nhttps://doi.org/10.1038/s41586-021-03652-7\nreceived: 5 february 2020\naccepted: 18 may 2021\npublished online: 16 june 2021\n\n check for updates\n\nedward h. nieh1,6, manuel schottdorf1,6, nicolas w. freeman1, ryan j. low1, sam lewallen1, \nsue ann koay1, lucas pinto1,5, jeffrey l. gauthier1, carlos d. brody1,2,3,7\u2009\u2709 & david w. tank1,2,4,7\u2009\u2709\n\nhippocampal neurons encode physical variables1\u20137 such as space1 or auditory \nfrequency6 in cognitive ", "available online at www.sciencedirect.com\n\nsciencedirect\n\noptogenetic approaches for dissecting\nneuromodulation and gpcr signaling in neural circuits\nskylar m spangler1,2,3,4 and michael r bruchas1,2,3,4\n\noptogenetics has revolutionized neuroscience by providing\nmeans to control cell signaling with spatiotemporal control in\ndiscrete cell types. in this review, we summarize four major\nclasses of optical tools to manipulate neuromodulatory gpcr\nsignaling: opsins (including engineered chimeric rece", "article\n\nhttps://doi.org/10.1038/s41467-021-21696-1\n\nopen\n\npredictive learning as a network mechanism for\nextracting low-dimensional latent space\nrepresentations\n\nstefano recanatesi\neric shea-brown1,2,7,8\n\n1\u2709, matthew farrell2, guillaume lajoie\n\n3,4, sophie deneve5, mattia rigotti\n\n6,8 &\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\narti\ufb01cial neural networks have recently achieved many successes in solving sequential\nprocessing and planning tasks. their success is often ascribed to the emergence of the task\u2019s", "new neural activity patterns emerge with\nlong-term learning\n\nemily r. obya,b,c,d,e, matthew d. golubb,f,g,h, jay a. hennigb,i,j, alan d. degenharta,b,c,d, elizabeth c. tyler-kabaraa,k,l,m,\nbyron m. yub,f,i,n,1, steven m. chaseb,i,n,1, and aaron p. batistaa,b,c,d,1,2\n\nadepartment of bioengineering, university of pittsburgh, pittsburgh, pa 15213; bcenter for the neural basis of cognition, university of pittsburgh and\ncarnegie mellon university, pittsburgh, pa 15213; cuniversity of pittsburgh brain", "contrastive hebbian learning in the  continuous hopfield  model \n\njavier r.  movellan \n\ndepartment of psychology \ncarnegie mellon  university \n\npittsburgh,  pa 15213 \n\nemail:  jm2z+@andrew.cmu.edu \n\nabstract \n\nthis  pape.r  shows  that  contrastive hebbian, \nthe algorithm used in mean field learning, can \nbe applied to any continuous hopfield model. \nthis implies that non-logistic activation func \ntions  as well  as  self connections  are  allowed. \ncontrary  to previous  approaches,  the  learn", "0\n2\n0\n2\n\n \nr\na\n\n \n\nm\n5\n \n \n]\nn\nn\n-\ns\ni\nd\n.\nt\na\nm\n-\nd\nn\no\nc\n[\n \n \n\n2\nv\n0\n9\n8\n0\n0\n\n.\n\n1\n1\n9\n1\n:\nv\ni\nx\nr\na\n\nmean-\ufb01eld inference methods for neural networks\n\nmarylou gabri\u00b4e1,2\n\n1center for data science, new york university\n\n2center for computational mathematics, flatiron institute\n\nabstract\n\nmachine learning algorithms relying on deep neural networks recently allowed a great leap\nforward in arti\ufb01cial intelligence. despite the popularity of their applications, the e\ufb03ciency of\nthese algorithms remain", "0\n2\n0\n2\n\n \nc\ne\nd\n1\n1\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n8\n7\n8\n2\n1\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\ndirect feedback alignment scales to\n\nmodern deep learning tasks and architectures\n\njulien launay1,2\n1lighton\n\nfran\u00e7ois boniface1\n\niacopo poli1\n2lpens, \u00e9cole normale sup\u00e9rieure\n{firstname}@lighton.ai\n\nflorent krzakala1,2,3\n3 idephics, epfl\n\nlair.lighton.ai/dfa-scales\n\nabstract\n\ndespite being the workhorse of deep learning, the backpropagation algorithm is\nno panacea. it enforces sequential layer updates, t", "biorxiv preprint \n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\nhttps://doi.org/10.1101/2020.11.02.365072\n; \n\nthis version posted november 4, 2020. \n\nthe copyright holder for this preprint\n\ndoi: \n\navailable under a\n\ncc-by 4.0 international license\n.\n\nstrong coupling and local control of\ndimensionality across brain areas\n\ndavid dahmen1,*, stefano recanatesi2,*, gabriel k. ocker3,4, xiaoxuan jia3", "i an update to this article is included at the end\n\nneuropeptides 47 (2013) 439\u2013450\n\ncontents lists available at sciencedirect\n\nneuropeptides\n\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / n p e p\n\nneuropeptides in learning and memory\n\n\u00e9va borb\u00e9ly, b\u00e1lint scheich, zsuzsanna helyes\n\n\u21d1\n\ndepartment of pharmacology and pharmacotherapy, faculty of medicine, university of p\u00e9cs, szigeti u. 12, h-7624 p\u00e9cs, hungary\nmolecular pharmacology research group, j\u00e1nos szent\u00e1goth", "neuroimage 80 (2013) 80\u2013104\n\ncontents lists available at sciverse sciencedirect\n\nneuroimage\n\nj o u r n a l h o m e p a ge : w ww . e l s e v i e r . c o m/ l o c a t e / y n i m g\n\npushing spatial and temporal resolution for functional and diffusion mri\nin the human connectome project\nkamil u\u011furbil a,\u204e, junqian xu a,b, edward j. auerbach a, steen moeller a, an t. vu a, julio m. duarte-carvajalino a,\nchristophe lenglet a, xiaoping wu a, sebastian schmitter a, pierre francois van de moortele a, jo", "journal of machine learning research 15 (2014) 1929-1958\n\nsubmitted 11/13; published 6/14\n\ndropout: a simple way to prevent neural networks from\n\nover\ufb01tting\n\nnitish srivastava\ngeo\ufb00rey hinton\nalex krizhevsky\nilya sutskever\nruslan salakhutdinov\ndepartment of computer science\nuniversity of toronto\n10 kings college road, rm 3302\ntoronto, ontario, m5s 3g4, canada.\n\neditor: yoshua bengio\n\nnitish@cs.toronto.edu\nhinton@cs.toronto.edu\nkriz@cs.toronto.edu\nilya@cs.toronto.edu\nrsalakhu@cs.toronto.edu\n\nabstr", "supporting information\nyamins et al. 10.1073/pnas.1403112111\nsi text\ndata collection. we collected neural data, assessed human be-\nhavior, and tested models on a common image set. in this section,\nwe discuss this image set and the data collection methods used.\narray electrophysiology. neural data were collected in the visual\ncortex of two awake behaving rhesus macaques (macaca mulatta,\n7 and 9 kg) using parallel multielectrode array electrophysiology\nrecording systems (cerebus system; blackrock ", "optimal degrees of synaptic connectivity\n\narticle\n\nhighlights\nd sparse synaptic wiring can optimize a neural representation\n\nfor associative learning\n\nd maximizing dimension predicts the degree of connectivity for\n\ncerebellum-like circuits\n\nd supervised plasticity of input connections is needed to\n\nexploit dense wiring\n\nd performance of a hebbian readout neuron is formally related\n\nto dimension\n\nauthors\n\nashok litwin-kumar,\nkameron decker harris, richard axel,\nhaim sompolinsky, l.f. abbott\n\ncorr", "biologically-plausible backpropagation through\narbitrary timespans via local neuromodulators\n\nyuhan helena liu1,2,3,*, stephen smith2,4, stefan mihalas1,2,3, eric shea-brown1,2,3, and uygar\n\ns\u00fcmb\u00fcl2,*\n\n1department of applied mathematics, university of washington, seattle, wa, usa\n\n2allen institute for brain science, 615 westlake ave n, seattle wa, usa\n\n3computational neuroscience center, university of washington, seattle, wa, usa\n\n4department of molecular and cellular physiology, stanford univer", "j. physiol. (1980), 302, pp. 463-482\nwith 8 text-figure8\nprinted in great britain\n\n463\n\npossible mechanisms for long-lasting potentiation of\n\nsynaptic transmission in hippocampal slices from\n\nguinea-pigs\n\nby p. andersen*, s. h. sundbergt, 0. sveent, j. w. swann\u00a7\n\nand h. wigstromii\n\nfrom the *institute of neurophysiology, university of oslo, norway and\n\nthe ildepartment of physiology, university of goteborg, sweden\n\n(received 28 june 1979)\n\nsummary\n\n1. long-lasting potentiation of synaptic transm", "siam j. control optim.\nvol. 40, no. 3, pp. 681\u2013698\n\nc(cid:2) 2001 society for industrial and applied mathematics\n\nlearning algorithms for markov decision processes\n\nwith average cost\u2217\n\nj. abounadi\u2020 , d. bertsekas\u2020 , and v. s. borkar\u2021\n\nabstract. this paper gives the \ufb01rst rigorous convergence analysis of analogues of watkins\u2019s\nq-learning algorithm, applied to average cost control of \ufb01nite-state markov chains. we discuss two\nalgorithms which may be viewed as stochastic approximation counterparts of", "credit  assignment through time: \nalternatives  to backpropagation \n\nyoshua bengio * \ndept.  informatique et \n\nrecherche  operationnelle \nuniversite de  montreal \nmontreal,  qc  h3c-3j7 \n\npaolo frasconi \n\ndip.  di sistemi e  informatica \n\nuniversita di  firenze \n50139  firenze  (italy) \n\nabstract \n\nlearning  to  recognize  or  predict  sequences  using  long-term  con \ntext  has  many  applications.  however,  practical  and  theoretical \nproblems  are  found  in  training  recurrent  neural  ne", "5\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n2\n2\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n9\n7\n5\n6\n0\n\n.\n\n6\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nunderstanding neural networks through deep visualization\n\njason yosinski\ncornell university\njeff clune\nanh nguyen\nuniversity of wyoming\nthomas fuchs\njet propulsion laboratory, california institute of technology\nhod lipson\ncornell university\n\nyosinski@cs.cornell.edu\n\njeffclune@uwyo.edu\nanguyen8@uwyo.edu\n\nfuchs@caltech.edu\n\nhod.lipson@cornell.edu\n\nabstract\n\nrecent years have produced great advances in\ntraining", "deep unsupervised learning using\nnonequilibrium thermodynamics\n\n5\n1\n0\n2\n\n \n\nv\no\nn\n8\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n8\nv\n5\n8\n5\n3\n0\n\n.\n\n3\n0\n5\n1\n:\nv\ni\nx\nr\na\n\njascha sohl-dickstein\nstanford university\neric a. weiss\nuniversity of california, berkeley\nniru maheswaranathan\nstanford university\nsurya ganguli\nstanford university\n\njascha@stanford.edu\n\neaweiss@berkeley.edu\n\nnirum@stanford.edu\n\nsganguli@stanford.edu\n\nabstract\n\na central problem in machine learning involves\nmodeling complex data-sets using high", "sergey zagoruyko and nikos komodakis: wide residual networks\n\n1\n\nwide residual networks\n\nsergey zagoruyko\nsergey.zagoruyko@enpc.fr\nnikos komodakis\nnikos.komodakis@enpc.fr\n\nuniversit\u00e9 paris-est, \u00e9cole des ponts\nparistech\nparis, france\n\nabstract\n\ndeep residual networks were shown to be able to scale up to thousands of layers\nand still have improving performance. however, each fraction of a percent of improved\naccuracy costs nearly doubling the number of layers, and so training very deep resid-\nual", "graph networks as learnable physics engines for inference and control\n\nalvaro sanchez-gonzalez 1 nicolas heess 1 jost tobias springenberg 1 josh merel 1 martin riedmiller 1\n\nraia hadsell 1 peter battaglia 1\n\n8\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n4\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n4\n2\n1\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nunderstanding and interacting with everyday\nphysical scenes requires rich knowledge about\nthe structure of the world, represented either im-\nplicitly in a value or policy function, or explic-\nitl", "biorxiv preprint \nthe copyright holder for this preprint (which was\nnot certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available \n\nthis version posted december 18, 2019. \n\nhttps://doi.org/10.1101/564476\n; \n\ndoi: \n\nunder a\n\ncc-by-nc-nd 4.0 international license\n.\n\nrecurrent neural networks learn robust representations by\n\ndynamically balancing compression and expansion\n\nmatthew farrell,1, 2 stefano recanatesi,1, ", "article\nrapid signalling in distinct dopaminergic \naxons during locomotion and reward\n\ndoi:10.1038/nature18942\n\nm. w. howe1 & d. a. dombeck1\n\ndopaminergic projection axons from the midbrain to the striatum are crucial for motor control, as their degeneration \nin parkinson disease results in profound movement deficits. paradoxically, most recording methods report rapid phasic \ndopamine signalling (~100-ms bursts) in response to unpredicted rewards, with little evidence for movement-related \nsigna", "research articles\n\nstrates are processed in mfas by individual full\nsets of active sites, according to the path of acp\ndescribed above. however, these studies have\nalso shown that a minority of substrates can be\nshuttled between the two sets of active sites,\neither by acp serving both mat domains or by\ndirect interaction of acp with both ks domains\n(6, 60\u201362). in light of the large 135 \u00e5 distance\nbetween the acp anchor point located in one\ncatalytic cleft and the mat in the other, the most\nplaus", "the journal of neuroscience, october 24, 2007 \u2022 27(43):11573\u201311586 \u2022 11573\n\nbehavioral/systems/cognitive\n\nefferent association pathways from the rostral prefrontal\ncortex in the macaque monkey\n\nmichael petrides1,2 and deepak n. pandya3,4,5\n1montreal neurological institute, mcgill university, montreal, quebec, canada h3a 2b4, 2department of psychology, mcgill university, montreal, quebec,\ncanada h3a 1b1, 3departments of anatomy and neurology, boston university school of medicine, boston, massachu", "neuron\n\narticle\n\nintracellular determinants of hippocampal\nca1 place and silent cell activity\nin a novel environment\n\nje\u00b4 ro\u02c6 me epsztein,1,2,3,4 michael brecht,1 and albert k. lee1,5,*\n1bernstein center for computational neuroscience, humboldt university, berlin 10115, germany\n2institut de neurobiologie de la me\u00b4 diterrane\u00b4 e, marseille 13273, france\n3institut national de la sante\u00b4 et de la recherche me\u00b4 dicale u901, marseille 13273, france\n4universite\u00b4 de la me\u00b4 diterrane\u00b4 e aix-marseille ii, ", "the intrinsic attractor manifold and population \ndynamics of a canonical cognitive circuit across \nwaking and sleep\n\nrishidev chaudhuri\u200a\n\n\u200a1,2,3,4*, berk ger\u00e7ek\u200a\n\n\u200a1,5,9, biraj pandey1,6,9, adrien peyrache\u200a\n\n\u200a7 and ila fiete\u200a\n\n\u200a1,8*\n\nneural circuits construct distributed representations of key variables\u2014external stimuli or internal constructs of quantities rel-\nevant for survival, such as an estimate of one\u2019s location in the world\u2014as vectors of population activity. although population \nactivity ", "generalized multidimensional scaling: a framework\nfor isometry-invariant partial surface matching\n\nalexander m. bronstein, michael m. bronstein, and ron kimmel\u2020\n\ndepartment of computer science, technion israel institute of technology, haifa 32000, israel\n\nedited by alexandre j. chorin, university of california, berkeley, ca, and approved december 5, 2005 (received for review october 3, 2005)\n\nan ef\ufb01cient algorithm for isometry-invariant matching of surfaces\nis presented. the key idea is computin", "t\u0006ai\u0002i\u0002g \b\u0006\u0003d\tc\b\u0007 \u0003f ex\u0004e\u0006\b\u0007 by \u0005i\u0002i\u0001izi\u0002g c\u0003\u0002\b\u0006a\u0007\bive\n\ndive\u0006ge\u0002ce\n\ngc\u0006u tr 2000\t004\n\nge\u0003(cid:11)\u0006ey e. \u0000i\u0002\b\u0003\u0002\n\nga\b\u0007by c\u0003\u0001\u0004\t\ba\bi\u0003\u0002a\u0000 \u0006e\t\u0006\u0003\u0007cie\u0002ce u\u0002i\b\n\nu\u0002ive\u0006\u0007i\by c\u0003\u0000\u0000ege \u0004\u0003\u0002d\u0003\u0002\n\n17 \t\tee\u0002 s\u0005\ta\u0006e\b \u0004\u0003\u0002d\u0003\u0002 wc1\u0006 3ar\b u.\u0003.\n\nh\b\b\u0004://www.ga\b\u0007by.\tc\u0000.ac.\tk/\n\nab\u0007\b\u0006ac\b\n\n\u0001\b i\u0007 \u0004\u0003\u0007\u0007ib\u0000e \b\u0003 c\u0003\u0001bi\u0002e \u0001\t\u0000\bi\u0004\u0000e \u0004\u0006\u0003babi\u0000i\u0007\bic \u0001\u0003de\u0000\u0007 \u0003f \bhe \u0007a\u0001e da\ba by \u0001\t\u0000\bi\u0004\u0000yi\u0002g\n\bhei\u0006 \u0004\u0006\u0003babi\u0000i\by di\u0007\b\u0006ib\t\bi\u0003\u0002\u0007 \b\u0003ge\bhe\u0006 a\u0002d \bhe\u0002 \u0006e\u0002\u0003\u0006\u0001a\u0000izi\u0002g. thi\u0007 i\u0007 a ve\u0006y e\u00e6cie\u0002\b\nway \b\u0003 \u0001\u0003de\u0000 high\tdi\u0001e\u0002\u0007i\u0003\u0002a\u0000 da\ba which \u0007i\u0001\t\u0000\ba\u0002e\u0003\t\u0007\u0000y \u0007a\bi\u0007(cid", "l\n\ni\n\ny\ng\no\no\nn\nh\nc\ne\nt\no\nb\ne\nr\nu\nt\na\nn\nm\no\nc\n.\ne\nr\nu\nt\na\nn\nw\nw\nw\n\n/\n\n.\n\n/\n/\n:\np\nt\nt\nh\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n8\n0\n0\n2\n\u00a9\n\n \n\np r i m e r\n\nwhat is the expectation maximization \nalgorithm?\n\nchuong b do & serafim batzoglou\n\nthe expectation maximization algorithm arises in many computational biology applications that involve probabilistic \nmodels. what is it good for, and how does it work?\n\nprobabilistic models, such as hidden markov \n\nmodels  or  bayesian  networks, ", "5\n1\n0\n2\n\n \n\nv\no\nn\n9\n2\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\n1\nv\n6\n1\n9\n3\n\n.\n\n0\n1\n4\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2015\n\nmemory networks\n\njason weston, sumit chopra & antoine bordes\nfacebook ai research\n770 broadway\nnew york, usa\n{jase,spchopra,abordes}@fb.com\n\nabstract\n\nwe describe a new class of learning models called memory networks. memory\nnetworks reason with inference components combined with a long-term memory\ncomponent; they learn how to use these jointly. the long-term mem", "0\n2\n0\n2\n\n \nt\nc\no\n2\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n2\n6\n3\n1\n1\n\n.\n\n5\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nstable and expressive recurrent vision models\n\ndrew linsley\u2217, alekh k ashok\u2217, lakshmi n govindarajan\u2217, rex liu, thomas serre\n\ncarney institute for brain science\n\ndepartment of cognitive linguistic & psychological sciences\n\nbrown university\n\nprovidence, ri 02912\n\n{drew_linsley,alekh_ashok,lakshmi_govindarajan,\n\nrex_liu,thomas_serre}@brown.edu\n\nabstract\n\nprimate vision depends on recurrent processing for reliab", "70 \n\nieee  transactions  on  audio  and electroacoustics,  vol.  au-15, no.  2, june  1967 \n\nthe use of  fast fourier  transform for the estimation \nof  power spectra: a method based  on  time  aver. \n\naging over  short, modified periodograms \n\npeter  d.  welch \n\nabstract-the  use of  the  fast fourier  transform  in power spec- \ntrum analysis is described.  principal advantages of  this  method  are a \nof  computations  and  in  required  core \nreduction  in  the  number \nstorage,  and  conveni", "burst-dependent synaptic plasticity can \ncoordinate learning in hierarchical circuits\n\nalexandre payeur\u200a\nand richard naud\u200a\n\n\u200a1,2,3,12,13, jordan guerguiev4,5,13, friedemann zenke\u200a\n\u200a1,2,3,11,14\u2009\u2709\n\n\u200a6, blake a. richards\u200a\n\n\u200a7,8,9,10,14\u2009\u2709 \n\nsynaptic plasticity is believed to be a key physiological mechanism for learning. it is well established that it depends on pre- and \npostsynaptic activity. however, models that rely solely on pre- and postsynaptic activity for synaptic changes have, so far, not ", "the journal of neuroscience, november 20, 2013 \u2022 33(47):18531\u201318539 \u2022 18531\n\nsystems/circuits\n\ncontrol of basal ganglia output by direct and indirect\npathway projection neurons\n\nbenjamin s. freeze,1,2,3 alexxai v. kravitz,1 nora hammack,1 joshua d. berke,4 and anatol c. kreitzer1,2,3,5\n1gladstone institute of neurological disease, san francisco, california 94158, 2biomedical sciences program and 3medical scientist training program,\nuniversity of california, san francisco, california 94117-1049, ", "backpropagation and the brain\n\ntimothy\u00a0p.\u00a0lillicrap \ngeoffrey\u00a0hinton\n\n , adam\u00a0santoro, luke\u00a0marris, colin\u00a0j.\u00a0akerman and \n\nabstract | during learning, the brain modifies synapses to improve behaviour. in the \ncortex, synapses are embedded within multilayered networks, making it difficult \nto determine the effect of an individual synaptic modification on the behaviour of \nthe system. the backpropagation algorithm solves this problem in deep artificial \nneural networks, but historically it has bee", "research article\n\nreinforcement learning of linking and\ntracing contours in recurrent\nneural networks\ntobias brosch1, heiko neumann1*, pieter r. roelfsema2,3,4\n\na11111\n\n1 university of ulm, institute of neural information processing, ulm, germany, 2 department of vision &\ncognition, netherlands institute for neuroscience (knaw), amsterdam, the netherlands, 3 department of\nintegrative neurophysiology, center for neurogenomics and cognitive research, vu university,\namsterdam, the netherlands, 4 ps", "understanding synthetic gradients and decoupled neural interfaces\n\nwojciech marian czarnecki 1 grzegorz \u00b4swirszcz 1 max jaderberg 1 simon osindero 1 oriol vinyals 1\n\nkoray kavukcuoglu 1\n\nabstract\n\nwhen training neural networks, the use of syn-\nthetic gradients (sg) allows layers or modules\nto be trained without update locking \u2013 without\nwaiting for a true error gradient to be backprop-\nagated \u2013 resulting in decoupled neural inter-\nfaces (dnis). this unlocked ability of being\nable to update parts ", "int j comput vis (2009) 81: 317\u2013330\ndoi 10.1007/s11263-008-0178-9\n\nspectral curvature clustering (scc)\nguangliang chen \u00b7 gilad lerman\n\nreceived: 15 november 2007 / accepted: 12 september 2008 / published online: 10 december 2008\n\u00a9 the author(s) 2008. this article is published with open access at springerlink.com\n\nabstract this paper presents novel\ntechniques for im-\nproving the performance of a multi-way spectral cluster-\ning framework (govindu in proceedings of the 2005 ieee\ncomputer society co", "neuron\n\nreport\n\nnmda receptor-dependent multidendrite ca2+ spikes\nrequired for hippocampal burst firing in vivo\n\nchristine grienberger,1 xiaowei chen,1,2 and arthur konnerth1,*\n1institute of neuroscience, center for integrated protein science and synergy cluster, technical university munich, biedersteiner stra\u00dfe 29,\nmunich 80802, germany\n2brain research center, third military medical university, chongqing 400038, china\n*correspondence: arthur.konnerth@lrz.tum.de\nhttp://dx.doi.org/10.1016/j.neuro", "comparing dynamics: deep neural networks versus glassy systems\n\nmarco baity-jesi 1 levent sagun 2 3 mario geiger 3 stefano spigler 3 2 g\u00b4erard ben arous 4\n\nchiara cammarota 5 yann lecun 4 6 7 matthieu wyart 3 giulio biroli 2 8\n\nabstract\n\nwe analyze numerically the training dynamics of\ndeep neural networks (dnn) by using methods\ndeveloped in statistical physics of glassy systems.\nthe two main issues we address are (1) the com-\nplexity of the loss landscape and of the dynam-\nics within it, and (2)", "submitted to iconip\u2019\t\b\n\neffect of batch learning in multilayer\n\nneural networks\n\nkenji fukumizu\n\nemail:fuku@brain.riken.go.jp\n\nlab. for information synthesis, riken brain science institute\n\nhirosawa \u0002-\u0001, wako, saitama, \u0003\u0005\u0001-\u0000\u0001\t\b, japan\n\nabstract\n\na (cid:12)rst step of the analysis of learning in multilayer networks.\n\nthis paper discusses batch gradient descent learning in mul-\ntilayer networks with a large number of statistical training\ndata. we emphasize on the di(cid:11)erence between regular c", "associative long short-term memory\n\n6\n1\n0\n2\n\n \n\ny\na\nm\n9\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n2\n3\n0\n3\n0\n\n.\n\n2\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nivo danihelka\ngreg wayne\nbenigno uria\nnal kalchbrenner\nalex graves\ngoogle deepmind\n\ndanihelka@google.com\ngregwayne@google.com\nburia@google.com\nnalk@google.com\ngravesa@google.com\n\nabstract\n\nwe investigate a new method to augment recur-\nrent neural networks with extra memory without\nincreasing the number of network parameters.\nthe system has an associative memory based on\ncom", "central cholinergic neurons are rapidly recruited\nby reinforcement feedback\n\narticle\n\ngraphical abstract\n\nauthors\nbala\u00b4 zs hangya, sachin p. ranade, maja\nlorenc, adam kepecs\n\ncorrespondence\nhangya.balazs@koki.mta.hu (b.h.),\nkepecs@cshl.edu (a.k.)\n\nin brief\nrecordings in basal forebrain cholinergic\nneurons during behavior show\nunexpectedly fast and precisely timed\nresponses to reward and punishment that\nare modulated by outcome expectations,\nsuggesting that the central cholinergic\nsystem may also", "no free lunch from deep learning in neuroscience:\n\na case study through models of the\n\nentorhinal-hippocampal circuit\n\nrylan schaeffer\ncomputer science\n\nstanford\n\nmikail khona\n\nphysics\n\nmit\n\nrschaef@cs.stanford.edu\n\nmikail@mit.edu\n\nila rani fiete\n\nbrain and cognitive sciences\n\nmit\n\nfiete@mit.edu\n\nabstract\n\nresearch in neuroscience, as in many scientific disciplines, is undergoing a re-\nnaissance based on deep learning. unique to neuroscience, deep learning models\ncan be used not only as a tool b", "opinion\n\ntrends in cognitive sciences vol.8 no.10 october 2004\n\nthe reverse hierarchy theory of visual\nperceptual learning\nmerav ahissar1 and shaul hochstein2\n\n1department of psychology and interdisciplinary center for neural computation, hebrew university, jerusalem, 91905, israel\n2department of neurobiology and interdisciplinary center for neural computation, hebrew university, jerusalem, 91905, israel\n\ntypically referred to as perceptual learning, whereas the\nlatter are termed priming. recent", "progress in brain research, vol. 149\nissn 0079-6123\ncopyright \u00df 2005 elsevier bv. all rights reserved\n\nchapter 11\n\ndrivers and modulators from push-pull and\n\nbalanced synaptic input\n\nl.f. abbott1,* and frances s. chance2\n\n1volen center and department of biology, brandeis university, waltham, ma 02454-9110, usa\n\n2department of neurobiology and behavior, university of california at irvine,\n\nirvine, ca 92697-4550, usa\n\nabstract: in 1998, sherman and guillery proposed that there are two types of inp", "ne36ch15-shenoy\n\nari\n\n10 june 2013\n\n15:31\n\nannu. rev. neurosci. 2013. 36:337\u201359\n\nfirst published online as a review in advance on\nmay 29, 2013\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev-neuro-062111-150509\ncopyright c(cid:2) 2013 by annual reviews.\nall rights reserved\n\ncortical control of arm\nmovements: a dynamical\nsystems perspective\n\nkrishna v. shenoy,1,2 maneesh sahani,1,3\nand mark m. churchland4\n1departments of electrical engi", "research article\n\nlocal online learning in recurrent\nnetworks with random feedback\njames m murray*\n\nzuckerman mind, brain and behavior institute, columbia university, new york,\nunited states\n\nabstract recurrent neural networks (rnns) enable the production and processing of time-\ndependent signals such as those involved in movement or working memory. classic gradient-based\nalgorithms for training rnns have been available for decades, but are inconsistent with biological\nfeatures of the brain, suc", "review\n\nspecial  issue:  hippocampus  and  memory\n\nthe  hippocampal\u2013striatal  axis  in\nlearning,  prediction  and  goal-directed\nbehavior\nc.m.a.  pennartz1,  r.  ito2,3,  p.f.m.j.  verschure4,5,  f.p.  battaglia1 and  t.w.  robbins6\n\n1 cognitive  and  systems  neuroscience  group,  swammerdam  institute  for  life  sciences,  center  for  neuroscience,  sciencepark  904,\n1098  xh,  amsterdam,  the  netherlands\n2 department  of  experimental  psychology,  university  of  oxford,  south  parks  ro", "global brain dynamics embed the motor command\nsequence of caenorhabditis elegans\n\narticle\n\ngraphical abstract\n\nauthors\nsaul kato, harris s. kaplan, tina\nschro\u00a8 del, ..., eviatar yemini, shawn\nlockery, manuel zimmer\n\ncorrespondence\nzimmer@imp.ac.at\n\nin brief\nsimultaneously recording the activity of\nnearly all neurons in the c. elegans brain\nreveals that most active neurons share\ninformation by engaging in coordinated,\ndynamical network activity that\ncorresponds to the sequential assembly\nof motor", "a r t i c l e s\n\nmetamers of the ventral stream\njeremy freeman1 & eero p simoncelli1\u20133\n\nthe human capacity to recognize complex visual patterns emerges in a sequence of brain areas known as the ventral stream, beginning \nwith primary visual cortex (v1). we developed a population model for mid-ventral processing, in which nonlinear combinations of v1 \nresponses are averaged in receptive fields that grow with eccentricity. to test the model, we generated novel forms of visual metamers, \nstimuli th", "9\n1\n0\n2\n\n \n\ny\na\nm\n1\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n0\n1\n6\n4\n0\n\n.\n\n5\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nexplainable ai for trees: from local explanations to global\n\nunderstanding\n\nscott m. lundberg1, gabriel erion1,2, hugh chen1, alex degrave1,2, jordan m. prutkin3,\n\nbala nair4,5, ronit katz6, jonathan himmelfarb6, nisha bansal6, and su-in lee1,*\n\n1paul g. allen school of computer science and engineering, university of washington\n\n2medical scientist training program, university of washington\n\n3division of card", "the neural autoregressive distribution estimator\n\nhugo larochelle\n\ndepartment of computer science\n\nuniversity of toronto\n\ntoronto, canada\n\niain murray\n\nschool of informatics\n\nuniversity of edinburgh\n\nedinburgh, scotland\n\nabstract\n\nwe describe a new approach for modeling the\ndistribution of high-dimensional vectors of dis-\ncrete variables. this model is inspired by the\nrestricted boltzmann machine (rbm), which\nhas been shown to be a powerful model of\nsuch distributions. however, an rbm typi-\ncall", "0\n2\n0\n2\n\n \n\nn\na\nj\n \n\n7\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n6\n8\n7\n1\n1\n\n.\n\n5\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nputting an end to end-to-end:\n\ngradient-isolated learning of representations\n\nsindy l\u00f6we\u2217\n\npeter o\u2019connor\n\nbastiaan s. veeling\u2217\n\namlab\n\nuniversity of amsterdam\n\nloewe.sindy@gmail.com, basveeling@gmail.com\n\nabstract\n\nwe propose a novel deep learning method for local self-supervised representation\nlearning that does not require labels nor end-to-end backpropagation but exploits\nthe natural order in data instea", "optimization theory of hebbian/anti-hebbian networks for pca and\n\nwhitening\n\ncengiz pehlevan1 and dmitri b. chklovskii1\n\n5\n1\n0\n2\n\n \n\nv\no\nn\n \n0\n3\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n8\n6\n4\n9\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014 in analyzing information streamed by sensory\norgans, our brains face challenges similar to those solved\nin statistical signal processing. this suggests that biologically\nplausible implementations of online signal processing algo-\nrithms may model neural computation. here, w", "published as a conference paper at iclr 2017\n\non large-batch training for deep learning:\ngeneralization gap and sharp minima\n\nnitish shirish keskar\u2217\nnorthwestern university\nevanston, il 60208\nkeskar.nitish@u.northwestern.edu\n\ndheevatsa mudigere\nintel corporation\nbangalore, india\ndheevatsa.mudigere@intel.com\n\njorge nocedal\nnorthwestern university\nevanston, il 60208\nj-nocedal@northwestern.edu\n\nmikhail smelyanskiy\nintel corporation\nsanta clara, ca 95054\nmikhail.smelyanskiy@intel.com\n\nping tak peter", "a r t i c l e s\n\nmodel-based choices involve prospective neural activity\nbradley b doll1,2, katherine d duncan2, dylan a simon3, daphna shohamy2,4 & nathaniel d daw1,3\n\ndecisions may arise via \u2018model-free\u2019 repetition of previously reinforced actions or by \u2018model-based\u2019 evaluation, which is  \nwidely thought to follow from prospective anticipation of action consequences using a learned map or model. while choices  \nand neural correlates of decision variables sometimes reflect knowledge of their co", "neuron\n\narticle\n\noptimal control of transient dynamics\nin balanced networks supports generation\nof complex movements\n\nguillaume hennequin,1,2,* tim p. vogels,1,3,4 and wulfram gerstner1,4\n1school of computer and communication sciences and brain mind institute, school of life sciences, ecole polytechnique fe\u00b4 de\u00b4 rale de\nlausanne (epfl), 1015 lausanne, switzerland\n2department of engineering, university of cambridge, cambridge cb2 1pz, uk\n3centre for neural circuits and behaviour, university of ox", "c(cid:13)copyright 2020\nmatthew s farrell\n\n\f", "9\n1\n0\n2\n\n \nt\nc\no\n7\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n3\n4\n4\n0\n0\n\n.\n\n6\n0\n9\n1\n:\nv\ni\nx\nr\na\n\ndimensionality compression and expansion in deep\n\nneural networks\n\ncenter for computational neuroscience\n\ncenter for computational neuroscience\n\nstefano recanatesi\u2217\n\nuniversity of washington\n\nseattle, wa\n\nstefanor@uw.edu\n\nmatthew farrell\u2217\n\nuniversity of washington\n\nseattle, wa\nmsf9@uw.edu\n\nmadhu advani\n\ncenter for brain science\n\nharvard university\ncambridge, ma\n\nmadvani@fas.harvard.edu\n\ntimothy moore\n\ncenter ", "letter\nemergence of reproducible spatiotemporal activity\nduring motor learning\n\ndoi:10.1038/nature13235\n\nandrew j. peters1, simon x. chen1 & takaki komiyama1,2\n\nthe motor cortex is capable of reliably driving complex movements1,2\nyet exhibits considerable plasticity during motor learning3\u201310. these\nobservations suggest that the fundamental relationship between\nmotor cortex activity and movement may not be fixed but is instead\nshaped by learning; however, to what extent and how motor learning\nsha", "the journal of neuroscience, september 9, 2015 \u2022 35(36):12477\u201312487 \u2022 12477\n\nsystems/circuits\n\ntraveling theta waves in the human hippocampus\n\nhonghui zhang1 and joshua jacobs2\n1school of biomedical engineering, sciences, and health systems, drexel university, philadelphia, pennsylvania 19104, and 2department of biomedical\nengineering, columbia university, new york, new york 10027\n\nthe hippocampal theta oscillation is strongly correlated with behaviors such as memory and spatial navigation, but ", "supervised contrastive learning\n\nprannay khosla \u2217\ngoogle research\nyonglong tian \u2020\n\nmit\n\npiotr teterwak \u2217\u2020\nboston university\nphillip isola \u2020\n\nmit\n\nchen wang \u2020\nsnap inc.\naaron maschinot\ngoogle research\n\naaron sarna \u2021\ngoogle research\n\nce liu\n\ngoogle research\n\ndilip krishnan\ngoogle research\n\nabstract\n\ncontrastive learning applied to self-supervised representation learning has seen\na resurgence in recent years, leading to state of the art performance in the unsu-\npervised training of deep image model", "article\n\ndoi: 10.1038/s41467-017-01827-3\n\nopen\n\nsupervised learning in spiking neural networks with\nforce training\nwilten nicola1 & claudia clopath1\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\npopulations of neurons display an extraordinary diversity in the behaviors they affect and\ndisplay. machine learning techniques have recently emerged that allow us to create networks\nof model neurons that display behaviors of similar complexity. here we demonstrate the\ndirect applicability of one such technique, the force metho", "neuron\n\narticle\n\npreference distributions of primary\nmotor cortex neurons re\ufb02ect control solutions\noptimized for limb biomechanics\n\ntimothy p. lillicrap1,2,* and stephen h. scott2,3,*\n1department of pharmacology, university of oxford, oxford ox1 3qt, uk\n2centre for neuroscience studies\n3department of biomedical and molecular sciences\nqueen\u2019s university, kingston, on k7l 3n6, canada\n*correspondence: timothy.lillicrap@pharm.ox.ac.uk (t.p.l.), steve.scott@queensu.ca (s.h.s.)\nhttp://dx.doi.org/10.10", "large-scale neural recordings call for\nnew insights to link brain and behavior\nanne e. urai1,2, brent doiron3, andrew m. leifer4 and anne k. churchland1,5\n\n1 cold spring harbor laboratory, cold spring harbor, ny, usa;\n\n2 leiden university, leiden, the netherlands; 3 university of chicago, chicago, il, usa;\n4 princeton university, princeton, nj, usa; 5 university of california los angeles, ca, usa\n\n2021-07-23\n\nabstract\nneuroscientists today can measure activity from more neurons than ever before,", "2\n2\n0\n2\n\n \n\ng\nu\na\n4\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n6\n2\n9\n4\n0\n\n.\n\n7\n0\n7\n1\n:\nv\ni\nx\nr\na\n\ntheoretical insights into the optimization landscape of\n\nover-parameterized shallow neural networks\n\nmahdi soltanolkotabi\u2217 adel javanmard\u2020 jason d. lee\u2020\n\njuly 15, 2017; revised july 2022\n\nabstract\n\nin this paper we study the problem of learning a shallow arti\ufb01cial neural network that best\n\ufb01ts a training data set. we study this problem in the over-parameterized regime where the\nnumber of observations are fewe", "axiomatic attribution for deep networks\n\nmukund sundararajan * 1 ankur taly * 1 qiqi yan * 1\n\n7\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n3\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n6\n3\n1\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe study the problem of attributing the pre-\ndiction of a deep network to its input features,\na problem previously studied by several other\nworks. we identify two fundamental axioms\u2014\nsensitivity and implementation invariance that\nattribution methods ought to satisfy. we show\nthat they are not satis\ufb01ed by m", "global versus local methods\n\nin nonlinear dimensionality reduction\n\nvin de silva\n\ndepartment of mathematics,\n\nstanford university,\nstanford. ca 94305\n\nsilva@math.stanford.edu\n\njoshua b. tenenbaum\n\ndepartment of brain and cognitive sciences,\n\nmassachusetts institute of technology,\n\ncambridge. ma 02139\njbt@ai.mit.edu\n\nabstract\n\nrecently proposed algorithms for nonlinear dimensionality reduction fall\nbroadly into two categories which have different advantages and disad-\nvantages: global (isomap [1]", "frontal cortex neuron types categorically \nencode single decision variables\n\nhttps://doi.org/10.1038/s41586-019-1816-9\nreceived: 8 may 2017\naccepted: 15 october 2019\npublished online: 4 december 2019\n\njunya hirokawa1,2,5, alexander vaughan1,5, paul masset1,3,4, torben ott1 & adam kepecs1*\n\nindividual neurons in many cortical regions have been found to encode specific, \nidentifiable features of the environment or body that pertain to the function of the \nregion1\u20133. however, in frontal cortex, whi", "a sparse coding model with synaptically local plasticity\nand spiking neurons can account for the diverse shapes\nof v1 simple cell receptive fields\n\njoel zylberberg1,2*, jason timothy murphy3, michael robert deweese1,2,3\n\n1 department of physics, university of california, berkeley, california, united states of america, 2 redwood center for theoretical neuroscience, university of california,\nberkeley, california, united states of america, 3 helen wills neuroscience institute, university of califor", "somatostatin-expressing neurons  \nin cortical networks\n\njoanna urban-ciecko and alison l.\u00a0barth\n\nabstract | somatostatin-expressing gabaergic neurons constitute a major class of \ninhibitory neurons in the mammalian cortex and are characterized by dense wiring \ninto the local network and high basal firing activity that persists in the absence of \nsynaptic input. this firing provides both gaba type a receptor (gabaar)- and \ngababr-mediated inhibition that operates at fast and slow timescales. the ", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nlearning  with  three  factors:  modulating  hebbian\nplasticity  with  errors\n\u0142ukasz  ku\u0013smierz,  takuya  isomura  and  taro  toyoizumi\n\nsynaptic  plasticity  is  a  central  theme  in  neuroscience.  a\nframework  of  three-factor  learning  rules  provides  a  powerful\nabstraction,  helping  to  navigate  through  the  abundance  of\nmodels  of  synaptic  plasticity.  it  is  well-known  that  the\ndopamine  modulation  of  lear", "unsupervised cross-lingual representation learning at scale\n\nalexis conneau\u2217 kartikay khandelwal\u2217\n\nnaman goyal vishrav chaudhary guillaume wenzek francisco guzm\u00b4an\n\nedouard grave myle ott luke zettlemoyer veselin stoyanov\n\nfacebook ai\n\nabstract\n\nthis paper shows that pretraining multilingual\nlanguage models at scale leads to signi\ufb01cant\nperformance gains for a wide range of cross-\nlingual transfer tasks. we train a transformer-\nbased masked language model on one hundred\nlanguages, using more than", "published as a conference paper at iclr 2022\n\nneural networks as kernel learners: the\nsilent alignment effect\n\nalexander atanasov\u2217 , blake bordelon\u2217 & cengiz pehlevan\nharvard university\ncambridge, ma 02138, usa\n{atanasov,blake bordelon,cpehlevan}@g.harvard.edu\n\nabstract\n\nneural networks in the lazy training regime converge to kernel machines. can\nneural networks in the rich feature learning regime learn a kernel machine with\na data-dependent kernel? we demonstrate that this can indeed happen due", "research article\n\nbiologically plausible learning in recurrent\nneural networks reproduces neural\ndynamics observed during cognitive tasks\nthomas miconi*\n\nthe neurosciences institute, california, united states\n\nabstract neural activity during cognitive tasks exhibits complex dynamics that flexibly encode\ntask-relevant variables. chaotic recurrent networks, which spontaneously generate rich dynamics,\nhave been proposed as a model of cortical computation during cognitive tasks. however, existing\nme", "a unified approach to linking experimental, statistical\nand computational analysis of spike train data\n\nliang meng1, mark a. kramer1, steven j. middleton2, miles a. whittington2, uri t. eden1*\n\n1 department of mathematics and statistics, boston university, boston, massachusetts, united states of america, 2 hull york medical school, york university, york, united\nkingdom\n\nabstract\n\na fundamental issue in neuroscience is how to identify the multiple biophysical mechanisms through which neurons\ngene", "letters to nature\n\n(average directional preference difference: 138.3 ^ 26.28, range: 89.18). for cell pairs in\nwhich two directions of plaid motion ful\ufb01lled these selection criteria, neuronal correlation\ncoef\ufb01cients (nccs) were averaged.\n\n16. kreiter, a. k. & singer, w. stimulus-dependent synchronization of neuronal responses in the visual\n\ncortex of the awake macaque monkey. j. neurosci. 16, 2381\u20132396 (1996).\n\n17. thiele, a., distler, c. & hoffmann, k. p. decision-related activity in the macaqu", "psychological review\n2013, vol. 120, no. 1, 190 \u2013229\n\n\u00a9 2013 american psychological association\n0033-295x/13/$12.00 doi: 10.1037/a0030852\n\ncognitive control over learning: creating, clustering, and generalizing\n\ntask-set structure\n\nanne g. e. collins and michael j. frank\n\nbrown university\n\nlearning and executive functions such as task-switching share common neural substrates, notably\nprefrontal cortex and basal ganglia. understanding how they interact requires studying how cognitive\ncontrol faci", "proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394,\n\nuppsala, sweden, 11-16 july 2010. c(cid:13)2010 association for computational linguistics\n\n384\n\nwordrepresentations:asimpleandgeneralmethodforsemi-supervisedlearningjosephturiand\u00b4epartementd\u2019informatiqueetrechercheop\u00b4erationnelle(diro)universit\u00b4edemontr\u00b4ealmontr\u00b4eal,qu\u00b4ebec,canada,h3t1j4lastname@iro.umontreal.calevratinovdepartmentofcomputerscienceuniversityofillinoisaturbana-champaignurbana,", "deep recurrent q-learning for partially observable mdps\n\nmatthew hausknecht and peter stone\n\ndepartment of computer science\nthe university of texas at austin\n{mhauskn, pstone}@cs.utexas.edu\n\n7\n1\n0\n2\n\n \n\nn\na\nj\n \n\n1\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n7\n2\n5\n6\n0\n\n.\n\n7\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\ndeep reinforcement learning has yielded pro\ufb01cient\ncontrollers for complex tasks. however,\nthese con-\ntrollers have limited memory and rely on being able\nto perceive the complete game screen at each deci-\nsion p", "e\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\n\nsynaptic homeostasis and input selectivity follow\nfrom a calcium-dependent plasticity model\n\nluk chong yeung\u2020\u2021\u00a7, harel z. shouval\u2020\u00b6, brian s. blais\u2020\u50a8, and leon n. cooper\u2020\u2021\u2020\u2020\n\n\u2020institute for brain and neural systems, departments of \u2021physics and \u2020\u2020neuroscience, brown university, providence, ri 02912; \u00b6department of neurobiology\nand anatomy, university of texas medical school, houston, tx 77030; and \u50a8department of science and technology, bryant university, smith\ufb01eld, ri 029", "node perturbation in vanilla deep networks\n\npeter latham\n\nfebruary 25, 2019\n\n1 setup\n\nour goal is to compare node perturbation to stochastic gradient descent (sgd). we\u2019ll do\nthis in the context of a vanilla feedforward network. we\u2019ll start with a network with added\nnoise, because we\u2019ll need it for node perturbation,\n\nxk+1 = \u03c6(hk + \u03bek)\n\nhk = wk \u00b7 xk .\n\n(1a)\n\n(1b)\n\nhere everything in bold is a vector or matrix, the nonlinearity \u03c6 is pointwise, k labels layer\n(note that wk is the weight from layer ", "research article\n\narea 2 of primary somatosensory cortex\nencodes kinematics of the whole arm\nraeed h chowdhury1,2*, joshua i glaser3,4,5, lee e miller1,6,7,8*\n\n1department of biomedical engineering, northwestern university, evanston, united\nstates; 2systems neuroscience institute, university of pittsburgh, pittsburgh, united\nstates; 3interdepartmental neuroscience program, northwestern university,\nchicago, united states; 4department of statistics, columbia university, new york,\nunited states; 5z", "deep bayesian active learning with image data\n\nyarin gal 1 2 riashat islam 1 zoubin ghahramani 1 3\n\nabstract\n\neven though active learning forms an important\npillar of machine learning, deep learning tools\nare not prevalent within it. deep learning poses\nseveral dif\ufb01culties when used in an active learn-\ning setting. first, active learning (al) methods\ngenerally rely on being able to learn and update\nmodels from small amounts of data. recent ad-\nvances in deep learning, on the other hand, are no-\n", "article\n\nhuman orbitofrontal cortex represents a cognitive\nmap of state space\n\nhighlights\nd we tested a novel theory of ofc function directly in humans\n\nwith fmri\n\nauthors\n\nnicolas w. schuck, ming bo cai,\nrobert c. wilson, yael niv\n\nd multivariate pattern analysis showed evidence for state\n\nencoding in ofc\n\ncorrespondence\nnschuck@princeton.edu\n\nd performance within and across participants was related to\n\nstate encoding in ofc\n\nd the \ufb01ndings provide strong support for the state\n\nrepresentation th", "alearningalgorithmforcontinuallyrunningfully\n\nrecurrentneuralnetworks\nronaldj.williams\ncollegeofcomputerscience\nnortheasternuniversity\nboston,massachusetts\u0000\u0002\u0001\u0001\u0005\nand\ndavidzipser\ninstituteforcognitivescience\nuniversityofcalifornia,sandiego\nlajolla,california\t\u0002\u0000\t\u0003\nappearsinneuralcomputation,\u0001,pp.\u0002\u0007\u0000-\u0002\b\u0000,\u0001\t\b\t.\nabstract\ntheexactformofagradient-followinglearningalgorithmforcompletelyrecurrentnet-\nworksrunningincontinuallysampledtimeisderivedandusedasthebasisforpractical\nalgorithmsfortemporalsupervised", "front. comput. sci., 2018, 12(6): 1140\u20131148\nhttps://doi.org/10.1007/s11704-016-6107-0\n\nconvolutional adaptive denoising autoencoders for hierarchical\n\nfeature extraction\n\nqianjun zhang, lei zhang\n\nmachine intelligence laboratory, college of computer science, sichuan university, chengdu 610065, china\n\nc(cid:2) higher education press and springer-verlag gmbh germany, part of springer nature 2018\n\nabstract convolutional neural networks (cnns) are typi-\ncal structures for deep learning and are widel", "the journal of neuroscience, june 5, 2013 \u2022 33(23):9565\u20139575 \u2022 9565\n\nsystems/circuits\n\nmatching recall and storage in sequence learning with\nspiking neural networks\n\njohanni brea, walter senn, and jean-pascal pfister\ndepartment of physiology, and center for cognition, learning, and memory, university of bern, ch-3012 bern, switzerland\n\nstoring and recalling spiking sequences is a general problem the brain needs to solve. it is, however, unclear what type of biologically\nplausible learning rule i", "neuroscience\nreview article\n\nm. heilbron, m. chait / neuroscience 389 (2018) 54\u201373\n\ngreat expectations: is there evidence for predictive coding in\nauditory cortex?\n\nmicha heilbron a,b* and maria chait c\na de\u00b4partement de biologie, e\u00b4 cole normale supe\u00b4rieure, paris 75005, france\nb universite\u00b4 pierre et marie curie p6, paris 75005, france\nc ear institute, university college london, london wc1x 8ee, united kingdom\n\nabstract\u2014predictive coding is possibly one of the most in\ufb02uential, comprehensive, a", "9424 \u2022 the journal of neuroscience, july 14, 2010 \u2022 30(28):9424 \u20139430\n\nbehavioral/systems/cognitive\n\nneuronal population coding of parametric working\nmemory\n\nomri barak,1 misha tsodyks,1 and ranulfo romo2\n1department of neurobiology, weizmann institute of science, rehovot 76100, israel, and 2instituto de fisiolog\u00eda celular-neurociencias, universidad\nnacional auto\u00b4noma de me\u00b4xico, 04510 mexico, d.f., mexico\n\ncomparing two sequentially presented stimuli is a widely used experimental paradigm for s", "research article\n\ndeep neural networks as a computational\nmodel for human shape sensitivity\n\njonas kubilius*, stefania bracci, hans p. op de beeck*\n\nbrain and cognition, university of leuven (ku leuven), leuven, belgium\n\n* jonas.kubilius@ppw.kuleuven.be (jk); hans.opdebeeck@ppw.kuleuven.be (hpodb)\n\na11111\n\nabstract\n\ntheories of object recognition agree that shape is of primordial importance, but there is no\nconsensus about how shape might be represented, and so far attempts to implement a\nmodel ", ".\n\nd\ne\nv\nr\ne\ns\ne\nr\n \ns\nt\nh\ng\ni\nr\n \nl\nl\n\na\n\n \n.\n\ne\nr\nu\nt\na\nn\n \nr\ne\ng\nn\ni\nr\np\ns\n\n \nf\no\n \nt\nr\na\np\n\n \n,\n.\n\nc\nn\n\ni\n \n,\n\na\nc\n\n \n\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n7\n1\n0\n2\n \n\u00a9\n\n \n\na r t i c l e s\n\nthe spatial structure of correlated neuronal variability\nrobert rosenbaum1,2, matthew a smith3\u20135, adam kohn6,7, jonathan e rubin5,8 & brent doiron5,8\n\nshared neural variability is ubiquitous in cortical populations. while this variability is presumed to arise from overlapping  \nsynaptic input, its precise relationship to", "from spiking neuron models to linear-nonlinear models\n\nsrdjan ostojic1,2*, nicolas brunel3\n\n1 center for theoretical neuroscience, columbia university, new york, new york, united states of america, 2 laboratoire de physique statistique, cnrs, universite\u00b4 pierre\net marie curie, universite\u00b4 paris-diderot, ecole normale supe\u00b4rieure, paris, france, 3 laboratory of neurophysics and physiology, cnrs umr 8119, universite\u00b4 paris descartes,\nparis, france\n\nabstract\n\nneurons transform time-varying inputs i", "4\n1\n0\n2\n\n \n\nv\no\nn\n4\n1\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n7\n7\n0\n4\n\n.\n\n1\n1\n4\n1\n:\nv\ni\nx\nr\na\n\na framework for studying synaptic plasticity\n\nwith neural spike train data\n\nby scott w. linderman\u2217, christopher h. stock, and ryan p. adams\u2020\n\nharvard university\n\nlearning and memory in the brain are implemented by complex,\ntime-varying changes in neural circuitry. the computational rules ac-\ncording to which synaptic weights change over time are the subject\nof much research, and are not precisely understoo", "interpreting neuronal population activity by reconstruction: uni\ufb01ed\nframework with application to hippocampal place cells\n\nkechen zhang, 1 iris ginzburg, 1 bruce l. mcnaughton, 2 and terrence j. sejnowski 1,3\n1computational neurobiology laboratory, howard hughes medical institute, the salk institute for biological studies,\nla jolla, 92037; 2division of neural systems, memory, and aging and department of psychology, arizona research\nlaboratories, university of arizona, tucson, arizona 85724; and ", "3\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n1\n2\n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n2\nv\n8\n0\n7\n4\n\n.\n\n7\n0\n2\n1\n:\nv\ni\nx\nr\na\n\njournal of arti\ufb01cial intelligence research 47 (2013) 253\u2013279\n\nsubmitted 02/13; published 06/13\n\nthe arcade learning environment:\n\nan evaluation platform for general agents\n\nmarc g. bellemare\nuniversity of alberta, edmonton, alberta, canada\n\nyavar naddaf\nempirical results inc., vancouver,\nbritish columbia, canada\n\njoel veness\nmichael bowling\nuniversity of alberta, edmonton, alberta, canada\n\nmg17@cs.ualberta.ca\n", "vol 462 | 17 december 2009 | doi:10.1038/nature08577\n\nletters\n\nstably maintained dendritic spines are associated\nwith lifelong memories\nguang yang1, feng pan1 & wen-biao gan1\n\nchanges in synaptic connections are considered essential for learn-\ning and memory formation1\u20136. however, it is unknown how neural\ncircuits undergo continuous synaptic changes during learning while\nmaintaining lifelong memories. here we show, by following post-\nsynaptic dendritic spines over time in the mouse cortex7,8, th", "j neurol (2012) 259:1062\u20131070\ndoi 10.1007/s00415-011-6299-z\n\no r i g i n a l c o m m u n i c a t i o n\n\nvivid visual mental imagery in the absence\nof the primary visual cortex\n\nholly bridge \u2022 stephen harrold \u2022 emily a. holmes \u2022\nmark stokes \u2022 christopher kennard\n\nreceived: 14 september 2011 / accepted: 18 october 2011 / published online: 8 november 2011\n\u00f3 the author(s) 2011. this article is published with open access at springerlink.com\n\nabstract the role of the primary visual cortex in visual\nme", "smoothing of, and parameter estimation from, noisy\nbiophysical recordings\n\nquentin j. m. huys1,2*, liam paninski2,3\n\n1 gatsby computational neuroscience unit, university college london, london, united kingdom, 2 center for theoretical neuroscience, columbia university, new york,\nnew york, united states of america, 3 statistics department, columbia university, new york, new york, united states of america\n\nabstract\n\nbiophysically detailed models of single cells are difficult to fit to real data. r", "cerebral cortex march 2014;24:677\u2013690\ndoi:10.1093/cercor/bhs348\nadvance access publication november 11, 2012\n\nemergence of complex computational structures from chaotic neural networks\nthrough reward-modulated hebbian learning\n\ngregor m. hoerzer, robert legenstein and wolfgang maass\n\ninstitute for theoretical computer science, graz university of technology, graz, austria\n\naddress correspondence to wolfgang maass. email: maass@igi.tugraz.at\n\nthis paper addresses the question how generic microcirc", "the journal of neuroscience, august 10, 2011 \u2022 31(32):11597\u201311616 \u2022 11597\n\nbehavioral/systems/cognitive\n\nmapping human cortical areas in vivo based on myelin\ncontent as revealed by t1- and t2-weighted mri\n\nmatthew f. glasser and david c. van essen\ndepartment of anatomy and neurobiology, washington university school of medicine, st. louis, missouri 63110\n\nnoninvasively mapping the layout of cortical areas in humans is a continuing challenge for neuroscience. we present a new method of\nmapping cor", "sampling and bayes' inference in scientific modelling and robustness \nauthor(s): george e. p. box \nsource: journal of the royal statistical society. series a (general), 1980, vol. 143, no. 4 \n(1980), pp. 383-430\n \npublished by: wiley for the royal statistical society \n\nstable url: https://www.jstor.org/stable/2982063\n\njstor is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide \nrange of content in a trusted digital archive. we use informa", "neuron, vol. 21, 1163\u20131175, november, 1998, copyright \u00aa1998 by cell press\n\ninvolvement of a postsynaptic\nprotein kinase a substrate in the expression\nof homosynaptic long-term depression\n\nkimihiko kameyama,*\u00a7 hey-kyoung lee,\u2020\u00a7\nmark f. bear,\u2020 and richard l. huganir*\u2021\n* howard hughes medical institute\ndepartment of neuroscience\nthe johns hopkins university\n\nschool of medicine\n\nbaltimore, maryland 21205\n\u2020 howard hughes medical institute\ndepartment of neuroscience\nbrown university\nprovidence, rhode ", "6\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n8\n9\n4\n3\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nimproved techniques for training gans\n\ntim salimans\n\ntim@openai.com\n\nian goodfellow\n\nian@openai.com\n\nwojciech zaremba\nwoj@openai.com\n\nvicki cheung\n\nvicki@openai.com\n\nalec radford\n\nalec.radford@gmail.com\n\nxi chen\n\npeter@openai.com\n\nabstract\n\nwe present a variety of new architectural features and training procedures that we\napply to the generative adversarial networks (gans) framework. we focus on two\napplicat", "a computational model of birdsong learning by auditory \nexperience and auditory feedback \n\n* \n\nkenji d ~ y a ' . ~  and terrence j. sejnow~ki~.~ \n\n'atr human information processing research laboratories, seika, soraku, \nkyoto 619-02, japan \n2howard hughes medical institute, the salk institute for biological studies, \nla jolla, california 92037, u.s.a. \n3department of biology, university of california, san diego, la jolla, \ncalifornia 92093, u.s.a. \n\nintroduction \n\nin addition to the goal of acqu", "ifac papersonline 54-9 (2021) 285\u2013290\n\ndesigning experiments for data-driven\ndesigning experiments for data-driven\ndesigning experiments for data-driven\ndesigning experiments for data-driven\ndesigning experiments for data-driven\ndesigning experiments for data-driven\ndesigning experiments for data-driven\ndesigning experiments for data-driven\n\ncontrol of nonlinear systems\ncontrol of nonlinear systems\ncontrol of nonlinear systems\ncontrol of nonlinear systems\ncontrol of nonlinear systems\ncontrol of ", "journal of machine learning research 13 (2012) 307-361\n\nsubmitted 12/10; revised 11/11; published 2/12\n\nnoise-contrastive estimation of unnormalized statistical models,\n\nwith applications to natural image statistics\n\nmichael u. gutmann\naapo hyv\u00a8arinen\ndepartment of computer science\ndepartment of mathematics and statistics\nhelsinki institute for information technology hiit\nuniversity of helsinki, finland\n\neditor: yoshua bengio\n\nmichael.gutmann@helsinki.fi\naapo.hyvarinen@helsinki.fi\n\nabstract\n\nwe ", "neural networks 16 (2003) 5\u20139\n\nneural networks letter\n\nwww.elsevier.com/locate/neunet\n\nmeta-learning in reinforcement learning\n\nnicolas schweighofera,*, kenji doyaa,b,1\n\nacrest, japan science and technology corporation, atr, human information science laboratories, 2-2-2 hikaridai, seika-cho,\n\nbatr human information science laboratories, 2-2-2 hikaridai, seika-cho, soraku-gun, kyoto 619-0288, japan\n\nsoraku-gun, kyoto 619-0288, japan\n\nreceived 6 september 2002; accepted 10 october 2002\n\nabstract\n\n", "perception & psychophysics\n2003, 65 (7), 1136-1144\n\na gradual spread of attention\nduring mental curve tracing\n\nnetherlands ophthalmic research institute, amsterdam, the netherlands\n\nr. houtkamp\n\nand amc, graduate school neurosciences amsterdam, amsterdam, the netherlands\n\namc, graduate school neurosciences amsterdam, amsterdam, the netherlands\n\nh. spekreijse\n\nnetherlands ophthalmic research institute, amsterdam, the netherlands\n\np. r. roelfsema\n\nand amc, graduate school neurosciences amsterdam, ", "13402 \u2022 the journal of neuroscience, september 30, 2015 \u2022 35(39):13402\u201313418\n\nsystems/circuits\n\nsimple learned weighted sums of inferior temporal\nneuronal firing rates accurately predict human core\nobject recognition performance\n\nx najib j. majaj,1,2* x ha hong,1,2,3* x ethan a. solomon,1,2 and x james j. dicarlo1,2\n1department of brain and cognitive sciences, 2mcgovern institute for brain research, and 3harvard\u2013massachusetts institute of technology division of\nhealth sciences and technology, ma", "tools and resources\n\nreal-time classification of experience-\nrelated ensemble spiking patterns for\nclosed-loop applications\ndavide ciliberti1,2,3*, fre\u00b4 de\u00b4 ric michon1,2,3, fabian kloosterman1,2,3,4*\n\n1neuro-electronics research flanders, leuven, belgium; 2brain and cognition, ku\nleuven, leuven, belgium; 3vib, leuven, belgium; 4imec, leuven, belgium\n\nabstract communication in neural circuits across the cortex is thought to be mediated by\nspontaneous temporally organized patterns of population a", "a\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\nt\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nhhs public access\nauthor manuscript\nscience. author manuscript; available in pmc 2016 march 20.\n\npublished in final edited form as:\n\nscience. 2015 july 10; 349(6244): 184\u2013187. doi:10.1126/science.aaa4056.\n\nsingle-trial spike trains in parietal cortex reveal discrete steps \nduring decision-making\n\nkenneth w. latimer1,2, jacob l. yates1,2, miriam", "leading edge\n\nreview\n\nthe molecular and systems\nbiology of memory\n\neric r. kandel,1,2,3,4,* yadin dudai,5 and mark r. mayford6\n1kavli institute for brain science\n2zuckerman mind brain behavior institute\n3howard hughes medical institute\n4departments of neuroscience, biochemistry and molecular biophysics, and psychiatry\ncollege of physicians and surgeons of columbia university, new york state psychiatric institute, 1051 riverside drive, new york,\nny 10032, usa\n5department of neurobiology, weizmann", "cerebellar granule cell axons support \nhigh-dimensional representations\n\nfrederic lanore\u200a\n\n\u200a1,2,4, n. alex cayco-gajic1,3,4, harsha gurnani1, diccon coyle1 and r. angus silver\u200a\n\n\u200a1\u2009\u2709\n\nin classical theories of cerebellar cortex, high-dimensional sensorimotor representations are used to separate neuronal activity \npatterns, improving associative learning and motor performance. recent experimental studies suggest that cerebellar granule \ncell (grc) population activity is low-dimensional. to examine", "neuron\n\nreview\n\ndopaminergic modulation of synaptic transmission\nin cortex and striatum\n\nnicolas x. tritsch1 and bernardo l. sabatini1,*\n1howard hughes medical institute, department of neurobiology, harvard medical school, 220 longwood avenue, boston, ma 02115, usa\n*correspondence: bsabatini@hms.harvard.edu\nhttp://dx.doi.org/10.1016/j.neuron.2012.09.023\n\namong the many neuromodulators used by the mammalian brain to regulate circuit function and plasticity,\ndopamine (da) stands out as one of the ", "4052 \u2022 the journal of neuroscience, march 4, 2015 \u2022 35(9):4052\u2013 4064\n\nsystems/circuits\n\nrole of the indirect pathway of the basal ganglia in\nperceptual decision making\n\nwei wei,1,2 x jonathan e. rubin,3 and xiao-jing wang1,2,4\n1center for neural science, new york university, new york, new york 10003, 2department of neurobiology and kavli institute for neuroscience, yale\nuniversity school of medicine, new haven, connecticut 06520, 3department of mathematics and center for the neural basis of cogn", "perspective\n\nhttps://doi.org/10.1038/s41467-023-37180-x\n\ncatalyzing next-generation arti\ufb01cial\nintelligence through neuroai\n\nreceived: 11 september 2022\n\naccepted: 3 march 2023\n\ncheck for updates\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\n1,29\n\n, sean escola 2,29, blake richards3,4,5,6,7,\n\nanthony zador\nbence \u00f6lveczky8, yoshua bengio 3, kwabena boahen9, matthew botvinick 10,\ndmitri chklovskii11, anne churchland 12, claudia clopath 13,\njames dicarlo 14, surya ganguli15, jeff ", "a path towards autonomous machine intelligence\n\nversion 0.9.2, 2022-06-27\n\nyann lecun\n\ncourant institute of mathematical sciences, new york university yann@cs.nyu.edu\n\nmeta - fundamental ai research yann@fb.com\n\njune 27, 2022\n\nabstract\n\nhow could machines learn as e\ufb03ciently as humans and animals? how could ma-\nchines learn to reason and plan? how could machines learn representations of percepts\nand action plans at multiple levels of abstraction, enabling them to reason, predict,\nand plan at mult", "report\n\nmismatch receptive fields in mouse visual cortex\n\nhighlights\nd v1 layer 2/3 neurons signal visuomotor mismatch in local\n\nauthors\n\npawel zmarz, georg b. keller\n\nparts of the visual \ufb01eld\n\nd resolution of mismatch receptive \ufb01elds matches that of\n\nvisual receptive \ufb01elds\n\nd mismatch receptive \ufb01elds are aligned to the visual retinotopy\n\ncorrespondence\ngeorg.keller@fmi.ch\n\nin brief\nzmarz and keller show that in v1 neurons,\nsensorimotor mismatch responses, like\nvisual responses, are con\ufb01ned to s", "physics- informed machine learning\n\n 1\n\n 1,2\u2009\u2709, ioannis\u00a0g.\u00a0kevrekidis3,4, lu\u00a0lu \n\n 5, paris\u00a0perdikaris6, \n\ngeorge\u00a0em\u00a0karniadakis \nsifan\u00a0wang7 and liu\u00a0yang \nabstract | despite great progress in simulating multiphysics problems using the numerical \ndiscretization of partial differential equations (pdes), one still cannot seamlessly incorporate noisy \ndata into existing algorithms, mesh generation remains complex, and high- dimensional problems \ngoverned by parameterized pdes cannot be tackled. mor", "journal of machine learning research 15 (2014) 3743-3773\n\nsubmitted 6/13; published 11/14\n\nwhat regularized auto-encoders learn from the\n\ndata-generating distribution\n\nguillaume alain\nyoshua bengio\ndepartment of computer science and operations research\nuniversity of montreal\nmontreal, h3c 3j7, quebec, canada\n\nguillaume.alain@umontreal.ca\nyoshua.bengio@umontreal.ca\n\neditors: aaron courville, rob fergus, and christopher manning\n\nabstract\n\nwhat do auto-encoders learn about the underlying data-gener", "single cortical neurons as deep arti\ufb01cial neural\nnetworks\n\narticle\n\ngraphical abstract\n\nauthors\n\ndavid beniaguev, idan segev,\nmichael london\n\ncorrespondence\ndavid.beniaguev@gmail.com\n\nin brief\nusing a modern machine learning\napproach, we show that the i/o\ncharacteristics of cortical pyramidal\nneurons can be approximated, at the\nmillisecond resolution (single spike\nprecision), by a temporally convolutional\nneural network with \ufb01ve to eight layers.\nthis computational complexity stems\nmainly from th", "behavioural brain research 206 (2010) 157\u2013165\n\ncontents lists available at sciencedirect\n\nbehavioural brain research\n\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / b b r\n\nreview\nstructure learning in action\ndaniel a. braun a,\u2217, carsten mehring b, daniel m. wolpert a\n\na computational and biological learning lab, department of engineering, university of cambridge, uk\nb bernstein center for computational neuroscience, freiburg, germany\n\na r t i c l e\n\ni n f o\n\na b ", "imagenet classi\ufb01cation with deep convolutional\n\nneural networks\n\nalex krizhevsky\n\nuniversity of toronto\n\nkriz@cs.utoronto.ca\n\nilya sutskever\n\nuniversity of toronto\n\nilya@cs.utoronto.ca\n\ngeoffrey e. hinton\nuniversity of toronto\n\nhinton@cs.utoronto.ca\n\nabstract\n\nwe trained a large, deep convolutional neural network to classify the 1.2 million\nhigh-resolution images in the imagenet lsvrc-2010 contest into the 1000 dif-\nferent classes. on the test data, we achieved top-1 and top-5 error rates of 37.", "neuron\n\narticle\n\na modeling framework for deriving\nthe structural and functional architecture\nof a short-term memory microcircuit\n\ndimitry fisher,1 itsaso olasagasti,2,6 david w. tank,3 emre r.f. aksay,4,* and mark s. goldman1,5,*\n1center for neuroscience, university of california, davis, ca 95618, usa\n2department of neurology, zurich university hospital, 8006 zurich, switzerland\n3princeton neuroscience institute and department of molecular biology, princeton university, princeton, nj 08544, usa", "7\n1\n0\n2\n\n \n\ny\na\nm\n2\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n3\n6\n1\n2\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\nunrolled generative adversarial networks\n\nluke metz\u2217\ngoogle brain\nlmetz@google.com\n\nben poole\u2020\nstanford university\npoole@cs.stanford.edu\n\ndavid pfau\ngoogle deepmind\npfau@google.com\n\njascha sohl-dickstein\ngoogle brain\njaschasd@google.com\n\nabstract\n\nwe introduce a method to stabilize generative adversarial networks (gans) by\nde\ufb01ning the generator objective with res", "reconciling modern machine learning practice\n\nand the bias-variance trade-o\ufb00\n\nmikhail belkina, daniel hsub, siyuan maa, and soumik mandala\n\nathe ohio state university, columbus, oh\n\nbcolumbia university, new york, ny\n\nseptember 12, 2019\n\nabstract\n\nbreakthroughs in machine learning are rapidly changing science and society, yet our fun-\ndamental understanding of this technology has lagged far behind. indeed, one of the central\ntenets of the \ufb01eld, the bias-variance trade-o\ufb00, appears to be at odds w", "fundamental tradeoffs between invariance and\n\nsensitivity to adversarial perturbations\n\nflorian tram\u00e8r 1 jens behrmann 2 nicholas carlini 3 nicolas papernot 3 j\u00f6rn-henrik jacobsen 4\n\nabstract\n\nadversarial examples are malicious inputs crafted\nto induce misclassi\ufb01cation. commonly studied\nsensitivity-based adversarial examples introduce\nsemantically-small changes to an input that result\nin a different model prediction. this paper studies\na complementary failure mode, invariance-based\nadversarial e", "long-term stability of cortical population \ndynamics underlying consistent behavior\n\njuan a. gallego\u200a\nand lee e. miller\u200a\n\n\u200a1,2,7,8*, matthew g. perich\u200a\n\u200a2,4,6,9*\n\n\u200a3,8, raeed h. chowdhury\u200a\n\n\u200a4, sara a. solla\u200a\n\n\u200a2,5,9  \n\nanimals readily execute learned behaviors in a consistent manner over long periods of time, and yet no equally stable neural \ncorrelate has been demonstrated. how does the cortex achieve this stable control? using the sensorimotor system as a model \nof cortical processing, we inv", "7\n1\n0\n2\n\n \nr\na\n\nm\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n1\n6\n1\n4\n0\n\n.\n\n0\n1\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\nwhy deep neural networks for function ap-\nproximation?\n\nshiyu liang & r. srikant\ncoordinated science laboratory\nand\ndepartment of electrical and computer engineering\nuniversity of illinois at urbana-champaign\nurbana, il 61801, usa\n{sliang26,rsrikant}@illinois.edu\n\nabstract\n\nrecently there has been much interest in understanding why deep neural networks\nare preferre", "b r i e f c o m m u n i c at i o n s\n\nexperience-dependent rescaling\nof entorhinal grids\ncaswell barry1\u20134, robin hayman3,4, neil burgess1,2 &\nkathryn j jeffery3,4\n\nthe \ufb01ring pattern of entorhinal \u2018grid cells\u2019 is thought to provide\nan intrinsic metric for space. we report a strong experience-\ndependent environmental in\ufb02uence: the spatial scales of the\ngrids (which are aligned and have \ufb01xed relative sizes within\neach animal) vary parametrically with changes to a familiar\nenvironment\u2019s size and sha", "direct feedback alignment based convolutional neural network training for\n\nlow-power online learning processor\n\ndonghyeon han\n\nhoi-jun yoo\n\nschool of electrical engineering\n\nkaist, daejeon, republic of korea\n\nschool of electrical engineering\n\nkaist, daejeon, republic of korea\n\nhdh4797@kaist.ac.kr\n\nhjyoo@kaist.ac.kr\n\nabstract\n\nthere were many algorithms to substitute the back-\npropagation (bp) in the deep neural network (dnn) train-\ning. however, they could not become popular because their\ntraini", "5\n1\n0\n2\n\n \nr\na\n\nm\n6\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n1\n0\n1\n2\n0\n\n.\n\n3\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nescaping from saddle points \u2013\n\nonline stochastic gradient for tensor decomposition\n\nrong ge\u2217\n\nfurong huang \u2020\n\nchi jin \u2021\n\nyang yuan \u00a7\n\nabstract\n\nwe analyze stochastic gradient descent for optimizing non-convex functions. in many cases for non-\nconvex functions the goal is to \ufb01nd a reasonable local minimum, and the main concern is that gradient\nupdates are trapped in saddle points. in this paper we identify stri", "statistical physics of inference: thresholds and algorithms\n\nlenka zdeborov\u00b4a1,\u2217, and florent krzakala2,\u2217\n\n1 institut de physique th\u00b4eorique,\n\ncnrs, cea, universit\u00b4e paris-saclay,\n\nf-91191, gif-sur-yvette, france\n\n2 laboratoire de physique statistique, cnrs, psl universit\u00b4es\n\nuniversit\u00b4e pierre & marie curie, 75005, paris, france.\n\necole normale sup\u00b4erieure. sorbonne universit\u00b4es\n\u2217 lenka.zdeborova@cea.fr and \ufb02orent.krzakala@ens.fr\n\nmany questions of fundamental interest in today\u2019s science can be", "neural networks 99 (2018) 56\u201367\n\ncontents lists available at sciencedirect\n\nneural networks\n\njournal homepage: www.elsevier.com/locate/neunet\n\nstdp-based spiking deep convolutional neural networks for object\nrecognition\nsaeed reza kheradpisheh a,b,*, mohammad ganjtabesh a, simon j. thorpe b,\ntimoth\u00e9e masquelier b\na department of computer science, school of mathematics, statistics, and computer science, university of tehran, tehran, iran\nb cerco umr 5549, cnrs \u2013universit\u00e9 toulouse 3, france\n\na r ", "5\n1\n0\n2\n\n \nr\np\na\n7\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n1\n4\n9\n0\n0\n\n.\n\n4\n0\n5\n1\n:\nv\ni\nx\nr\na\n\na simple way to initialize recurrent networks of\n\nrecti\ufb01ed linear units\n\nquoc v. le, navdeep jaitly, geoffrey e. hinton\n\ngoogle\n\nabstract\n\nlearning long term dependencies in recurrent networks is dif\ufb01cult due to van-\nishing and exploding gradients. to overcome this dif\ufb01culty, researchers have de-\nveloped sophisticated optimization techniques and network architectures. in this\npaper, we propose a simpler solution", "fast gradient-descent methods for temporal-difference learning\n\nwith linear function approximation\n\nrichard s. sutton,\u2217 hamid reza maei,\u2217 doina precup,\u2020 shalabh bhatnagar,\u2021 david silver,\u2217 csaba szepesv\u00b4ari,\u2217\neric wiewiora\u2217\n\u2217reinforcement learning and arti\ufb01cial intelligence laboratory, university of alberta, edmonton, canada\n\u2020school of computer science, mcgill university, montreal, canada\n\u2021department of computer science and automation, indian institute of science, bangalore, india\n\nabstract\n\nsutt", "petreska et al. dynamical segmentation of single trials from population neural data\n\nnips 2011 pre-conference version\n\ndynamical segmentation of single trials\n\nfrom population neural data\n\nbiljana petreska\n\ngatsby computational neuroscience unit\n\nuniversity college london\n\nbiljana@gatsby.ucl.ac.uk\n\nbyron m. yu\nece and bme\n\ncarnegie mellon university\n\nbyronyu@cmu.edu\n\njohn p. cunningham\ndept of engineering\n\nuniversity of cambridge\njpc74@cam.ac.uk\n\ngopal santhanam, stephen i. ryu\u2020, krishna v. shen", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/51769785\n\nentorhinal cortex layer iii input to the hippocampus is crucial for temporal\nassociation memory\n\nreads\n777\n\nalex rivest\nmassachusetts institute of technology\n\n7 publications\u00a0\u00a0\u00a01,381 citations\u00a0\u00a0\u00a0\n\nsee profile\n\narticle\u00a0\u00a0in\u00a0\u00a0science \u00b7 november 2011\n\ndoi: 10.1126/science.1210125\u00a0\u00b7\u00a0source: pubmed\n\ncitations\n279\n\n5 authors, including:\n\njunghyup suh\nharvard medical school\n\n23 publicat", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/293648042\n\nbiological pattern formation: from basic mechanisms to complex structures\n\narticle\u00a0\u00a0in\u00a0\u00a0review of modern physics \u00b7 january 1994\n\ncitations\n166\n\n2 authors:\n\nandr\u00e9 koch\ndynamic phenomena s\u00e0rl\n\n16 publications\u00a0\u00a0\u00a01,238 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nsome of the authors of this publication are also working on these related projects:\n\nhypervelocity view project\n\nmorphogenesis view proje", "0\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n8\n1\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n9\n3\n7\n0\n1\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nfourier features let networks learn\n\nhigh frequency functions in low dimensional domains\n\nmatthew tancik1\u2217\n\npratul p. srinivasan1,2\u2217\n\nben mildenhall1\u2217\n\nsara fridovich-keil1\n\nnithin raghavan1 utkarsh singhal1 ravi ramamoorthi3\n\njonathan t. barron2 ren ng1\n\n1university of california, berkeley\n\n2google research\n\n3university of california, san diego\n\nabstract\n\nwe show that passing input points through a simple f", "neuron\n\nreview\n\nre-evaluating circuit mechanisms\nunderlying pattern separation\n\nn. alex cayco-gajic1 and r. angus silver1,*\n1department of neuroscience, physiology and pharmacology, university college london, gower street, london wc1e 6bt, uk\n*correspondence: a.silver@ucl.ac.uk\nhttps://doi.org/10.1016/j.neuron.2019.01.044\n\nwhen animals interact with complex environments, their neural circuits must separate overlapping patterns\nof activity that represent sensory and motor information. pattern sep", "adversarial images for the primate brain\n\nli yuan,1,2,6, will xiao,1,3,6,* gabriel kreiman,4 francis e.h. tay,5 jiashi feng,2\n\nmargaret s. livingstone1,*\n\n1department of neurobiology, harvard medical school, boston, ma 02115, u.s.a.\n\n2department of electrical and computer engineering, national university of singapore, singapore 117583\n\n3department of molecular and cellular biology, harvard university, cambridge, ma 02134, u.s.a.\n\n4department of ophthalmology, boston children\u2019s hospital, boston, ", "article\nrobust neuronal dynamics in premotor \ncortex during motor planning\n\ndoi:10.1038/nature17643\n\nnuo li1*, kayvon daie1*, karel svoboda1 & shaul druckmann1\n\nneural activity maintains representations that bridge past and future events, often over many seconds. network models \ncan produce persistent and ramping activity, but the positive feedback that is critical for these slow dynamics can cause \nsensitivity to perturbations. here we use electrophysiology and optogenetic perturbations in the ", "letter\nsynaptic amplification by dendritic spines enhances\ninput cooperativity\n\ndoi:10.1038/nature11554\n\nmark t. harnett1*, judit k. makara1,2*, nelson spruston1, william l. kath3 & jeffrey c. magee1\n\ndendritic spines are the nearly ubiquitous site of excitatory synaptic\ninput onto neurons1,2 and as such are critically positioned to influ-\nence diverse aspects of neuronal signalling. decades of theoretical\nstudies have proposed that spines may function as highly effective\nand modifiable chemical", "\u00a9 2019. published by the company of biologists ltd | journal of experimental biology (2019) 222, jeb188912. doi:10.1242/jeb.188912\n\nreview\n\norigin and role of path integration in the cognitive representations\nof the hippocampus: computational insights into open questions\nfrancesco savelli1,* and james j. knierim1,2,*\n\nabstract\npath integration is a straightforward concept with varied connotations\nthat are important to different disciplines concerned with navigation,\nsuch as ethology, cognitive s", "adversarially trained neural representations may already be as robust as\n\ncorresponding biological neural representations\n\n2\n2\n0\n2\n\n \n\nn\nu\nj\n \n9\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n8\n2\n2\n1\n1\n\n.\n\n6\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nchong guo 1 michael j. lee 1 2 3 guillaume leclerc 4 joel dapello 1 2 5 yug rao 6 aleksander madry 4 7\n\njames j. dicarlo 1 2 3\n\nabstract\n\nvisual systems of primates are the gold standard\nof robust perception. there is thus a general be-\nlief that mimicking the neural representation", "ne43ch19_mccormick\n\narjats.cls\n\njune 24, 2020\n\n9:45\n\nannual review of neuroscience\nneuromodulation of brain\nstate and behavior\n\ndavid a. mccormick,1 dennis b. nestvogel,1\nand biyu j. he2\n1institute of neuroscience, university of oregon, eugene, oregon 97403, usa;\nemail: davidmc@uoregon.edu\n2departments of neurology, neuroscience and physiology, and radiology, neuroscience\ninstitute, new york university school of medicine, new york, ny 10016, usa\n\nannu. rev. neurosci. 2020. 43:391\u2013415\n\nfirst publ", "3 august 2000 volume 406 issue no 6795 \n\nwhose scans are they, anyway?\n\nraw data are useful for researchers wishing to replicate the results of an experiment. care needs to be taken when, as\nwith brain-imaging measurements, such data can be misused or misinterpreted.\n\nlike motherhood and apple pie, the concept of sharing primary\n\ndata is widely recognized among scientists as a good thing. the\ndifficulty lies in putting this laudable aim into practice \u2014 and \nthe current controversy surrounding th", "continual learning through synaptic intelligence\n\nfriedemann zenke * 1 ben poole * 1 surya ganguli 1\n\nabstract\n\nwhile deep learning has led to remarkable ad-\nvances across diverse applications, it struggles\nin domains where the data distribution changes\nover the course of learning.\nin stark contrast,\nbiological neural networks continually adapt to\nchanging domains, possibly by leveraging com-\nplex molecular machinery to solve many tasks\nin this study, we introduce in-\nsimultaneously.\ntelligent s", "report\n\nmedial entorhinal cortex lesions only partially\ndisrupt hippocampal place cells and hippocampus-\ndependent place memory\n\ngraphical abstract\n\nauthors\n\njena b. hales, magdalene i. schlesiger, ...,\nstefan leutgeb, robert e. clark\n\ncorrespondence\nsleutgeb@ucsd.edu (s.l.),\nreclark@ucsd.edu (r.e.c.)\n\nin brief\nto address whether the medial entorhinal\ncortex (mec) is necessary for spatial cod-\ning and hippocampus-dependent mem-\nory, hales et al. selectively removed the\nentire mec in rats. this l", "current biology 23, 2121\u20132129, november 4, 2013 \u00aa2013 elsevier ltd all rights reserved http://dx.doi.org/10.1016/j.cub.2013.09.013\n\ndistinct roles of the cortical layers\nof area v1 in figure-ground segregation\n\narticle\n\nmatthew w. self,1,* timo van kerkoerle,1 hans supe` r,2,3,4\nand pieter r. roelfsema1,5,6\n1department of vision & cognition, netherlands institute for\nneuroscience, meibergdreef 47, 1105 ba, amsterdam, the\nnetherlands\n2department of basic psychology, university of barcelona,\npasse", "opinion\n\ntrends in neurosciences vol.27 no.12 december 2004\n\nthe bayesian brain: the role of\nuncertainty in neural coding and\ncomputation\n\ndavid c. knill and alexandre pouget\n\ncenter for visual science and the department of brain and cognitive science, university of rochester, ny 14627, usa\n\nto use sensory information ef\ufb01ciently to make judgments\nand guide action in the world, the brain must represent\nand use information about uncertainty in its computations\nfor perception and action. bayesian m", "3\n2\n0\n2\n\n \n\ng\nu\na\n1\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n8\n9\n8\n6\n1\n\n.\n\n8\n0\n3\n2\n:\nv\ni\nx\nr\na\n\ntransformers as support vector machines\n\ndavoud ataee tarzanagh1\u22c6 yingcong li2\u22c6\n\nchristos thrampoulidis3\n\nsamet oymak4\u2020\n\nabstract\n\nsince its inception in \u201cattention is all you need\u201d, the transformer architecture has led to revolutionary advance-\nments in natural language processing. the attention layer within the transformer admits a sequence of input tokens\nx and makes them interact through pairwise similar", "foundations and trends r(cid:13) in machine learning\nan introduction to\nvariational autoencoders\n\nsuggested citation: diederik p. kingma and max welling (2019), \u201can introduction to\nvariational autoencoders\u201d, foundations and trends r(cid:13) in machine learning: vol. xx, no.\nxx, pp 1\u201318. doi: 10.1561/xxxxxxxxx.\n\ndiederik p. kingma\ngoogle\ndurk@google.com\nmax welling\nuniversiteit van amsterdam, qualcomm\nmwelling@qti.qualcomm.com\n\n9\n1\n0\n2\n\n \nc\ne\nd\n1\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n1\n9\n6\n2\n0\n\n.\n\n6\n", "letters to nature\n\nused are important. previous estimates of parasite diversity were\nrestricted by dna sequence availability and used a limited number\nof genes and/or sequences from genbank. databases frequently\ncontain erroneous dna sequences23, and it is dif\ufb01cult to verify the\naccuracy of the sequences and the identities of the parasites from\nwhich they were obtained. although synonymous sites and non-\ncoding sequences are assumed to be relatively neutral, the degree of\ndeviation from neutrali", "structure learning in human sequential\ndecision-making\n\ndaniel e. acun\u02dc a1*, paul schrater1,2\n\n1 department of computer science and engineering, university of minnesota, minneapolis, minnesota, united states of america, 2 department of psychology, university\nof minnesota, minneapolis, minnesota, united states of america\n\nabstract\n\nstudies of sequential decision-making in humans frequently find suboptimal performance relative to an ideal actor that has\nperfect knowledge of the model of how reward", "elifesciences.org\n\nshort report\n\nretroactive modulation of spike timing-\ndependent plasticity by dopamine\nzuzanna brzosko, wolfram schultz, ole paulsen*\n\ndepartment of physiology, development and neuroscience, physiological\nlaboratory, university of cambridge, cambridge, united kingdom\n\nabstract most reinforcement learning models assume that the reward signal arrives after the\nactivity that led to the reward, placing constraints on the possible underlying cellular mechanisms.\nhere we show that d", "proc. natl acad. sci. usa\nvol. 79, pp. 2554-2558, april 1982\nbiophysics\n\nneural networks and physical systems with emergent collective\ncomputational abilities\n\n(associative memory/parallel processing/categorization/content-addressable memory/fail-soft devices)\n\nj. j. hopfield\ndivision of chemistry and biology, california institute of technology, pasadena, california 91125; and bell laboratories, murray hill, new jersey 07974\ncontributed by john j. hopfweld, january 15, 1982\n\ncomputational proper", "9\n1\n0\n2\n\n \n\nn\na\nj\n \n\n0\n2\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n4\nv\n0\n5\n1\n8\n0\n\n.\n\n4\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ndeep learning in spiking neural networks\n\namirhossein tavanaei\u2217, masoud ghodrati\u2020, saeed reza kheradpisheh\u2021,\n\ntimoth\u00b4ee masquelier\u00a7 and anthony maida\u2217\n\n\u2217center for advanced computer studies, university of louisiana at lafayette\n\n\u2020department of physiology, monash university, clayton, vic, australia\n\n\u2021department of computer science, faculty of mathematical sciences and computer,\n\nlafayette, louisiana, la 70504,", "reinforcement learning using a continuous time\nactor-critic framework with spiking neurons\n\nnicolas fre\u00b4 maux1, henning sprekeler1,2, wulfram gerstner1*\n1 school of computer and communication sciences and school of life sciences, brain mind institute, e\u00b4cole polytechnique fe\u00b4de\u00b4rale de lausanne, 1015 lausanne epfl,\nswitzerland, 2 theoretical neuroscience lab, institute for theoretical biology, humboldt-universita\u00a8t zu berlin, berlin, germany\n\nabstract\n\nanimals repeat rewarded behaviors, but the ", "bert: pre-training of deep bidirectional transformers for\n\nlanguage understanding\n\njacob devlin ming-wei chang kenton lee kristina toutanova\n\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\ngoogle ai language\n\n9\n1\n0\n2\n\n \n\ny\na\nm\n4\n2\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n2\nv\n5\n0\n8\n4\n0\n\n.\n\n0\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe introduce a new language representa-\ntion model called bert, which stands for\nbidirectional encoder representations from\ntransformers. unlike recent language repre-\nsentation models", "neurobiology of learning and memory 70, 119\u2013136 (1998)\narticle no. nl983843\n\nthe basal ganglia and chunking\n\nof action repertoires\n\nann m. graybiel\n\ndepartment of brain and cognitive sciences, massachusetts institute of technology,\n\ncambridge, massachusetts 02139\n\nthe basal ganglia have been shown to contribute to habit and stimulus\u2013response\n(s\u2013r) learning. these forms of learning have the property of slow acquisition and, in\nhumans, can occur without conscious awareness. this paper proposes tha", "neuroimage 48 (2009) 21\u201328\n\ncontents lists available at sciencedirect\n\nneuroimage\n\nj o u r n a l h o m e p a g e : w w w. e l s e v i e r. c o m / l o c a t e / y n i m g\n\nage-associated alterations in cortical gray and white matter signal intensity and gray\nto white matter contrast\nd.h. salat a,b,\u204e, s.y. lee a,c, a.j. van der kouwe a,b, d.n. greve a,b, b. fischl a,b,d, h.d. rosas a,c\na department of radiology, massachusetts general hospital, boston, ma, usa\nb athinoula a. martinos center for bi", "rstb.royalsocietypublishing.org\n\nresearch\n\ncite this article: lu h, park h, poo m-m.\n2014 spike-timing-dependent bdnf secretion\nand synaptic plasticity. phil. trans. r. soc. b\n369: 20130132.\nhttp://dx.doi.org/10.1098/rstb.2013.0132\n\none contribution of 35 to a discussion meeting\nissue \u2018synaptic plasticity in health and disease\u2019.\n\nsubject areas:\nneuroscience\n\nkeywords:\nstdp, bdnf, synaptic plasticity, tltp\n\nauthor for correspondence:\nmu-ming poo\ne-mail: mpoo@berkeley.edu\n\n\u2020present address: depart", "network: computation in neural systems\n\nissn: 0954-898x (print) 1361-6536 (online) journal homepage: https://www.tandfonline.com/loi/inet20\n\nmaximum likelihood estimation of cascade point-\nprocess neural encoding models\n\nliam paninski\n\nto cite this article: liam paninski (2004) maximum likelihood estimation of cascade point-\nprocess neural encoding models, network: computation in neural systems, 15:4, 243-262,\ndoi: 10.1088/0954-898x_15_4_002\n\nto link to this article:  https://doi.org/10.1088/095", "sequence parallelism: long sequence training from\n\nsystem perspective\n\nshenggui li1,\n\nfuzhao xue1\u2020 chaitanya baranwal1 yongbin li1 yang you1\n\n1department of computer science, national university of singapore\n\n2\n2\n0\n2\n\n \n\ny\na\nm\n1\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n2\n1\n3\n1\n\n.\n\n5\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\ntransformer achieves promising results on various tasks. however, self-attention\nsuffers from quadratic memory requirements with respect to the sequence length.\nexisting work focuses on reduci", "research article\nstimulus-driven population activity patterns\nin macaque primary visual cortex\n\nbenjamin r. cowley1,2, matthew a. smith2,3,4,5, adam kohn6,7, byron m. yu2,8,9*\n\n1 machine learning department, carnegie mellon university, pittsburgh, pennsylvania, united states of\namerica, 2 center for neural basis of cognition, carnegie mellon university, pittsburgh, pennsylvania,\nunited states of america, 3 department of ophthalmology, university of pittsburgh, pittsburgh,\npennsylvania, united st", "annu. rev. neurosci. 2000. 23:649\u2013711\ncopyright q 2000 by annual reviews. all rights reserved\n\nsynaptic plasticity and memory:\nan evaluation of the hypothesis\n\ns. j. martin, p. d. grimwood, and r. g. m. morris\ndepartment and centre for neuroscience, the university of edinburgh, crichton street,\nedinburgh, eh8 9le, united kingdom, e-mail: stephen.martin@ed.ac.uk,\npaulg@cfn.ed.ac.uk, r.g.m.morris@ed.ac.uk\n\nkey words ltp, ltd, depotentiation, learning, hippocampus, amygdala, cortex\nabstract changin", "siam j. optim.\nvol. 19, no. 4, pp. 1574\u20131609\n\nc(cid:2) 2009 society for industrial and applied mathematics\n\nrobust stochastic approximation approach to\n\nstochastic programming\u2217\n\n\u2020\na. nemirovski\n\n, a. juditsky\n\n\u2021\n\n, g. lan\n\n\u2020\n\n, and a. shapiro\n\n\u2020\n\nabstract. in this paper we consider optimization problems where the objective function is given\nin a form of the expectation. a basic di\ufb03culty of solving such stochastic optimization problems is\nthat the involved multidimensional integrals (expectations", "article\n\ndoi: 10.1038/s41467-017-01109-y\n\nopen\n\nsparse synaptic connectivity is required for\ndecorrelation and pattern separation in feedforward\nnetworks\nn. alex cayco-gajic1, claudia clopath2 & r. angus silver\n\n1\n\nfunction of the brain. the divergent feedforward\npattern separation is a fundamental\nnetworks thought to underlie this computation are widespread, yet exhibit remarkably similar\nsparse synaptic connectivity. marr-albus theory postulates that such networks separate\noverlapping activity", "a biologically plausible neural network for\n\nslow feature analysis\n\ndavid lipshutz\u21e41\n\ncharlie windolf\u21e41,2\n\nsiavash golkar 1\n\ndmitri b. chklovskii 1,3\n\n1 center for computational neuroscience, flatiron institute\n\n2 department of statistics, columbia university\n3 neuroscience institute, nyu medical center\n\n{dlipshutz,sgolkar,dchklovskii}@flatironinstitute.org\n\nc.windolf@columbia.edu\n\nabstract\n\nlearning latent features from time series data is an important problem in both\nmachine learning and brain", "neuron\n\nreport\n\nsensorimotor mismatch signals\nin primary visual cortex of the behaving mouse\n\ngeorg b. keller,1,2,* tobias bonhoeffer,1 and mark hu\u00a8 bener1,*\n1max planck institute of neurobiology, 82152 munich-martinsried, germany\n2present address: friedrich miescher institute for biomedical research, maulbeerstrasse 66, ch-4058 basel, switzerland\n*correspondence: georg@neuro.mpg.de (g.b.k.), mark@neuro.mpg.de (m.h.)\ndoi 10.1016/j.neuron.2012.03.040\n\nsummary\n\nstudies in anesthetized animals have", "review\n\nj neurophysiol 106: 1068 \u20131077, 2011.\nfirst published june 15, 2011; doi:10.1152/jn.00429.2011.\n\ndistinct functions for direct and transthalamic corticocortical connections\n\ns. murray sherman1 and r. w. guillery2\n1department of neurobiology, the university of chicago, chicago, illinois; and 2medical research council anatomical\nneuropharmacology unit, oxford, united kingdom\n\nsubmitted 11 may 2011; accepted in \ufb01nal form 9 june 2011\n\nsherman sm, guillery rw. distinct functions for direct an", "8\n1\n0\n2\n\n \n\np\ne\ns\n1\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n4\n8\n1\n4\n0\n\n.\n\n9\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nsearching for ef\ufb01cient multi-scale\n\narchitectures for dense image prediction\n\nliang-chieh chen maxwell d. collins\n\nbarret zoph\n\nflorian schroff\n\nyukun zhu\n\nhartwig adam\n\ngeorge papandreou\n\njonathon shlens\n\ngoogle inc.\n\nabstract\n\nthe design of neural network architectures is an important component for achiev-\ning state-of-the-art performance with machine learning systems across a broad\narray of tasks. much wo", "pergamon\n\nneural networks 13 (2000) 411\u2013430\n\ninvited article\n\nneural\n\nnetworks\n\nwww.elsevier.com/locate/neunet\n\nindependent component analysis: algorithms and applications\n\na. hyva\u00a8rinen, e. oja*\n\nneural networks research centre, helsinki university of technology, p.o. box 5400, fin-02015 hut, helsinki, finland\n\nreceived 28 march 2000; accepted 28 march 2000\n\nabstract\n\na fundamental problem in neural network research, as well as in many other disciplines, is \ufb01nding a suitable representation of m", "3\n2\n0\n2\n\n \n\ny\na\nm\n1\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n0\n5\n0\n0\n2\n\n.\n\n5\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nlet\u2019s verify step by step\n\n\u2217\nhunter lightman\n\n\u2217\nvineet kosaraju\n\nyura burda\n\n\u2217\n\nharri edwards\n\nbowen baker\n\nteddy lee\n\njan leike\n\njohn schulman\n\nilya sutskever\n\n\u2217\nkarl cobbe\n\nopenai\n\nabstract\n\nin recent years, large language models have greatly improved in their\nability to perform complex multi-step reasoning. however, even state-\nof-the-art models still regularly produce logical mistakes. to train more\nreli", "articles\n\nhttps://doi.org/10.1038/s41593-018-0147-8\n\nprefrontal cortex as a meta-reinforcement \nlearning system\n\njane x. wang!\njoel z. leibo1, demis hassabis1,4 and matthew botvinick!\n\n!1,5, zeb kurth-nelson1,2,5, dharshan kumaran1,3, dhruva tirumala1, hubert soyer1,  \n\n!1,4*\n\nover the past 20 years, neuroscience research on reward-based learning has converged on a canonical model, under which the \nneurotransmitter dopamine \u2018stamps in\u2019 associations between situations, actions and rewards by modu", "version of record: https://www.sciencedirect.com/science/article/pii/s0959438817300910\nmanuscript_5171e60c85fbd8babd41ddcbde28ec6f\n\nthe temporal paradox of hebbian learning and homeostatic plasticity\n\nfriedemann zenkea,\u2217, wulfram gerstnerb, surya gangulia\n\nbbrain mind institute, school of life sciences and school of computer and communication sciences, ecole polytechnique\n\nadepartment of applied physics, stanford university, stanford, ca 94305, usa\n\nf\u00e9d\u00e9rale de lausanne, ch-1015 lausanne epfl, s", "a r t i c l e s\n\nlimits on the memory storage capacity of\nbounded synapses\n\nstefano fusi & l f abbott\n\nmemories maintained in patterns of synaptic connectivity are rapidly overwritten and destroyed by ongoing plasticity related to the\nstorage of new memories. short memory lifetimes arise from the bounds that must be imposed on synaptic ef\ufb01cacy in any realistic\nmodel. we explored whether memory performance can be improved by allowing synapses to traverse a large number of states\nbefore reaching t", "learning identi\ufb01able and interpretable latent models\n\nof high-dimensional neural activity using pi-vae\n\nding zhou\n\ndepartment of statistics\n\ncolumbia university\n\ndz2336@columbia.edu\n\nxue-xin wei\n\ndepartment of neuroscience\n\nut austin\n\nweixx@utexas.edu\n\nabstract\n\nthe ability to record activities from hundreds of neurons simultaneously in the brain\nhas placed an increasing demand for developing appropriate statistical techniques\nto analyze such data. recently, deep generative models have been prop", "original research\npublished: 01 march 2021\ndoi: 10.3389/fncom.2021.640235\n\nbehavioral time scale plasticity of\nplace fields: mathematical analysis\n\nian cone 1,2 and harel z. shouval 1*\n\n1 department of neurobiology and anatomy, university of texas medical school, houston, tx, united states, 2 applied\nphysics program, rice university, houston, tx, united states\n\ntraditional synaptic plasticity experiments and models depend on tight\ntemporal\ncorrelations between pre- and postsynaptic activity. the", "7\n1\n0\n2\n\n \n\ng\nu\na\n5\n\n \n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n6\n2\n0\n8\n0\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nlearning arbitrary dynamics in ef\ufb01cient, balanced\n\nspiking networks using local plasticity rules\n\naireza alemi\u2020\n\ngroup for neural theory, ens\n\n29 rue d\u2019ulm, paris\u201375005, france\n\n\u2020alireza.alemi@{ens.fr, gmail.com}\n\nchristian k. machens\n\nchampalimaud centre for the unknown\n\navenida brasilia, 1400-038 lisbon, portugal\n\nsophie den\u00e8ve\u2217\n\ngroup for neural theory, ens\n\n29 rue d\u2019ulm, paris\u201375005, france\n\njean-ja", "institute of mathematical statistics is collaborating with jstor to digitize, preserve, and extend access to\nthe annals of mathematical statistics.\n\nwww.jstor.org\n\n\u00ae\n\n\f", "1\n2\n0\n2\n\n \nr\na\n\nm\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n3\n4\n3\n1\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\ntowards understanding hierarchical learning:\n\nbene\ufb01ts of neural representations\n\nminshuo chen\u2217\n\nyu bai\u2020\n\njason d. lee\u2021\n\ntuo zhao\u00a7\n\nhuan wang\u00b6\n\ncaiming xiong\u00b6\n\nrichard socher\u00b6\n\nmarch 8, 2021\n\nabstract\n\ndeep neural networks can empirically perform e\ufb03cient hierarchical learning, in which the\nlayers learn useful representations of the data. however, how they make use of the intermediate\nrepresentations are not exp", "distance metric learning, with application\n\nto clustering with side-information\n\neric p. xing, andrew y. ng, michael i. jordan and stuart russell\n\nuniversity of california, berkeley\n\nberkeley, ca 94720\n\n\u0000 epxing,ang,jordan,russell\nabstract\n\n@cs.berkeley.edu\n\nmany algorithms rely critically on being given a good metric over their\ninputs. for instance, data can often be clustered in many \u201cplausible\u201d\nways, and if a clustering algorithm such as k-means initially fails to \ufb01nd\none that is meaningful t", "complex dynamics in simple neural networks:\nunderstanding gradient flow in phase retrieval\n\nstefano sarao mannelli1, giulio biroli2, chiara cammarota3,4,\nflorent krzakala5, pierfrancesco urbani1, and lenka zdeborov\u00e16\n\nabstract\n\ndespite the widespread use of gradient-based algorithms for optimizing high-\ndimensional non-convex functions, understanding their ability of \ufb01nding good\nminima instead of being trapped in spurious ones remains to a large extent an\nopen problem. here we focus on gradient ", "visualizing the loss landscape of neural nets\n\nhao li1, zheng xu1, gavin taylor2, christoph studer3, tom goldstein1\n\n1university of maryland, college park 2united states naval academy 3cornell university\n\n{haoli,xuzh,tomg}@cs.umd.edu, taylor@usna.edu, studer@cornell.edu\n\nabstract\n\nneural network training relies on our ability to \ufb01nd \u201cgood\u201d minimizers of highly\nnon-convex loss functions. it is well-known that certain network architecture\ndesigns (e.g., skip connections) produce loss functions tha", "adversarial examples that fool both computer\n\nvision and time-limited humans\n\ngamaleldin f. elsayed\u2217\n\ngoogle brain\n\ngamaleldin.elsayed@gmail.com\n\nshreya shankar\nstanford university\n\nbrian cheung\nuc berkeley\n\n8\n1\n0\n2\n\n \n\ny\na\nm\n2\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n5\n9\n1\n8\n0\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nnicolas papernot\n\npennsylvania state university\n\nalex kurakin\ngoogle brain\n\nian goodfellow\ngoogle brain\n\njascha sohl-dickstein\n\ngoogle brain\n\njaschasd@google.com\n\nabstract\n\nmachine learning models are vu", "e\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\ne\nr\nu\nt\na\nn\nm\no\nc\n.\ne\nr\nu\nt\na\nn\nw\nw\nw\n\n/\n\n.\n\n/\n/\n:\np\nt\nt\nh\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n6\n0\n0\n2\n\u00a9\n\n \n\na r t i c l e s\n\nthe tempotron: a neuron that learns spike\ntiming\u2013based decisions\n\nrobert gu\u00a8tig1\u20134 & haim sompolinsky1,2,5\n\nthe timing of action potentials in sensory neurons contains substantial information about the eliciting stimuli. although the\ncomputational advantages of spike timing\u2013based neuronal codes have long been recognized, it is u", "the hsic bottleneck: deep learning without back-propagation\n\nwan-duo kurt ma\n\nj.p. lewis w. bastiaan kleijn\n\nvictoria university\n\nmawand@ecs.vuw.ac.nz, jplewis@google.com, bastiaan.kleijn@ecs.vuw.ac.nz\n\n9\n1\n0\n2\n\n \nc\ne\nd\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n8\n5\n1\n0\n\n.\n\n8\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe introduce the hsic (hilbert-schmidt independence crite-\nrion) bottleneck for training deep neural networks. the hsic\nbottleneck is an alternative to the conventional cross-entropy\nloss and backpropa", "evolving images for visual neurons using a deep\ngenerative network reveals coding principles and\nneuronal preferences\n\narticle\n\ngraphical abstract\n\nauthors\ncarlos r. ponce, will xiao,\npeter f. schade, till s. hartmann,\ngabriel kreiman, margaret s. livingstone\n\ncorrespondence\ncrponce@wustl.edu (c.r.p.),\nmlivingstone@hms.harvard.edu (m.s.l.)\n\nin brief\nneurons guided the evolution of their own\nbest stimuli with a generative deep neural\nnetwork.\n\nhighlights\nd a generative deep neural network and a g", "article\n\ndoi:10.1038/nature11347\n\ndivision and subtraction by distinct\ncortical inhibitory networks in vivo\n\nnathan r. wilson1*, caroline a. runyan1*, forea l. wang1 & mriganka sur1\n\nbrain circuits process information through specialized neuronal subclasses interacting within a network. revealing their\ninterplay requires activating specific cells while monitoring others in a functioning circuit. here we use a new platform\nfor two-way light-based circuit interrogation in visual cortex in vivo to ", "motor cortex embeds muscle-like commands in an\nuntangled population response\n\narticle\n\nhighlights\nd motor cortex displays a signature of a smooth dynamical\n\nsystem: low tangling\n\nd low tangling explains the previously puzzling dominant\n\nsignals in motor cortex\n\nd low tangling confers noise robustness and predicts\n\npopulation activity patterns\n\nd motor cortex embeds output commands in structure that\n\nreduces tangling\n\nauthors\n\nabigail a. russo, sean r. bittner,\nsean m. perkins, ...,\nlaurence f. a", "international journal of computer vision 40(1), 49\u201371, 2000\nc(cid:176) 2000 kluwer academic publishers. manufactured in the netherlands.\n\na parametric texture model based on joint statistics\n\nof complex wavelet coef\ufb01cients\n\ncenter for neural science, and courant institute of mathematical sciences, new york university,\n\njavier portilla and eero p. simoncelli\n\nnew york, ny 10003, usa\n\nreceived november 12, 1999; revised june 9, 2000\n\nabstract. we present a universal statistical model for texture i", "extracting and composing robust features with denoising\n\nautoencoders\n\npascal vincent\nhugo larochelle\nyoshua bengio\npierre-antoine manzagol\nuniversit\u00b4e de montr\u00b4eal, dept. iro, cp 6128, succ. centre-ville, montral, qubec, h3c 3j7, canada\n\nvincentp@iro.umontreal.ca\nlarocheh@iro.umontreal.ca\nbengioy@iro.umontreal.ca\nmanzagop@iro.umontreal.ca\n\nabstract\n\nprevious work has shown that the di\ufb03cul-\nties in learning deep generative or discrim-\ninative models can be overcome by an ini-\ntial unsupervised l", "grid cells, place cells, and geodesic generalization for\nspatial reinforcement learning\n\nnicholas j. gustafson1*, nathaniel d. daw1,2\n\n1 center for neural science, new york university, new york, new york, united states of america, 2 department of psychology, new york university, new york, new york,\nunited states of america\n\nabstract\n\nreinforcement learning (rl) provides an influential characterization of the brain\u2019s mechanisms for learning to make\nadvantageous choices. an important problem, thou", "a r t i c l e s\n\nconnectivity reflects coding: a model of voltage-based \nstdp with homeostasis\n\nclaudia clopath1, lars b\u00fcsing1,2, eleni vasilaki1,2 & wulfram gerstner1\n\nelectrophysiological connectivity patterns in cortex often have a few strong connections, which are sometimes bidirectional, \namong a lot of weak connections. to explain these connectivity patterns, we created a model of spike timing\u2013dependent plasticity \n(stdp) in which synaptic changes depend on presynaptic spike arrival and th", "8\n1\n0\n2\n\n \n\nn\na\nj\n \n\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n1\n4\n7\n8\n0\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\ntrain longer, generalize better: closing the\n\ngeneralization gap in large batch training of neural\n\nnetworks\n\nelad hoffer\u2217,\n\nitay hubara\u2217,\n\ndaniel soudry\n\ntechnion - israel institute of technology, haifa, israel\n\n{elad.hoffer, itayhubara, daniel.soudry}@gmail.com\n\nabstract\n\nbackground: deep learning models are typically trained using stochastic gradient\ndescent or one of its variants. these methods update ", "article\n\ncommunicated by yuri dabaghian\n\nquantifying information conveyed by large\nneuronal populations\n\njohn a. berkowitz\njaberkow@ucsd.edu\ndepartment of physics, university of california san diego, san diego,\nca 92093, u.s.a.\n\ntatyana o. sharpee\nsharpee@salk.edu\ncomputational neurobiology laboratory, salk institute for biological studies, la\njolla, ca 92037, and department of physics, university of california san diego,\nsan diego, ca 92093, u.s.a.\n\nquantifying mutual information between inputs", "proc. nati. acad. sci. usa\nvol. 88, pp. 4433-4437, may 1991\nneurobiology\n\na more biologically plausible learning rule for neural networks\n\n(reinforcement learning/coordinate transformation/posterior parietal cortex/sensorimotor integration/hebbian synapses)\n\npietro mazzonit*, richard a. andersent\u00a7, and michael i. jordant\ntdepartment of brain and cognitive sciences, and tharvard-mit division of health sciences and technology, massachusetts institute of technology,\ncambridge, ma 02139\n\ncommunicate", "\u0000\u0002\u0001\u0004\u0003\n\n\u0005\u0007\u0006\t\b\n\u0000\f\u000b\u000e\r\u000f\u000b\u0010\u0001\u0012\u0011\u0013\u000b\u0010\u0001\u0015\u0014\u0016\r\u0017\u0006\u0019\u0018\u001b\u001a\u001c\r\u0017\u001d\u001e\u0001\t\b \u001f!\u000b\u0013\"$#\u001e%&\u0006\u0004\u0006\t%\u0016'(\u0001\u0004\u001f*)\n\n+,\u0006\u0012)-%&\u001a\u0013\u0001.\u000b\u0010/0\u0003\n\n\u0011213%&\u001a\n\n45%&\u001f0\u001f6\b7\u00148\u000b\u000e\u0001\u0015%&\u001f\u001e\u0001\u0015\u00119\u000b;:<\b \u0001=\u001f>13%&\u001a?\u00147\b \u0003\n\n\b \u001f!\u000b(@a\b\u0016\r\u0017\u001a\u0013\u001f\u001e\u0001=\u001f>)\n\nb6c$d\u000ee$fhgji\u0010k\u0010l,mnfhfhmhe$o\u0019p\n\nc?fnfnrts?r\u0007cvu\nb\u001ec$]czedfrgevpcz\\rt]\\dih\u0002d\u0010mkj9rt]\\plmkznm\n\nc?oxw\u000ey[z\\r^]>_a`tmnr^d\u0010`tr\n\no-c?ppzqc$dsrutwv\n\nx\u0013yfz?zg{\n\n|\u0013}l}q~\u0080\u007f\u0004\u0081h\u0082?\u0083\u0085\u0084\u0017\u0086\u0088\u0087\u0004\u0089\u008b\u008ap\u008c\u008e\u008dq\u008f\u0010\u0090^\u008f\u008b\u0087\u0015\u0091\u0092\u008d\\\u008c\u008e\u008dp\u0093\u0015\u0094$\u0095\t\u0096l}l}t\u0097\u0099\u0098\u0004\u0098\t\u009a\t\u009b\u009c\u0098=\u009d\t\u009e\\\u0096e\u009f.\u009a\u0004\u009a=\u0098l\u0097\n\n\u00a0\u0017\u00a1$\u00a2\u0015\u00a3\u0004\u00a4n\u00a5\u00a7\u00a6c\u00a3\n\n\u00a8\u000e\u00a9\u00a7\u00aa\u00ac\u00abu\u00adp\u00ae\u00b0\u00af\u00b0\u00aa\u00ac\u00b1\u0015\u00b2\u008e\u00b3\u000f\u00b4\u00a7\u00ae\u00b5\u00b3\t\u00ab\u0092\u00b3\t\u00b6\\\u00af\u00b0\u00ab\u00b7\u00ad!\u00b8c\u00b3\t\u00b6\u00b9\u00b3\u0015\u00ae\u0080\u00adc\u00b2$\u00b1\u0015\u00b2\u00ba\u00adc\u00ab\u00b0\u00ab\u00b7\u00bbc\u00bca\u00adc\u00ab\u00b0\u00ab", "the geometry of abstraction in the hippocampus\nand prefrontal cortex\n\narticle\n\ngraphical abstract\n\nauthors\nsilvia bernardi, marcus k. benna,\nmattia rigotti, je\u00b4 ro\u02c6 me munuera,\nstefano fusi, c. daniel salzman\n\ncorrespondence\nsf2237@columbia.edu (s.f.),\ncds2005@columbia.edu (c.d.s.)\n\nin brief\ndifferent types of cognitive, emotional,\nand behavioral \ufb02exibility\u2014generalization\nin novel situations and the ability to\ngenerate many different responses to\ncomplex patterns of inputs\u2014place\ndifferent demand", "r e v i e w s\n\ntop-down influences on visual \nprocessing\n\ncharles d.\u00a0gilbert1 and wu li2\n\nabstract | re-entrant or feedback pathways between cortical areas carry rich and varied \ninformation about behavioural context, including attention, expectation, perceptual \ntasks, working memory and motor commands. neurons receiving such inputs effectively \nfunction as adaptive processors that are able to assume different functional states \naccording to the task being executed. recent data suggest that the", "article\n\ndoi:10.1038/nature10918\n\nchoice-specific sequences in parietal\ncortex during a virtual-navigation\ndecision task\n\nchristopher d. harvey1,3,4{, philip coen1,4 & david w. tank1,2,3,4\n\nthe posterior parietal cortex (ppc) has an important role in many cognitive behaviours; however, the neural circuit\ndynamics underlying ppc function are not well understood. here we optically imaged the spatial and temporal activity\npatterns of neuronal populations in mice performing a ppc-dependent task that", "6\n1\n0\n2\n\n \nc\ne\nd\n1\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n5\nv\n6\n9\n5\n1\n0\n\n.\n\n9\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ndirect feedback alignment provides learning in\n\ndeep neural networks\n\narild n\u00f8kland\n\ntrondheim, norway\n\narild.nokland@gmail.com\n\nabstract\n\narti\ufb01cial neural networks are most commonly trained with the back-propagation\nalgorithm, where the gradient for learning is provided by back-propagating the error,\nlayer by layer, from the output layer to the hidden layers. a recently discovered\nmethod called feedback-", "24. l. vician et al., proc. natl. acad. sci. u.s.a. 92, 2164\n\n(1995).\n\n25. m. yoshihara et al., data not shown.\n26. w. li, j. t. ohlmeyer, m. e. lane, d. kalderon, cell 80,\n\n553 (1995).\n\n27. y. zhong, v. budnik, c.-f. wu, j. neurosci. 12, 644\n\n(1992).\n\n28. g. w. davis, c. m. schuster, c. s. goodman, neuron\n\n19, 561 (1997).\n\n29. d. o. hebb, the organization of behavior (wiley,\n\nnew york, 1949).\n\n30. u. frey, r. g. morris, nature 385, 533 (1997).\n\n31. k. c. martin et al., cell 91, 927 (1997).\n32. ", "j\no\nu\nr\nn\na\nl\n \no\nf\n \nc\no\nm\np\nu\nt\na\nt\ni\no\nn\na\nl\n \nn\ne\nu\nr\no\ns\nc\ni\ne\nn\nc\ne\n,\n \n1\n,\n \n3\n1\n3\n-\n3\n2\n1\n \n(\n1\n9\n9\n4\n)\n \n(cid:14)\n9\n \n1\n9\n9\n4\n \nk\nl\nu\nw\ne\nr\n \na\nc\na\nd\ne\nm\ni\nc\n \np\nu\nb\nl\ni\ns\nh\ne\nr\ns\n,\n \nb\no\ns\nt\no\nn\n.\n \nm\na\nn\nu\nf\na\nc\nt\nu\nr\ne\nd\n \ni\nn\n \nt\nh\ne\n \nn\ne\nt\nh\ne\nr\nl\na\nn\nd\ns\n.\n \nw\nh\ne\nn\n \ni\nn\nh\ni\nb\ni\nt\ni\no\nn\n \nn\no\nt\n \ne\nx\nc\ni\nt\na\nt\ni\no\nn\n \ns\ny\nn\nc\nh\nr\no\nn\ni\nz\ne\ns\n \nn\ne\nu\nr\na\nl\n \nf\ni\nr\ni\nn\ng\n \nc\na\nr\nl\n \nv\na\nn\n \nv\nr\ne\ne\ns\nw\ni\nj\nk\n \na\nn\nd\n \nl\n.\nf\n.\n \na\nb\nb\no\nt\nt\n \nc\ne\nn\nt\ne\nr\n \nf\no\nr\n \nc\no\nm\np\nl\ne\nx\n \ns", "published as a conference paper at iclr 2020\n\nthe curious case of\nneural text degeneration\n\nari holtzman\u2020\u2021\n\njan buys\u00a7\u2020\n\nli du\u2020\n\nmaxwell forbes\u2020\u2021\n\nyejin choi\u2020\u2021\n\n\u2020paul g. allen school of computer science & engineering, university of washington\n\u2021allen institute for arti\ufb01cial intelligence\n\u00a7department of computer science, university of cape town\n{ahai,dul2,mbforbes,yejin}@cs.washington.edu, jbuys@cs.uct.ac.za\n\nabstract\n\ndespite considerable advances in neural language modeling, it remains an open\nque", "task representations in neural networks trained to \nperform many cognitive tasks\n\nguangyu\u00a0robert\u00a0yang\u200a\nand xiao-jing\u00a0wang\u200a\n\n\u200a1,2, madhura\u00a0r.\u00a0joglekar1,6, h.\u00a0francis\u00a0song1,7, william\u00a0t.\u00a0newsome3,4  \n\u200a1,5*\n\nthe brain has the ability to flexibly perform many tasks, but the underlying mechanism cannot be elucidated in traditional \nexperimental and modeling studies designed for one task at a time. here, we trained single network models to perform 20 \ncognitive tasks that depend on working memory, dec", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/2022.12.30.522267\n; \n\nthis version posted december 31, 2022. \n\nthe copyright holder for this\n\npreprint (which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in \n\nperpetuity. it is made available under a\n\ncc-by-nc 4.0 international license\n.\n\na sensory-motor theory of the neocortex based on\n\nactive predictive coding\n\nrajesh p. n. rao\n\ncenter for neurotechnology &\n\npaul g. allen sch", "article\n\na neural population mechanism for rapid learning\n\ngraphical abstract\n\nauthors\n\ninputs\n\npremotor\ncortex\n\nmotor\ncortex\n\nbehavior\n\nmatthew g. perich, juan a. gallego,\nlee e. miller\n\ncorrespondence\nlm@northwestern.edu\n\nin brief\nbehavioral adaptation occurs rapidly,\neven after single errors. perich et al.\npropose that the premotor cortex can\nexploit an \u2018\u2018output-null\u2019\u2019 subspace to\nlearn to adjust its output to downstream\nregions in response to behavioral errors.\n\npremotor population\n\nactivity", "neuron\n\nreview\n\nacetylcholine as a neuromodulator:\ncholinergic signaling shapes\nnervous system function and behavior\n\nmarina r. picciotto,1,2,3,* michael j. higley,2,3 and yann s. mineur1\n1department of psychiatry\n2department of neurobiology\n3program in cellular neuroscience, neurodegeneration and repair\nyale university school of medicine, new haven, ct 06511, usa\n*correspondence: marina.picciotto@yale.edu\nhttp://dx.doi.org/10.1016/j.neuron.2012.08.036\n\nacetylcholine in the brain alters neuronal", "cognition 152 (2016) 160\u2013169\n\ncontents lists available at sciencedirect\n\ncognition\n\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / c o g n i t\n\noriginal articles\nneural signature of hierarchically structured expectations predicts\nclustering and transfer of rule sets in reinforcement learning\nanne gabrielle eva collins a,b,\u21d1\n\n, michael joshua frank a\n\na department of cognitive, linguistic and psychological sciences, brown institute for brain science, brown univers", "prl 116, 038701 (2016)\n\np h y s i c a l r e v i e w l e t t e r s\n\nweek ending\n\n22 january 2016\n\nheterogeneous preference and local nonlinearity in consensus decision making\n\nandrew t. hartnett,1,2,* emmanuel schertzer,3,4 simon a. levin,2 and iain d. couzin2,5,6\n\n1department of physics, princeton university, princeton, new jersey 08544, usa\n\n2department of ecology and evolutionary biology, princeton university, princeton, new jersey 08544, usa\n\n3upmc universit\u00e9 paris 06, laboratoire de probabil", "3\n2\n0\n2\n\n \n\nn\na\nj\n \n\n9\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n8\n2\n0\n8\n0\n\n.\n\n1\n0\n3\n2\n:\nv\ni\nx\nr\na\n\na survey of meta-reinforcement learning\n\njacob beck\u2217\n\njacob.beck@cs.ox.ac.uk\n\nuniversity of oxford\n\nristo vuorio\u2217\n\nristo.vuorio@cs.ox.ac.uk\n\nuniversity of oxford\n\nevan zheran liu\n\nzheng xiong\n\nluisa zintgraf\u2020\n\nevanliu@cs.stanford.edu\n\nzheng.xiong@cs.ox.ac.uk\n\nzintgraf@deepmind.com\n\nstanford university\n\nuniversity of oxford\n\nuniversity of oxford\n\nchelsea finn\n\ncbfinn@cs.stanford.edu\n\nstanford university\n\nshim", "research article\ninferring spikes from calcium imaging in\ndopamine neurons\n\nweston flemingid1\u262f, sean jewell2\u262f, ben engelhard1, daniela m. witten2*, ilana\nb. witten1*\n\n1 princeton neuroscience institute, princeton university, princeton, new jersey, united states of america,\n2 department of statistics & biostatistics, university of washington, seattle, washington, united states of\namerica\n\n\u262f these authors contributed equally to this work.\n* iwitten@princeton.edu (ibw); dwitten@uw.edu (dmw)\n\nabstra", "unitary evolution recurrent neural networks\n\nmartin arjovsky \u2217\namar shah \u2217\nyoshua bengio\nuniversidad de buenos aires, university of cambridge,\nuniversit\u00b4e de montr\u00b4eal. yoshua bengio is a cifar senior fellow.\n\u2217indicates \ufb01rst authors. ordering determined by coin \ufb02ip.\n\nmarjovsky@dc.uba.ar\nas793@cam.ac.uk\n\nabstract\n\nrecurrent neural networks (rnns) are notori-\nously dif\ufb01cult to train. when the eigenvalues\nof the hidden to hidden weight matrix deviate\nfrom absolute value 1, optimization becomes dif-", "e\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\ne\nr\nu\nt\na\nn\nm\no\nc\n.\ne\nr\nu\nt\na\nn\nw\nw\nw\n\n/\n\n.\n\n/\n/\n:\np\nt\nt\nh\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n4\n0\n0\n2\n\u00a9\n\n \n\na r t i c l e s\n\ncomputational subunits in thin dendrites of\npyramidal cells\n\nalon polsky1, bartlett w mel2 & jackie schiller1\n\nthe thin basal and oblique dendrites of cortical pyramidal neurons receive most of the synaptic inputs from other cells, but their\nintegrative properties remain uncertain. previous studies have most often reported globa", "cerebellar motor learning: when is cortical\nplasticity not enough?\n\njohn porrill, paul dean*\n\ndepartment of psychology, sheffield university, sheffield, united kingdom\n\nclassical marr-albus theories of cerebellar learning employ only cortical sites of plasticity. however, tests of these\ntheories using adaptive calibration of the vestibulo\u2013ocular reflex (vor) have indicated plasticity in both cerebellar\ncortex and the brainstem. to resolve this long-standing conflict, we attempted to identify the", "9improvedtemporaldifferencemethodswithlinearfunctionapproximationdimitrip.bertsekasandangelianedichmassachusettsinstituteoftechnologyalphatech,inc.viveks.borkartatainstituteoffundamentalresearcheditor\u2019ssummary:thischapterconsiderstemporaldifferencealgorithmswithinthecontextofin\ufb01nite-horizon\ufb01nite-statedynamicprogrammingproblemswithdis-countedcostandlinearcostfunctionapproximation.thisproblemarisesasasubprobleminthepolicyiterationmethodofdynamicprogramming.additionaldiscussionsofsuchproblemscanbef", "8368 \u2022 the journal of neuroscience, august 9, 2006 \u2022 26(32):8368 \u2013 8376\n\nbehavioral/systems/cognitive\n\nreward-related cortical inputs define a large striatal\nregion in primates that interface with associative\ncortical connections, providing a substrate for\nincentive-based learning\n\nsuzanne n. haber,1 ki-sok kim,2 philippe mailly,3 and roberta calzavara1\n1department of pharmacology and physiology, university of rochester school of medicine, rochester, new york 14642, 2department of public health,", "article\n\nhttps://doi.org/10.1038/s41467-020-14578-5\n\nopen\n\nseparability and geometry of object manifolds\nin deep neural networks\n\nuri cohen\n\n1,6, sueyeon chung\n\n2,3,4,6, daniel d. lee5 & haim sompolinsky1,2*\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\nstimuli are represented in the brain by the collective population responses of sensory neu-\nrons, and an object presented under varying conditions gives rise to a collection of neural\npopulation responses called an \u2018object manifold\u2019. changes in the object repr", "open\n\na local learning rule for \nindependent component analysis\n\ntakuya isomura1,2,3 & taro toyoizumi1\n\nreceived: 16 february 2016\naccepted: 26 may 2016\npublished: 21 june 2016\n\nhumans can separately recognize independent sources when they sense their superposition. this \ndecomposition is mathematically formulated as independent component analysis (ica). while a few \nbiologically plausible learning rules, so-called local learning rules, have been proposed to achieve \nica, their performance varie", "spike-based reinforcement learning in continuous state\nand action space: when policy gradient methods fail\n\neleni vasilaki1,2*, nicolas fre\u00b4 maux1, robert urbanczik3, walter senn3, wulfram gerstner1\n\n1 laboratory of computational neuroscience, epfl, lausanne, switzerland, 2 department of computer science, university of sheffield, sheffield, united kingdom,\n3 department of physiology, university of bern, bern, switzerland\n\nabstract\n\nchanges of synaptic connections between neurons are thought to b", "journal of mathematical psychology 56 (2012) 1\u201312\n\ncontents lists available at sciverse sciencedirect\n\njournal of mathematical psychology\n\njournal homepage: www.elsevier.com/locate/jmp\n\ntutorial\na tutorial on bayesian nonparametric models\nsamuel j. gershman a,\u21e4, david m. blei b\na department of psychology and princeton neuroscience institute, princeton university, princeton nj 08540, usa\nb department of computer science, princeton university, princeton nj 08540, usa\n\na r t i c l e\n\ni n f o\n\na b s", "3\n2\n0\n2\n\n \n\nn\na\nj\n \n\n9\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n6\nv\n4\n9\n7\n9\n0\n\n.\n\n1\n1\n1\n2\n:\nv\ni\nx\nr\na\n\njournal of arti\ufb01cial intelligence research 76 (2023) 201-264\n\nsubmitted 08/2022; published 01/2023\n\na survey of zero-shot generalisation in deep\n\nreinforcement learning\n\nrobert kirk\nuniversity college london, gower st, london\nwc1e 6bt, united kingdom\namy zhang\nuniversity of california, berkeley, berkeley\nca, united states\nmeta ai research,\nedward grefenstette\nuniversity college london, gower st, london\nwc1e ", "article\n\ndoi:10.1038/nature15257\n\nlabelling and optical erasure of synaptic\nmemory traces in the motor cortex\n\nakiko hayashi-takagi1,2, sho yagishita1,3, mayumi nakamura1, fukutoshi shirai1, yi i. wu4, amanda l. loshbaugh5,6,\nbrian kuhlman5,6, klaus m. hahn5,7 & haruo kasai1,3\n\ndendritic spines are the major loci of synaptic plasticity and are considered as possible structural correlates of memory.\nnonetheless, systematic manipulation of specific subsets of spines in the cortex has been unattain", "r e p o r t s\n\n35. r. n. shepard, psychon. bull. rev. 1, 2 (1994).\n36. j. b. tenenbaum, adv. neural info. proc. syst. 10, 682\n\n(1998).\n\n37. t. martinetz, k. schulten, neural netw. 7, 507 (1994).\n38. v. kumar, a. grama, a. gupta, g. karypis, introduc-\ntion to parallel computing: design and analysis of\nalgorithms (benjamin/cummings, redwood city, ca,\n1994), pp. 257\u2014297.\n\n39. d. beymer, t. poggio, science 272, 1905 (1996).\n40. available at www.research.att.com/;yann/ocr/mnist.\n41. p. y. simard, y. ", "vol 443|7 september 2006|doi:10.1038/nature05078\n\nletters\n\nexperience-dependent representation of visual\ncategories in parietal cortex\ndavid j. freedman1 & john a. assad1\n\ncategorization is a process by which the brain assigns meaning to\nsensory stimuli. through experience, we learn to group stimuli\ninto categories, such as \u2018chair\u2019, \u2018table\u2019 and \u2018vehicle\u2019, which are\ncritical for rapidly and appropriately selecting behavioural\nresponses1,2. although much is known about the neural represen-\ntation ", "a r t i c l e s\n\nrobust timing and motor patterns by taming chaos in \nrecurrent neural networks\nrodrigo laje1,5 & dean v buonomano1\u20134\nthe brain\u2019s ability to tell time and produce complex spatiotemporal motor patterns is critical for anticipating the next ring of a \ntelephone or playing a musical instrument. one class of models proposes that these abilities emerge from dynamically changing \npatterns of neural activity generated in recurrent neural networks. however, the relevant dynamic regimes o", "original research\npublished: 04 april 2019\ndoi: 10.3389/fncom.2019.00018\n\ndeep learning with asymmetric\nconnections and hebbian updates\n\nyali amit*\n\ndepartment of statistics, university of chicago, chicago, il, united states\n\nwe show that deep networks can be trained using hebbian updates yielding similar\nperformance to ordinary back-propagation on challenging image datasets. to overcome\nthe unrealistic symmetry in connections between layers, implicit in back-propagation,\nthe feedback weights ar", "anrv314-ne30-11\n\nari\n\n10 may 2007\n\n20:1\n\nmultiple dopamine\nfunctions at different\ntime courses\nwolfram schultz\ndepartment of physiology, development, and neuroscience, university of\ncambridge, cambridge cb2 3dy, united kingdom; email: ws234@cam.ac.uk\n\nannu. rev. neurosci. 2007. 30:259\u201388\n\nfirst published online as a review in advance on\nmarch 12, 2007\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev.neuro.28.061604.135722\ncopyright c(ci", "sanhueza and lisman molecular brain 2013, 6:10\nhttp://www.molecularbrain.com/content/6/1/10\n\nr ev i e w\nthe camkii/nmdar complex as a molecular\nmemory\nmagdalena sanhueza1 and john lisman2*\n\nopen access\n\nabstract\ncamkii is a major synaptic protein that is activated during the induction of long-term potentiation (ltp) by the ca2+\ninflux through nmdars. this activation is required for ltp induction, but the role of the kinase in the maintenance\nof ltp is less clear. elucidating the mechanisms of ma", "journal of machine learning research 11 (2010) 2287-2322\n\nsubmitted 7/09; revised 4/10; published 8/10\n\nspectral regularization algorithms for learning large incomplete\n\nmatrices\n\nrahul mazumder\ntrevor hastie\u2217\ndepartment of statistics\nstanford university\nstanford, ca 94305\nrobert tibshirani\u2020\ndepartment of health, research and policy\nstanford university\nstanford, ca 94305\n\neditor: tommi jaakkola\n\nrahulm@stanford.edu\nhastie@stanford.edu\n\ntibs@stanford.edu\n\nabstract\n\nwe use convex relaxation techni", "neuron, vol. 32, 899\u2013910, december 6, 2001, copyright \uf8e92001 by cell press\n\ntemporal precision and temporal drift in brain\nand behavior of zebra finch song\n\nzhiyi chi1,3 and daniel margoliash2\n1 department of statistics\n2 department of organismal biology and anatomy\nuniversity of chicago\nchicago, illinois 60637\n\nsummary\n\nin the zebra finch forebrain nucleus robustus archistri-\natalis (ra), neurons burst during singing. we showed\nthat the internal structure of spike bursts was regu-\nlated with a p", "an introduction to mds\n\nflorian wickelmaier\n\nsound quality research unit, aalborg university, denmark\n\nmay 4, 2003\n\nauthor\u2019s note\n\nthis manuscript was completed while the author was employed at the sound qual-\n\nity research unit at aalborg university. this unit receives \ufb01nancial support from\n\ndelta acoustics & vibration, br\u00a8uel & kj\u00e6r, and bang & olufsen, as well as from\n\nthe danish national agency for industry and trade (efs) and the danish technical\n\nresearch council (stvf). the author would l", "articles\n\nvol 459 | 28 may 2009 | doi:10.1038/nature08010\n\nhippocampal theta oscillations are\ntravelling waves\n\nevgueniy v. lubenov1 & athanassios g. siapas1\n\ntheta oscillations clock hippocampal activity during awake behaviour and rapid eye movement (rem) sleep. these\noscillations are prominent in the local field potential, and they also reflect the subthreshold membrane potential and strongly\nmodulate the spiking of hippocampal neurons. the prevailing view is that theta oscillations are synchr", "acute silencing of hippocampal ca3 reveals  \na dominant role in place field responses\n\nheydar\u00a0davoudi\u200a\n\n\u200a1,2,3,4 and david\u00a0j.\u00a0foster\u200a\n\n\u200a1,2,3*\n\nneurons  in  hippocampal  output  area  ca1  are  thought  to \nexhibit  redundancy  across  cortical  and  hippocampal  inputs. \nhere we show instead that acute silencing of ca3 terminals \ndrastically reduces place field responses for many ca1 neu-\nrons, while a smaller number are unaffected or have increased \nresponses. these results imply that ca3 is t", "available online at www.sciencedirect.com\n\nsciencedirect\n\ncurrent opinion in\n\nneurobiology\n\nfrom lazy to rich to exclusive task representations in\nneural networks and neural codes\nmatthew farrell1, stefano recanatesi2 and eric shea-brown2\n\nabstract\nneural circuits\u2014both in the brain and in \u201cartificial\u201d neural\nnetwork models\u2014learn to solve a remarkable variety of tasks,\nand there is a great current opportunity to use neural networks\nas models for brain function. key to this endeavor is the ability", "the successor representation in human \nreinforcement learning\n\ni. momennejad\u200a\n\n\u200a1*, e. m. russek2, j. h. cheong3, m. m. botvinick\u200a\n\n\u200a4, n. d. daw1 and s. j. gershman\u200a\n\n\u200a5\n\ntheories of reward learning in neuroscience have focused on two families of algorithms thought to capture deliberative versus \nhabitual choice. \u2018model-based\u2019 algorithms compute the value of candidate actions from scratch, whereas \u2018model-free\u2019 algo-\nrithms make choice more efficient but less flexible by storing pre-computed act", "neuron, vol. 33, 163\u2013175, january 17, 2002, copyright \uf8e92002 by cell press\n\nthalamic relay functions and\ntheir role in corticocortical communication:\ngeneralizations from the visual system\n\nreview\n\nr.w. guillery1 and s. murray sherman2,3\n1department of anatomy\nuniversity of wisconsin school of medicine\n1300 university avenue\nmadison, wisconsin 53706\n2 department of neurobiology\nstate university of new york\nstony brook, new york 11794\n\nsummary\n\nall neocortical areas receive thalamic inputs. some\nt", "neuron\n\nperspective\n\ncanonical microcircuits for predictive coding\n\nandre m. bastos,1,2,6 w. martin usrey,1,3,4 rick a. adams,8 george r. mangun,2,3,5 pascal fries,6,7 and karl j. friston8,*\n1center for neuroscience\n2center for mind and brain\n3department of neurology\n4department of neurobiology, physiology and behavior\n5department of psychology\nuniversity of california, davis, davis, ca 95618 usa\n6ernst stru\u00a8 ngmann institute (esi) for neuroscience in cooperation with max planck society, deutsch", "letter\nrepetitive motor learning induces coordinated\nformation of clustered dendritic spines in vivo\n\ndoi:10.1038/nature10844\n\nmin fu1, xinzhu yu1, ju lu2 & yi zuo1\n\nmany lines of evidence suggest that memory in the mammalian\nbrain is stored with distinct spatiotemporal patterns1,2. despite\nrecent progresses in identifying neuronal populations involved\nin memory coding3\u20135, the synapse-level mechanism is still poorly\nunderstood. computational models and electrophysiological data\nhave shown that f", "a r t i c l e s\n\nthalamic nuclei convey diverse contextual information \nto layer 1 of visual cortex\nmorgane m roth1,4, johannes c dahmen2\u20134, dylan r muir1,4, fabia imhof1, francisco j martini1 & sonja b hofer1,2\nsensory perception depends on the context in which a stimulus occurs. prevailing models emphasize cortical feedback as the \nsource of contextual modulation. however, higher order thalamic nuclei, such as the pulvinar, interconnect with many cortical \nand subcortical areas, suggesting a r", "neural networks 15 (2002) 665\u2013687\n\n2002 special issue\n\nwww.elsevier.com/locate/neunet\n\ncontrol of exploitation \u2013 exploration meta-parameter\n\nin reinforcement learning\n\nshin ishiia,b,*, wako yoshidaa,b, junichiro yoshimotoa,b\n\nanara institute of science and technology, 8916-5 takayama-cho, ikoma, nara 630-0101, japan\n\nbcrest, japan science and technology corporation, japan\n\nreceived 10 october 2001; accepted 16 april 2002\n\nabstract\n\nin reinforcement learning (rl), the duality between exploitation", "acta numerica (2021), pp. 327\u2013444\ndoi:10.1017/s0962492921000052\n\nprinted in the united kingdom\n\nneural network approximation\n\nronald devore\n\ndepartment of mathematics, texas a&m university,\n\ncollege station, tx 77843, usa\ne-mail: rdevore@math.tamu.edu\n\ndepartment of operations research and financial engineering,\n\nboris hanin\n\nprinceton university, nj 08544, usa\n\ne-mail: bhanin@princeton.edu\n\nguergana petrova\n\ndepartment of mathematics, texas a&m university,\n\ncollege station, tx 77843, usa\ne-mail", "available  online  at  www.sciencedirect.com\n\nsequence  learning  and  the  role  of  the  hippocampus  in  rodent\nnavigation\ndavid  j  foster1 and  james  j  knierim1,2\n\nthe  hippocampus  has  long  been  associated  with  navigation\nand  spatial  representations,  but  it  has  been  dif\ufb01cult  to  link\ndirectly  the  neurophysiological  correlates  of  hippocampal\nplace  cells  with  navigational  planning  and  action.  in  recent\nyears,  large-scale  population  recordings  of  place  cells ", "article\n\na top-down cortical circuit for accurate sensory\nperception\n\nhighlights\nd somatosensory (s1) and secondary motor (m2) cortices form\n\na top-down circuit\n\nd sensory stimulation induces sequential s1 to m2 and m2 to\n\ns1 input patterns\n\nd m2 evokes a dendritic spike and persistent \ufb01ring in s1 layer 5\n\n(l5) neurons\n\nd optogenetic inhibition of m2 to s1 axons degrades accurate\n\nsensory perception\n\nauthors\n\nsatoshi manita, takayuki suzuki, ...,\nmatthew e. larkum,\nmasanori murayama\n\ncorresponde", "6\n1\n0\n2\n\n \n\nv\no\nn\n6\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n1\n4\n1\n5\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\ntraining spiking deep networks\n\nfor neuromorphic hardware\n\ncentre for theoretical neuroscience\n\ncentre for theoretical neuroscience\n\neric hunsberger\n\nuniversity of waterloo\nwaterloo, on n2l 3g1\n\nchris eliasmith\n\nuniversity of waterloo\nwaterloo, on n2l 3g1\n\nehunsber@uwaterloo.ca\n\nceliasmith@uwaterloo.ca\n\nabstract\n\nwe describe a method to train spiking deep networks that can be run using leaky\nintegrate-and-\ufb01re", "2\n2\n0\n2\n\n \nr\na\n\nm\n4\n\n \n\n \n \n]\np\ns\n.\ns\ns\ne\ne\n[\n \n \n\n2\nv\n2\n3\n6\n2\n1\n\n.\n\n0\n1\n0\n2\n:\nv\ni\nx\nr\na\n\nbiologically plausible single-layer networks for nonnegative\n\nindependent component analysis\n\ndavid lipshutz1, cengiz pehlevan2, and dmitri b. chklovskii1,3\n\n1center for computational neuroscience, flatiron institute\n\n2john a. paulson school of engineering and applied sciences, harvard university\n\n3neuroscience institute, nyu medical center\n\nmarch 8, 2022\n\nabstract\n\nan important problem in neuroscience is t", "st02ch15-salakhutdinov\n\nari\n\n14 march 2015\n\n8:3\n\nlearning deep generative\nmodels\nruslan salakhutdinov\ndepartments of computer science and statistical sciences, university of toronto,\ntoronto m5s 3g4, canada; email: rsalakhu@cs.toronto.edu\n\nannu. rev. stat. appl. 2015. 2:361\u201385\n\nthe annual review of statistics and its application is\nonline at statistics.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev-statistics-010814-020120\ncopyright c(cid:2) 2015 by annual reviews.\nall rights reserved\n\nk", "the journal of neuroscience, july 27, 2016 \u2022 36(30):7817\u20137828 \u2022 7817\n\nbehavioral/cognitive\n\na probability distribution over latent causes, in the\norbitofrontal cortex\n\nx stephanie c. y. chan, x yael niv,* and x kenneth a. norman*\nprinceton neuroscience institute, princeton university, princeton, new jersey 08544\n\nthe orbitofrontal cortex (ofc) has been implicated in both the representation of \u201cstate,\u201d in studies of reinforcement learning and\ndecision making, and also in the representation of \u201csc", "iffil\n\nemi\n\n1!\n\n'-0'-tmw*wmm\n\ns\n\n*\n\nscreen of the microscope. between 150 to 300\nnerve fibers were analyzed per cross section.\n\n13. c. f. eldridge, m. bartlett, r. p. bunge, p. m. wood,\nj. cell biol. 105, 1023 (1987); c. f. eldridge, m. b.\nbunge, r. p. bunge, j. neurosci. 9, 625 (1989).\n\n14. addition of progesterone (dissolved in ethanol; final\nconcentration of ethanol, 0.1 %) daily to a final con-\ncentration of 20 nm to culture medium for 2 weeks\ndid not increase the area occupied by the neurit", "neuron\n\nreport\n\nsynaptic integration gradients\nin single cortical pyramidal cell dendrites\n\ntiago branco1,2,* and michael ha\u00a8 usser1,2,*\n1wolfson institute for biomedical research\n2department of neuroscience, physiology and pharmacology, university college london, gower street, london wc1e 6bt, uk\n*correspondence: t.branco@ucl.ac.uk (t.b.), m.hausser@ucl.ac.uk (m.h.)\ndoi 10.1016/j.neuron.2011.02.006\n\nsummary\n\ncortical pyramidal neurons receive thousands of\nsynaptic inputs arriving at different d", "9\n1\n0\n2\n\n \n\nb\ne\nf\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n4\n5\n0\n2\n0\n\n.\n\n0\n1\n8\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2019\n\ngradient descent provably optimizes\nover-parameterized neural networks\n\nsimon s. du\u2217\nmachine learning department\ncarnegie mellon university\nssdu@cs.cmu.edu\n\nbarnab\u00b4as pocz\u00b4os\nmachine learning department\ncarnegie mellon university\nbapozos@cs.cmu.edu\n\nxiyu zhai\u2217\ndepartment of eecs\nmassachusetts institute of technology\nxiyuzhai@mit.edu\n\naarti singh\nmachine learning de", "revealing the hidden networks of interaction in mobile\nanimal groups allows prediction of complex\nbehavioral contagion\n\nsara brin rosenthala,1, colin r. twomeyb,1, andrew t. hartnetta, hai shan wub, and iain d. couzinb,c,d,2\n\ndepartments of aphysics and becology and evolutionary biology, princeton university, princeton, nj 08544; cdepartment of collective behaviour, max planck\ninstitute for ornithology, d-78547 konstanz, germany; and dchair of biodiversity and collective behavior, department of ", "9050 \u2022 the journal of neuroscience, july 15, 2009 \u2022 29(28):9050 \u20139058\n\nbehavioral/systems/cognitive\n\nthe foveal confluence in human visual cortex\n\nmark m. schira,1,2,3 christopher w. tyler,4 michael breakspear,2,3,5 and branka spehar1\nschools of 1psychology and 2psychiatry, university of new south wales, sydney, new south wales 2052, australia, 3the black dog institute, prince of\nwales hospital, randwick, new south wales 2031, australia, 4the smith-kettlewell eye research institute, san francisc", "cellpose: a generalist algorithm for cellular \nsegmentation\n\ncarsen stringer, tim wang, michalis michaelos and marius pachitariu\u200a\n\n\u200a\u2009\u2709\n\nmany biological applications require the segmentation of cell bodies, membranes and nuclei from microscopy images. deep \nlearning has enabled great progress on this problem, but current methods are specialized for images that have large training \ndatasets. here we introduce a generalist, deep learning-based segmentation method called cellpose, which can precisel", "8\n9\n9\n1\n\n \n\np\ne\ns\n4\n\n \n\n \n \n]\nh\np\n-\np\nm\no\nc\n.\ns\nc\ni\ns\ny\nh\np\n[\n \n \n\n2\nv\n8\n0\n0\n3\n0\n8\n9\n/\ns\nc\ni\ns\ny\nh\np\n:\nv\ni\nx\nr\na\n\ntechnical report no. 9805, department of statistics, university of toronto\n\nannealed importance sampling\n\nradford m. neal\n\ndepartment of statistics and department of computer science\n\nuniversity of toronto, toronto, ontario, canada\n\nhttp://www.cs.utoronto.ca/\u223cradford/\n\nradford@stat.utoronto.ca\n\nfirst version:\nrevised:\n\n18 february 1998\n1 september 1998\n\nabstract. simulated annealing ", "this is the accepted manuscript made available via chorus. the article has been\n\npublished as:\n\nmachine learning hidden symmetries\n\nziming liu and max tegmark\n\nphys. rev. lett. 128, 180201 \u2014 published  6 may 2022\n\ndoi: 10.1103/physrevlett.128.180201\n\n\f", "9\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n1\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n8\n8\n4\n6\n0\n\n.\n\n2\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nfeedback alignment in deep convolutional net-\nworks\n\ntheodore h. moskovitz1,2,*, ashok litwin-kumar1, and l.f. abbott1\n\n1mortimer b. zuckerman mind, brain and behavior institute, department of neuroscience, columbia\nuniversity, new york, ny\n2department of computer science, columbia university, new york, ny\n*t.moskovitz@columbia.edu\n\nabstract\n\nseveral recent studies have identi\ufb01ed similarities between neural r", "research article\n\nneural dynamics at successive stages of\nthe ventral visual stream are consistent\nwith hierarchical error signals\nelias b issa\u2020*, charles f cadieu\u2021, james j dicarlo\n\ndepartment of brain and cognitive sciences, mcgovern institute for brain\nresearch, massachusetts institute of technology, cambridge, united states\n\nabstract ventral visual stream neural responses are dynamic, even for static image\npresentations. however, dynamical neural models of visual cortex are lacking as most p", "article\n\nbayesian computation through cortical latent\ndynamics\n\ngraphical abstract\n\nauthors\n\ntwo-prior time interval reproduction task\n\nhansem sohn, devika narain,\nnicolas meirhaeghe, mehrdad jazayeri\n\nin-vivo\n\nestimation\n\nproduction\n\nready\n\nset\n\ngo\n\nmonkey\nfrontal \ncortex\n\nshort prior\nlong prior\n\nsample interval\n\nin-silico\n\nrecurrent\n\nneural\nnetwork\nmodel\n\ncurved manifold hypothesis\n\ncorrespondence\nmjaz@mit.edu\n\nin brief\nsohn et al. found that prior beliefs warp\nneural representations in the fr", "3\n1\n0\n2\n\n \nc\ne\nd\n9\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n0\n6\n5\n\n.\n\n2\n1\n3\n1\n:\nv\ni\nx\nr\na\n\nplaying atari with deep reinforcement learning\n\nvolodymyr mnih koray kavukcuoglu david silver alex graves\n\nioannis antonoglou\n\ndaan wierstra martin riedmiller\n\ndeepmind technologies\n\n{vlad,koray,david,alex.graves,ioannis,daan,martin.riedmiller} @ deepmind.com\n\nabstract\n\nwe present the \ufb01rst deep learning model to successfully learn control policies di-\nrectly from high-dimensional sensory input using reinforceme", "a r t i c l e s\n\nexperience-dependent spatial expectations in mouse \nvisual cortex\naris fiser1,2,4, david mahringer1,2,4, hassana k oyibo1,2,4, anders v petersen3, marcus leinweber1 &  \ngeorg b keller1,2\n\nin generative models of brain function, internal representations are used to generate predictions of sensory input, yet little is \nknown about how internal models influence sensory processing. here we show that, with experience in a virtual environment, the \nactivity of neurons in layer 2/3 of ", "review\npublished: 25 october 2018\ndoi: 10.3389/fnins.2018.00774\n\ndeep learning with spiking neurons:\nopportunities and challenges\n\nmichael pfeiffer* and thomas pfeil\n\nbosch center for arti\ufb01cial intelligence, robert bosch gmbh, renningen, germany\n\nspiking neural networks (snns) are inspired by information processing in biology, where\nsparse and asynchronous binary signals are communicated and processed in a massively\nparallel fashion. snns on neuromorphic hardware exhibit favorable properties suc", "exascale deep learning for climate analytics\nthorsten kurth\u2217\nmayur mudigonda\u2217\ntkurth@lbl.gov\nmudigonda@berkeley.edu\nnathan luehr\u2020\nmichael matheson\u2021\nmathesonma@ornl.gov\nnluehr@nvidia.com\njack deslippe\u2217\nmichael houston\u2020\njrdeslippe@lbl.gov\nmhouston@nvidia.com\n\nsean treichler\u2020\nsean@nvidia.com\neverett phillips\u2020\nephillips@nvidia.com\nmassimiliano fatica\u2020\nmfatica@nvidia.com\n\njoshua romero\u2020\njoshr@nvidia.com\nankur mahesh\u2217\namahesh@lbl.gov\n\nprabhat@lbl.gov\n\nprabhat\u2217\n\n8\n1\n0\n2\n\n \nt\nc\no\n3\n\n \n\n \n \n]\n\nc\nd\n.\ns\nc\n", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/2021.12.21.473757\n; \n\nthis version posted june 25, 2022. \n\nthe copyright holder for this preprint\n\n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\navailable under a\n\ncc-by-nc-nd 4.0 international license\n.\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\nisolating salient variations of int", "0\n2\n0\n2\n\n \n\np\ne\ns\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n8\n8\n6\n0\n1\n\n.\n\n1\n1\n9\n1\n:\nv\ni\nx\nr\na\n\nrethinking softmax with cross-entropy:\n\nneural network classifier as\nmutual information estimator\n\na preprint\n\nzhenyue qin1,*, dongwoo kim1,2,*, and tom gedeon1\n\n1australian national university, australia\n\n2pohang university of science and technology, republic of korea\n\nzhenyue.qin@anu.edu.au, dongwookim@postech.ac.kr, tom@cs.anu.edu.au\n\n*equal contribution and correspondence\n\nabstract\n\ncross-entropy loss wi", "9\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n1\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n4\n5\n5\n4\n0\n\n.\n\n6\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nprincipled training of neural networks\n\nwith direct feedback alignment\n\njulien launay & iacopo poli\n\nlighton\n\njulien@lighton.ai\niacopo@lighton.ai\n\nflorent krzakala\n\nlighton, laboratoire de physique de l\u2019ens,\n\nuniversit\u00e9 psl, cnrs, sorbonne universit\u00e9, universit\u00e9\n\nparis-diderot, sorbonne paris cit\u00e9, paris, france\n\nflorent.krzakala@ens.fr, florent@lighton.ai\n\nabstract\n\nthe backpropagation algorithm has lon", "research article\n\nattention stabilizes the shared gain of v4\npopulations\nneil c rabinowitz1*, robbe l goris1, marlene cohen2, eero p simoncelli1*\n\n1center for neural science, howard hughes medical institute, new york university,\nnew york, united states; 2department of neuroscience and center for the neural\nbasis of cognition, university of pittsburgh, pittsburgh, united states\n\nabstract responses of sensory neurons represent stimulus information, but are also influenced\nby internal state. for ex", "research article\n\non pixel-wise explanations for non-linear\nclassifier decisions by layer-wise relevance\npropagation\nsebastian bach1,2\u262f*, alexander binder2,5\u262f, gr\u00e9goire montavon2, frederick klauschen3,\nklaus-robert m\u00fcller2,4*, wojciech samek1,2*\n\n1 machine learning group, fraunhofer heinrich hertz institute, berlin, germany, 2 machine learning group,\ntechnische universit\u00e4t berlin, berlin, germany, 3 charit\u00e9 university hospital, berlin, germany,\n4 department of brain and cognitive engineering, ko", "13522 \u2022 the journal of neuroscience, december 10, 2008 \u2022 28(50):13522\u201313531\n\nbehavioral/systems/cognitive\n\nthe neural basis for combinatorial coding in a cortical\npopulation response\n\nleslie c. osborne,1* stephanie e. palmer,3* stephen g. lisberger,1,2 and william bialek3\n1sloan-swartz center for theoretical neurobiology, w. m. keck foundation center for integrative neuroscience, and department of physiology, and\n2howard hughes medical institute, university of california at san francisco, san fr", "research article\ndopamine neurons do not constitute an\nobligatory stage in the final common path for\nthe evaluation and pursuit of brain\nstimulation reward\n\nivan trujillo-pisanty\u00a4, kent conover, pavel solis, daniel palacios, peter shizgalid*\n\ncentre for studies in behavioural neurobiology, concordia university, montreal, que\u00b4bec, canada\n\n\u00a4 current address: center for the neurobiology of addiction, pain, and emotion, department of\nanesthesiology and pain medicine, department of pharmacology, univ", "deepfool: a simple and accurate method to fool deep neural networks\n\nseyed-mohsen moosavi-dezfooli, alhussein fawzi, pascal frossard\n\n\u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\n{seyed.moosavi,alhussein.fawzi,pascal.frossard} at epfl.ch\n\n6\n1\n0\n2\n\n \nl\nu\nj\n \n\n4\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n9\n9\n5\n4\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nstate-of-the-art deep neural networks have achieved im-\npressive results on many image classi\ufb01cation tasks. how-\never, these same architectures have been shown to be", "2\n1\n0\n2\n\n \nl\nu\nj\n \n\n2\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n9\n0\n2\n6\n\n.\n\n2\n1\n1\n1\n:\nv\ni\nx\nr\na\n\nbuilding high-level features\n\nusing large scale unsupervised learning\n\nquoc v. le\nmarc\u2019aurelio ranzato\nrajat monga\nmatthieu devin\nkai chen\ngreg s. corrado\nje\ufb00 dean\nandrew y. ng\n\nquocle@cs.stanford.edu\nranzato@google.com\nrajatmonga@google.com\nmdevin@google.com\nkaichen@google.com\ngcorrado@google.com\njeff@google.com\nang@cs.stanford.edu\n\nabstract\n\n1. introduction\n\nwe consider the problem of building high-\nlevel, cl", "9\n1\n0\n2\n\n \nr\na\n\nm\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n9\n6\n3\n0\n1\n\n.\n\n5\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nstable recurrent models\n\njohn miller\u2217\n\nmoritz hardt\u2020\n\nmarch 5, 2019\n\nabstract\n\nstability is a fundamental property of dynamical systems, yet to this date it has had little\nbearing on the practice of recurrent neural networks.\nin this work, we conduct a thorough\ninvestigation of stable recurrent models. theoretically, we prove stable recurrent neural networks\nare well approximated by feed-forward networks for t", "opinion\n\na  cellular  mechanism  for  cortical\nassociations:  an  organizing  principle\nfor  the  cerebral  cortex\n\nmatthew  larkum\n\nneurocure  cluster  of  excellence,  department  of  biology,  humboldt  university,  charite\u00b4  platz  1,  10117,  berlin,  germany\n\na  basic  feature  of  intelligent  systems  such  as  the  cere-\nbral  cortex  is  the  ability  to  freely  associate  aspects  of\nperceived  experience  with  an  internal  representation  of\nthe  world  and  make  predictions  abo", "eligibility traces provide a data-inspired alternative\n\nto backpropagation through time\n\nguillaume bellec*, franz scherr*, elias hajek, darjan salaj, anand subramoney,\n\nrobert legenstein & wolfgang maass\ninstitute of theoretical computer science\ngraz university of technology, austria\n\n{bellec,scherr,salaj,hajek,legenstein,maass}@igi.tugraz.at\n\n* equal contributions\n\nabstract\n\nlearning in recurrent neural networks (rnns) is most often implemented by\ngradient descent using backpropagation through ", "improving generalisation for temporal\n\ndifference learning:\n\nthe successor representation\n\npeter dayan\n\ncomputational neurobiology laboratory\n\nthe salk institute\n\npo box 85800, san diego ca 92186-5800\n\nabstract\n\nestimation of returns over time, the focus of temporal difference (td) algorithms,\nimposes particular constraints on good function approximators or representations.\nappropriate generalisation between states is determined by how similar their succes-\nsors are, and representations should f", "deep residual learning for image recognition\n\nkaiming he\n\nxiangyu zhang\n\nshaoqing ren\n\njian sun\n\nmicrosoft research\n\n{kahe, v-xiangz, v-shren, jiansun}@microsoft.com\n\nabstract\n\ndeeper neural networks are more dif\ufb01cult to train. we\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. we explicitly reformulate the layers as learn-\ning residual functions with reference to the layer inputs, in-\nstead of learning unreferenced", "neuroscience  letters  680  (2018)  88\u201393\n\ncontents  lists  available  at  sciencedirect\n\nneuroscience\n\n \n\nletters\n\nj o  u r  n a l  h o  m e   p  a g e :  w w w . e l s e v i e r . c o m / l o c a t e / n e u l e t\n\nreview   article\nbarlow   versus   hebb:   when   is   it   time   to   abandon   the   notion   of   feature\ndetectors   and   adopt   the   cell   assembly   as   the   unit   of   cognition?\n\nhoward   eichenbaum\nboston\ncenter\n\nmemory\n\nbrain,\n\nand\n\nfor\n\n \n\n \n\n \n\n \n\n \n\n \n\nuniversit", "distinctive image features\n\nfrom scale-invariant keypoints\n\ndavid g. lowe\n\ncomputer science department\nuniversity of british columbia\n\nvancouver, b.c., canada\n\nlowe@cs.ubc.ca\n\njanuary 5, 2004\n\nabstract\n\nthis paper presents a method for extracting distinctive invariant features from\nimagesthatcanbeusedtoperformreliablematchingbetweendifferentviewsof\nan object or scene. the features are invariant to image scale and rotation, and\nareshowntoprovide robust matching across aasubstantial range ofaf\ufb01ned", "pergamorr\n\n@\n\nprogress in neurobiology, vol. 50, pp.381 to 425, 1996\ncopyright~ 1996elsevierscienceltd. all rightsreserved\nprinted in great britain\n\n0301-0082/96/$32.00\n\npii: s0301-0082(96)00042-1\n\nthe basal ganglia: focused selection and\ninhibition of competing motor programs\n\ndepartment\n\nof neurology, box 8111, washington university school of medicine,\n\n660 s. euclid ave.,\n\nst. louis, mo 63110, u.s.a.\n\njonathan w. mink\n\n(received\n\n17 january\n\n1996; accepted\n\n23 june 1996)\n\nabstract-thebasalgan", "doi: 10.21105/joss.01003\nsoftware\n\n\u2022 review\n\u2022 repository\n\u2022 archive\n\nsubmitted: 17 september 2018\npublished: 01 november 2018\nlicense\nauthors of papers retain copy-\nright and release the work un-\nder a creative commons attri-\nbution 4.0 international license\n(cc-by).\n\nfixedpointfinder: a tensorflow toolbox for identifying\nand characterizing fixed points in recurrent neural\nnetworks\nmatthew d. golub1,2 and david sussillo1,2,3,4\n1 department of electrical engineering, stanford university 2 stanford", "letter\n\ncommunicated by nathaniel daw\n\nattention-gated reinforcement learning of internal\nrepresentations for classi\ufb01cation\n\npieter r. roelfsema\np.roelfsema@ioi.knaw.nl\nnetherlands ophthalmic research institute, 1105 ba amsterdam, netherlands, and\ncenter for neurogenomics and cognitive research, department of experimental\nneurophysiology, vrije universiteit, 1081 hv amsterdam, netherlands\n\narjen van ooyen\narjen.van.ooyen@falw.vu.nl\nnetherlands institute for brain research, 1105 az amsterdam, net", "neuroscience  and  biobehavioral  reviews  68  (2016)  862\u2013879\n\ncontents  lists  available  at  sciencedirect\n\nneuroscience\n\n \n\nand\n\n \n\nbiobehavioral\n\n \n\nreviews\n\nj o u r n a l  h  o m  e p a  g e :  w w w . e l s e v i e r . c o m / l o c a t e / n e u b i o r e v\n\nactive   inference   and   learning\nkarl   friston a,\u2217,   thomas   fitzgerald a,b,   francesco   rigoli a,   philipp   schwartenbeck a,b,c,d,\njohn   o\u2019doherty e,   giovanni   pezzulo f\na the  wellcome  trust  centre  for  neuroimagin", "phil. trans. r. soc. b (2008) 363, 3845\u20133857\ndoi:10.1098/rstb.2008.0158\npublished online 1 october 2008\n\ncortical mechanisms for reinforcement learning\n\nin competitive games\nhyojung seo and daeyeol lee*\n\ndepartment of neurobiology, yale university school of medicine, 333 cedar street,\n\nshm b404, new haven, ct 06510, usa\n\ngame theory analyses optimal strategies for multiple decision makers interacting in a social group.\nhowever, the behaviours of individual humans and animals often deviate system", "understanding dopamine and reinforcement learning:\nthe dopamine reward prediction error hypothesis\n\npaul w. glimcher1\n\ncenter for neuroeconomics, new york university, new york, ny 10003\n\nedited by donald w. pfaff, the rockefeller university, new york, ny, and approved december 9, 2010 (received for review october 14, 2010)\n\na number of recent advances have been achieved in the study of\nmidbrain dopaminergic neurons. understanding these advances\nand how they relate to one another requires a deep ", "r e v i e w s\n\npath integration and the neural basis \nof the \u2018cognitive map\u2019\n\nbruce l. mcnaughton*\u00b6, francesco p. battaglia\u00a7, ole jensen||, edvard i. moser\u00b6 \nand may-britt moser\u00b6\nabstract | the hippocampal formation can encode relative spatial location, without \nreference to external cues, by the integration of linear and angular self-motion (path \nintegration). theoretical studies, in conjunction with recent empirical discoveries, suggest \nthat the medial entorhinal cortex (mec) might perform s", "orthogonal representations for robust context-\ndependent task performance in brains and neural\nnetworks\n\narticle\n\nhighlights\nd we trained arti\ufb01cial neural networks and humans on context-\n\ndependent decision tasks\n\nd initial weight variance determined the network\u2019s\n\nrepresentational geometry\n\nd human fronto-parietal representations were similar to those\n\nof low-variance networks\n\nd theory of nonlinear gating explains how these are formed in\n\nneural networks and brains\n\nauthors\n\ntimo flesch, keno ", "the journal of neuroscience, november 12, 2014 \u2022 34(46):15497\u201315504 \u2022 15497\n\nsymposium\n\nattention, reward, and information seeking\n\nx jacqueline gottlieb,1,2 mary hayhoe,3 okihide hikosaka,6 and antonio rangel4,5\n1department of neuroscience and 2kavli institute for brain science, columbia university, new york, new york 10032, 3university of texas at austin,\naustin, texas 78712, 4division of humanities and social sciences and 5computational and neural systems, california institute of technology, ", "neurocomputing 48 (2002) 17\u201337\n\nwww.elsevier.com/locate/neucom\n\nerror-backpropagation in temporally encoded networks of\n\nspiking neurons\n\nsander m. bohtea;\u2217, joost n. koka;c , han la poutr\u0003ea;b\n\nacwi, kruislaan 413, 1098 sj amsterdam, the netherlands\n\nbschool of technology management, eindhoven university of technology, the netherlands\n\ncliacs, leiden university, p.o. box 9512, 2300 ra leiden, the netherlands\n\nreceived 27 october 2000; accepted 6 june 2001\n\nabstract\n\nfor a network of spiking neu", "journal of mathematical psychology 76 (2017) 198\u2013211\n\ncontents lists available at sciencedirect\n\njournal of mathematical psychology\n\njournal homepage: www.elsevier.com/locate/jmp\n\na tutorial on the free-energy framework for modelling perception\nand learning\nrafal bogacz\u2217\n\nmrc unit for brain network dynamics, university of oxford, mansfield road, oxford, ox1 3th, uk\nnuffield department of clinical neurosciences, university of oxford, john radcliffe hospital, oxford, ox3 9du, uk\n\nh i g h l i g h t", "neural networks 15 (2002) 507\u2013521\n\n2002 special issue\n\nwww.elsevier.com/locate/neunet\n\ndopamine-dependent plasticity of corticostriatal synapses\n\njohn n.j. reynolds, jeffery r. wickens*\n\nthe neuroscience research centre and department of anatomy and structural biology, school of medical sciences, university of otago, p.o. box 913,\n\nreceived 6 november 2001; revised 25 february 2002; accepted 25 february 2002\n\ndunedin, new zealand\n\nabstract\n\nknowledge of the effect of dopamine on corticostriatal ", "a scale-dependent measure of system\ndimensionality\n\narticle\n\nhighlights\nd the scale-dependent dimensionality uni\ufb01es widely used\n\nmeasures of dimensionality\n\nd dynamical systems show distinct dimensionality properties\n\nat different scales\n\nd the scale-dependent dimensionality allows us to identify\n\ncritical scales of the system\n\nd fundamental trends in dimensionality of neural activity\n\ndepend on the brain state\n\nauthors\n\nstefano recanatesi, serena bradde,\nvijay balasubramanian,\nnicholas a. stein", "perspectives\n\no p i n i o n\n\nthe short-latency dopamine signal: \na role in discovering novel actions?\n\npeter redgrave and kevin gurney\n\nabstract | an influential concept in contemporary computational neuroscience is \nthe reward prediction error hypothesis of phasic dopaminergic function. it \nmaintains that midbrain dopaminergic neurons signal the occurrence of \nunpredicted reward, which is used in appetitive learning to reinforce existing \nactions that most often lead to reward. however, the ava", "0\n2\n0\n2\n\n \nt\nc\no\n0\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n5\nv\n9\n5\n3\n5\n0\n\n.\n\n9\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nactivation relaxation: a local dynamical\n\napproximation to backpropagation in the brain\n\nberen millidge\n\nschool of informatics\nuniversity of edinburgh\nberen@millidge.name\n\nalexander tschantz\n\nsackler center for consciousness science\n\nschool of engineering and informatics\n\nuniversity of sussex\n\ntschantz.alec@gmail.com\n\nanil k seth\n\nchristopher l buckley\n\nsackler center for consciousness science\n\nevolutionary an", "when networks do and don\u2019t learn to represent task features \u2013\n\nimplications for neural codes\n\nmatthew farrell, stefano recanatesi, and eric shea-brown\n\nmarch 2023\n\nabstract\n\nneural circuits \u2013 both in the brain and in \u201cartifical\u201d neural network models \u2013 learn to solve a remarkable\nvariety of tasks, and there is great potential in using neural networks as models for brain function. to better\ncharacterize this potential, we survey the literature to gain insight on a fundamental question: what are\nt", "the neuronal code for number\n\nandreas nieder\nabstract | humans and non-human primates share an elemental quantification system that \nresides in a dedicated neural network in the parietal and frontal lobes. in this cortical network, \n\u2018number neurons\u2019 encode the number of elements in a set, its cardinality or numerosity, \nirrespective of stimulus appearance across sensory motor systems, and from both spatial and \ntemporal presentation arrays. after numbers have been extracted from sensory input, t", "www.nature.com/cr\nwww.cell-research.com\n\nopen\n\narticle\na novel somatosensory spatial navigation system outside the\nhippocampal formation\nxiaoyang long1 and sheng-jia zhang1\n\nspatially selective \ufb01ring of place cells, grid cells, boundary vector/border cells and head direction cells constitutes the basic building\nblocks of a canonical spatial navigation system centered on the hippocampal-entorhinal complex. while head direction cells can be\nfound throughout the brain, spatial tuning outside the hi", "diverse feature visualizations reveal invariances\n\nin early layers of deep neural networks\n\nsantiago a. cadena, marissa a. weis, leon a. gatys,\n\nmatthias bethge, and alexander s. ecker\n\ncenter for integrative neuroscience and institute for theoretical physics\n\nbernstein center for computational neuroscience\n\nuniversity of t\u00fcbingen, germany\n\n{first.last}@bethgelab.org\n\nabstract. visualizing features in deep neural networks (dnns) can\nhelp understanding their computations. many previous studies ai", "7\n1\n0\n2\n\n \nc\ne\nd\n \n0\n3\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n2\n6\n0\n0\n0\n\n.\n\n1\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ndendritic error backpropagation\n\nin deep cortical microcircuits\n\njo\u02dcao sacramento1*, rui ponte costa1, yoshua bengio2, walter senn1*\n\n1department of physiology\n\nuniversity of bern, switzerland\n\n2montreal institute for learning algorithms\nuniversit\u00b4e de montr\u00b4eal, quebec, canada\n\nabstract\n\nanimal behaviour depends on learning to associate sensory stimuli with the desired motor com-\nmand. understanding ho", "r e v i e w s\n\nthe free-energy principle:  \na unified brain theory?\n\nkarl friston\n\nabstract | a free-energy principle has been proposed recently that accounts for action, \nperception and learning. this review looks at some key brain theories in the biological (for \nexample, neural darwinism) and physical (for example, information theory and optimal \ncontrol theory) sciences from the free-energy perspective. crucially, one key theme runs \nthrough each of these theories \u2014 optimization. furthermore", "5\n1\n0\n2\n\n \nr\np\na\n0\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n9\n6\n2\n3\n\n.\n\n6\n0\n4\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2015\n\nscheduled denoising autoencoders\n\nkrzysztof j. geras\nschool of informatics\nuniversity of edinburgh\nk.j.geras@sms.ed.ac.uk\n\ncharles sutton\nschool of informatics\nuniversity of edinburgh\ncsutton@inf.ed.ac.uk\n\nabstract\n\nwe present a representation learning method that learns features at multiple dif-\nferent levels of scale. working within the unsupervised framework of d", "a r t i c l e s\n\nspecific evidence of low-dimensional continuous \nattractor dynamics in grid cells\nkijung yoon1, michael a buice1, caswell barry2\u20134, robin hayman4, neil burgess2,3 & ila r fiete1\nwe examined simultaneously recorded spikes from multiple rat grid cells, to explain mechanisms underlying their activity.  \namong grid cells with similar spatial periods, the population activity was confined to lie close to a two-dimensional (2d) manifold: \ngrid cells differed only along two dimensions o", "a distributional perspective on reinforcement learning\n\nmarc g. bellemare * 1 will dabney * 1 r\u00b4emi munos 1\n\n7\n1\n0\n2\n\n \nl\nu\nj\n \n\n1\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n7\n8\n8\n6\n0\n\n.\n\n7\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nin this paper we argue for the fundamental impor-\ntance of the value distribution: the distribution\nof the random return received by a reinforcement\nlearning agent. this is in contrast to the com-\nmon approach to reinforcement learning which\nmodels the expectation of this return, or value.\nal", "topics in cognitive science 7 (2015) 217\u2013229\ncopyright \u00a9 2015 cognitive science society, inc. all rights reserved.\nissn:1756-8757 print / 1756-8765 online\ndoi: 10.1111/tops.12142\n\nrational use of cognitive resources: levels of analysis\n\nbetween the computational and the algorithmic\n\nthomas l. grif\ufb01ths,a falk lieder,a noah d. goodmanb\n\nadepartment of psychology, university of california, berkeley\n\nbdepartment of psychology, stanford university\n\nreceived 22 july 2013; received in revised form 24 f", "3\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n1\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n7\n8\n4\n0\n0\n\n.\n\n2\n0\n3\n2\n:\nv\ni\nx\nr\na\n\njournal of latex class files, vol. 14, no. 8, august 2015\n\n1\n\na comprehensive survey of continual learning:\n\ntheory, method and application\n\nliyuan wang, xingxing zhang, hang su, jun zhu, fellow, ieee\n\nabstract\u2014to cope with real-world dynamics, an intelligent agent needs to incrementally acquire, update, accumulate, and exploit\nknowledge throughout its lifetime. this ability, known as continual learning, pro", "advanced review\n\npredictive coding\nyanping huang and rajesh p. n. rao\u2217\n\npredictive coding is a unifying framework for understanding redundancy reduction\nand ef\ufb01cient coding in the nervous system. by transmitting only the unpredicted\nportions of an incoming sensory signal, predictive coding allows the nervous\nsystem to reduce redundancy and make full use of the limited dynamic range of\nneurons. starting with the hypothesis of ef\ufb01cient coding as a design principle in the\nsensory system, predictive", "communicated by fernando pineda \n\nan  efficient gradient-based algorithm for on-line \ntraining of  recurrent network trajectories \n\nronald j. williams \njing peng \ncollege of  computer  science, northeastern  university, boston, m a  02225 usa \n\na novel variant of  the familiar backpropagation-through-time approach \nto training recurrent networks is described.  this algorithm is intended \nto be used on arbitrary recurrent networks that run continually without \never being  reset to  an initial sta", "reviews\n\nmaking memories last: the synaptic \ntagging and capture hypothesis\n\nroger l. redondo*\u2021 and richard g. m. morris*\n\nabstract | the synaptic tagging and capture hypothesis of protein synthesis-dependent \nlong-term potentiation asserts that the induction of synaptic potentiation creates only  \nthe potential for a lasting change in synaptic efficacy, but not the commitment to such a \nchange. other neural activity, before or after induction, can also determine whether \npersistent change occur", "letters\n\nvol 441|15 june 2006|doi:10.1038/nature04766\n\ncortical substrates for exploratory decisions in\nhumans\nnathaniel d. daw1*, john p. o\u2019doherty2*\u2020, peter dayan1, ben seymour2 & raymond j. dolan2\n\ndecision making in an uncertain environment poses a con\ufb02ict\nbetween the opposing demands of gathering and exploiting infor-\nmation. in a classic illustration of this \u2018exploration\u2013exploitation\u2019\ndilemma1, a gambler choosing between multiple slot machines\nbalances the desire to select what seems, on t", "research article\ndimensionality in recurrent spiking networks:\nglobal trends in activity and local origins in\nconnectivity\n\nstefano recanatesiid1*, gabriel koch ockerid2, michael a. buice1,2,3, eric shea-\nbrown1,2,3\n\n1 center for computational neuroscience, university of washington, seattle, washington, united states of\namerica, 2 allen institute for brain science, seattle, washington, united states of america, 3 department of\napplied mathematics, university of washington, seattle, washington, u", "r e v i e w\n\nfocus on big data\n\ndimensionality reduction for large-scale \nneural recordings\n\njohn p cunningham1 & byron m yu2\u20134\n\nmost sensory, cognitive and motor functions depend on the interactions of many neurons. in recent years, there has been \nrapid development and increasing use of technologies for recording from large numbers of neurons, either sequentially or \nsimultaneously. a key question is what scientific insight can be gained by studying a population of recorded neurons beyond \nstu", "nonlinear ica using auxiliary variables\n\nand generalized contrastive learning\n\n9\n1\n0\n2\n\n \n\nb\ne\nf\n4\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n1\n5\n6\n8\n0\n\n.\n\n5\n0\n8\n1\n:\nv\ni\nx\nr\na\n\naapo hyv\u00a8arinen 1,2\n\nhiroaki sasaki 3,1\n\nrichard e. turner 4\n\n1 the gatsby unit\n\nucl, uk\n\n2 dept. of cs and hiit\nuniv. helsinki, finland\n\n3div. of info. sci.\n\nnaist, japan\n\n4 univ. cambridge &\nmicrosoft research, uk\n\nabstract\n\nnonlinear ica is a fundamental problem\nfor unsupervised representation learning, em-\nphasizing the capa", "partition functions from\n\nrao-blackwellized tempered sampling\n\n6\n1\n0\n2\n\n \n\ny\na\nm\n5\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n2\n1\n9\n1\n0\n\n.\n\n3\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ndavid e. carlson\u22171,2\npatrick stinson\u22172\nari pakman\u22171,2\nliam paninski1,2\n1 department of statistics\n2 grossman center for the statistics of mind\ncolumbia university, new york, ny, 10027\n\nabstract\n\npartition functions of probability distributions\nare important quantities for model evaluation\nand comparisons. we present a new method\nto compute pa", "generative modeling by estimating gradients of the\n\ndata distribution\n\nyang song\n\nstanford university\n\nyangsong@cs.stanford.edu\n\nstefano ermon\n\nstanford university\n\nermon@cs.stanford.edu\n\n0\n2\n0\n2\n\n \nt\nc\no\n0\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n0\n6\n5\n0\n\n.\n\n7\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe introduce a new generative model where samples are produced via langevin\ndynamics using gradients of the data distribution estimated with score matching.\nbecause gradients can be ill-de\ufb01ned and hard to estimate ", "6\n1\n0\n2\n\n \nl\nu\nj\n \n\n8\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n2\n8\n3\n9\n0\n\n.\n\n3\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ndeep networks with stochastic depth\n\ngao huang*, yu sun*, zhuang liu\u2020, daniel sedra, kilian q. weinberger\n\n{gh349, ys646, dms422, kqw4}@cornell.edu, cornell university\n\n* authors contribute equally\n\nliuzhuang13@mails.tsinghua.edu.cn, tsinghua university\n\n\u2020\n\nabstract. very deep convolutional networks with hundreds of layers\nhave led to signi\ufb01cant reductions in error on competitive benchmarks.\nalthough the unma", "knowledge-based systems 199 (2020) 105972\n\ncontents lists available at sciencedirect\n\nknowledge-based systems\n\njournal homepage: www.elsevier.com/locate/knosys\n\ninterpretable neural networks based on continuous-valued logic and\nmulticriteria decision operators\norsolya csisz\u00e1r a,b,\u2217, g\u00e1bor csisz\u00e1r c, j\u00f3zsef dombi d\n\na faculty of basic sciences, university of applied sciences esslingen, esslingen, germany\nb institute of applied mathematics, \u00f3buda university, budapest, hungary\nc institute of materi", "neuron, vol. 36, 241\u2013263, october 10, 2002, copyright \uf8e92002 by cell press\n\ngetting formal with\ndopamine and reward\n\nreview\n\nwolfram schultz1,2,3\n1institute of physiology\nuniversity of fribourg\nch-1700 fribourg\nswitzerland\n2 department of anatomy\nuniversity of cambridge\ncambridge cb2 3dy\nunited kingdom\n\nrecent neurophysiological studies reveal that neurons\nin certain brain structures carry specific signals about\npast and future rewards. dopamine neurons display\na short-latency, phasic reward sign", "j neurophysiol 98: 3648 \u20133665, 2007.\nfirst published october 10, 2007; doi:10.1152/jn.00364.2007.\n\nreinforcement learning with modulated spike timing\u2013dependent\nsynaptic plasticity\n\nmichael a. farries1 and adrienne l. fairhall2\n1department of biology, university of texas at san antonio, san antonio, texas; and 2department of physiology and biophysics,\nuniversity of washington, seattle, washington\n\nsubmitted 2 april 2007; accepted in \ufb01nal form 9 october 2007\n\nfarries ma, fairhall al. reinforcement", "bombesin-like peptide recruits disinhibitory cortical\ncircuits and enhances fear memories\n\narticle\n\ngraphical abstract\n\nauthors\nsarah melzer, elena r. newmark,\ngrace or mizuno, ..., james levasseur,\nlin tian, bernardo l. sabatini\n\ncorrespondence\nbsabatini@hms.harvard.edu\n\nin brief\ncritical function of neuropeptides in\ncortex-dependent behaviors is\ndemonstrated by local and long-range\nsources of the neuropeptide, grp,\nselectively recruiting disinhibitory cortical\nmicrocircuits in auditory cortex ", "j neurophysiol 108: 624 \u2013 644, 2012.\nfirst published april 11, 2012; doi:10.1152/jn.00371.2011.\n\nbehavioral and neural correlates of visuomotor adaptation observed through\na brain-computer interface in primary motor cortex\n\nsteven m. chase,1,3 robert e. kass,2,3 and andrew b. schwartz1,3\n1department of neurobiology, university of pittsburgh, pittsburgh, pennsylvania; 2department of statistics, carnegie mellon\nuniversity, pittsburgh, pennsylvania; and 3center for the neural basis of cognition, ca", "circuit mechanisms for the maintenance and \nmanipulation of information in working memory\n\nnicolas y. masse\u200a\nand david j. freedman\u200a\n\n\u200a1,3*\n\n\u200a1*, guangyu r. yang2,4, h. francis song2,5, xiao-jing wang\u200a\n\n\u200a2  \n\nrecently it has been proposed that information in working memory (wm) may not always be stored in persistent neuronal \nactivity but can be maintained in \u2018activity-silent\u2019 hidden states, such as synaptic efficacies endowed with short-term synap-\ntic plasticity. to test this idea computational", "13326 \u2022 the journal of neuroscience, october 6, 2010 \u2022 30(40):13326 \u201313337\n\ndevelopment/plasticity/repair\n\nfunctional requirements for reward-modulated\nspike-timing-dependent plasticity\n\nnicolas fre\u00b4maux,* henning sprekeler,* and wulfram gerstner\nschool of computer and communication sciences and brain-mind institute, ecole polytechnique fe\u00b4de\u00b4rale de lausanne, ch-1015 lausanne, switzerland\n\nrecent experiments have shown that spike-timing-dependent plasticity is influenced by neuromodulation. we ", "math. program., ser. b (2015) 151:249\u2013281\ndoi 10.1007/s10107-015-0893-2\n\nfull length paper\n\nrecent advances in trust region algorithms\n\nya-xiang yuan1\n\nreceived: 29 september 2014 / accepted: 17 february 2015 / published online: 15 march 2015\n\u00a9 springer-verlag berlin heidelberg and mathematical optimization society 2015\n\nabstract trust region methods are a class of numerical methods for optimization.\nunlike line search type methods where a line search is carried out in each iteration,\ntrust regi", "vol 442|13 july 2006|doi:10.1038/nature04968\n\nletters\n\na high-performance brain\u2013computer interface\ngopal santhanam1*, stephen i. ryu1,2*, byron m. yu1, afsheen afshar1,3 & krishna v. shenoy1,4\n\nrecent studies have demonstrated that monkeys1\u20134 and humans5\u20139\ncan use signals from the brain to guide computer cursors. brain\u2013\ncomputer interfaces (bcis) may one day assist patients suffering\nfrom neurological injury or disease, but relatively low system\nperformance remains a major obstacle. in fact, the", "high-precision coding in visual cortex\n\narticle\n\ngraphical abstract\n\nauthors\ncarsen stringer, michalis michaelos,\ndmitri tsyboulski, sarah e. lindo,\nmarius pachitariu\n\ncorrespondence\npachitarium@janelia.hhmi.org\n\nin brief\nlarge-scale recordings in mouse primary\nvisual cortex and higher order visual\nareas uncover neural representations\nmore precise than behavioral\ndiscrimination thresholds, suggesting\nvisual perception is limited by non-\nsensory brain networks.\n\nhighlights\nd large-scale recording", "letter\n\ncommunicated by peter dayan\n\nthe successor representation and temporal context\n\nsamuel j. gershman\nsjgershm@princeton.edu\nchristopher d. moore\nchrmoore@gmail.com\nmichael t. todd\nmttodd@princeton.edu\nkenneth a. norman\nknorman@princeton.edu\ndepartment of psychology and princeton neuroscience institute,\nprinceton university, princeton, nj 08540\n\nper b. sederberg\nsederberg.1@osu.edu\ndepartment of psychology, the ohio state university, columbus oh 43210, u.s.a.\n\nthe successor representation w", "annu. rev. neurosci. 2000. 23:473\u2013500\ncopyright q 2000 by annual reviews. all rights reserved\n\nneuronal coding\nof prediction errors\n\nwolfram schultz1 and anthony dickinson2\n1institute of physiology and program in neuroscience, university of fribourg, ch-1700\nfribourg, switzerland; e-mail: wolfram.schultz@unifr.ch\n2department of experimental psychology, university of cambridge, cambridge cb2\n3eb, united kingdom; e-mail: ad15@cus.cam.ac.uk\n\nkey words\n\nlearning, plasticity, dopamine, reward, attent", "perspectives\n\nand her attitude would remain negative. but\nalice might join bob if he wants to go to the\nrestaurant. by visiting the restaurant again,\nalice gets a chance to change her opinion.\nalice\u2019s attitude will depend on bob\u2019s, but\nonly because he influenced the probability\nof her revisiting the restaurant. \n\nfinally, the number of your friends who\nengage in some activity can also influence\nyour estimate of the value of this activity. if\nyou have many friends who start firms, for\nexample, yo", "learning curves for stochastic gradient descent\n\nin linear feedforward networks\n\njustin werfel\ndept. of eecs\n\nmit\n\ncambridge, ma 02139\njkwerfel@mit.edu\n\nxiaohui xie\n\nh. sebastian seung\n\nhhmi\n\nmit\n\ndept. of brain & cog. sci.\n\ncambridge, ma 02139\n\nseung@mit.edu\n\ndept. of molecular biology\n\nprinceton university\nprinceton, nj 08544\n\nxhx@princeton.edu\n\nabstract\n\ngradient-following learning methods can encounter problems of imple-\nmentation in many applications, and stochastic variants are frequently\n", "research article\n\nexplaining data-driven document classifications1\n\ndepartment of engineering management, faculty of applied economics, university of antwerp, prinsstraat 13,\n\n2018 antwerp, belgium  {david.martens@ua.ac.be}\n\ndavid martens\n\ndepartment of information, operations and management sciences, stern school of business, new york university,\n\n44 west 4th street, new york, ny  10012-1126  u.s.a.  {fprovost@stern.nyu.edu}\n\nfoster provost\n\nmany  document  classification  applications  require", "0\n2\n0\n2\n\n \nt\nc\no\n3\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n3\n2\n1\n7\n0\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nkernelized information bottleneck leads to\n\nbiologically plausible 3-factor hebbian learning in\n\ndeep networks\n\ngatsby computational neuroscience unit\n\ngatsby computational neuroscience unit\n\nroman pogodin\n\nuniversity college london\n\nlondon, w1t 4jg\n\nroman.pogodin.17@ucl.ac.uk\n\npeter e. latham\n\nuniversity college london\n\nlondon, w1t 4jg\n\npel@gatsby.ucl.ac.uk\n\nabstract\n\nthe state-of-the art machine learning ap", "neural networks 16 (2003) 1429\u20131451\n\nwww.elsevier.com/locate/neunet\n\nthe general inef\ufb01ciency of batch training for gradient\n\ndescent learning\n\nd. randall wilsona,*, tony r. martinezb,1\n\nafonix corporation, 180 west election road suite 200, draper, ut, usa\n\nbcomputer science department, 3361 tmcb, brigham young university, provo, ut 84602, usa\n\nreceived 10 july 2001; revised 8 april 2003; accepted 8 april 2003\n\nabstract\n\ngradient descent training of neural networks can be done in either a batch o", "two routes to scalable credit assignment without weight symmetry\n\ndaniel kunin * 1 aran nayebi * 2 javier sagastuy-brena * 1\n\nsurya ganguli 3 jonathan m. bloom 4 5 daniel l. k. yamins 6 7 8\n\nabstract\n\nthe neural plausibility of backpropagation has\nlong been disputed, primarily for its use of non-\nlocal weight transport \u2014 the biologically dubi-\nous requirement that one neuron instantaneously\nmeasure the synaptic weights of another. until re-\ncently, attempts to create local learning rules that\nav", "p\nh\ny\ns\ni\nc\na\n \n9\nd\n \n(\n1\n9\n8\n3\n)\n \n1\n8\n9\n-\n2\n0\n8\n \nn\no\nr\nt\nh\n-\nh\no\nl\nl\na\nn\nd\n \np\nu\nb\nl\ni\ns\nh\ni\nn\ng\n \nc\no\nm\np\na\nn\ny\n \nm\ne\na\ns\nu\nr\ni\nn\ng\n \nt\nh\ne\n \ns\nt\nr\na\nn\ng\ne\nn\ne\ns\ns\n \no\nf\n \ns\nt\nr\na\nn\ng\ne\n \na\nt\nt\nr\na\nc\nt\no\nr\ns\n \np\ne\nt\ne\nr\n \ng\nr\na\ns\ns\nb\ne\nr\ng\ne\nr\nt\n \na\nn\nd\n \ni\nt\na\nm\na\nr\n \np\nr\no\nc\na\nc\nc\ni\na\n \nd\ne\np\na\nr\nt\nm\ne\nn\nt\n \no\nf\n \nc\nh\ne\nm\ni\nc\na\nl\n \np\nh\ny\ns\ni\nc\ns\n,\n \nw\ne\ni\nz\nm\na\nn\nn\n \ni\nn\ns\nt\ni\nt\nu\nt\ne\n \no\nf\n \ns\nc\ni\ne\nn\nc\ne\n,\n \nr\ne\nh\no\nv\no\nt\n \n7\n6\n1\n0\n0\n,\n \ni\ns\nr\na\ne\nl\n \nr\ne\nc\ne\ni\nv\ne\nd\n \n1\n6\n \nn\no\nv\ne\nm\nb\n", "..............................................................\neffective leadership and decision-\nmaking in animal groups on the move\n\niain d. couzin1,2, jens krause3, nigel r. franks4 & simon a. levin1\n\n1department of ecology and evolutionary biology, princeton university,\nprinceton, new jersey 08544, usa\n2department of zoology, south parks road, university of oxford, oxford\nox1 3ps, uk\n3centre for biodiversity and conservation, school of biology, university of leeds,\nleeds ls2 9jt, uk\n4centre ", "p1: fxz\njanuary 12, 2001\n\n14:38\n\nannual reviews\n\nar121-07\n\nannu. rev. neurosci. 2001. 24:167\u2013202\ncopyright c(cid:13) 2001 by annual reviews. all rights reserved\n\nan integrative theory of prefrontal\ncortex function\n\nearl k. miller\ncenter for learning and memory, riken-mit neuroscience research center and\ndepartment of brain and cognitive sciences, massachusetts institute of technology,\ncambridge, massachusetts 02139; e-mail: ekm@ai.mit.edu\n\njonathan d. cohen\ncenter for the study of brain, mind, a", "7\n1\n0\n2\n\n \nr\na\n\nm\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n4\n0\n1\n8\n0\n\n.\n\n5\n0\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\ndeep predictive coding networks for video\nprediction and unsupervised learning\n\nwilliam lotter, gabriel kreiman & david cox\nharvard university\ncambridge, ma 02215, usa\n{lotter,davidcox}@fas.harvard.edu\ngabriel.kreiman@tch.harvard.edu\n\nabstract\n\nwhile great strides have been made in using deep learning algorithms to solve\nsupervised learning tasks, the problem of un", "brain-inspired learning on neuromorphic\n\n1\n\nsubstrates\n\nfriedemann zenke\u2020, emre o. neftci\u2020\n\n\u2020 all authors contributed equally\n\n0\n2\n0\n2\n\n \nt\nc\no\n2\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n1\n3\n9\n1\n1\n\n.\n\n0\n1\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\u2014neuromorphic hardware strives to emulate\nbrain-like neural networks and thus holds the promise for\nscalable, low-power information processing on temporal\ndata streams. yet, to solve real-world problems, these\nnetworks need to be trained. however, training on neuro-\nmorphic sub", "7\n1\n0\n2\n\n \nr\na\n\nm\n3\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n3\nv\n4\n2\n7\n1\n0\n\n.\n\n1\n0\n7\n1\n:\nv\ni\nx\nr\na\n\ndeepstack: expert-level arti\ufb01cial intelligence in\n\nheads-up no-limit poker\n\nmatej morav\u02c7c\u00b4\u0131k\u2660,\u2665,\u2020, martin schmid\u2660,\u2665,\u2020, neil burch\u2660, viliam lis\u00b4y\u2660,\u2663,\n\ndustin morrill\u2660, nolan bard\u2660, trevor davis\u2660,\n\nkevin waugh\u2660, michael johanson\u2660, michael bowling\u2660,\u2217\n\n\u2660department of computing science, university of alberta,\n\nedmonton, alberta, t6g2e8, canada\n\n\u2665department of applied mathematics, charles university,\n\n\u2663department of", "0\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n1\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n2\n5\n1\n0\n\n.\n\n0\n1\n9\n1\n:\nv\ni\nx\nr\na\n\ngated linear networks\n\njoel veness * 1 tor lattimore * 1 david budden * 1 avishkar bhoopchand * 1 christopher mattern 1\n\nagnieszka grabska-barwinska 1 eren sezener 1 jianan wang 1 peter toth 1 simon schmitt 1 marcus hutter 1\n\nabstract\n\npaper\n\npresents\n\na\nneural\n\nnew family\n\nof\nthis\nbackpropagation-free\narchitectures,\ngated linear networks (glns). what dis-\ntinguishes glns from contemporary neural\nnetworks is", "published as a conference paper at iclr 2019\n\nbayesian deep convolutional networks with\nmany channels are gaussian processes\n\njaehoon lee \u2021 \u2217 yasaman bahri \u2021 greg yang\u25e6,\n\nroman novak \u2020, lechao xiao \u2020 \u2217 ,\njiri hron(cid:5), daniel a. abola\ufb01a,\ngoogle brain, \u25e6microsoft research ai, (cid:5) department of engineering, university of cambridge\n{romann, xlc, jaehlee, yasamanb, \u25e6gregyang@microsoft.com,\n(cid:5)jh2084@cam.ac.uk, danabo, jpennin, jaschasd}@google.com\n\njascha sohl-dickstein\n\njeffrey penningto", "trade-off between curvature tuning and position\ninvariance in visual area v4\n\ntatyana o. sharpeea,b,1, minjoon kouha,b,c,2, and john h. reynoldsc\n\nacomputational and csystems neurobiology laboratories, salk institute for biological studies, la jolla, ca 92037; and bcenter for theoretical biological\nphysics, university of california, san diego, la jolla, ca 92093\n\nedited by charles gross, princeton university, princeton, nj, and approved may 30, 2013 (received for review october 9, 2012)\n\nhumans ", "rapid communication\n\nhippocampus 00:00\u201300 (2015)\n\nstatistical learning of temporal community\n\nstructure in the hippocampus\n\nanna c. schapiro,* nicholas b. turk-browne,\nkenneth a. norman, and matthew m. botvinick\n\nabstract:\nthe hippocampus is involved in the learning and repre-\nsentation of temporal statistics, but little is understood about the kinds\nof statistics it can uncover. prior studies have tested various forms of\nstructure that can be learned by tracking the strength of transition prob-", "ieee transactions on evolutionary computation, vol. 11, no. 2, april 2007\n\n265\n\nintrinsic motivation systems for autonomous\n\nmental development\n\npierre-yves oudeyer, fr\u00e9d\u00e9ric kaplan, and verena v. hafner\n\nabstract\u2014exploratory activities seem to be intrinsically re-\nwarding for children and crucial for their cognitive development.\ncan a machine be endowed with such an intrinsic motivation\nsystem? this is the question we study in this paper, presenting a\nnumber of computational systems that try to", "predictive coding of dynamical variables in balanced\nspiking networks\n\nmartin boerlin1, christian k. machens2, sophie dene` ve1*\n1 group for neural theory, de\u00b4partement d\u2019e\u00b4tudes cognitives, e\u00b4cole normale supe\u00b4rieure, paris, france, 2 champalimaud neuroscience programme, champalimaud centre\nfor the unknown, lisbon, portugal\n\nabstract\n\ntwo observations about the cortex have puzzled neuroscientists for a long time. first, neural responses are highly variable.\nsecond, the level of excitation and i", "a task-optimized neural network replicates human\nauditory behavior, predicts brain responses, and\nreveals a cortical processing hierarchy\n\narticle\n\nhighlights\nd a deep neural network optimized for speech and music tasks\n\nperformed as well as human listeners\n\nd the optimization produced separate music and speech\n\npathways after a shared front end\n\nd the network made human-like error patterns and predicted\n\nauditory cortical responses\n\nd network predictions suggest hierarchical organization in\n\nhu", "research\n\nneuroscience\n\nring attractor dynamics in the\ndrosophila central brain\n\nsung soo kim,* herv\u00e9 rouault,* shaul druckmann,\u2020 vivek jayaraman\u2020\n\nring attractors are a class of recurrent networks hypothesized to underlie the representation\nof heading direction. such network structures, schematized as a ring of neurons whose\nconnectivity depends on their heading preferences, can sustain a bump-like activity pattern\nwhose location can be updated by continuous shifts along either turn direction. ", "variational learning for recurrent spiking networks\n\ndanilo jimenez rezende\n\nbrain mind institute\n\n\u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\n1015 lausanne epfl, switzerland\ndanilo.rezende@epfl.ch\n\nschool of computer and communication sciences, brain mind institute\n\ndaan wierstra\n\n\u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\n1015 lausanne epfl, switzerland\n\ndaan.wierstra@epfl.ch\n\nschool of computer and communication sciences, brain mind institute\n\nwulfram gerstner\n\n\u00b4ecole polytechnique f\u00b4ed\u00b4er", "0\n2\n0\n2\n\n \n\ng\nu\na\n1\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n8\n1\n8\n0\n\n.\n\n8\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nprevalence of neural collapse during the terminal\nphase of deep learning training\n\nvardan papyana,1, x.y. hanb,1, and david l. donohoa,2\nadepartment of statistics, stanford university; bschool of operations research and information engineering, cornell university\n\nthis manuscript was compiled on august 24, 2020\n\nmodern practice for training classi\ufb01cation deepnets involves a termi-\nnal phase of training (tpt)", "research article\n\none-shot learning and behavioral\neligibility traces in sequential decision\nmaking\nmarco p lehmann1,2*, he a xu3, vasiliki liakoni1,2, michael h herzog3\u2020,\nwulfram gerstner1,2\u2020, kerstin preuschoff4\u2020\n\n1brain-mind-institute, school of life sciences, e\u00b4 cole polytechnique fe\u00b4 de\u00b4 rale de\nlausanne, lausanne, switzerland; 2school of computer and communication\nsciences, e\u00b4 cole polytechnique fe\u00b4 de\u00b4 rale de lausanne, lausanne, switzerland;\n3laboratory of psychophysics, school of life s", "7\n1\n0\n2\n\n \nc\ne\nd\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n0\n1\n3\n0\n0\n\n.\n\n2\n1\n7\n1\n:\nv\ni\nx\nr\na\n\ndeep learning with permutation-invariant operator\n\nfor multi-instance histopathology classi\ufb01cation\n\njakub m. tomczak\n\nuniversity of amsterdam\n\nmaximilian ilse\n\nuniversity of amsterdam\n\nmax welling\n\nuniversity of amsterdam\n\nabstract\n\nthe computer-aided analysis of medical scans is a longstanding goal in the medical\nimaging \ufb01eld. currently, deep learning has became a dominant methodology\nfor supporting pathologis", "published as a conference paper at iclr 2017\n\ndeep variational information bottleneck\n\nalexander a. alemi, ian fischer, joshua v. dillon, kevin murphy\ngoogle research\n{alemi,iansf,jvdillon,kpmurphy}@google.com\n\nabstract\n\nwe present a variational approximation to the information bottleneck of tishby\net al. (1999). this variational approach allows us to parameterize the informa-\ntion bottleneck model using a neural network and leverage the reparameterization\ntrick for ef\ufb01cient training. we call th", "biorxiv preprint \nthe copyright holder for this preprint (which was\nnot certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available \n\nthis version posted november 12, 2017. \n\nhttps://doi.org/10.1101/214262\n; \n\ndoi: \n\nunder a\n\ncc-by-nc-nd 4.0 international license\n.\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\na theory of multineuronal dimensionality", "3\n2\n0\n2\n\n \nt\nc\no\n4\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n6\n4\n0\n6\n1\n\n.\n\n0\n1\n3\n2\n:\nv\ni\nx\nr\na\n\na unified, scalable framework for\n\nneural population decoding\n\nmehdi azabou1,\u2217, vinam arora1, venkataramana ganesh1, ximeng mao2,3 ,\n\nsantosh nachimuthu1, michael j. mendelson1, blake richards2,4,\n\nmatthew g. perich2,3, guillaume lajoie2,3, eva l. dyer1,\u2217\n\n1 georgia tech, 2 mila, 3 universit\u00e9 de montr\u00e9al, 4 mcgill university\n\nabstract\n\nour ability to use deep learning approaches to decipher neural activity wo", "letter\n\ncommunicated by mark van rossum\n\nidenti\ufb01cation of stable spike-timing-dependent\nplasticity from spiking activity with generalized\nmultilinear modeling\n\nbrian s. robinson\nbsrobins@usc.edu\ntheodore w. berger\nberger@usc.edu\ndong song\ndsong@usc.edu\ndepartment of biomedical engineering, university of southern california,\nlos angeles, ca 90089, u.s.a.\n\ncharacterization of long-term activity-dependent plasticity from behav-\niorally driven spiking activity is important for understanding the unde", "neuron, vol. 22, 435\u2013450, march, 1999, copyright \u00aa 1999 by cell press\n\nthe neural code of the retina\n\nreview\n\nmarkus meister* and michael j. berry ii\ndepartment of molecular and cellular biology\nharvard university\ncambridge, massachusetts 02138\n\nintroduction\nhow do action potentials represent information?\naction potentials are the standard signal conveyed be-\ntween neurons in the central nervous system. it is a long-\nstanding question how these spikes represent sensory\ninput, internal states of ", "longshort-termmemoryandlearning-to-learninnetworksofspikingneuronsguillaumebellec*,darjansalaj*,anandsubramoney*,robertlegenstein&wolfgangmaassinstitutefortheoreticalcomputersciencegrazuniversityoftechnology,austria{bellec,salaj,subramoney,legenstein,maass}@igi.tugraz.at*equalcontributionsabstractrecurrentnetworksofspikingneurons(rsnns)underlietheastoundingcomput-ingandlearningcapabilitiesofthebrain.butcomputingandlearningcapabilitiesofrsnnmodelshaveremainedpoor,atleastincomparisonwitharti\ufb01cialn", "journal of the experimental analysis of behavior\n\n2005, 84, 555\u2013579\n\nnumber 3 (november)\n\ndynamic response-by-response models of matching behavior in rhesus monkeys\n\nbrian lau and paul w. glimcher\n\nnew york university\n\nwe studied the choice behavior of 2 monkeys in a discrete-trial task with reinforcement contingencies\nsimilar to those herrnstein (1961) used when he described the matching law. in each session, the\nmonkeys experienced blocks of discrete trials at different relative-reinforcer fre", "i an update to this article is included at the end\n\nll\n\nprimer\narti\ufb01cial neural networks\nfor neuroscientists: a primer\n\nguangyu robert yang1,* and xiao-jing wang2,*\n1center for theoretical neuroscience, columbia university, new york, ny, usa\n2center for neural science, new york university, new york, ny, usa\n*correspondence: robert.yang@columbia.edu (g.r.y.), xjwang@nyu.edu (x.-j.w.)\nhttps://doi.org/10.1016/j.neuron.2020.09.005\n\nsummary\n\narti\ufb01cial neural networks (anns) are essential tools in mac", "backpropagation through time: what it \ndoes and how to do it \n\npaul  j. werbos \n\nbackpropagation  is  now the most  widely  used tool in the field \nof artificial neural  networks.  at  the core of backpropagation is  a \nmethod  for  calculating derivatives  exactly  and  efficiently  in any \nlarge  system made  up of elementary  subsystems or  calculations \nwhich are represented  by  known,  differentiable  functions;  thus, \nbackpropagation  has  many  applications  which  do  not  involve \nneu", "an anatomically constrained model for path\nintegration in the bee brain\n\narticle\n\nhighlights\nd central complex neuroarchitecture suf\ufb01ces as a path\n\nintegration circuit\n\nd compass heading and forward speed information converge\n\nin the bee central complex\n\nd columnar noduli neurons plausibly encode a distributed\n\nmemory of the home vector\n\nd the circuit additionally compares current and desired\n\nheadings to initiate steering\n\nauthors\n\nthomas stone, barbara webb,\nandrea adden, ..., luca scimeca,\ner", "deep learning via hessian-free optimization\n\njames martens\nuniversity of toronto, ontario, m5s 1a1, canada\n\njmartens@cs.toronto.edu\n\nabstract\n\nwe develop a 2nd-order optimization method\nbased on the \u201chessian-free\u201d approach, and apply\nit to training deep auto-encoders. without using\npre-training, we obtain results superior to those\nreported by hinton & salakhutdinov (2006) on\nthe same tasks they considered. our method is\npractical, easy to use, scales nicely to very large\ndatasets, and isn\u2019t limi", "neural system identi\ufb01cation for large populations\n\nseparating \u201cwhat\u201d and \u201cwhere\u201d\n\ndavid a. klindt * 1-3, alexander s. ecker * 1,2,4,6, thomas euler 1-3, matthias bethge 1,2,4-6\n\n* authors contributed equally\n\n1 centre for integrative neuroscience, university of t\u00fcbingen, germany\n\n2 bernstein center for computational neuroscience, university of t\u00fcbingen, germany\n\n3 institute for ophthalmic research, university of t\u00fcbingen, germany\n4 institute for theoretical physics, university of t\u00fcbingen, germa", "clique topology reveals intrinsic geometric structure in\nneural correlations\n\nchad giustia,b, eva pastalkovac, carina curtob,d,1, and vladimir itskovb,d,1,2\n\nawarren center for network and data science, departments of bioengineering and mathematics, university of pennsylvania, philadelphia, pa 19104;\nbdepartment of mathematics, university of nebraska, lincoln, ne 68588; cjanelia research campus, howard hughes medical institute, ashburn, va\n20147; and ddepartment of mathematics, the pennsylvania ", "perspective\n\nhttps://doi.org/10.1038/s41467-019-11786-6\n\nopen\n\na critique of pure learning and what arti\ufb01cial neural\nnetworks can learn from animal brains\nanthony m. zador1\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\narti\ufb01cial neural networks (anns) have undergone a revolution, catalyzed by better super-\nvised learning algorithms. however, in stark contrast to young animals (including humans),\ntraining such networks requires enormous numbers of labeled examples, leading to the belief\nthat animals must rely ", "model-agnostic meta-learning for fast adaptation of deep networks\n\n7\n1\n0\n2\n\n \nl\nu\nj\n \n\n8\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n0\n4\n3\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nchelsea finn 1 pieter abbeel 1 2 sergey levine 1\n\nabstract\n\nwe propose an algorithm for meta-learning that\nis model-agnostic, in the sense that it is com-\npatible with any model trained with gradient de-\nscent and applicable to a variety of different\nlearning problems, including classi\ufb01cation, re-\ngression, and reinforcement learning. the goal", "small stimuli, barely covering the neurons\u2019 receptive\nfields, are used.\n\n29. a. renart et al., science 327, 587 (2010).\n30. v. braitenberg, a. sch\u00fcz, anatomy of the cortex:\nstatistics and geometry (springer, berlin, 1991).\n31. b. hellwig, a. sch\u00fcz, a. aertsen, biol. cybern. 71,\n\n1 (1994).\na. reyes, nature 448, 802 (2007).\n\n32. j. de la rocha, b. doiron, e. shea-brown, k. josi\u0107,\n33. y. c. yu, r. s. bultje, x. wang, s. h. shi, nature 458,\n34. j. hertz, neural comput., published online 20 october\n\n", "appl. comput. harmon. anal. 14 (2003) 257\u2013275\n\nwww.elsevier.com/locate/acha\n\ngrassmannian frames with applications\n\nto coding and communication\n\nthomas strohmer a,\u2217,1 and robert w. heath jr. b\n\na department of mathematics, university of california, davis, ca 95616-8633, usa\n\nb department of electrical and computer engineering, the university of texas at austin, austin, tx 78727, usa\n\nreceived 19 july 2002; revised 28 february 2003; accepted 26 march 2003\n\ncommunicated by henrique malvar\n\nabstrac", "downloaded from \n\nhttp://cshperspectives.cshlp.org/\n\n at nyu med ctr library on september 5, 2023 - published by cold spring\n\nharbor laboratory press \n\nnmda receptor-dependent long-term\npotentiation and long-term depression\n(ltp/ltd)\n\nchristian lu\u00a8 scher1 and robert c. malenka2\n\n1department of basic neurosciences and clinic of neurology, university of geneva and geneva university\nhospital, 1211 geneva, switzerland\n2nancy pritzker laboratory, department of psychiatry and behavioral sciences, stan", "r e v i e w\n\nthe distinct modes of vision offered by\nfeedforward and recurrent processing\n\nvictor a.f. lamme and pieter r. roelfsema\n\nan analysis of response latencies shows that when an image is presented to the visual system,\nneuronal activity is rapidly routed to a large number of visual areas.however,the activity of cortical\nneurons is not determined by this feedforward sweep alone. horizontal connections within areas,\nand higher areas providing feedback, result in dynamic changes in tuning.", "deep physical neural networks trained with \nbackpropagation\n\nhttps://doi.org/10.1038/s41586-021-04223-6\nreceived: 19 may 2021\naccepted: 9 november 2021\npublished online: 26 january 2022\nopen access\n\n check for updates\n\nlogan g. wright1,2,4\u2009\u2709, tatsuhiro onodera1,2,4\u2009\u2709, martin m. stein1, tianyu wang1, \ndarren t. schachter3, zoey hu1 & peter l. mcmahon1\u2009\u2709\n\ndeep-learning models have become pervasive tools in science and engineering. \nhowever, their energy requirements now increasingly limit their sc", "biorxiv preprint \nthe copyright holder for this preprint (which was\nnot certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available \n\nthis version posted december 2, 2018. \n\nhttps://doi.org/10.1101/338947\n; \n\ndoi: \n\nunder a\n\ncc-by 4.0 international license\n.\n\nmanifold-tiling localized receptive fields are\n\noptimal in similarity-preserving neural networks\n\nanirvan m. sengupta\u2020\u2021\n\nmariano tepper\u2021\u21e4\n\ncengiz pehlevan\u2021\u21e4\n\n", "co11ch22_ganguli\n\narjats.cls\n\nfebruary 13, 2020\n\n10:27\n\nannual review of condensed matter physics\nstatistical mechanics of\ndeep learning\n\nyasaman bahri,1 jonathan kadmon,2\njeffrey pennington,1 sam s. schoenholz,1\njascha sohl-dickstein,1 and surya ganguli1,2\n1google brain, google inc., mountain view, california 94043, usa\n2department of applied physics, stanford university, stanford, california 94035, usa;\nemail: sganguli@stanford.edu\n\nkeywords\nneural networks, machine learning, dynamical phase t", "research article\n\nthe effects of chloride dynamics on\nsubstantia nigra pars reticulata responses\nto pallidal and striatal inputs\nryan s phillips1,2, ian rosner2,3, aryn h gittis2,3, jonathan e rubin1,2*\n\n1department of mathematics, university of pittsburgh, pittsburgh, united states;\n2center for the neural basis of cognition, pittsburgh, united states; 3department\nof biological sciences, carnegie mellon university, pittsburgh, united states\n\nabstract as a rodent basal ganglia (bg) output nucleus", "on the relation between universality, characteristic kernels and\n\nrkhs embedding of measures\n\nbharath k. sriperumbudur\ndept. of ece, uc san diego\n\nla jolla, usa.\n\nbharathsv@ucsd.edu\n\nkenji fukumizu\n\nthe institute of statistical\nmathematics, tokyo, japan.\n\nfukumizu@ism.ac.jp\n\ngert r. g. lanckriet\n\ndept. of ece, uc san diego\n\nla jolla, usa.\n\ngert@ece.ucsd.edu\n\nabstract\n\ntion in a rkhs that has the representation,\n\nuniversal kernels have been shown to play\nan important role in the achievability of ", "1\n2\n0\n2\n\n \nt\nc\no\n5\n2\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n2\nv\n0\n7\n4\n0\n1\n\n.\n\n0\n1\n1\n2\n:\nv\ni\nx\nr\na\n\ninterpreting deep learning models in natural language processing:\n\na review\n\nxiaofei sun1, diyi yang2, xiaoya li1, tianwei zhang3,\n\nyuxian meng1, han qiu4, guoyin wang5, eduard hovy6, jiwei li1,7\n\n1shannon.ai, 2georgia institute of technology\n\n3nanyang technological university, 4tsinghua university\n\n5amazon alexa ai, 6carnegie mellon university, 7zhejiang university\n\nabstract\n\nneural network models have achiev", "memory-ef\ufb01cient backpropagation through time\n\naudr\u00afunas gruslys\ngoogle deepmind\n\naudrunas@google.com\n\nr\u00e9mi munos\n\ngoogle deepmind\n\nmunos@google.com\n\nivo danihelka\n\ngoogle deepmind\n\ndanihelka@google.com\n\nmarc lanctot\n\ngoogle deepmind\n\nlanctot@google.com\n\nalex graves\n\ngoogle deepmind\n\ngravesa@google.com\n\nabstract\n\nwe propose a novel approach to reduce memory consumption of the backpropa-\ngation through time (bptt) algorithm when training recurrent neural networks\n(rnns). our approach uses dynamic ", "12366 \u2022 the journal of neuroscience, september 15, 2010 \u2022 30(37):12366 \u201312378\n\nbehavioral/systems/cognitive\n\nan approximately bayesian delta-rule model explains the\ndynamics of belief updating in a changing environment\n\nmatthew r. nassar,1 robert c. wilson,2 benjamin heasly,1 and joshua i. gold1\n1department of neuroscience, university of pennsylvania, philadelphia, pennsylvania 19104, and 2department of psychology, princeton university,\nprinceton, new jersey 08540\n\nmaintaining appropriate belief", "9\n1\n0\n2\n\n \nr\na\n\n \n\nm\n2\n1\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n5\n6\n9\n7\n0\n\n.\n\n2\n1\n8\n1\n:\nv\ni\nx\nr\na\n\ndeep learning with asymmetric connections and hebbian\n\nupdates\n\nyali amit\n\nuniversity of chicago\n\nmarch 14, 2019\n\nabstract\n\nwe show that deep networks can be trained using hebbian updates yielding similar\nperformance to ordinary back-propagation on challenging image datasets. to overcome\nthe unrealistic symmetry in connections between layers, implicit in back-propagation,\nthe feedback weights are separate fro", "m\na\nc\nh\ni\nn\ne\n \nl\ne\na\nr\nn\ni\nn\ng\n,\n \n2\n2\n,\n \n2\n8\n3\n-\n2\n9\n0\n \n(\n1\n9\n9\n6\n)\n \no\n \n1\n9\n9\n6\n \nk\nl\nu\nw\ne\nr\n \na\nc\na\nd\ne\nm\ni\nc\n \np\nu\nb\nl\ni\ns\nh\ne\nr\ns\n,\n \nb\no\ns\nt\no\nn\n.\n \nm\na\nn\nu\nf\na\nc\nt\nu\nr\ne\nd\n \ni\nn\n \nt\nh\ne\n \nn\ne\nt\nh\ne\nr\nl\na\nn\nd\ns\n.\n \nt\ne\nc\nh\nn\ni\nc\na\nl\n \nn\no\nt\ne\n \ni\nn\nc\nr\ne\nm\ne\nn\nt\na\nl\n \nm\nu\nl\nt\ni\n-\ns\nt\ne\np\n \nq\n-\nl\ne\na\nr\nn\ni\nn\ng\n \nj\ni\nn\ng\n \np\ne\nn\ng\n \nc\no\nl\nl\ne\ng\ne\n \no\nf\n \ne\nn\ng\ni\nn\ne\ne\nr\ni\nn\ng\n,\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n \no\nf\n \nc\na\nl\ni\nf\no\nr\nn\ni\na\n,\n \nr\ni\nv\ne\nr\ns\ni\nd\ne\n,\n \nc\na\n \n9\n2\n5\n2\n1\n \nr\no\nn\na\nl\nd\n \nj\n", "neuron, vol. 44, 23\u201330, september 30, 2004, copyright \uf8e92004 by cell press\n\nspike timing-dependent\nplasticity of neural circuits\n\nreview\n\nyang dan* and mu-ming poo*\ndivision of neurobiology\ndepartment of molecular and cell biology and\nhelen wills neuroscience institute\nuniversity of california, berkeley\nberkeley, california 94720\n\nrecent findings of spike timing-dependent plasticity\n(stdp) have stimulated much interest among experi-\nmentalists and theorists. beyond the traditional corre-\nlation-b", "article\n\nattractor dynamics in networks with learning rules\ninferred from in vivo data\n\nhighlights\nd a network model with a learning rule inferred from data\n\nauthors\n\nulises pereira, nicolas brunel\n\nexhibits attractor dynamics\n\nd the storage capacity of the model is close to optimal\n\nd attractors can be \ufb01xed points or chaotic, depending on\n\nparameters\n\nd chaotic dynamics lead to highly irregular activity but stable\n\nmemory\n\ncorrespondence\nnicolas.brunel@duke.edu\n\nin brief\na network model with a ", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/248544743\n\nattention as an organ system\n\narticle \u00b7 february 2008\n\ndoi: 10.1017/cbo9780511541681.005\n\ncitations\n299\n\n2 authors:\n\nmichael posner\nuniversity of oregon\n\n462 publications\u00a0\u00a0\u00a0111,722 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n2,362\n\njin fan\ncity university of new york - queens college\n\n224 publications\u00a0\u00a0\u00a020,326 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nsome of the authors of this publication are also wo", "3\n2\n0\n2\n\n \n\nn\na\nj\n \n4\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n4\nv\n8\n3\n3\n1\n0\n\n.\n\n6\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nbiologically-plausible backpropagation through\narbitrary timespans via local neuromodulators\n\nyuhan helena liu1,2,3,*, stephen smith2,4, stefan mihalas1,2,3, eric shea-brown1,2,3, and uygar\n\ns\u00fcmb\u00fcl2,*\n\n1department of applied mathematics, university of washington, seattle, wa, usa\n\n2allen institute for brain science, 615 westlake ave n, seattle wa, usa\n\n3computational neuroscience center, university of ", "expectation backpropagation: parameter-free\ntraining of multilayer neural networks with\n\ncontinuous or discrete weights\n\ndaniel soudry1, itay hubara2, ron meir2\n\n(1) department of statistics, columbia university\n\n(2) department of electrical engineering, technion, israel institute of technology\n\ndaniel.soudry@gmail.com,itayhubara@gmail.com,rmeir@ee.technion.ac.il\n\nabstract\n\nmultilayer neural networks (mnns) are commonly trained using gradient\ndescent-based methods, such as backpropagation (bp). ", "ica with reconstruction cost for ef\ufb01cient\n\novercomplete feature learning\n\nquoc v. le, alexandre karpenko, jiquan ngiam and andrew y. ng\n\n{quocle,akarpenko,jngiam,ang}@cs.stanford.edu\ncomputer science department, stanford university\n\nabstract\n\nindependent components analysis (ica) and its variants have been successfully\nused for unsupervised feature learning. however, standard ica requires an or-\nthonoramlity constraint to be enforced, which makes it dif\ufb01cult to learn overcom-\nplete features. in ", "neuron, vol. 25, 707\u2013715, march, 2000, copyright \u00aa 2000 by cell press\n\nexperience-dependent asymmetric shape\nof hippocampal receptive fields\n\nmayank r. mehta,* michael c. quirk,\u2020\nand matthew a. wilson\ncenter for learning and memory\nriken-mit neuroscience research center\ndepartment of brain and cognitive sciences\ndepartment of biology\nmassachusetts institute of technology\ncambridge, massachusetts 02139\n\nand abbott, 1996; tsodyks et al., 1996) works. further,\nwhile these previous studies provided ", "hypothesis and theory article\npublished: 09 january 2014\ndoi: 10.3389/fncom.2013.00194\n\ntime representation in reinforcement learning models of\nthe basal ganglia\nsamuel j. gershman 1*, ahmed a. moustafa 2 and elliot a. ludvig 3,4\n\n1 department of brain and cognitive sciences, massachusetts institute of technology, cambridge, ma, usa\n2 school of social sciences and psychology, marcs institute for brain and behaviour, university of western sydney, sydney, nsw, australia\n3 princeton neuroscience in", "letters to nature\n\n17. crevier, d. w. & meister, m. synchronous period-doubling in \ufb02icker vision of salamander and man.\n\nj. neurophysiol. 79, 1869\u20131878 (1998).\n\n18. merigan, w. h. & maunsell, j. h. how parallel are the primate visual pathways? annu. rev. neurosci.\n\n16, 369\u2013402 (1993).\n\n19. sparks, d. l. translation of sensory signals into commands for control of saccadic eye movements:\n\nrole of primate superior colliculus. physiol. rev. 66, 118\u2013171 (1986).\n\n20. knudsen, e. i. auditory and visual", "8\n1\n0\n2\n\n \n\nv\no\nn\n3\n\n \n\n \n \n]\n\no\nc\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n6\n6\n9\n6\n0\n\n.\n\n0\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nbiologically plausible online principal component\n\nanalysis without recurrent neural dynamics\n\nvictor minden1, cengiz pehlevan1, and dmitri b. chklovskii*1,2\n\n1center for computational biology, flatiron institute, new york, ny 10010\n\nemail: {vminden, cpehlevan, mitya}@flatironinstitute.org\n\n2neuroscience institute, nyu langone medical center, new york, ny 10016\n\nabstract\u2014arti\ufb01cial neural networks that lea", "7\n1\n0\n2\n\n \nc\ne\nd\n4\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n9\n6\n9\n8\n0\n\n.\n\n2\n1\n7\n1\n:\nv\ni\nx\nr\na\n\nmean field residual networks: on the edge of chaos\n\ngreg yang\u2217\n\nmicrosoft research ai\n\ngregyang@microsoft.com\n\nsamuel s. schoenholz\n\ngoogle brain\n\nschsam@google.com\n\nabstract\n\nwe study randomly initialized residual networks using mean \ufb01eld theory and the\ntheory of difference equations. classical feedforward neural networks, such as\nthose with tanh activations, exhibit exponential behavior on the average when\np", "on spectral  clustering: \n\nanalysis  and an algorithm \n\nandrew y.  ng \n\ncs  division \nu.c.  berkeley \n\nmichael i. jordan \n\ncs  div.  &  dept.  of stat. \n\nu.c.  berkeley \n\nang@cs.berkeley.edu \n\njordan@cs.berkeley.edu \n\nyair  weiss \n\nschool of cs  &  engr. \n\nthe hebrew univ. \nyweiss@cs.huji.ac.il \n\nabstract \n\ndespite many empirical successes of spectral  clustering  methods \nalgorithms  that  cluster  points  using  eigenvectors  of  matrices  de \nrived  from  the  data-\nthere  are  several  unres", "article \n\ncommunicated by john platt and simon haykin \n\nan information-maximization approach to \nblind separation and blind deconvolution \n\nanthony j.  bell \nterrence i. sejnowski \nhoward  hughes medical institute, \ncomputational neurobiology  laboratory, the salk institute, \n10010 n. torrey pines road, la jolla, ca 92037 usa and \ndepartment  of biology, university of california, san diego, la jolla, c a  92093 u s a  \n\nwe  derive  a  new  self-organizing  learning  algorithm  that  maximizes \nt", "cerebral cortex october 2007;17:2443--2452\ndoi:10.1093/cercor/bhl152\nadvance access publication january 13, 2007\n\nsolving the distal reward problem through\nlinkage of stdp and dopamine signaling\n\neugene m. izhikevich\n\nthe neurosciences institute, 10640 john jay hopkins drive,\nsan diego, ca 92121, usa\n\nin pavlovian and instrumental conditioning, reward typically comes\nseconds after reward-triggering actions, creating an explanatory\nconundrum known as \u2018\u2018distal reward problem\u2019\u2019: how does the brain\n", "1\n2\n0\n2\n\n \nt\nc\no\n1\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n4\nv\n8\n5\n2\n9\n0\n\n.\n\n1\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nmaximum likelihood training of\n\nscore-based diffusion models\n\nyang song\u02da\n\ncomputer science department\n\nstanford university\n\nyangsong@cs.stanford.edu\n\nconor durkan\u02da\n\nschool of informatics\nuniversity of edinburgh\n\nconor.durkan@ed.ac.uk\n\niain murray\n\nschool of informatics\nuniversity of edinburgh\ni.murray@ed.ac.uk\n\nstefano ermon\n\ncomputer science department\n\nstanford university\n\nermon@cs.stanford.edu\n\nabstrac", "spectral methods for neural characterization using\n\ngeneralized quadratic models\n\nil memming park\u2217123, evan archer\u221713, nicholas priebe14, & jonathan w. pillow123\n\n1. center for perceptual systems, 2. dept. of psychology,\n\n3. division of statistics & scienti\ufb01c computation, 4. section of neurobiology,\n\n{memming@austin., earcher@, nicholas@, pillow@mail.} utexas.edu\n\nthe university of texas at austin\n\nabstract\n\nwe describe a set of fast, tractable methods for characterizing neural responses\nto high", "neuropsychopharmacology reviews (2008) 33, 18\u201341\n& 2008 nature publishing group all rights reserved 0893-133x/08 $30.00\n...............................................................................................................................................................\n18\nwww.neuropsychopharmacology.org\n\nreview\n\nsynaptic plasticity: multiple forms, functions, and\nmechanisms\n\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\f\n\nami citri1 and robert c malenka*,1\n1departmen", "behavioral and brain sciences (2001) 24, 629\u2013640\nprinted in the united states of america\n\ngeneralization, similarity, \nand bayesian inference\n\njoshua b. tenenbaum and thomas l. griffiths\ndepartment of psychology, stanford university, stanford, ca94305-2130\njbt@psych.stanford.edu\nhttp://www-psych.stanford.edu/~jbt\nhttp://www-psych.stanford.edu/~gruffydd/\n\ngruffydd@psych.stanford.edu\n\nabstract: shepard has argued that a universal law should govern generalization across different domains of percept", "randaugment: practical automated data augmentation\n\nwith a reduced search space\n\nekin d. cubuk \u2217, barret zoph\u2217, jonathon shlens, quoc v. le\n\n{cubuk, barretzoph, shlens, qvl}@google.com\n\ngoogle research, brain team\n\n9\n1\n0\n2\n\n \n\nv\no\nn\n4\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n9\n1\n7\n3\n1\n\n.\n\n9\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nrecent work has shown that data augmentation has the\npotential to signi\ufb01cantly improve the generalization of deep\nlearning models. recently, automated augmentation strate-\ngies have led ", "reports\n\nuninformed individuals promote\ndemocratic consensus in animal groups\n\niain d. couzin,1* christos c. ioannou,1\u2020 g\u00fcven demirel,2 thilo gross,2\u2021 colin j. torney,1\nandrew hartnett,1 larissa conradt,3\u00a7 simon a. levin,1 naomi e. leonard4\n\nconflicting interests among group members are common when making collective decisions,\nyet failure to achieve consensus can be costly. under these circumstances individuals may be\nsusceptible to manipulation by a strongly opinionated, or extremist, minority.", "neuron\n\narticle\n\nspontaneous events outline the realm\nof possible sensory responses\nin neocortical populations\n\nartur luczak,1,2 peter bartho\u00b4 ,1 and kenneth d. harris1,*\n1center for molecular and behavioural neuroscience, rutgers university, 197 university avenue, newark, nj 07102, usa\n2present address: ccbn, university of lethbridge, lethbridge, ab t1k 3m4, canada\n*correspondence: kdharris@rutgers.edu\ndoi 10.1016/j.neuron.2009.03.014\n\nsummary\n\nneocortical assemblies produce complex activity\npa", "original research\npublished: 12 june 2017\ndoi: 10.3389/fncom.2017.00048\n\ncliques of neurons bound into\ncavities provide a missing link\nbetween structure and function\n\nmichael w. reimann 1 \u2020, max nolte 1 \u2020, martina scolamiero 2, katharine turner 2,\nrodrigo perin 3, giuseppe chindemi 1, pawe\u0142 d\u0142otko 4\u2021, ran levi 5\u2021, kathryn hess 2*\u2021 and\nhenry markram 1, 3*\u2021\n\n1 blue brain project, \u00e9cole polytechnique f\u00e9d\u00e9rale de lausanne, geneva, switzerland, 2 laboratory for topology and\nneuroscience, brain mind i", "forms of prediction in the \nnervous\u00a0system\n\nchristoph\u00a0teufel \n\n  and paul\u00a0c.\u00a0fletcher \n\n \n\nabstract | the idea that predictions shape how we perceive and comprehend the \nworld has become increasingly influential in the field of systems neuroscience.  \nit also forms an important framework for understanding neuropsychiatric disorders, \nwhich are proposed to be the result of disturbances in the mechanisms through \nwhich prior information influences perception and belief, leading to the production \n", "hessian eigenmaps: locally linear embedding\ntechniques for high-dimensional data\n\ndavid l. donoho* and carrie grimes\n\ndepartment of statistics, stanford university, stanford, ca 94305-4065\n\ncontributed by david l. donoho, march 19, 2003\n\nwe describe a method for recovering the underlying parametriza-\ntion of scattered data (mi) lying on a manifold m embedded in\nhigh-dimensional euclidean space. the method, hessian-based\nlocally linear embedding, derives from a conceptual framework of\nlocal isome", "a r t i c l e s\n\nsynapses with short-term plasticity are optimal \nestimators of presynaptic membrane potentials\njean-pascal pfister1, peter dayan2 & m\u00e1t\u00e9 lengyel1\nthe trajectory of the somatic membrane potential of a cortical neuron exactly reflects the computations performed on its afferent \ninputs. however, the spikes of such a neuron are a very low-dimensional and discrete projection of this continually evolving signal. \nwe explored the possibility that the neuron\u2032s efferent synapses perform ", "0\n2\n0\n2\n\n \nr\np\na\n \n2\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n4\nv\n9\n8\n8\n0\n0\n\n.\n\n6\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nlearning to solve the credit assignment\nproblem\n\nbenjamin james lansdell\ndepartment of bioengineering\nuniversity of pennsylvania\npennsylvania, pa 19104\nlansdell@seas.upenn.edu\n\nkonrad paul kording\ndepartment of bioengineering\nuniversity of pennsylvania\npennsylvania, pa 19104\n\nprashanth ravi prakash\ndepartment of bioengineering\nuniversity of pennsylvania\npennsylvania, pa 19104\n\nabstract\n\nbackpropagation ", "r e v i e w s\n\n n e u r o i m ag i n g\n\ndecoding mental states from brain \nactivity in humans\n\njohn-dylan haynes*\u2021\u00a7 and geraint rees\u2021\u00a7\nabstract | recent advances in human neuroimaging have shown that it is possible to \naccurately decode a person\u2019s conscious experience based only on non-invasive measurements \nof their brain activity. such \u2018brain reading\u2019 has mostly been studied in the domain of visual \nperception, where it helps reveal the way in which individual experiences are encoded in the \nh", "visualizing and understanding\n\nconvolutional networks\n\nmatthew d. zeiler and rob fergus\n\ndept. of computer science,\nnew york university, usa\n{zeiler,fergus}@cs.nyu.edu\n\nabstract. large convolutional network models have recently demon-\nstrated impressive classi\ufb01cation performance on the imagenet bench-\nmark krizhevsky et al. [18]. however there is no clear understanding of\nwhy they perform so well, or how they might be improved. in this paper\nwe explore both issues. we introduce a novel visualiza", "2\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n8\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n1\n8\n8\n3\n\n.\n\n6\n0\n2\n1\n:\nv\ni\nx\nr\na\n\ndanco: dimensionality from angle and norm\n\nconcentration\n\nc. ceruti, s. bassis, a. rozza, g. lombardi, e. casiraghi, and p. campadelli\n\ndipartimento di scienze dell\u2019informazione, universit`a degli studi di milano,\n\nvia comelico 39-41, 20135 milano, italy\n\nclaudio.ceruti@unimi.it\n\nabstract. in the last decades the estimation of the intrinsic dimen-\nsionality of a dataset has gained considerable importance. desp", "neuron\n\nreport\n\nrepresentation of geometric borders\nin the developing rat\n\ntale l. bjerknes,1 edvard i. moser,1 and may-britt moser1,*\n1kavli institute for systems neuroscience, norwegian university of science and technology, no-7489 trondheim, norway\n*correspondence: maybm@ntnu.no\nhttp://dx.doi.org/10.1016/j.neuron.2014.02.014\n\nsummary\n\nlocal space is represented by a number of function-\nally speci\ufb01c cell types, including place cells in the\nhippocampus and grid cells, head direction cells,\nand ", "a large-scale circuit mechanism for hierarchical\ndynamical processing in the primate cortex\n\narticle\n\nhighlights\nd large-scale model of the macaque cortex with a gradient of\n\nsynaptic excitation\n\nd sensory areas show fast responses while cognitive areas\n\nshow slow integrative activity\n\nd multiple temporal hierarchies in the same anatomical network\n\nd functional connectivity analysis needs to incorporate inter-\n\nareal heterogeneity\n\nauthors\n\nrishidev chaudhuri, kenneth\nknoblauch, marie-alice gari", "https://doi.org/10.1038/s41583-023-00740-7\n\n check for updates\n\nreconstructing computational \nsystem dynamics from neural data \nwith recurrent neural networks\n\nsections\n\nintroduction\n\ndynamical systems theory \nprimer\n\ndynamical systems theory \nand recurrent neural networks \nin neuroscience\n\nreconstructing trajectories \nfrom time series data\n\ndynamical systems \nreconstruction\n\nevaluating dynamical system \nreconstructions\n\noutlook and future challenges\n\ndaniel durstewitz\u2009\nabstract\n\n \u20091,2,3 \n\n, geo", "understanding black-box decisions with su\ufb03cient input subsets\n\nwhat made you do this?\n\nbrandon carter*\n\njonas mueller*\n\nsiddhartha jain\n\nmit computer science and arti\ufb01cial intelligence laboratory\n\ndavid gi\ufb00ord\n\nabstract\n\nlocal explanation frameworks aim to rational-\nize particular decisions made by a black-box\nprediction model. existing techniques are of-\nten restricted to a speci\ufb01c type of predictor\nor based on input saliency, which may be un-\ndesirably sensitive to factors unrelated to the\nmod", "1024 \u2022 the journal of neuroscience, january 21, 2015 \u2022 35(3):1024 \u20131037\n\nsystems/circuits\n\ndistribution and function of hcn channels in the apical\ndendritic tuft of neocortical pyramidal neurons\n\nmark t. harnett,1 jeffrey c. magee,1 and stephen r. williams2\n1howard hughes medical institute, janelia farm research campus, ashburn, virginia 20147, and 2queensland brain institute, the university of\nqueensland, brisbane qld 4072, australia\n\nthe apical tuft is the most remote area of the dendritic tre", "linear dynamical neural population models through\n\nnonlinear embeddings\n\nyuanjun gao\u21e4 1 , evan archer\u21e412, liam paninski12, john p. cunningham12\n\ndepartment of statistics1 and grossman center2\n\ncolumbia university\n\nnew york, ny, united states\n\nyg2312@columbia.edu, evan@stat.columbia.edu,\nliam@stat.columbia.edu, jpc2181@columbia.edu\n\nabstract\n\na body of recent work in modeling neural activity focuses on recovering low-\ndimensional latent features that capture the statistical structure of large-sca", "learning algorithms and signal processing \nfor brain-inspired computing\n\ncengiz pehlevan and dmitri b. chklovskii\n\nneuroscience-inspired online unsupervised  \nlearning algorithms\nartificial neural networks\n\ninventors of the original artificial neural networks (anns) de-\n\nrived their inspiration from biology [1]. however, today, most \nanns, such as backpropagation-based convolutional deep-\nlearning networks, resemble natural nns only superficially. \ngiven that, on some tasks, such anns achieve hu", "0\n2\n0\n2\n\n \n\nn\na\nj\n \n\n9\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n1\n9\n3\n5\n0\n\n.\n\n4\n0\n9\n1\n:\nv\ni\nx\nr\na\n\ndeep learning without weight transport\n\nmohamed akrout\n\nuniversity of toronto, triage\n\ncollin wilson\n\nuniversity of toronto\n\npeter c. humphreys\n\ndeepmind\n\ntimothy lillicrap\n\ndeepmind, university college london\n\ndouglas tweed\n\nuniversity of toronto, york university\n\nabstract\n\ncurrent algorithms for deep learning probably cannot run in the brain because\nthey rely on weight transport, where forward-path neurons t", "7\n1\n0\n2\n\n \nr\np\na\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n0\n6\n1\n0\n0\n\n.\n\n1\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nnips 2016 tutorial:\n\ngenerative adversarial networks\n\nian goodfellow\n\nopenai, ian@openai.com\n\nabstract\n\nthis report summarizes the tutorial presented by the author at nips\n2016 on generative adversarial networks (gans). the tutorial describes:\n(1) why generative modeling is a topic worth studying, (2) how generative\nmodels work, and how gans compare to other generative models, (3)\nthe details of how gans work, ", "the synaptic plasticity and memory\nhypothesis: encoding, storage\nand persistence\n\nrstb.royalsocietypublishing.org\n\ntomonori takeuchi, adrian j. duszkiewicz and richard g. m. morris\n\ncentre for cognitive and neural systems, university of edinburgh, 1 george square, edinburgh eh8 9jz, uk\n\nreview\n\ncite this article: takeuchi t, duszkiewicz aj,\nmorris rgm. 2014 the synaptic plasticity and\nmemory hypothesis: encoding, storage\nand persistence. phil. trans. r. soc. b 369:\n20130288.\nhttp://dx.doi.org/10", "adapting deep network features to capture psychological representations\n\njoshua c. peterson (jpeterson@berkeley.edu)\njoshua t. abbott (joshua.abbott@berkeley.edu)\n\nthomas l. grif\ufb01ths (thomas grif\ufb01ths@berkeley.edu)\n\ndepartment of psychology, university of california, berkeley, ca 94720 usa\n\n6\n1\n0\n2\n\n \n\ng\nu\na\n6\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n4\n6\n1\n2\n0\n\n.\n\n8\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\ndeep neural networks have become increasingly successful at\nsolving classic perception problems such as object r", "ne39ch09-ulanovsky\n\nari\n\n11 june 2016\n\n9:8\n\n3-d maps and compasses\nin the brain\narseny finkelstein, liora las, and nachum ulanovsky\ndepartment of neurobiology, weizmann institute of science, rehovot 76100, israel;\nemail: nachum.ulanovsky@weizmann.ac.il\n\nannu. rev. neurosci. 2016. 39:171\u201396\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev-neuro-070815-013831\ncopyright c(cid:2) 2016 by annual reviews.\nall rights reserved\n\nkeywords\nspatial", "a low-power band of neuronal spiking activity \ndominated by local single units improves the \nperformance of brain\u2013machine interfaces\n\n\u200a1, alex k. vaskov\u200a\n\n\u200a4, \n\u200a1, autumn j. bullard1, chrono s. nu1, jonathan c. kao5,6, krishna v. shenoy7,8,9,10,11,12, \n\n\u200a2, matthew s. willsey1,3, elissa j. welle\u200a\n\nsamuel r. nason\u200a\nphilip p. vu\u200a\ntaekwang jang4,13, hun-seok kim4, david blaauw4, parag g. patil\u200a\ncynthia a. chestek\u200a\n\n\u200a1, hyochan an\u200a\n\n\u200a1,3,14,15 and \n\n\u200a1,2,4,15\u2009\u2709\n\nthe large power requirement of curren", "neuron\n\narticle\n\naction selection and action value\nin frontal-striatal circuits\n\nmoonsang seo,1 eunjeong lee,1 and bruno b. averbeck1,*\n1laboratory of neuropsychology, national institute of mental health, national institutes of health, bethesda, md 20892-4415, usa\n*correspondence: bruno.averbeck@nih.gov\ndoi 10.1016/j.neuron.2012.03.037\n\nsummary\n\nreinforcement\n\nthe role that frontal-striatal circuits play in normal\nbehavior remains unclear. two of the leading hypoth-\neses suggest that these circu", "j neurophysiol 93: 2600 \u20132613, 2005.\nfirst published december 29, 2004; doi:10.1152/jn.00803.2004.\n\ncalcium time course as a signal for spike-timing\u2013dependent plasticity\n\njonathan e. rubin,1,2,3 richard c. gerkin,2,3 guo-qiang bi,2,3,4 and carson c. chow1,2,3,4,5\n1department of mathematics, 2center for the neural basis of cognition, 3center for neuroscience at university of pittsburgh, and\n4department of neurobiology, university of pittsburgh, pittsburgh, pennsylvania; and 5laboratory of biologi", "the geometry of decision-making in individuals and\ncollectives\n\nvivek h. sridhara,b,c,1\nnir s. govi\n\n, liang lia,b,c,1\n, and iain d. couzina,b,c,1\n\n, dan gorbonosa,b,c, m\u00e1t\u00e9 nagya,b,c,d,e,f, bianca r. schellg\n\n, timothy sorochkinh,i\n\n,\n\nadepartment of collective behaviour, max planck institute of animal behavior, 78464 konstanz, germany; bcentre for the advanced study of collective\nbehaviour, university of konstanz, 78464 konstanz, germany; cdepartment of biology, university of konstanz, 78464 k", "annales de la facult\u00e9 des sciences de toulouse\n\npascal massart\nsome applications of concentration\ninequalities to statistics\nannales de la facult\u00e9 des sciences de toulouse 6e s\u00e9rie, tome 9, no 2\n(2000), p. 245-303\n<http://www.numdam.org/item?id=afst_2000_6_9_2_245_0>\n\n\u00a9 universit\u00e9 paul sabatier, 2000, tous droits r\u00e9serv\u00e9s.\nl\u2019acc\u00e8s aux archives de la revue \u00ab annales de la facult\u00e9 des sciences de\ntoulouse \u00bb (http://picard.ups-tlse.fr/~annales/) implique l\u2019accord avec les\nconditions g\u00e9n\u00e9rales d\u2019uti", "fairwashing: the risk of rationalization\n\nulrich a\u00a8\u0131vodji 1 hiromi arai 2 3 olivier fortineau 4 s\u00b4ebastien gambs 1 satoshi hara 5 alain tapp 6 7\n\nabstract\n\nblack-box explanation is the problem of explain-\ning how a machine learning model \u2013 whose in-\nternal logic is hidden to the auditor and gen-\nerally complex \u2013 produces its outcomes. cur-\nrent approaches for solving this problem include\nmodel explanation, outcome explanation as well\nas model inspection. while these techniques can\nbe bene\ufb01cial b", "cortical preparatory activity indexes \nlearned motor memories\n\nhttps://doi.org/10.1038/s41586-021-04329-x\nreceived: 25 january 2020\naccepted: 9 december 2021\npublished online: 26 january 2022\n\n check for updates\n\nxulu sun1,2,9\u2009\u2709, daniel j. o\u2019shea2,3,9, matthew d. golub2,3, eric m. trautmann2,3, \nsaurabh vyas2,4, stephen i. ryu3,5,6 & krishna v. shenoy2,3,4,6,7,8\u2009\u2709\n\nthe brain\u2019s remarkable ability to learn and execute various motor behaviours \nharnesses the capacity of neural populations to genera", "kickback cuts backprop\u2019s red-tape:\n\nbiologically plausible credit assignment in neural networks\n\ndavid balduzzi\n\ndavid.balduzzi@vuw.ac.nz\n\nvictoria university of wellington\n\nhastagiri vanchinathan\n\nhastagiri@inf.ethz.ch\n\neth zurich\n\njoachim buhmann\njbuhmann@inf.ethz.ch\n\neth zurich\n\nabstract\n\nerror backpropagation is an extremely effective algorithm\nfor assigning credit in arti\ufb01cial neural networks. however,\nweight updates under backprop depend on lengthy recursive\ncomputations and require separa", "article\n\ndoi:10.1038/nature12112\n\nhippocampal place-cell sequences depict\nfuture paths to remembered goals\n\nbrad e. pfeiffer1 & david j. foster1\n\neffective navigation requires planning extended routes to remembered goal locations. hippocampal place cells have\nbeen proposed to have a role in navigational planning, but direct evidence has been lacking. here we show that before\ngoal-directed navigation in an open arena, the rat hippocampus generates brief sequences encoding spatial trajectories\nstr", "neuron, vol. 48, 661\u2013673, november 23, 2005, copyright \u00aa2005 by elsevier inc. doi 10.1016/j.neuron.2005.09.032\n\ntransient dynamics versus fixed points in\nodor representations by locust antennal\nlobe projection neurons\n\nofer mazor1 and gilles laurent*\ncomputation and neural systems program\ndivision of biology\ncalifornia institute of technology\npasadena, california 91125\n\nsummary\n\nprojection neurons (pns) in the locust antennal lobe\nexhibit odor-speci\ufb01c dynamic responses. we studied a\npn populatio", "thephysicsofoptimaldecisionmaking:aformalanalysisofmodelsofperformanceintwo-alternativeforced-choicetasksrafalbogacz,ericbrown,jeffmoehlis,philipholmes,andjonathand.cohenprincetonuniversityinthisarticle,theauthorsconsideroptimaldecisionmakingintwo-alternativeforced-choice(tafc)tasks.theybeginbyanalyzing6modelsoftafcdecisionmakingandshowthatallbutonecanbereducedtothedriftdiffusionmodel,implementingthestatisticallyoptimalalgorithm(mostaccurateforagivenspeedorfastestforagivenaccuracy).theyprovefurt", "neural residual flow fields\n\nfor efficient video representations\n\ndaniel rho1[0000\u22120002\u22128568\u22129489], junwoo cho1, jong hwan ko1,2\u22c6, and\n\neunbyung park1,2\u22c6\n\n1 department of artificial intelligence, sungkyunkwan university\n\n2 department of electrical and computer engineering, sungkyunkwan university\n\n{daniel231,jwcho000,jhko,epark}@skku.edu\n\nabstract. neural fields have emerged as a powerful paradigm for repre-\nsenting various signals, including videos. however, research on improving\nthe parameter ", "7\n1\n0\n2\n\n \n\nn\na\nj\n \n\n3\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n8\n3\n5\n6\n0\n\n.\n\n1\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nunder review as a conference paper at iclr 2017\n\noutrageously large neural networks:\nthe sparsely-gated mixture-of-experts layer\n\nnoam shazeer1, azalia mirhoseini\u2217\u20201, krzysztof maziarz\u22172, andy davis1, quoc le1, geoffrey\n\nhinton1 and jeff dean1\n\n1google brain, {noam,azalia,andydavis,qvl,geoffhinton,jeff}@google.com\n\n2jagiellonian university, cracow, krzysztof.maziarz@student.uj.edu.pl\n\nabstract\n\nthe capacity ", "ne39ch02-moser\n\nari\n\n26 may 2016\n\n13:5\n\nten years of grid cells\ndavid c. rowland, yasser roudi, may-britt moser,\nand edvard i. moser\nkavli institute for systems neuroscience and centre for neural computation, norwegian\nuniversity of science and technology, 7491 trondheim, norway;\nemail: david.c.rowland@ntnu.no, yasser.roudi@ntnu.no, may-britt.moser@ntnu.no,\nedvard.moser@ntnu.no\n\nannu. rev. neurosci. 2016. 39:19\u201340\n\nfirst published online as a review in advance on\nmarch 9, 2016\n\nthe annual review", "dynamical mean-\ufb01eld theory for stochastic gradient\n\ndescent in gaussian mixture classi\ufb01cation\n\nfrancesca mignacco1\n\nflorent krzakala2,3\n\npierfrancesco urbani1\n\nlenka zdeborov\u00e11,4\n\n1 institut de physique th\u00e9orique, universit\u00e9 paris-saclay, cnrs, cea, gif-sur-yvette, france\n2 laboratoire de physique, cnrs, \u00e9cole normale sup\u00e9rieure, psl university, paris, france\n\n3 idephics laboratory, epfl, switzerland\n\n4 spoc laboratory, epfl, switzerland\n\ncorrespondence to: francesca.mignacco@ipht.fr\n\nabstract\n\n", "j neurophysiol 98: 1733\u20131750, 2007.\nfirst published june 27, 2007; doi:10.1152/jn.01265.2006.\n\na model of v4 shape selectivity and invariance\n\ncharles cadieu,1 minjoon kouh,1 anitha pasupathy,2 charles e. connor,3 maximilian riesenhuber,4 and\ntomaso poggio1\n1center for biological and computational learning, mcgovern institute, massachusetts institute of technology, cambridge,\nmassachusetts; 2department of biological structure, university of washington, seattle, washington; 3department of neurosc", "journal of machine learning research 10 (2009) 207-244\n\nsubmitted 12/07; revised 9/08; published 2/09\n\ndistance metric learning for large margin\n\nnearest neighbor classi\ufb01cation\n\nkilian q. weinberger\nyahoo! research\n2821 mission college blvd\nsanta clara, ca 9505\n\nlawrence k. saul\ndepartment of computer science and engineering\nuniversity of california, san diego\n9500 gilman drive, mail code 0404\nla jolla, ca 92093-0404\n\neditor: sam roweis\n\nkilian@yahoo-inc.com\n\nsaul@cs.ucsd.edu\n\nabstract\n\nthe accu", "letter\n\ncommunicated by joshua gold\n\nthe basal ganglia and cortex implement optimal decision\nmaking between alternative actions\n\nrafal bogacz\nr.bogacz@bristol.ac.uk\ndepartment of computer science, university of bristol, bristol bs8 1ub, u.k.\n\nkevin gurney\nk.gurney@shef.ac.uk\ndepartment of psychology, university of shef\ufb01eld, shef\ufb01eld s10 2tp, u.k.\n\nneurophysiological studies have identi\ufb01ed a number of brain regions\ncritically involved in solving the problem of action selection or deci-\nsion makin", "resurrecting the sigmoid in deep learning through\n\ndynamical isometry: theory and practice\n\njeffrey pennington\n\ngoogle brain\n\nsamuel s. schoenholz\n\ngoogle brain\n\napplied physics, stanford university and google brain\n\nsurya ganguli\n\nabstract\n\nit is well known that weight initialization in deep networks can have a dramatic\nimpact on learning speed. for example, ensuring the mean squared singular value\nof a network\u2019s input-output jacobian is o(1) is essential for avoiding exponentially\nvanishing or", "the  journal  of  neuroscience,  march  1995,  15(3):  1669-1682 \n\nthe  morphoelectrotonic \ndendritic  function \n\ntransform: \n\na  graphical  approach \n\nto \n\nanthony  m.  zador,\u2019 \n\u2018salk \ninstitute  of  life  sciences,  hebrew  university, \n\ninstitute,  san  diego,  california \n\nhagai  agmon-snir,2 \n\nand \n\nldan  segev2 \n92037,  and  2department \n\njerusalem, \n\nisrael  91904 \n\nof  neurobiology \n\nand  center \n\nfor  neural  computation, \n\nis \n\njust \nsimilarly \n\nelectrotonic \nneuronal \nvelop \nscribe \no", "letters\n\nvol 457 | 26 february 2009 | doi:10.1038/nature07709\n\nthe subcellular organization of neocortical excitatory\nconnections\nleopoldo petreanu1, tianyi mao1, scott m. sternson1 & karel svoboda1\n\nunderstanding cortical circuits will require mapping the connec-\ntions between specific populations of neurons1, as well as deter-\nmining the dendritic locations where the synapses occur2. the\ndendrites of individual cortical neurons overlap with numerous\ntypes of local and long-range excitatory axo", "a r t i c l e s\n\ndifferential triggering of spontaneous glutamate \nrelease by p/q-, n- and r-type ca2+ channels\nyaroslav s ermolyuk1,5, felicity g alder1,5, rainer surges1,4,5, ivan y pavlov1, yulia timofeeva2,3,  \ndimitri m kullmann1 & kirill e volynski1\n\nthe role of voltage-gated ca2+ channels (vgccs) in spontaneous miniature neurotransmitter release is incompletely understood. \nwe found that stochastic opening of p/q-, n- and r-type vgccs accounts for ~50% of all spontaneous glutamate release", "em algorithms for pca and spca\n\nsam roweis\u0000\nabstract\n\ni present an expectation-maximization (em) algorithm for principal\ncomponent analysis (pca). the algorithm allows a few eigenvectors and\neigenvalues to be extracted from large collections of high dimensional\ndata. it is computationally very ef\ufb01cient in space and time. it also natu-\nrally accommodates missing information. i also introduce a new variant\nof pca called sensible principal component analysis (spca) which de-\n\ufb01nes a proper density m", "the journal of neuroscience, december 10, 2008 \u2022 28(50):13457\u201313466 \u2022 13457\n\ncellular/molecular\n\nspine neck plasticity controls postsynaptic calcium signals\nthrough electrical compartmentalization\n\n\u00e5sa grunditz,1* niklaus holbro,1* lei tian,1 yi zuo,2 and thomas g. oertner1\n1friedrich miescher institute, maulbeerstrasse 66, ch-4058 basel, switzerland, and 2department for molecular cell and developmental biology, university\nof california, santa cruz, santa cruz, california 95064\n\ndendritic spines", "a basal ganglia-forebrain circuit in the songbird\nbiases motor output to avoid vocal errors\n\naaron s. andalman and michale s. fee1\n\nmcgovern institute for brain research, department of brain and cognitive sciences, massachusetts institute of technology, cambridge, ma 02139\n\nedited by fernando nottebohm, the rockefeller university, millbrook, ny, and approved may 29, 2009 (received for review march 24, 2009)\n\nin songbirds, as in mammals, basal ganglia-forebrain circuits are\nnecessary for the lear", "ne43ch05_magee\n\narjats.cls\n\njune 22, 2020\n\n9:51\n\nannual review of neuroscience\nsynaptic plasticity forms\nand functions\njeffrey c. magee and christine grienberger\ndepartment of neuroscience and howard hughes medical institute, baylor college of\nmedicine, houston, texas 77030, usa; email: jcmagee@bcm.edu\n\nannu. rev. neurosci. 2020. 43:95\u2013117\n\nfirst published as a review in advance on\nfebruary 19, 2020\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nhttps://doi.org/10.1146/", "reviews\n\nextracting information from neuronal \npopulations: information theory  \nand decoding approaches\n\nrodrigo quian quiroga* and stefano panzeri\u2021\u00a7\n\nabstract | to a large extent, progress in neuroscience has been driven by the study of \nsingle-cell responses averaged over several repetitions of stimuli or behaviours. however, \nthe brain typically makes decisions based on single events by evaluating the activity of large \nneuronal populations. therefore, to further understand how the brain pro", "the journal of neuroscience, december 15, 1998, 18(24):10464\u201310472\n\nsynaptic modi\ufb01cations in cultured hippocampal neurons:\ndependence on spike timing, synaptic strength, and\npostsynaptic cell type\n\nguo-qiang bi and mu-ming poo\ndepartment of biology, university of california at san diego, la jolla, california 92093\n\nin cultures of dissociated rat hippocampal neurons, persistent\npotentiation and depression of glutamatergic synapses were\ninduced by correlated spiking of presynaptic and postsynaptic", "lettercommunicatedbygarrettstanleybayesianpopulationdecodingofmotorcorticalactivityusingakalmanfilterweiwuweiwu@dam.brown.eduyungaogao@dam.brown.edudivisionofappliedmathematics,brownuniversity,providence,ri02912,u.s.a.eliebienenstockelie@dam.brown.edudivisionofappliedmathematicsanddepartmentofneuroscience,brownuniversity,providence,ri02912,u.s.a.johnp.donoghuejohndonoghue@brown.edudepartmentofneuroscience,brownuniversity,providence,ri02912,u.s.a.michaelj.blackblack@cs.brown.edudepartmentofcomput", "siam j. sci. comput.\nvol. 26, no. 1, pp. 313\u2013338\n\nc(cid:2) 2004 society for industrial and applied mathematics\n\nprincipal manifolds and nonlinear dimensionality\n\n\u2217\nreduction via tangent space alignment\n\n\u2020\nzhenyue zhang\n\n\u2021\nand hongyuan zha\n\nabstract. we present a new algorithm for manifold learning and nonlinear dimensionality\nreduction. based on a set of unorganized data points sampled with noise from a parameterized man-\nifold, the local geometry of the manifold is learned by constructing an ap", "institute of mathematical statistics is collaborating with jstor to digitize, preserve, and extend access to\nstatistical science.\n\nwww.jstor.org\n\n\u00ae\n\n\f", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nanalyzing  biological  and  arti\ufb01cial  neural  networks:\nchallenges  with  opportunities  for  synergy?\ndavid  gt  barrett1,3,  ari  s  morcos1,3,4 and  jakob  h  macke2\n\ndeep  neural  networks  (dnns)  transform  stimuli  across  multiple\nprocessing  stages  to  produce  representations  that  can  be  used\nto  solve  complex  tasks,  such  as  object  recognition  in  images.\nhowever,  a  full  understanding  of  how  they  a", "made: masked autoencoder for distribution estimation\n\n5\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n5\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n9\n0\n5\n3\n0\n\n.\n\n2\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nmathieu germain\nuniversit\u00b4e de sherbrooke, canada\nkarol gregor\ngoogle deepmind\niain murray\nuniversity of edinburgh, united kingdom\nhugo larochelle\nuniversit\u00b4e de sherbrooke, canada\n\nmathieu.germain2@usherbrooke.ca\n\nkarol.gregor@gmail.com\n\ni.murray@ed.ac.uk\n\nhugo.larochelle@usherbrooke.ca\n\nabstract\n\nthere has been a lot of recent interest in designing\nneural", "\u0006ajjahi\u0014j\u0006\u0014\u0006=jkha\n\nsynaptic plasticity in a \ncerebellum-like structure \ndepends on temporal order \ncurtis c. bell*, victor z. han*, yoshiko sugawarat \n& kirsty grant:!: \n* r.  s.  dow neurological sciences institute,  good samaritan hospital and \nmedical center,  1120 n. w  20th avenue, portland,  oregon  97209,  usa \nt institut alfred fessard,  cnrs 91190 gif-sur-yvette,  france \n:j:  department of physiology,  teikyo  university school of medicine,  kaga 2-11-1, \nitabashi-ku,  tokyo  173, japa", "learning to simulate complex physics with graph networks\n\nalvaro sanchez-gonzalez * 1 jonathan godwin * 1 tobias pfaff * 1 rex ying * 1 2 jure leskovec 2\n\npeter w. battaglia 1\n\n0\n2\n0\n2\n\n \n\np\ne\ns\n4\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n0\n4\n9\n0\n\n.\n\n2\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nhere we present a machine learning framework\nand model implementation that can learn to\nsimulate a wide variety of challenging physi-\ncal domains, involving \ufb02uids, rigid solids, and\ndeformable materials interacting with one ", "bmc neuroscience\n\nbiomed central\n\noral presentation\nan online hebbian learning rule that performs independent \ncomponent analysis\nclaudia clopath*1, andr\u00e9 longtin2 and wulfram gerstner1\n\nopen access\n\naddress: 1lcn, epfl, lausanne, 1015, switzerland and 2cnd, university of ottawa, ottawa, ontario, canada\n\nemail: claudia clopath* - claudia.clopath@epfl.ch\n* corresponding author    \n\nfrom seventeenth annual computational neuroscience meeting: cns*2008\nportland, or, usa. 19\u201324 july 2008\n\npublished: ", "neuroscience  research  74  (2012)  177\u2013183\n\ncontents  lists  available  at  sciverse  sciencedirect\n\nneuroscience\n\n \n\nresearch\n\nj o  u  r  n  a l  h o m  e p  a g e :  w w w . e l s e v i e r . c o m / l o c a t e / n e u r e s\n\nupdate   article\nlearning   to   represent   reward   structure:   a   key   to   adapting   to   complex\nenvironments\nhiroyuki   nakahara a,\u2217,   okihide   hikosaka b\n\na laboratory  for  integrated  theoretical  neuroscience,  riken  brain  science  institute,  wako,  s", "deep generative stochastic networks trainable by backprop\n\nyoshua bengio\u2217\n\u00b4eric thibodeau-laufer\nguillaume alain\nd\u00b4epartement d\u2019informatique et recherche op\u00b4erationnelle, universit\u00b4e de montr\u00b4eal,\u2217& canadian inst. for advanced research\njason yosinski\ndepartment of computer science, cornell university\n\nfind.us@on.the.web\n\nabstract\n\ntraining principle for\nwe introduce a novel\nis an alternative to\nprobabilistic models that\nmaximum likelihood. the proposed generative\nstochastic networks (gsn) framew", "article\nlocus coeruleus and dopaminergic \nconsolidation of everyday memory\n\ntomonori takeuchi1*, adrian j. duszkiewicz1*, alex sonneborn2*, patrick a. spooner1, miwako yamasaki3, \nmasahiko watanabe3, caroline c. smith2, guill\u00e9n fern\u00e1ndez4, karl deisseroth5, robert w. greene2,6 & richard g. m. morris1,7\n\ndoi:10.1038/nature19325\n\nthe retention of episodic-like memory is enhanced, in humans and animals, when something novel happens shortly \nbefore or after encoding. using an everyday memory task in", "23; right 36, 13, and 27); superior frontal gyrus (left\n29, 31, and 45; right 17, 35, and 37).\n\n17. although the improvement in wm performance with\ncholinergic enhancement was a nonsigni(cid:222)cant trend\nin the current study (p 5 0.07), in a previous study\n(9) with a larger sample (n 5 13) the effect was\nhighly signi(cid:222)cant (p , 0.001). in the current study,\nwe analyzed rt data for six of our seven subjects\nbecause the behavioral data for one subject were\nunavailable due to a computer fa", "learning phrase representations using rnn encoder\u2013decoder\n\nfor statistical machine translation\n\nkyunghyun cho\n\nbart van merri\u00a8enboer caglar gulcehre\n\nuniversit\u00b4e de montr\u00b4eal\n\ndzmitry bahdanau\n\njacobs university, germany\n\n4\n1\n0\n2\n\n \n\np\ne\ns\n3\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n3\nv\n8\n7\n0\n1\n\n.\n\n6\n0\n4\n1\n:\nv\ni\nx\nr\na\n\nfirstname.lastname@umontreal.ca\n\nd.bahdanau@jacobs-university.de\n\nfethi bougares holger schwenk\n\nuniversit\u00b4e du maine, france\n\nyoshua bengio\n\nuniversit\u00b4e de montr\u00b4eal, cifar senior fellow\n\nfirst", "neuron, vol. 45, 599\u2013611, february 17, copyright \u00a92005 by elsevier inc. doi 10.1016/j.neuron.2005.02.001\n\ncascade models of synaptically stored memories\n\nstefano fusi,1 patrick j. drew,2 and l.f. abbott2,*\n1institute of physiology\nuniversity of bern\nb\u00fchlplatz 5\nch-3012, bern\nswitzerland\n2 volen center and\ndepartment of biology\nbrandeis university\nwaltham, massachusetts 02454\n\nsummary\n\nstoring memories of ongoing, everyday experiences\nrequires a high degree of plasticity, but retaining\nthese memo", "massively parallel methods for deep reinforcement learning\n\n5\n1\n0\n2\n\n \nl\nu\nj\n \n\n6\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n9\n2\n4\n0\n\n.\n\n7\n0\n5\n1\n:\nv\ni\nx\nr\na\n\narun nair, praveen srinivasan, sam blackwell, cagdas alcicek, rory fearon, alessandro de maria, vedavyas\npanneershelvam, mustafa suleyman, charles beattie, stig petersen, shane legg, volodymyr mnih, koray\nkavukcuoglu, david silver\n{arunsnair, prav, blackwells, cagdasalcicek, roryf, ademaria, darthveda, mustafasul, cbeattie,\nsvp, legg, vmnih, korayk, ", "lettercommunicatedbypeterdayanmakingworkingmemorywork:acomputationalmodeloflearningintheprefrontalcortexandbasalgangliarandallc.o\u2019reillyoreilly@psych.colorado.edumichaelj.frankfrankmj@psych.colorado.edudepartmentofpsychology,universityofcoloradoboulder,boulder,co80309,u.s.a.theprefrontalcortexhaslongbeenthoughttosubservebothworkingmemory(theholdingofinformationonlineforprocessing)andexecutivefunctions(decidinghowtomanipulateworkingmemoryandperformprocessing).althoughmanycomputationalmodelsofwork", "2\n1\n0\n2\n\n \nl\nu\nj\n \n\n3\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n0\n8\n5\n0\n\n.\n\n7\n0\n2\n1\n:\nv\ni\nx\nr\na\n\nimproving neural networks by preventing\n\nco-adaptation of feature detectors\n\ng. e. hinton\u2217, n. srivastava, a. krizhevsky, i. sutskever and r. r. salakhutdinov\n\ndepartment of computer science, university of toronto,\n6 king\u2019s college rd, toronto, ontario m5s 3g4, canada\n\n\u2217to whom correspondence should be addressed; e-mail: hinton@cs.toronto.edu\n\nwhen a large feedforward neural network is trained on a small training ", "12176 \u2022 the journal of neuroscience, november 7, 2007 \u2022 27(45):12176 \u201312189\n\nbehavioral/systems/cognitive\n\nneural ensembles in ca3 transiently encode paths forward\nof the animal at a decision point\n\nadam johnson1 and a. david redish2\n1center for cognitive sciences and 2department of neuroscience, univeristy of minnesota, minneapolis, minnesota 55455\n\nneural ensembles were recorded from the ca3 region of rats running on t-based decision tasks. examination of neural representations\nof space at fas", "(2006).\n\n7, 365 (2005).\n\n16. c. j. marshall, cell 80, 179 (1995).\n17. s. sasagawa, y. ozaki, k. fujita, s. kuroda, nat. cell biol.\n18. l. o. murphy, j. blenis, trends biochem. sci. 31, 268\n19. m. villedieu et al., gynecol. oncol. 101, 507 (2006).\n20. b. k. choi, c. h. choi, h. l. oh, y. k. kim, neurotoxicity\n21. s. d. santos, p. j. verveer, p. i. bastiaens, nat. cell biol.\n22. a. acharya, s. b. ruvinov, j. gal, c. vinson, biochemistry\n23. m. ramezani-rad, curr. genet. 43, 161 (2003).\n24. c. wu, ", "cell-type\u2013speci\ufb01c neuromodulation guides synaptic\ncredit assignment in a spiking neural network\n\nyuhan helena liua,b,c,1\n\n, stephen smithb,d\n\n, stefan mihalasa,b,c, eric shea-browna,b,c, and uygar s\u00fcmb\u00fclb,1\n\nadepartment of applied mathematics, university of washington, seattle, wa 98195; ballen institute for brain science, seattle, wa 98109; ccomputational\nneuroscience center, university of washington, seattle, wa 98195; and ddepartment of molecular and cellular physiology, stanford university, ", "neuron\n\nperspective\n\nwhat is a cognitive map?\norganizing knowledge for flexible behavior\n\ntimothy e.j. behrens,1,2,* timothy h. muller,1 james c.r. whittington,1 shirley mark,2 alon b. baram,1\nkimberly l. stachenfeld,3 and zeb kurth-nelson3,4\n1wellcome centre for integrative neuroimaging, centre for functional magnetic resonance imaging of the brain, university of oxford,\njohn radcliffe hospital, oxford ox3 9du, uk\n2wellcome centre for human neuroimaging, institute of neurology, university colle", "operant matching is a generic outcome of synaptic\nplasticity based on the covariance between reward\nand neural activity\n\nyonatan loewenstein* and h. sebastian seung\n\nhoward hughes medical institute and the department of brain and cognitive sciences, massachusetts institute of technology, cambridge, ma 02139\n\nedited by william t. newsome, stanford university school of medicine, stanford, ca, and approved august 12, 2006 (received for review june 23, 2005)\n\nthe probability of choosing an alternati", "circuit models of low-dimensional shared\nvariability in cortical networks\n\narticle\n\nhighlights\nd low-dimensional shared variability can be generated in\n\nspatial network models\n\nd synaptic spatial and temporal scales determine the\n\ndimensions of shared variability\n\nd depolarizing inhibitory neurons suppresses the population-\n\nwide \ufb02uctuations\n\nd modeling the attentional modulation of variability within and\n\nbetween brain areas\n\nauthors\n\nchengcheng huang, douglas a. ruff,\nryan pyle, robert rosenba", "siam review\nvol. 53, no. 2, pp. 217\u2013288\n\nc(cid:1) 2011 society for industrial and applied mathematics\n\nfinding structure with randomness:\nprobabilistic algorithms for\nconstructing approximate\n\u2217\nmatrix decompositions\n\nn. halko\np. g. martinsson\nj. a. tropp\n\n\u2020\n\u2020\n\u2021\n\nabstract. low-rank matrix approximations, such as the truncated singular value decomposition and\nthe rank-revealing qr decomposition, play a central role in data analysis and scienti\ufb01c\ncomputing. this work surveys and extends recent rese", "https://doi.org/10.1038/s41583-023-00693-x\n\n check for updates\n check for updates\n\na unifying perspective on neural \nmanifolds and circuits for cognition\n\nsections\n\nintroduction\n\ncircuit\u2013manifold convergence: \nhead direction system\n\ntowards convergence in  \ngrid cells\n\ncircuits with mixed selectivity\n\nconclusions and perspectives\n\nchristopher langdon\u2009\nabstract\n\n \u20091,2,3, mikhail genkin\u2009\n\n \u20092,3 & tatiana a. engel\u2009\n\n \u20091,2 \n\ntwo different perspectives have informed efforts to explain the link \nbetwe", "learning important features through propagating activation differences\n\navanti shrikumar 1 peyton greenside 1 anshul kundaje 1\n\nabstract\n\nthe purported \u201cblack box\u201d nature of neural\nnetworks is a barrier to adoption in applica-\ntions where interpretability is essential. here\nwe present deeplift (deep learning impor-\ntant features), a method for decomposing the\noutput prediction of a neural network on a spe-\nci\ufb01c input by backpropagating the contributions\nof all neurons in the network to every fea", "article\n\ndoi: 10.1038/s41467-018-06560-z\n\nopen\n\ncortical population activity within a preserved\nneural manifold underlies multiple motor behaviors\n\njuan a. gallego\nsara a. solla1,5 & lee e. miller\n\n1,3,6\n\n1,2, matthew g. perich\n\n3, stephanie n. naufel3, christian ethier\n\n4,\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\npopulations of cortical neurons \ufb02exibly perform different functions; for the primary motor\ncortex (m1) this means a rich repertoire of motor behaviors. we investigate the \ufb02exibility of\nm1 movem", "b\nr\na\ni\nn\n \nr\ne\ns\ne\na\nr\nc\nh\n \nr\ne\nv\ni\ne\nw\ns\n \ne\nl\ns\ne\nv\ni\ne\nr\n \nb\nr\na\ni\nn\n \nr\ne\ns\ne\na\nr\nc\nh\n \nr\ne\nv\ni\ne\nw\ns\n \n2\n1\n \n(\n1\n9\n9\n6\n)\n \n2\n1\n9\n-\n2\n4\n5\n \nf\nu\nl\nl\n-\nl\ne\nn\ng\nt\nh\n \nr\ne\nv\ni\ne\nw\n \nt\nh\ne\n \na\ns\nc\ne\nn\nd\ni\nn\ng\n \nn\ne\nu\nr\no\nm\no\nd\nu\nl\na\nt\no\nr\ny\n \ns\ny\ns\nt\ne\nm\ns\n \ni\nn\n \nl\ne\na\nr\nn\ni\nn\ng\n \nb\ny\n \nr\ne\ni\nn\nf\no\nr\nc\ne\nm\ne\nn\nt\n:\n \nc\no\nm\np\na\nr\ni\nn\ng\n \nc\no\nm\np\nu\nt\na\nt\ni\no\nn\na\nl\n \nc\no\nn\nj\ne\nc\nt\nu\nr\ne\ns\n \nw\ni\nt\nh\n \ne\nx\np\ne\nr\ni\nm\ne\nn\nt\na\nl\n \nf\ni\nn\nd\ni\nn\ng\ns\n \nc\ny\nr\ni\ne\nl\n \nm\n.\na\n.\n \np\ne\nn\nn\na\nr\nt\nz\n \n*\n \np\nh\ny\ns\n", "gradient descent for spiking neural networks\n\ndongsung huh\nsalk institute\n\nla jolla, ca 92037\n\nhuh@salk.edu\n\nterrence j. sejnowski\n\nsalk institute\n\nla jolla, ca 92037\nterry@salk.edu\n\nabstract\n\nmost large-scale network models use neurons with static nonlinearities that pro-\nduce analog output, despite the fact that information processing in the brain is\npredominantly carried out by dynamic neurons that produce discrete pulses called\nspikes. research in spike-based computation has been impeded by ", "8\n1\n0\n2\n\n \nr\na\n\nm\n \n1\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n0\n7\n7\n7\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2018\n\nemergence of grid-like representations by\ntraining recurrent neural networks to\nperform spatial localization\n\nchristopher j. cueva\u2217, xue-xin wei\u2217\ncolumbia university\nnew york, ny 10027, usa\n{ccueva,weixxpku}@gmail.com\n\nabstract\n\ndecades of research on the neural code underlying spatial navigation have re-\nvealed a diverse set of neural response properties. ", "letter\n\ncommunicated by michael hasselmo\n\nhow inhibitory oscillations can train neural networks and\npunish competitors\n\n\u2217\n\n\u2217\n\nkenneth a. norman\nknorman@princeton.edu\nehren newman\nenewman@princeton.edu\ngreg detre\ngdetre@princeton.edu\nsean polyn\npolyn@psych.upenn.edu\ndepartment of psychology, princeton university, princeton, nj 08544, u.s.a.\n\nwe present a new learning algorithm that leverages oscillations in the\nstrength of neural inhibition to train neural networks. raising inhibition\ncan be used", "the journal of neuroscience, february 4, 2004 \u2022 24(5):1089 \u20131100 \u2022 1089\n\nbehavioral/systems/cognitive\n\nlinearity of cortical receptive fields measured with\nnatural sounds\n\nchristian k. machens, michael s. wehr, and anthony m. zador\ncold spring harbor laboratory, cold spring harbor, new york 11724\n\nhow do cortical neurons represent the acoustic environment? this question is often addressed by probing with simple stimuli such as\nclicks or tone pips. such stimuli have the advantage of yielding easi", "6\n1\n0\n2\n\n \n\nv\no\nn\n2\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n3\nv\n9\n8\n2\n0\n0\n\n.\n\n4\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nin press at behavioral and brain sciences.\n\nbuilding machines that learn and think like people\n\nbrenden m. lake,1 tomer d. ullman,2,4 joshua b. tenenbaum,2,4 and samuel j. gershman3,4\n\n1center for data science, new york university\n\n2department of brain and cognitive sciences, mit\n\n3department of psychology and center for brain science, harvard university\n\n4center for brains minds and machines\n\nabstract\n\nrecent", "a r t i c l e s\n\nbrief optogenetic inhibition of dopamine neurons \nmimics endogenous negative reward prediction errors\nchun yun chang1, guillem r esber2, yasmin marrero-garcia1, hau-jie yau1, antonello bonci1,3,4 &  \ngeoffrey schoenbaum1,3,5\n\ncorrelative studies have strongly linked phasic changes in dopamine activity with reward prediction error signaling. but causal \nevidence that these brief changes in firing actually serve as error signals to drive associative learning is more tenuous. altho", "a r t i c l e s\n\nneural antecedents of self-initiated actions  \nin secondary motor cortex\nmasayoshi murakami, m in\u00eas vicente, gil m costa & zachary f mainen\nthe neural origins of spontaneous or self-initiated actions are not well understood and their interpretation is controversial.  \nto address these issues, we used a task in which rats decide when to abort waiting for a delayed tone. we recorded neurons  \nin the secondary motor cortex (m2) and interpreted our findings in light of an integratio", "training neural networks without gradients:\n\na scalable admm approach\n\ngavin taylor1\nryan burmeister1\nzheng xu2\nbharat singh2\nankit patel3\ntom goldstein2\n1united states naval academy, annapolis, md usa\n2university of maryland, college park, md usa\n3rice university, houston, tx usa\n\nabstract\n\nwith the growing importance of large network\nmodels and enormous training datasets, gpus\nhave become increasingly necessary to train neu-\nral networks. this is largely because conven-\ntional optimization alg", "cognitive psychology 96 (2017) 1\u201325\n\ncontents lists available at sciencedirect\n\ncognitive psychology\n\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / c o g p s y c h\n\nwhere do hypotheses come from?\nishita dasgupta a,\u21d1\n\n, eric schulz b, samuel j. gershman c\n\na department of physics and center for brain science, harvard university, united states\nb department of experimental psychology, university college london, united kingdom\nc department of psychology and center f", "original research\npublished: 17 december 2015\ndoi: 10.3389/fncom.2015.00149\n\nneural network model of memory\nretrieval\n\nstefano recanatesi 1, mikhail katkov 1, sandro romani 2 and misha tsodyks 1, 3*\n\n1 department of neurobiology, weizmann institute of science, rehovot, israel, 2 janelia farm research campus, howard\nhughes medical institute, ashburn, va, usa, 3 department of neurotechnologies, lobachevsky state university of nizhny\n\nnovgorod, nizhny novgorod, russia\n\nhuman memory can store large ", "deep reinforcement learning with double q-learning\n\nhado van hasselt and arthur guez and david silver\n\ngoogle deepmind\n\n5\n1\n0\n2\n\n \nc\ne\nd\n8\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n1\n6\n4\n6\n0\n\n.\n\n9\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nthe popular q-learning algorithm is known to overestimate\naction values under certain conditions. it was not previously\nknown whether, in practice, such overestimations are com-\nmon, whether they harm performance, and whether they can\ngenerally be prevented. in this paper, we answer ", "chapter\n\n22\n\nmedial prefrontal cortex\nand the adaptive regulation\nof reinforcement learning\nparameters\nmehdi khamassi*,{,{,},1, pierre enel*,{, peter ford dominey*,{, emmanuel procyk*,{\n\u204e\ninserm u846, stem cell and brain research institute, bron, france\n{\nuniversite\u00b4 de lyon, lyon 1, umr-s 846, lyon, france\n{institut des syste`mes intelligents et de robotique, universite\u00b4 pierre et marie\ncurie-paris 6, paris cedex 05, france\n}\ncnrs umr 7222, paris, france\n1corresponding author. tel.: \u00fe33-1442728", "article\n\nreceived 17 feb 2017 | accepted 26 may 2017 | published 13 jul 2017\n\ndoi: 10.1038/ncomms16091\n\nopen\n\nprecise inhibitory microcircuit assembly of\ndevelopmentally related neocortical interneurons\nin clusters\nxin-jun zhang1, zhizhong li1, zhi han2,3, khadeejah t. sultan1,4, kun huang2 & song-hai shi1,4\n\ngaba-ergic interneurons provide diverse inhibitions that are essential for the operation of\nneuronal circuits in the neocortex. however, the mechanisms that control the functional\norganizat", "pre-train, prompt, and predict: a systematic survey of\nprompting methods in natural language processing\n\npengfei liu\n\nweizhe yuan\n\njinlan fu\n\ncarnegie mellon university\n\npliu3@cs.cmu.edu\n\ncarnegie mellon university\nweizhey@cs.cmu.edu\n\nnational university of singapore\njinlanjonna@gmail.com\n\nzhengbao jiang\n\ncarnegie mellon university\nzhengbaj@cs.cmu.edu\n\nhiroaki hayashi\n\ncarnegie mellon university\nhiroakih@cs.cmu.edu\n\ngraham neubig\n\ncarnegie mellon university\ngneubig@cs.cmu.edu\n\nabstract\n\nthis pap", "a\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\nt\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nhhs public access\nauthor manuscript\nneural comput. author manuscript; available in pmc 2021 november 17.\n\npublished in final edited form as:\nneural comput. 2021 april 13; 33(5): 1300\u20131328. doi:10.1162/neco_a_01374.\n\ncontrastive similarity matching for supervised learning\n\nshanshan qin,\njohn a. paulson school of engineering and applied sciences,", "assembling old tricks for new tasks: a neural model\n\nof instructional learning and control\n\ntsung-ren huang, thomas e. hazy, seth a. herd,\n\nand randall c. o\u02bcreilly\n\nabstract\n\u25a0 we can learn from the wisdom of others to maximize success.\nhowever, it is unclear how humans take advice to flexibly\nadapt behavior. on the basis of data from neuroanatomy, neuro-\nphysiology, and neuroimaging, a biologically plausible model is\ndeveloped to illustrate the neural mechanisms of learning from\ninstructions. th", "biorxiv preprint \nthe copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is \n\nhttps://doi.org/10.1101/2023.07.18.549575\n; \n\nthis version posted july 21, 2023. \n\ndoi: \n\nmade available under a\n\ncc-by-nc 4.0 international license\n.\n\nnonlinear manifolds underlie neural population\n\nactivity during behaviour\n\nc\u00b4atia fortunato1, jorge bennasar-v\u00b4azquez1, junchol park2, joanna c. ch", "trap of feature diversity in the learning of mlps\n\ndongrui liua\u2217\n\nshaobo wangb\u2217 jie rena kangrui wanga\n\nhuiqi denga quanshi zhanga \u2020\n\nsheng yina\n\nashanghai jiao tong university\n\nbharbin institute of technology\n\n2\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n0\n8\n9\n0\n0\n\n.\n\n2\n1\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nin this paper, we focus on a typical two-phase phenomenon in the learning of\nmulti-layer perceptrons (mlps), and we aim to explain the reason for the decrease\nof feature diversity in the \ufb01rst ", "architectural universality of neural tangent kernel training dynamics\n\ntensor programs iib:\n\ngreg yang 1 * etai littwin 2 *\n\nabstract\n\nyang (2020a) recently showed that the neural\ntangent kernel (ntk) at initialization has an\nin\ufb01nite-width limit for a large class of architec-\ntures including modern staples such as resnet\nand transformers. however, their analysis does\nnot apply to training. here, we show the same neu-\nral networks (in the so-called ntk parametriza-\ntion) during training follow a ", "gaussian gated linear networks\n\ndavid budden\u2217 adam h. marblestone\u2217 eren sezener\u2217\n\ntor lattimore greg wayne\u2020\n\njoel veness\u2020\n\ndeepmind\n\naixi@google.com\n\nabstract\n\nwe propose the gaussian gated linear network (g-gln), an extension to the\nrecently proposed gln family of deep neural networks. instead of using back-\npropagation to learn features, glns have a distributed and local credit assignment\nmechanism based on optimizing a convex objective. this gives rise to many\ndesirable properties including u", "7\n1\n0\n2\n\n \nr\np\na\n9\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n6\n9\n7\n5\n0\n\n.\n\n4\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nquantifying interpretability of deep visual representations\n\nnetwork dissection:\n\ndavid bau\u2217, bolei zhou\u2217, aditya khosla, aude oliva, and antonio torralba\n\n{davidbau, bzhou, khosla, oliva, torralba}@csail.mit.edu\n\ncsail, mit\n\nabstract\n\nlamps in places net\n\nwheels in object net\n\npeople in video net\n\nwe propose a general framework called network dissec-\ntion for quantifying the interpretability of latent repres", "letter\n\ncommunicated by lea duncker\n\nprobing the relationship between latent linear dynamical\nsystems and low-rank recurrent neural network models\n\nadrian valente\nadrian.valente@ens.fr\nsrdjan ostojic\nsrdjan.ostojic@ens.fr\nlaboratoire de neurosciences cognitives et computationnelles, inserm u960,\necole normale superieure\u2013psl research university, 75005 paris, france\n\njonathan w. pillow\npillow@princeton.edu\nprinceton neuroscience institute, princeton university, princeton,\nnj 08544, u.s.a.\n\na large", "letter\n\ncommunicated by laurence abbott\n\nspike-timing-dependent hebbian plasticity as temporal\ndifference learning\n\nrajesh p. n. rao\ndepartment of computer science and engineering, university of washington,\nseattle, wa 98195-2350, u.s.a.\n\nterrence j. sejnowski\nhoward hughes medical institute, the salk institute for biological studies, la jolla,\nca 92037, u.s.a., and department of biology, university of california at san diego,\nla jolla, ca 92037, u.s.a.\n\na spike-timing-dependent hebbian mechanis", "empirical evaluation of\n\ngated recurrent neural networks\n\non sequence modeling\n\n4\n1\n0\n2\n\n \nc\ne\nd\n1\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n5\n5\n5\n3\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\njunyoung chung\n\ncaglar gulcehre\n\nuniversit\u00b4e de montr\u00b4eal\n\nkyunghyun cho\n\nyoshua bengio\n\nuniversit\u00b4e de montr\u00b4eal\ncifar senior fellow\n\nabstract\n\nin this paper we compare different types of recurrent units in recurrent neural net-\nworks (rnns). especially, we focus on more sophisticated units that implement\na gating mechanism, such as", "published as a conference paper at iclr 2021\n\ntowards nonlinear disentanglement in\nnatural data with temporal sparse coding\n\ndavid klindt\u2217\nuniversity of t\u00fcbingen\nklindt.david@gmail.com\n\nlukas schott\u2217\nuniversity of t\u00fcbingen\nlukas.schott@bethgelab.org\n\nyash sharma\u2217\nuniversity of t\u00fcbingen\nyash.sharma@bethgelab.org\n\nivan ustyuzhaninov\nuniversity of t\u00fcbingen\nivan.ustyuzhaninov@bethgelab.org\n\nmatthias bethge\u2021\nuniversity of t\u00fcbingen\nmatthias.bethge@bethgelab.org\n\nwieland brendel\nuniversity of t\u00fcbingen\n", "the journal of neuroscience, december 8, 2010 \u2022 30(49):16601\u201316608 \u2022 16601\n\nbehavioral/systems/cognitive\n\nexpectation and surprise determine neural population\nresponses in the ventral visual stream\n\ntobias egner,1,2 jim m. monti,3 and christopher summerfield4\n1department of psychology and neuroscience, and 2center for cognitive neuroscience, duke university, durham, north carolina 27708, 3department of\npsychology, university of illinois, beckman institute, urbana-champaign, illinois 61801, and 4", "1\n2\n0\n2\n\n \nt\nc\no\n7\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n0\n2\n6\n9\n0\n\n.\n\n6\n0\n1\n2\n:\nv\ni\nx\nr\na\n\ndisentangling identi\ufb01able features from noisy data\n\nwith structured nonlinear ica\n\nhermanni h\u00e4lv\u00e41 \u2217\n\nsylvain le corff2\n\nluc leh\u00e9ricy3\n\njonathan so4\n\nyongjie zhu1\n\nelisabeth gassiat5 \u2020\n\naapo hyv\u00e4rinen1 \u2020\n\n1department of computer science, university of helsinki, finland\n\n2 samovar, t\u00e9l\u00e9com sudparis, d\u00e9partement citi, institut polytechnique de paris, palaiseau, france\n\n3laboratoire j. a. dieudonn\u00e9, universi", "learning and generalization in overparameterized neural\n\nnetworks, going beyond two layers\n\nzeyuan allen-zhu\n\nzeyuan@csail.mit.edu\nmicrosoft research ai\n\nyuanzhi li\n\nyingyu liang\n\nyuanzhil@stanford.edu\n\nyliang@cs.wisc.edu\n\nstanford university\n\nuniversity of wisconsin-madison\n\nnovember 12, 2018\n\n(version 6)\u2217\n\nabstract\n\nthe fundamental learning theory behind neural networks remains largely open. what classes\nof functions can neural networks actually learn? why doesn\u2019t the trained network over\ufb01t wh", "biorxiv preprint \n\nhttps://doi.org/10.1101/2022.05.17.492325\n; \n\ndoi: \nwas not certified by peer review) is the author/funder. all rights reserved. no reuse allowed without permission. \n\nthis version posted may 18, 2022. \n\nthe copyright holder for this preprint (which\n\ninferring neural activity before plasticity: a foundation\nfor learning beyond backpropagation\nyuhang song1,2,*, beren millidge2, tommaso salvatori1, thomas lukasiewicz1,*, zhenghua xu1,3, and\nrafal bogacz2,*\n\n1department of comput", "the journal of neuroscience, september 20, 2006 \u2022 26(38):9673\u20139682 \u2022 9673\n\nbehavioral/systems/cognitive\n\ntriplets of spikes in a model of\nspike timing-dependent plasticity\n\njean-pascal pfister and wulfram gerstner\nlaboratory of computational neuroscience, school of computer and communication sciences and brain-mind institute, ecole polytechnique fe\u00b4de\u00b4rale de\nlausanne, ch-1015 lausanne, switzerland\n\nclassical experiments on spike timing-dependent plasticity (stdp) use a protocol based on pairs o", "motor cortical representation of speed and direction\nduring reaching\n\ndaniel w. moran and andrew b. schwartz\nthe neurosciences institute, san diego, california 92121\n\nmoran, daniel w. and andrew b. schwartz. motor cortical repre-\nsentation of speed and direction during reaching. j. neurophysiol. 82:\n2676 \u20132692, 1999. the motor cortical substrate associated with reach-\ning was studied as monkeys moved their hands from a central position\nto one of eight targets spaced around a circle. single-cell ", "the statistical structure of the hippocampal code\nfor space as a function of time, context, and value\n\narticle\n\ngraphical abstract\n\nauthors\njae sung lee, john j. briguglio,\njeremy d. cohen, sandro romani,\nalbert k. lee\n\ncorrespondence\nleej13@janelia.hhmi.org (j.s.l.),\nbriguglioj@janelia.hhmi.org (j.j.b.),\nsandro.romani@gmail.com (s.r.),\nleea@janelia.hhmi.org (a.k.l.)\n\nin brief\na uni\ufb01ed response pattern of\nhippocampal place cells, driven by\nindividual cell-intrinsic mechanisms,\nquantitatively pre", "variational inference: a review for statisticians\n\ndavid m. blei\n\ndepartment of computer science and statistics\n\ncolumbia university\n\nalp kucukelbir\n\ndepartment of computer science\n\ncolumbia university\n\njon d. mcauliffe\n\ndepartment of statistics\n\nuniversity of california, berkeley\n\nmay 11, 2018\n\nabstract\n\none of the core problems of modern statistics is to approximate dif\ufb01cult-to-compute\nprobability densities. this problem is especially important in bayesian statistics, which\nframes all inferenc", "6\n1\n0\n2\n\n \n\nb\ne\nf\n2\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n2\n4\n3\n6\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2016\n\nactor-mimic\ndeep multitask and transfer reinforcement\nlearning\n\nemilio parisotto, jimmy ba, ruslan salakhutdinov\ndepartment of computer science\nuniversity of toronto\ntoronto, ontario, canada\n{eparisotto,jimmy,rsalakhu}@cs.toronto.edu\n\nabstract\n\nthe ability to act in multiple environments and transfer previous knowledge to\nnew situations can be considered a critic", "5\n1\n0\n2\n\n \n\nv\no\nn\n5\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n5\n2\n5\n7\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\ndifference target propagation\n\ndong-hyun lee1, saizheng zhang1, asja fischer1, and\n\nyoshua bengio1,2\n\n1 universit\u00b4e de montr\u00b4eal, quebec, canada\n\n2 cifar senior fellow\n\nabstract. back-propagation has been the workhorse of recent successes of deep\nlearning but it relies on in\ufb01nitesimal effects (partial derivatives) in order to per-\nform credit assignment. this could become a serious issue as one considers\ndeepe", "published as a conference paper at iclr 2018\n\non the information bottleneck\ntheory of deep learning\n\nandrew m. saxe, yamini bansal, joel dapello, madhu advani\nharvard university\n{asaxe,madvani}@fas.harvard.edu,{ybansal,dapello}@g.harvard.edu\n\nartemy kolchinsky, brendan d. tracey\nsanta fe institute\n{artemyk,tracey.brendan}@gmail.com\n\ndavid d. cox\nharvard university\nmit-ibm watson ai lab\ndavidcox@fas.harvard.edu\ndavid.d.cox@ibm.com\n\nabstract\n\nthe practical successes of deep neural networks have no", "exponential expressivity in deep neural networks\n\nthrough transient chaos\n\nben poole1, subhaneil lahiri1, maithra raghu2, jascha sohl-dickstein2, surya ganguli1\n\n{benpoole,sulahiri,sganguli}@stanford.edu, {maithra,jaschasd}@google.com\n\n1stanford university, 2google brain\n\nabstract\n\nwe combine riemannian geometry with the mean \ufb01eld theory of high dimensional\nchaos to study the nature of signal propagation in generic, deep neural networks\nwith random weights. our results reveal an order-to-chaos e", "biorxiv preprint \n\nthe copyright holder for this preprint (which was\nnot certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available \n\nthis version posted september 16, 2018. \n\nhttps://doi.org/10.1101/418939\n; \n\ndoi: \n\nunder a\n\ncc-by-nc-nd 4.0 international license\n.\n\nprobing variability in a cognitive map using manifold inference from neural dynamics\nryan j. low(cid:89),1, sam lewallen(cid:89),1,3, dmitriy aronov1", "cognitive \n\nscience \n\n11, \n\n23-63 \n\n(1987) \n\ncompetitive learning: \n\nfrom interactive  activation  to \n\nadaptive resonance \n\nstephen \n\ngrossberg \nboston university \n\nmechanistic \n\ncomparisons \n\nare  mode \n\nbetween \n\nseveral \n\nnetwork \n\nond \ncognitive \n\nof \nresonance, \n\nand \n\nprocessing: \nback \nond \n\narticle \nlearning. \nshown \n\nof  rumelhart \n\nall \n\nthe  models \n\nin  grossberg \n\n(1976b) \n\ncompetitive \ninput \n\nenvironment \n\nlearning \n\nexpectancies, \n\ntop-down \n\nexpectancies; \n\nsituation, \n\nreprese", " \n\nparallel distributed processing copyrighted material \f", "audio adversarial examples: targeted attacks on speech-to-text\n\nnicholas carlini\n\ndavid wagner\nuniversity of california, berkeley\n\n8\n1\n0\n2\n\n \nr\na\n\n \n\nm\n0\n3\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n4\n4\n9\n1\n0\n\n.\n\n1\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014we construct targeted audio adversarial examples\non automatic speech recognition. given any audio waveform,\nwe can produce another that\nis over 99.9% similar, but\ntranscribes as any phrase we choose (recognizing up to 50\ncharacters per second of audio). we apply our whit", "report\n\ndendritic spines prevent synaptic voltage clamp\n\nhighlights\nd high spine neck resistance prevents voltage-clamp control of\n\nauthors\n\nlou beaulieu-laroche, mark t. harnett\n\nexcitatory synapses.\n\nd voltage-clamp measurements are severely distorted for\n\nspiny neurons.\n\nd large single-spine ampa conductance saturates synaptic\n\ncurrent.\n\ncorrespondence\nharnett@mit.edu\n\nin brief\nbeaulieu-laroche and harnett\ndemonstrate that voltage clamp, the main\napproach to investigate synaptic\nphysiology, i", "e\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\n\nl\n\ny\ng\no\no\ni\ns\ny\nh\np\n\nf\no\n\nl\n\na\nn\nr\nu\no\nj\n\ne\nh\nt\n\nj physiol 591.22 (2013) pp 5425\u20135431\n\ns y m p o s i u m r e v i e w\n\n5425\n\nthe evolutionary origin of the vertebrate basal ganglia\nand its role in action selection\n\nsten grillner, brita robertson and marcus stephenson-jones\n\ndepartment of neuroscience, karolinska institutet, se-17177 stockholm, sweden\n\nabstract the group of nuclei within the basal ganglia of the forebrain is central to the control of\nmovement. we present ", "toroidal topology of population activity in \ngrid cells\n\nhttps://doi.org/10.1038/s41586-021-04268-7\nreceived: 24 february 2021\naccepted: 19 november 2021\npublished online: 12 january 2022\nopen access\n\n check for updates\n\nrichard j. gardner1,6\u2009\u2709, erik hermansen2,6, marius pachitariu3, yoram burak4,5, nils a. baas2\u2009\u2709, \nbenjamin a. dunn1,2\u2009\u2709, may-britt moser1 & edvard i. moser1\u2009\u2709\n\nthe medial entorhinal cortex is part of a neural system for mapping the position of an \nindividual within a physical en", "reliability of spike timing in neocortical neurons \nauthor(s): zachary f. mainen and terrence j. sejnowski \nsource: science, jun. 9, 1995, new series, vol. 268, no. 5216 (jun. 9, 1995), pp. 1503-\n1506\npublished by: american association for the advancement of science \n\n \n\nstable url: https://www.jstor.org/stable/2887763\n\njstor is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide \nrange of content in a trusted digital archive. we use infor", "siam review\nvol. 64, no. 1, pp. 153\u2013178\n\nc\\bigcirc  2022 society for industrial and applied mathematics\n\ndimensionality reduction via\ndynamical systems: the case of t-sne\\ast \n\ngeorge c. linderman\\dagger \nstefan steinerberger\\ddagger \n\nabstract. t-distributed stochastic neighborhood embedding (t-sne), a clustering and visualization\nmethod proposed by van der maaten and hinton in 2008, has rapidly become a standard\ntool in the natural sciences. despite its overwhelming success, it has a distinct ", "ne34ch19-gerfen\n\nari\n\n13 may 2011\n\n10:57\n\nannu. rev. neurosci. 2011. 34:441\u201366\n\nfirst published online as a review in advance on\napril 4, 2011\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev-neuro-061010-113641\ncopyright c(cid:2) 2011 by annual reviews.\nall rights reserved\n\n0147-006x/11/0721-0441$20.00\n\nmodulation of striatal\nprojection systems by\ndopamine\n\ncharles r. gerfen1 and d. james surmeier2\n1laboratory of systems neuroscience, ", "0270~6474/82/0201-0032$02.00/o \ncopyright \nprinted \n\nin  u.s.a. \n\n0  society \n\nfor  neuroscience \n\ntheory  for  the  development \norientation \nvisual  cortex1 \n\nspecificity \n\nof  neuron \n\nand  binocular \n\nthe \n\njournal \n\nof  neuroscience \n\nvol.  2,  no. \n\n1,  pp.  32-48 \n1982 \n\njanuary \n\nselectivity: \n\ninteraction \n\nin \n\nelie  l.  bienenstock,2 \n\nleon  n  cooper,3 \n\nand  paul  w.  munro \n\ncenter \n\nfor  neural \n\nscience,  department \n\nof  physics,  and  division \nisland \n\nrhode \n\nof applied  math", "learning representations that support extrapolation\n\ntaylor w. webb 1 zachary dulberg 2 steven m. frankland 2 alexander a. petrov 3 randall c. o\u2019reilly 4\n\njonathan d. cohen 2\n\nabstract\n\nextrapolation \u2013 the ability to make inferences that\ngo beyond the scope of one\u2019s experiences \u2013 is a\nhallmark of human intelligence. by contrast, the\ngeneralization exhibited by contemporary neural\nnetwork algorithms is largely limited to interpola-\ntion between data points in their training corpora.\nin this paper", "7\n1\n0\n2\n\n \n\nv\no\nn\n5\n2\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n2\nv\n4\n7\n8\n7\n0\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\na uni\ufb01ed approach to interpreting model\n\npredictions\n\nscott m. lundberg\n\nsu-in lee\n\npaul g. allen school of computer science\n\npaul g. allen school of computer science\n\nuniversity of washington\n\nseattle, wa 98105\n\nslund1@cs.washington.edu\n\ndepartment of genome sciences\n\nuniversity of washington\n\nseattle, wa 98105\n\nsuinlee@cs.washington.edu\n\nabstract\n\nunderstanding why a model makes a certain prediction can be", "3\n2\n0\n2\n\n \n\ny\na\nm\n5\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n7\n5\n1\n2\n0\n\n.\n\n0\n1\n2\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2023\n\nthe influence of learning rule on represen-\ntation dynamics in wide neural networks\n\nblake bordelon & cengiz pehlevan\nschool of engineering and applied science\nharvard university\ncambridge, ma 02138, usa\n{blake bordelon,cpehlevan}@g.harvard.edu\n\nabstract\n\nit is unclear how changing the learning rule of a deep neural network alters its\nlearning dynamics and re", "bernoulli 19(4), 2013, 1378\u20131390\ndoi: 10.3150/12-bejsp17\n\non statistics, computation and scalability\n\nmichael i. jordan\n\ndepartment of statistics and department of eecs, university of california, berkeley, ca, usa.\ne-mail: jordan@stat.berkeley.edu; url:www.cs.berkeley.edu/~jordan\n\nhow should statistical procedures be designed so as to be scalable computationally to the massive datasets\nthat are increasingly the norm? when coupled with the requirement that an answer to an inferential question\nbe ", "the journal of neuroscience, november 15, 1996, 16(22):7376 \u20137389\n\ncontextual modulation in primary visual cortex\n\nkarl zipser,1 victor a. f. lamme,2 and peter h. schiller1\n1the department of brain and cognitive sciences, massachusetts institute of technology, cambridge, massachusetts\n02139, and 2graduate school of neurosciences, department of medical physics, amc, university of amsterdam,\namsterdam, the netherlands, and the netherlands ophthalmic research institute, 1100 ac amsterdam, the\nnethe", "hippocampal place cell assemblies are\nspeed-controlled oscillators\n\ncaroline geisler, david robbe, michae\u00a8 l zugaro*, anton sirota, and gyo\u00a8 rgy buzsa\u00b4 ki\u2020\n\ncenter for molecular and behavioral neuroscience, rutgers, the state university of new jersey, 197 university avenue, newark, nj 07102\n\nedited by nancy j. kopell, boston university, boston, ma, and approved march 28, 2007 (received for review november 14, 2006)\n\nthe phase of spikes of hippocampal pyramidal cells relative to the\nlocal \ufb01eld \u242a ", "research article\n\ntraining excitatory-inhibitory recurrent\nneural networks for cognitive tasks: a\nsimple and flexible framework\nh. francis song1\u262f, guangyu r. yang1\u262f, xiao-jing wang1,2*\n\n1 center for neural science, new york university, new york, new york, united states of america, 2 nyu-\necnu institute of brain and cognitive science, nyu shanghai, shanghai, china\n\n\u262f these authors contributed equally to this work.\n* xjwang@nyu.edu\n\nabstract\n\nthe ability to simultaneously record from large numbers", "6\n1\n0\n2\n\n \n\nv\no\nn\n0\n1\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n2\nv\n9\n7\n7\n2\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nunderreviewasaconferencepaperaticlr2017rl2:fastreinforcementlearningviaslowreinforcementlearningyanduan\u2020\u2021,johnschulman\u2020\u2021,xichen\u2020\u2021,peterl.bartlett\u2020,ilyasutskever\u2021,pieterabbeel\u2020\u2021\u2020ucberkeley,departmentofelectricalengineeringandcomputerscience\u2021openai{rocky,joschu,peter}@openai.com,peter@berkeley.edu,{ilyasu,pieter}@openai.comabstractdeepreinforcementlearning(deeprl)hasbeensuccessfulinlearningsophis-ticatedbehavi", "simulating a primary visual cortex at the front of\ncnns improves robustness to image perturbations\n\nmartin schrimpf1,2,4, franziska geiger2,5,6,7, david d. cox8,3, james j. dicarlo1,2,4\n\njoel dapello\u2217,1,2,3, tiago marques\u2217,1,2,4\n\n\u2217joint \ufb01rst authors (equal contribution)\n\n1department of brain and cognitive sciences, mit, cambridge, ma02139\n\n2mcgovern institute for brain research, mit, cambridge, ma02139\n\n3school of engineering and applied sciences, harvard university, cambridge, ma02139\n\n4center ", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/083824\n; \n\nthis version posted july 4, 2017. \n\nthe copyright holder for this preprint (which was not\n\ncertified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available under \n\na\n\ncc-by-nd 4.0 international license\n.\n\n \n\nthe successor representation in human reinforcement learning \n\nmomennejad i1*, russek em2*, cheong jh3, botvinick mm4, daw nd1, gershman sj5 \n\n \n1        ", "neuron\n\narticle\n\ntranslaminar inhibitory cells recruited\nby layer 6 corticothalamic neurons\nsuppress visual cortex\n\ndante s. bortone,1 shawn r. olsen,1 and massimo scanziani1,*\n1howard hughes medical institute, center for neural circuits and behavior, neurobiology section and department of neuroscience,\nuniversity of california san diego, la jolla, ca 92093-0634, usa\n*correspondence: massimo@ucsd.edu\nhttp://dx.doi.org/10.1016/j.neuron.2014.02.021\n\nsummary\n\nin layer 6 (l6), a principal output lay", "journal of neural engineeringpaper \u2022 open accessdecoding hand kinematics from populationresponses in sensorimotor cortex during graspingto cite this article: elizaveta v okorokova et al 2020 j. neural eng. 17 046035 view the article online for updates and enhancements.you may also likemechanical design of a low-cost abs handprosthesis using the finite element methoda bastarrechea, q estrada, j zubrzycki etal.-enhancing gesture decoding performanceusing signals from posterior parietalcortex: a st", "9\n1\n0\n2\n\n \nl\nu\nj\n \n\n5\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n6\nv\n1\n7\n9\n2\n0\n\n.\n\n9\n0\n5\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2016\n\ncontinuous control with deep reinforcement\nlearning\n\ntimothy p. lillicrap\u2217, jonathan j. hunt\u2217, alexander pritzel, nicolas heess,\ntom erez, yuval tassa, david silver & daan wierstra\ngoogle deepmind\nlondon, uk\n{countzero, jjhunt, apritzel, heess,\netom, tassa, davidsilver, wierstra} @ google.com\n\nabstract\n\nwe adapt the ideas underlying the success of deep q-learning to", "3\n1\n0\n2\n\n \n\ng\nu\na\n5\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n3\n4\n3\n\n.\n\n8\n0\n3\n1\n:\nv\ni\nx\nr\na\n\nestimating or propagating gradients through\nstochastic neurons for conditional computation\n\nyoshua bengio, nicholas l\u00b4eonard and aaron courville\nd\u00b4epartement d\u2019informatique et recherche op\u00b4erationnelle\n\nuniversit\u00b4e de montr\u00b4eal\n\nabstract\n\nstochastic neurons and hard non-linearities can be useful for a number of rea-\nsons in deep learning models, but in many cases they pose a challenging problem:\nhow to estimat", "neuron\n\narticle\n\nstructural and molecular remodeling\nof dendritic spine substructures\nduring long-term potentiation\n\nmiquel bosch,1,2,* jorge castro,2,5 takeo saneyoshi,3,5 hitomi matsuno,3 mriganka sur,2 and yasunori hayashi1,2,3,4,*\n1riken-mit neuroscience research center\n2the picower institute for learning and memory, department of brain and cognitive sciences\nmassachusetts institute of technology, cambridge, ma 02139, usa\n3brain science institute, riken, wako, saitama 351-0198, japan\n4saitam", "a r t i c l e s\n\nthe descending corticocollicular pathway mediates \nlearning-induced auditory plasticity\n\nvictoria m bajo1, fernando r nodal1, david r moore1,2 & andrew j king1\n\ndescending projections from sensory areas of the cerebral cortex are among the largest pathways in the brain, suggesting that \nthey are important for subcortical processing. although corticofugal inputs have been shown to modulate neuronal responses in \nthe thalamus and midbrain, the behavioral importance of these change", "research article\n\nthe computational nature of memory\nmodification\nsamuel j gershman1*, marie-h monfils2, kenneth a norman3, yael niv3\n\n1department of psychology and center for brain science, harvard university,\ncambridge, united states; 2department of psychology, university of texas, austin,\nunited states; 3princeton neuroscience institute and department of psychology,\nprinceton university, princeton, united states\n\nabstract retrieving a memory can modify its influence on subsequent behavior. we", "innovative methodology\n\nj neurophysiol 102: 614 \u2013 635, 2009.\nfirst published april 8, 2009; doi:10.1152/jn.90941.2008.\n\ngaussian-process factor analysis for low-dimensional single-trial analysis\nof neural population activity\n\nbyron m. yu,1,2,4 john p. cunningham,1 gopal santhanam,1 stephen i. ryu,1,3 krishna v. shenoy,1,2,*\nand maneesh sahani4,*\n1department of electrical engineering, 2neurosciences program, and 3department of neurosurgery, stanford university, stanford,\ncalifornia; and 4gatsby c", "language models are few-shot learners\n\ntom b. brown\u2217\n\nbenjamin mann\u2217\n\nnick ryder\u2217\n\nmelanie subbiah\u2217\n\njared kaplan\u2020\n\nprafulla dhariwal\n\narvind neelakantan\n\npranav shyam\n\ngirish sastry\n\namanda askell\n\nsandhini agarwal\n\nariel herbert-voss\n\ngretchen krueger\n\ntom henighan\n\nrewon child\n\naditya ramesh\n\ndaniel m. ziegler\n\njeffrey wu\n\nclemens winter\n\nchristopher hesse\n\nmark chen\n\neric sigler\n\nmateusz litwin\n\nscott gray\n\nbenjamin chess\n\njack clark\n\nchristopher berner\n\nsam mccandlish\n\nalec radford\n\nilya su", "6\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n7\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n0\n4\n3\n5\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nexponential expressivity in deep neural networks\n\nthrough transient chaos\n\nben poole\n\nstanford university\n\npoole@cs.stanford.edu\n\nsubhaneil lahiri\nstanford university\nsulahiri@stanford.edu\n\nmaithra raghu\n\ngoogle brain and cornell university\n\nmaithrar@gmail.com\n\njascha sohl-dickstein\n\ngoogle brain\n\njaschasd@google.com\n\nsurya ganguli\n\nstanford university\n\nsganguli@stanford.edu\n\nabstract\n\nwe combine riemann", "ieee transactions on visualization and computer graphics  vol. 23,  no. 1,  january 2017 \n\n461\n\nembedded data representations\n\nwesley willett, yvonne jansen, pierre dragicevic\n\nfig. 1. examples of embedded data representations1: (a) dye-based \ufb02ow visualization on a 1/48 scale airplane model, (b) yelp\u2019s\nmonocle application uses a mobile phone to display business ratings in front of the establishments, (c) concept image of an augmented\nreality visualization of urban wind \ufb02ow [42], (d) a visualizat", "nature|vol 445|15 february 2007\n\ncollective minds \n\nessay\n\nputting the pieces together\n\niain couzin\n\nby tapping into social cues, individuals in a group may gain access to higher-order computational \ncapacities that mirror the group\u2019s responses to its environment.\ndetected by only a relatively small propor-\ntion of group members due to limitations \nin individual sensory capabilities, often \nfurther restricted by crowding. close \nbehavioural coupling among near neigh-\nbours, however, allows a loc", "shape perception reduces activity in human primary\nvisual cortex\n\nscott o. murray*\u2020, daniel kersten\u2021, bruno a. olshausen*\u00a7, paul schrater\u2021\u00b6, and david l. woods\u50a8**\n*center for neuroscience, \u00a7department of psychology, and \u50a8department of neurology, university of california, davis, ca 95616; departments of\n\u2021psychology and \u00b6computer science and engineering, university of minnesota, minneapolis, mn 55455; and **neurology service (127e),\ndepartment of veterans affairs northern california health care sy", "neuron\n\nperspective\n\nnavigating the neural space\nin search of the neural code\n\nmehrdad jazayeri1,* and arash afraz1\n1department of brain & cognitive sciences, mcgovern institute for brain research, massachusetts institute of technology, cambridge, ma\n02139, usa\n*correspondence: mjaz@mit.edu\nhttp://dx.doi.org/10.1016/j.neuron.2017.02.019\n\nthe advent of powerful perturbation tools, such as optogenetics, has created new frontiers for probing causal\ndependencies in neural and behavioral states. thes", "research article\ndetection and analysis of spatiotemporal\npatterns in brain activity\n\nrory g. townsendid1,2, pulin gong1,2*\n\n1 school of physics, the university of sydney, nsw, australia, 2 arc centre of excellence for integrative\nbrain function, the university of sydney, nsw, australia\n\n* pulin.gong@sydney.edu.au\n\nabstract\n\nthere is growing evidence that population-level brain activity is often organized into propa-\ngating waves that are structured in both space and time. such spatiotemporal pa", "5\n1\n0\n2\n\n \n\nb\ne\nf\n7\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n8\n5\n5\n6\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\nunder review as a conference paper at iclr 2015\n\nrandom walk initialization for training\nvery deep feedforward networks\n\ndavid sussillo\ngoogle inc.\nmountain view, ca, 94303, usa\nsussillo@google.com\n\nl.f. abbott\ndepartments of neuroscience and physiology and cellular biophysics\ncolumbia university\nnew york, ny, 10032, usa\nlfabbott@columbia.edu\n\nabstract\n\ntraining very deep networks is an important open problem i", "predictive coding approximates backprop along\n\narbitrary computation graphs\n\n0\n2\n0\n2\n\n \nt\nc\no\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n2\n8\n1\n4\n0\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nberen millidge\n\nschool of informatics\nuniversity of edinburgh\nberen@millidge.name\n\nalexander tschantz\n\nsackler centre for consciousness science\n\nschool of engineering and informatics\n\nuniversity of sussex\n\ntschantz.alec@gmail.com\n\nchristopher l buckley\n\nevolutionary and adaptive systems research group\n\nschool of engineering and informa", "r e v i e w s\n\n s l e e p\n\nthe memory function of sleep\n\nsusanne diekelmann and jan born\n\nabstract | sleep has been identified as a state that optimizes the consolidation of newly \nacquired information in memory, depending on the specific conditions of learning and the \ntiming of sleep. consolidation during sleep promotes both quantitative and qualitative \nchanges of memory representations. through specific patterns of neuromodulatory activity \nand electric field potential oscillations, slow-wav", "two-moment decision models and expected utility maximization \nauthor(s): jack meyer \nsource: the american economic review, jun., 1987, vol. 77, no. 3 (jun., 1987), pp. 421-\n430\npublished by: american economic association \n\n \n\nstable url: https://www.jstor.org/stable/1804104\n \nreferences \nlinked references are available on jstor for this article: \nhttps://www.jstor.org/stable/1804104?seq=1&cid=pdf-\nreference#references_tab_contents \nyou may need to log in to jstor to access the linked references.", "n\ne\nu\nr\na\nl\n \nn\ne\nt\nw\no\nr\nk\ns\n,\n \nv\no\nl\n.\n \n5\n,\n \np\np\n.\n \n3\n-\n1\n7\n,\n \n1\n9\n9\n2\n \n0\n8\n9\n3\n-\n6\n0\n8\n0\n/\n9\n2\n \n$\n5\n.\n0\n0\n \n+\n \n.\n0\n0\n \np\nr\ni\nn\nt\ne\nd\n \ni\nn\n \nt\nh\ne\n \nu\ns\na\n.\n \na\nl\nl\n \nr\ni\ng\nh\nt\ns\n \nr\ne\ns\ne\nr\nv\ne\nd\n.\n \nc\no\np\ny\nr\ni\ng\nh\nt\n \n\u00a9\n \n1\n9\n9\n2\n \np\ne\nr\ng\na\nm\no\nn\n \np\nr\ne\ns\ns\n \np\nl\nc\n \ni\nn\nv\ni\nt\ne\nd\n \na\nr\nt\ni\nc\nl\ne\n \no\nb\nj\ne\nc\nt\ni\nv\ne\n \nf\nu\nn\nc\nt\ni\no\nn\n \nf\no\nr\nm\nu\nl\na\nt\ni\no\nn\n \no\nf\n \nt\nh\ne\n \nb\nc\nm\n \nt\nh\ne\no\nr\ny\n \no\nf\n \nv\ni\ns\nu\na\nl\n \nc\no\nr\nt\ni\nc\na\nl\n \np\nl\na\ns\nt\ni\nc\ni\nt\ny\n:\n \ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n \n", "deep learning models of the retinal response to\n\nnatural scenes\n\nlane t. mcintosh\u22171, niru maheswaranathan\u22171, aran nayebi1,\n\nsurya ganguli2,3, stephen a. baccus3\n\n1neurosciences phd program, 2department of applied physics, 3neurobiology department\n\n{lmcintosh, nirum, anayebi, sganguli, baccus}@stanford.edu\n\nstanford university\n\nabstract\n\na central challenge in sensory neuroscience is to understand neural computations\nand circuit mechanisms that underlie the encoding of ethologically relevant, nat", "distributed deep q-learning\n\nkevin chavez1, hao yi ong1, and augustus hong1\n\nabstract\u2014 we propose a distributed deep learning model\nto successfully learn control policies directly from high-\ndimensional sensory input using reinforcement learning. the\nmodel is based on the deep q-network, a convolutional neural\nnetwork trained with a variant of q-learning. its input is\nraw pixels and its output\nis a value function estimating\nfuture rewards from taking an action given a system state.\nto distribute", "reviewarticleintrinsicdimensionestimation:relevanttechniquesandabenchmarkframeworkp.campadelli,1e.casiraghi,1c.ceruti,1anda.rozza21dipartimentodiinformatica,universit`adeglistudidimilano,viacomelico39,20135milano,italy2researchgroup,hyerasoftware,viamattei2,coccaglio,25030brescia,italycorrespondenceshouldbeaddressedtoe.casiraghi;casiraghi@di.unimi.itreceived25february2015;accepted17may2015academiceditor:sangminleecopyright\u00a92015p.campadellietal.thisisanopenaccessarticledistributedunderthecreative", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/2826691\n\nthe role of constraints in hebbian learning\n\narticle\u00a0\u00a0in\u00a0\u00a0neural computation \u00b7 july 1997\n\ndoi: 10.1162/neco.1994.6.1.100\u00a0\u00b7\u00a0source: citeseer\n\ncitations\n360\n\n2 authors, including:\n\nkenneth d miller\ncolumbia university\n\n123 publications\u00a0\u00a0\u00a010,715 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n734\n\nall content following this page was uploaded by kenneth d miller on 19 december 2013.\n\nthe user has ", "foundations and trends r(cid:1) in\nmachine learning\nvol. 1, nos. 1\u20132 (2008) 1\u2013305\nc(cid:1) 2008 m. j. wainwright and m. i. jordan\ndoi: 10.1561/2200000001\n\ngraphical models, exponential families, and\n\nvariational inference\n\nmartin j. wainwright1 and michael i. jordan2\n\n1 department of statistics, and department of electrical engineering and\n\ncomputer science, university of california, berkeley 94720, usa,\nwainwrig@stat.berkeley.edu\n\n2 department of statistics, and department of electrical enginee", "letter\nnonlinear dendritic integration of sensory and motor\ninput during an active sensing task\n\ndoi:10.1038/nature11601\n\nning-long xu1, mark t. harnett1, stephen r. williams2, daniel huber1{, daniel h. o\u2019connor1{, karel svoboda1 & jeffrey c. magee1\n\nactive dendrites provide neurons with powerful processing capabi-\nlities. however, little is known about the role of neuronal dendrites\nin behaviourally related circuit computations. here we report that a\nnovel global dendritic nonlinearity is invol", "connectivity\t\r \u00a0 reflects\t\r \u00a0 coding:\t\r \u00a0 a\t\r \u00a0 model\t\r \u00a0 of\t\r \u00a0 voltage-\u00ad\u2010based\t\r \u00a0 spike-\u00ad\u2010\ntiming-\u00ad\u2010dependent-\u00ad\u2010plasticity\t\r \u00a0with\t\r \u00a0homeostasis\t\r \u00a0\n\nclaudia\t\r \u00a0clopath,\t\r \u00a0lars\t\r \u00a0b\u00fcsing*,\t\r \u00a0eleni\t\r \u00a0vasilaki,\t\r \u00a0wulfram\t\r \u00a0gerstner\t\r \u00a0\n\t\r \u00a0\nlaboratory\t\r \u00a0of\t\r \u00a0computational\t\r \u00a0neuroscience\t\r \u00a0\nbrain-\u00ad\u2010mind\t\r \u00a0institute\t\r \u00a0and\t\r \u00a0school\t\r \u00a0of\t\r \u00a0computer\t\r \u00a0and\t\r \u00a0communication\t\r \u00a0sciences\t\r \u00a0\necole\t\r \u00a0polytechnique\t\r \u00a0f\u00e9d\u00e9rale\t\r \u00a0de\t\r \u00a0lausanne\t\r \u00a0\n1015\t\r \u00a0lausanne\t\r \u00a0epfl,\t\r \u00a0switzerland", "improved preconditioner for\nhessian free optimization\n\nolivier chapelle\n\nyahoo! labs\n\nsanta clara, ca\n\ndumitru erhan\n\nyahoo! labs\nsunnyvale, ca\n\nchap@yahoo-inc.com\n\ndumitru@yahoo-inc.com\n\nabstract\n\nwe investigate the use of hessian free optimization for learning deep au-\ntoencoders. one of the critical components in that algorithm is the choice\nof the preconditioner. we argue in this paper that the jacobi precondi-\ntioner leads to faster optimization and we show how it can be accurately\nand e\ufb03ci", "8\n1\n0\n2\n\n \nt\nc\no\n \n6\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n3\n9\n3\n1\n1\n\n.\n\n0\n1\n8\n1\n:\nv\ni\nx\nr\na\n\ndendritic cortical microcircuits\n\napproximate the backpropagation algorithm\n\njo\u00e3o sacramento\u2217\n\ndepartment of physiology\n\nuniversity of bern, switzerland\nsacramento@pyl.unibe.ch\n\nyoshua bengio\u2021\n\nmila and universit\u00e9 de montr\u00e9al, canada\n\nyoshua.bengio@mila.quebec\n\nrui ponte costa\u2020\n\ndepartment of physiology\n\nuniversity of bern, switzerland\n\ncosta@pyl.unibe.ch\n\nwalter senn\n\ndepartment of physiology\n\nuniversi", "physical review x 11, 021064 (2021)\n\ntransient chaotic dimensionality expansion by recurrent networks\nchristian keup ,1,2,*,\u2020\n\ntobias k\u00fchn ,1,2,3,* david dahmen ,1 and moritz helias 1,4\n\n1institute of neuroscience and medicine (inm-6) and institute for advanced simulation (ias-6) and jara\n\ninstitut brain structure-function relationships (inm-10), j\u00fclich research centre, j\u00fclich, germany\n\n3laboratoire de physique de l\u2019ens, laboratoire msc de l\u2019universit\u00b4e de paris, cnrs, paris, france\n\n4department", "published as a conference paper at iclr 2020\n\nfantastic generalization measures\nand where to find them\n\nyiding jiang\u2217\u2020, behnam neyshabur\u2217, hossein mobahi, dilip krishnan, samy bengio\ngoogle research\n{ydjiang,neyshabur,hmobahi,dilipkay,bengio}@google.com\n\nabstract\n\ngeneralization of deep networks has lately been of great interest, resulting in a\nnumber of theoretically and empirically motivated complexity measures. how-\never, most papers proposing such measures study only a small set of models, l", "c h a p t e r\n\n15\n\nvalue learning through reinforcement:\n\nthe basics of dopamine and reinforcement\n\nlearning\n\nnathaniel d. daw and philippe n. tobler\n\no u t l i n e\n\nintroduction\n\nlearning: prediction and prediction errors\n\nfunctional anatomy of dopamine and striatum\n\nresponses of dopamine neurons to outcomes\nsequential predictions: from rescorla\u0000wagner to\ntemporal difference learning\n\n283\n\n283\n\n285\n\n287\n\n289\n\ntemporal difference learning and the dopamine\nresponse\n\nfrom error-driven learning to ", "1 of 27\n\ntext  \nonly \n\nmythosthe\n\nof model \ninterpretability \n\nin machine learning, the \nconcept of interpretability is \nboth important and slippery.\n\nzachary c. lipton \n\nsupervised machine-learning models boast \n\nremarkable predictive capabilities. but can you \ntrust your model? will it work in deployment? \nwhat else can it tell you about the world? \nmodels should be not only good, but also \ninterpretable, yet the task of interpretation appears \nunderspecified. the academic literature has provi", "5\n1\n0\n2\n\n \n\ny\na\nm\n8\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n7\n9\n5\n4\n0\n\n.\n\n5\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nu-net: convolutional networks for biomedical\n\nimage segmentation\n\nolaf ronneberger, philipp fischer, and thomas brox\n\ncomputer science department and bioss centre for biological signalling studies,\n\nuniversity of freiburg, germany\n\nronneber@informatik.uni-freiburg.de,\n\nwww home page: http://lmb.informatik.uni-freiburg.de/\n\nabstract. there is large consent that successful training of deep net-\nworks requires ", "barcodes: the persistent topology of data\n\nrobert ghrist\n\nabstract. this article surveys recent work of carlsson and collaborators on\napplications of computational algebraic topology to problems of feature de-\ntection and shape recognition in high-dimensional data. the primary math-\nematical tool considered is a homology theory for point-cloud data sets \u2014\npersistent homology \u2014 and a novel representation of this algebraic charac-\nterization \u2014 barcodes. we sketch an application of these techniques", "r e v i e w s\n\nprimary visual cortex and\nvisual awareness\n\nfrank tong\n\nthe primary visual cortex (v1) is probably the best characterized area of primate cortex, but\nwhether this region contributes directly to conscious visual experience is controversial. early\nneurophysiological and neuroimaging studies found that visual awareness was best correlated\nwith neural activity in extrastriate visual areas, but recent studies have found similarly powerful\neffects in v1. lesion and inactivation studies ", "j\no\nu\nr\nm\nd\n \n~\n4\nn\ne\nu\nr\no\n,\nw\ni\ne\nn\nc\ne\nm\ne\nt\nh\no\nd\n~\n,\n \n1\n1\n \n(\n1\n9\n8\n4\n)\n \n4\n7\n-\n6\n0\n \n4\n7\n \ne\nl\ns\ne\nv\ni\ne\nr\n \nn\ns\nm\n \n0\n0\n3\n8\n4\n \nd\ne\nv\ne\nl\no\np\nm\ne\nn\nt\ns\n \no\nf\n \na\n \nw\na\nt\ne\nr\n-\nm\na\nz\ne\n \np\nr\no\nc\ne\nd\nu\nr\ne\n \nf\no\nr\n \ns\nt\nu\nd\ny\ni\nn\ng\n \ns\np\na\nt\ni\na\nl\n \nl\ne\na\nr\nn\ni\nn\ng\n \ni\nn\n \nt\nh\ne\n \nr\na\nt\n \nr\ni\nc\nh\na\nr\nd\n \nm\no\nr\nr\ni\ns\n \nm\nr\nc\n \nc\no\ng\nn\ni\nt\ni\nv\ne\n \nn\ne\nu\nr\no\ns\nc\ni\ne\nn\nc\ne\n \ng\nr\no\nu\np\n,\n \np\n.\nw\nc\nh\no\nl\no\ng\ni\nc\na\nl\n \nl\na\nb\no\nr\na\nt\no\nr\ny\n,\n \nu\nn\ni\nt\n~\ne\nr\ns\ni\no\n'\n \no\n \nf\n \ns\nt\n.\n \na\nn\nd\nr\ne\nw\ns\n", "movinets: mobile video networks for ef\ufb01cient video recognition\n\ndan kondratyuk*, liangzhe yuan, yandong li, li zhang, mingxing tan, matthew brown, boqing gong\n\n{dankondratyuk,lzyuan,yandongli,zhl,tanmingxing,mtbr,bgong}@google.com\n\ngoogle research\n\n1\n2\n0\n2\n\n \nr\np\na\n8\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n1\n1\n5\n1\n1\n\n.\n\n3\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe present mobile video networks (movinets), a fam-\nily of computation and memory ef\ufb01cient video networks\nthat can operate on streaming video for online ", "7\n1\n0\n2\n\n \n\ny\na\nm\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n7\n4\n8\n8\n0\n\n.\n\n4\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nparseval networks: improving robustness to adversarial examples\n\nmoustapha cisse 1 piotr bojanowski 1 edouard grave 1 yann dauphin 1 nicolas usunier 1\n\nabstract\n\nwe introduce parseval networks, a form of deep\nneural networks in which the lipschitz constant\nof linear, convolutional and aggregation layers\nis constrained to be smaller than 1. parseval\nnetworks are empirically and theoretically mo-\ntivated by ", "mice alternate between discrete strategies during \nperceptual decision-making\n\u200a1,2\u2009\u2709, nicholas a. roy2, iris r. stone\u200a\n\n\u200a2, the international brain laboratory*, \n\n\u200a3, anne k. churchland\u200a\n\n\u200a4, alexandre pouget\u200a\n\n\u200a5 and jonathan w. pillow\u200a\n\n\u200a2,6\u2009\u2709\n\nzoe c. ashwood\u200a\nanne e. urai\u200a\n\nclassical models of perceptual decision-making assume that subjects use a single, consistent strategy to form decisions, or \nthat decision-making strategies evolve slowly over time. here we present new analyses suggesting ", "article\n\nhttps://doi.org/10.1038/s41467-022-34938-7\n\nsleep-like unsupervised replay reduces\ncatastrophic forgetting in arti\ufb01cial neural\nnetworks\n\nreceived: 24 may 2021\n\naccepted: 10 november 2022\n\ntimothy tadros\nmaxim bazhenov 1,2\n\n1,2, giri p. krishnan2, ramyaa ramyaa3 &\n\ncheck for updates\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\narti\ufb01cial neural networks are known to suffer from catastrophic forgetting:\nwhen learning multiple tasks sequentially, they perform well on the", "university of pennsylvania \nuniversity of pennsylvania \nscholarlycommons \nscholarlycommons \n\ngeneral robotics, automation, sensing and \nperception laboratory \n\nlab papers (grasp) \n\n1-2008 \n\nbarcodes: the persistent topology of data \nbarcodes: the persistent topology of data \n\nrobert ghrist \nuniversity of pennsylvania, ghrist@seas.upenn.edu \n\nfollow this and additional works at: https://repository.upenn.edu/grasp_papers \n\nrecommended citation \nrecommended citation \nrobert ghrist, \"barcodes: the p", "\f", "a r t i c l e s\n\nsignals in inferotemporal and perirhinal cortex suggest \nan untangling of visual target information\nmarino pagan, luke s urban, margot p wohl & nicole c rust\nfinding sought visual targets requires our brains to flexibly combine working memory information about what we are looking for \nwith visual information about what we are looking at. to investigate the neural computations involved in finding visual targets, \nwe recorded neural responses in inferotemporal cortex (it) and peri", "28\n\nbatch and online learning algorithms for nonconvex\nneyman-pearson classi\ufb01cation\n\ngilles gasso, litis insa\naristidis pappaioannou, marina spivak, and l \u00b4eon bottou, nec labs\n\nwe describe and evaluate two algorithms for neyman-pearson (np) classi\ufb01cation problem which has been\nrecently shown to be of a particular importance for bipartite ranking problems. np classi\ufb01cation is a non-\nconvex problem involving a constraint on false negatives rate. we investigated batch algorithm based on\ndc program", "the tale of the neuroscientists and the computer: why\nmechanistic theory matters\njoshua w. brown*\n\nopinion article\npublished: 31 october 2014\ndoi: 10.3389/fnins.2014.00349\n\npsychological and brain sciences, indiana university, bloomington, in, usa\n*correspondence: jwmbrown@indiana.edu\n\nedited by:\nangela r. laird, florida international university, usa\nreviewed by:\nalexander j. shackman, university of maryland, usa\nkimberly louise ray, university of california, davis, usa\n\nkeywords: computational ", "foundation models for decision making:\nproblems, methods, and opportunities\n\nsherry yang\u22171,2 ofir nachum1 yilun du3\n\njason wei1 pieter abbeel2 dale schuurmans1,4\n\n1google research, brain team, 2uc berkeley, 3mit, 4university of alberta\n\nfoundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in\na wide range of vision and language tasks. when such models are deployed in real world environments,\nthey inevitably interface with other entities and agents. fo", "vol 458 | 19 march 2009 | doi:10.1038/nature07842\n\narticles\n\nactivation of camkii in single dendritic\nspines during long-term potentiation\n\nseok-jin r. lee1, yasmin escobedo-lozoya1, erzsebet m. szatmari1 & ryohei yasuda1\n\ncalcium/calmodulin-dependent kinase ii (camkii) plays a central part in long-term potentiation (ltp), which underlies some\nforms of learning and memory. here we monitored the spatiotemporal dynamics of camkii activation in individual dendritic\nspines during ltp using two-photo", "stable population coding for working memory\ncoexists with heterogeneous neural\ndynamics in prefrontal cortex\n\njohn d. murraya, alberto bernacchiab, nicholas a. royc, christos constantinidisd, ranulfo romoe,f,1,\nand xiao-jing wangg,h,1\n\nadepartment of psychiatry, yale university school of medicine, new haven, ct 06510; bdepartment of engineering, university of cambridge, cambridge\ncb2 1pz, united kingdom; cprinceton neuroscience institute, princeton university, princeton, nj 08544; ddepartment of", "journal  of  neuroscience  methods  263  (2016)  36\u201347\n\ncontents  lists  available  at  sciencedirect\n\njournal\n\n \n\nof\n\n \n\nneuroscience\n\n \n\nmethods\n\nj o  u r  n a l  h o m e  p  a g e :  w w w . e l s e v i e r . c o m / l o c a t e / j n e u m e t h\n\ncomputational   neuroscience\na   bayesian   nonparametric   approach   for   uncovering   rat   hippocampal\npopulation   codes   during   spatial   navigation\nscott   w.   linderman a,   matthew   j.   johnson a,b,   matthew   a.   wilson c,   zhe  ", "vol 457 | 8 january 2009 | doi:10.1038/nature07467\n\nletters\n\nneural processing of auditory feedback during vocal\npractice in a songbird\ngeorg b. keller1 & richard h. r. hahnloser1\n\nsongbirds are capable of vocal learning and communication1,2 and\nare ideally suited to the study of neural mechanisms of complex\nsensory and motor processing. vocal communication in a noisy bird\ncolony and vocal learning of a specific song template both require\nthe ability to monitor auditory feedback3,4 to distinguis", "9\n1\n0\n2\n\n \n\nn\na\nj\n \n\n2\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n8\n4\n7\n3\n0\n\n.\n\n7\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nrepresentation learning with\ncontrastive predictive coding\n\naaron van den oord\n\ndeepmind\n\navdnoord@google.com\n\noriol vinyals\n\ndeepmind\n\nvinyals@google.com\n\nyazhe li\ndeepmind\n\nyazhe@google.com\n\nabstract\n\nwhile supervised learning has enabled great progress in many applications, unsu-\npervised learning has not seen such widespread adoption, and remains an important\nand challenging endeavor for arti\ufb01cial intell", "y\nr\na\nt\nn\ne\nm\nm\no\nc\n\ne\ne\ns\n\ne\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\n\na triplet spike-timing\u2013dependent plasticity model\ngeneralizes the bienenstock\u2013cooper\u2013munro rule\nto higher-order spatiotemporal correlations\n\njulijana gjorgjievaa,1,2, claudia clopathb, juliette audetc, and jean-pascal p\ufb01sterd,e\n\nadepartment of applied mathematics and theoretical physics, university of cambridge, cambridge cb3 0wa, united kingdom; blaboratory of neurophysics\nand physiology, universit\u00e9 paris descartes, 75270 paris, france; clabo", "mobilenetv2: inverted residuals and linear bottlenecks\n\nmark sandler andrew howard menglong zhu andrey zhmoginov liang-chieh chen\n\n{sandler, howarda, menglong, azhmogin, lcchen}@google.com\n\ngoogle inc.\n\n9\n1\n0\n2\n\n \nr\na\n\n \n\nm\n1\n2\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n4\nv\n1\n8\n3\n4\n0\n\n.\n\n1\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\napplications.\n\nin this paper we describe a new mobile architecture,\nmobilenetv2, that improves the state of the art perfor-\nmance of mobile models on multiple tasks and bench-\nmarks as well as acros", "progressive neural networks\n\nandrei a. rusu*, neil c. rabinowitz*, guillaume desjardins*, hubert soyer,\n\njames kirkpatrick, koray kavukcuoglu, razvan pascanu, raia hadsell\n\n* these authors contributed equally to this work\n\n2\n2\n0\n2\n\n \nt\nc\no\n2\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n1\n7\n6\n4\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\n{andreirusu, ncr, gdesjardins, soyer, kirkpatrick, korayk, razp, raia}@google.com\n\ngoogle deepmind\n\nlondon, uk\n\nabstract\n\nlearning to solve complex sequences of tasks\u2014while both leveraging ", "corrected: publisher correction\n\na diverse range of factors affect the nature  \nof neural representations underlying  \nshort-term memory\n\na.\u00a0emin\u00a0orhan\u200a\n\n\u200a1* and wei\u00a0ji\u00a0ma2,3\n\nsequential and persistent activity models are two prominent models of short-term memory in neural circuits. in persistent \nactivity models, memories are represented in persistent or nearly persistent activity patterns across a population of neurons, \nwhereas in sequential models, memories are represented dynamically by a s", "the journal of neuroscience, december 6, 2006 \u2022 26(49):12717\u201312726 \u2022 12717\n\ncellular/molecular\n\nplasticity compartments in basal dendrites of neocortical\npyramidal neurons\n\nurit gordon,* alon polsky,* and jackie schiller\ndepartment of physiology, technion medical school, haifa 31096, israel\n\nsynaptic plasticity rules widely determine how cortical networks develop and store information. using confocal imaging and dual site\nfocal synaptic stimulation, we show that basal dendrites, which receive th", "neuron\n\narticle\n\nperceptual learning reduces interneuronal\ncorrelations in macaque visual cortex\n\nyong gu,1 sheng liu,1 christopher r. fetsch,1 yun yang,1 sam fok,1 adhira sunkara,1 gregory c. deangelis,2\nand dora e. angelaki1,*\n1department of anatomy and neurobiology, washington university school of medicine, st. louis, mo 63110, usa\n2department of brain and cognitive sciences, university of rochester, rochester, ny 14627, usa\n*correspondence: angelaki@pcg.wustl.edu\ndoi 10.1016/j.neuron.2011.06", "neuron, vol. 27, 205\u2013218, august, 2000, copyright \u00aa 2000 by cell press\n\nfunctions of the primate temporal\nlobe cortical visual areas in invariant\nvisual object and face recognition\n\nreview\n\nedmund t. rolls*\nuniversity of oxford\ndepartment of experimental psychology\nsouth parks road\noxford, ox1 3ud\nunited kingdom\n\nthere is now good evidence that neural systems in tem-\nporal cortical visual areas process information about\nfaces. because a large number of neurons are devoted\nto this class of stimul", "2\n2\n0\n2\n\n \n\nv\no\nn\n9\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n7\n1\n5\n9\n0\n\n.\n\n3\n0\n2\n2\n:\nv\ni\nx\nr\na\n\ntensorf: tensorial radiance fields\n\nanpei chen1\u22c6 zexiang xu2\u22c6 andreas geiger3\n\njingyi yu1 hao su4\n\n1shanghaitech university\n\n2adobe research\n\n3university of t\u00a8ubingen and mpi-is, t\u00a8ubingen\n\n4uc san diego\n\nhttps://apchenstu.github.io/tensorf/\n\nabstract. we present tensorf, a novel approach to model and recon-\nstruct radiance fields. unlike nerf that purely uses mlps, we model the\nradiance field of a scene as ", "siam review\nvol. 60, no. 2, pp. 223\u2013311\n\nc(cid:2) 2018 society for industrial and applied mathematics\n\noptimization methods for\n\u2217\nlarge-scale machine learning\n\n\u2020\nl\u00b4eon bottou\n\u2021\nfrank e. curtis\n\u00a7\njorge nocedal\n\nabstract. this paper provides a review and commentary on the past, present, and future of numeri-\ncal optimization algorithms in the context of machine learning applications. through case\nstudies on text classi\ufb01cation and the training of deep neural networks, we discuss how op-\ntimization ", "review\npublished: 22 august 2017\ndoi: 10.3389/fnana.2017.00071\n\na laminar organization for selective\ncortico-cortical communication\n\nrinaldo d. d\u2019souza * and andreas burkhalter\n\ndepartment of neuroscience, washington university school of medicine, st. louis, mo, united states\n\nthe neocortex is central to mammalian cognitive ability, playing critical roles in sensory\nperception, motor skills and executive function. this thin, layered structure comprises\ndistinct, functionally specialized areas th", "research | reports\n\nnew zealand; inra and agence nationale de la recherche project\nsheepsnpqtl, france; european union through the seventh\nframework programme quantomics (kbbe222664) and 3sr\n(kbbe245140) projects; the ole r\u00f8mer grant from danish natural\nscience research council, bgi-shenzhen, china; the earmarked\nfund for modern china wool & cashmere technology research\nsystem (no.nycytx-40-3); and the australian department of\nagriculture food and fisheries, filling the research gap project,\n\u201cho", "article\n\nreceived 7 oct 2014 | accepted 5 nov 2014 | published 18 dec 2014\n\ndoi: 10.1038/ncomms6768\n\nopen\n\ncompetition between items in working\nmemory leads to forgetting\njarrod a. lewis-peacock1 & kenneth a. norman2\n\nswitching attention from one thought to the next propels our mental lives forward. however,\nit is unclear how this thought-juggling affects our ability to remember these thoughts.\nhere we show that competition between the neural representations of pictures in working\nmemory can imp", "6\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n2\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n4\nv\n3\n8\n9\n1\n0\n\n.\n\n8\n0\n5\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2016\n\ndigging deep into the layers of cnns:\nin\nsearch of how cnns achieve view invariance\n\namrbakry@cs.rutgers.edu\n\namr bakry\u2217\ntarek el-gaaly\u2217\n\ntgaaly@cs.rutgers.edu\n\nmohamed elhoseiny\u2217\n\nm.elhoseiny@cs.rutgers.edu\n\nahmed elgammal\n\nelgammal@cs.rutgers.edu\n\n* indicates co-\ufb01rst authors\n\ncomputer science department, rutgers university\n\nabstract\n\nthis paper is focused on st", "image-to-image translation with conditional adversarial networks\n\nphillip isola\n\njun-yan zhu\n\ntinghui zhou\n\nalexei a. efros\n\nberkeley ai research (bair) laboratory, uc berkeley\n{isola,junyanz,tinghuiz,efros}@eecs.berkeley.edu\n\n8\n1\n0\n2\n\n \n\nv\no\nn\n6\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n3\nv\n4\n0\n0\n7\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nfigure 1: many problems in image processing, graphics, and vision involve translating an input image into a corresponding output image.\nthese problems are often treated with applicatio", "metastable attractors explain the variable timing of\nstable behavioral action sequences\n\narticle\n\nhighlights\nd behavioral sequences in freely moving rats revealed large\n\nvariability in action timing\n\nd actions were preceded by onset of speci\ufb01c neural patterns in\n\nsecondary motor cortex\n\nd metastable attractors in a network model can explain the\n\norigin of timing variability\n\nd transitions between attractors are driven by low-dimensional\n\ncorrelated variability\n\nauthors\n\nstefano recanatesi,\nulise", "m\no\nc\n\n.\n\ne\nr\nu\nt\na\nn\n\n.\ni\n\nc\ns\no\nr\nu\ne\nn\n\n/\n/\n:\np\nt\nt\nh\n \n\u2022\n \n.\n\nc\nn\n\ni\n \n\n \n\na\nc\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n0\n0\n0\n2\n\u00a9\n\n \n\n \n\n\u00a9 2000 nature america inc. \u2022 http://neurosci.nature.com\n\narticles\n\ndirect cortical control of muscle\nactivation in voluntary arm\nmovements: a model\n\nemanuel todorov\n\ngatsby computational neuroscience unit, university college london, 17 queen square london wc1n 3 ar, uk\n\ncorrespondence should be directed to e.t. (emo@gatsby.ucl.ac.uk)\n\nwhat neural activity in motor cortex repr", "7546 \u2022 the journal of neuroscience, july 13, 2016 \u2022 36(28):7546 \u20137556\n\nsystems/circuits\n\nstimulus dependence of correlated variability across\ncortical areas\n\nx douglas a. ruff and x marlene r. cohen\ndepartment of neuroscience and center for the neural basis of cognition, university of pittsburgh, pittsburgh, pennsylvania 15213\n\nthe way that correlated trial-to-trial variability between pairs of neurons in the same brain area (termed spike count or noise correlation,\nrsc) depends on stimulus or t", "ne39ch07-freedman\n\nari\n\n28 may 2016\n\n9:18\n\nneuronal mechanisms of visual\ncategorization: an abstract\nview on decision making\n\ndavid j. freedman1,2 and john a. assad3,4\n1department of neurobiology, university of chicago, chicago, illinois 60637;\nemail: dfreedman@uchicago.edu\n2the grossman institute for neuroscience, quantitative biology, and human behavior,\nuniversity of chicago, chicago, illinois 60637\n3department of neurobiology, harvard medical school, boston, massachusetts 02115;\nemail: jassa", "https://doi.org/10.1038/s41593-019-0480-6\n\nthe next generation of approaches to investigate \nthe link between synaptic plasticity and learning\n\nyann humeau\u200a\n\n\u200a1,2 and daniel choquet\u200a\n\n\u200a1,2,3\n\nactivity-dependent synaptic plasticity has since long been proposed to represent the subcellular substrate of learning and \nmemory, one of the most important behavioral processes through which we adapt to our environment. despite the undisputed \nimportance of synaptic plasticity for brain function, its exac", "beyond plasticity: the dynamic impact \nof electrical synapses on neural circuits\n\npepe\u00a0alcam\u00ed1,2,3 and alberto\u00a0e.\u00a0pereda3,4*\nabstract | electrical synapses are found in vertebrate and invertebrate nervous systems. \nthe\u00a0cellular basis of these synapses is the gap junction, a group of intercellular channels that \nmediate direct communication between adjacent neurons. similar to chemical synapses, \nelectrical connections are modifiable and their variations in strength provide a mechanism for \nrecon", "a simple normative network approximates\nlocal non-hebbian learning in the cortex\n\nsiavash golkar 1\n\ndavid lipshutz 1\n\nyanis bahroun 1\n\nanirvan m. sengupta 1,2\n\ndmitri b. chklovskii 1,3\n\n1 center for computational neuroscience, flatiron institute\n2 department of physics and astronomy, rutgers university\n\n3 neuroscience institute, nyu medical center\n\n{sgolkar,dlipshutz,ybahroun,mitya}@flatironinstitute.org\n\nanirvans.physics@gmail.com\n\nabstract\n\nto guide behavior, the brain extracts relevant featur", "review\n\ncommunicated by steven nowlan\n\na unifying review of linear gaussian models\n\n\u2217\nsam roweis\ncomputation and neural systems, california institute of technology, pasadena, ca\n91125, u.s.a.\n\n\u2217\nzoubin ghahramani\ndepartment of computer science, university of toronto, toronto, canada\n\nfactor analysis, principal component analysis, mixtures of gaussian clus-\nters, vector quantization, kalman \ufb01lter models, and hidden markov mod-\nels can all be uni\ufb01ed as variations of unsupervised learning under a s", "m\na\nc\nh\ni\nn\ne\n \nl\ne\na\nr\nn\ni\nn\ng\n,\n \n8\n,\n \n2\n2\n9\n-\n2\n5\n6\n \n(\n1\n9\n9\n2\n)\n \n\u00a9\n \n1\n9\n9\n2\n \nk\nl\nu\nw\ne\nr\n \na\nc\na\nd\ne\nm\ni\nc\n \np\nu\nb\nl\ni\ns\nh\ne\nr\ns\n,\n \nb\no\ns\nt\no\nn\n.\n \nm\na\nn\nu\nf\na\nc\nt\nu\nr\ne\nd\n \ni\nn\n \nt\nh\ne\n \nn\ne\nt\nh\ne\nr\nl\na\nn\nd\ns\n.\n \ns\ni\nm\np\nl\ne\n \ns\nt\na\nt\ni\ns\nt\ni\nc\na\nl\n \ng\nr\na\nd\ni\ne\nn\nt\n-\nf\no\nl\nl\no\nw\ni\nn\ng\n \na\nl\ng\no\nr\ni\nt\nh\nm\ns\n \nf\no\nr\n \nc\no\nn\nn\ne\nc\nt\ni\no\nn\ni\ns\nt\n \nr\ne\ni\nn\nf\no\nr\nc\ne\nm\ne\nn\nt\n \nl\ne\na\nr\nn\ni\nn\ng\n \nr\no\nn\na\nl\nd\n \nj\n.\n \nw\ni\nl\nl\ni\na\nm\ns\n \nr\nj\nw\n@\nc\no\nr\nw\ni\nn\n.\nc\nc\ns\n.\nn\no\nr\nt\nh\ne\na\ns\nt\ne\nr\nn\n.\ne\n", "report\n\na role for the locus coeruleus in hippocampal ca1\nplace cell reorganization during spatial reward\nlearning\n\nhighlights\nd lc-ca1 projections exhibit increased activity near a new\n\nreward location\n\nauthors\n\nalexandra mansell kaufman,\ntristan geiller, attila losonczy\n\nd activation of lc-ca1 near the reward induces place cell\n\noverrepresentation\n\ncorrespondence\nal2856@columbia.edu\n\nd inhibition of lc-ca1 axons suppresses place cell\n\noverrepresentation\n\nin brief\nkaufman et al. imaged and\nopto", "adaptive optimal training of animal behavior\n\nji hyun bak1,4 jung yoon choi2,3 athena akrami3,5 ilana witten2,3 jonathan w. pillow2,3\n\n1department of physics, 2department of psychology, princeton university\n\n3princeton neuroscience institute, princeton university\n\n4school of computational sciences, korea institute for advanced study\n\n5howard hughes medical institute\n\njhbak@kias.re.kr, {jungchoi,aakrami,iwitten,pillow}@princeton.edu\n\nabstract\n\nneuroscience experiments often require training anima", "towards evaluating the robustness\n\nof neural networks\n\nnicholas carlini\n\ndavid wagner\nuniversity of california, berkeley\n\nabstract\n\noriginal adversarial\n\noriginal adversarial\n\n7\n1\n0\n2\n\n \nr\na\n\n \n\nm\n2\n2\n\n \n \n]\n\nr\nc\n.\ns\nc\n[\n \n \n\n2\nv\n4\n4\n6\n4\n0\n\n.\n\n8\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nneural networks provide state-of-the-art results for most\nmachine learning tasks. unfortunately, neural networks are\nvulnerable to adversarial examples: given an input x and any\ntarget classi\ufb01cation t, it is possible to \ufb01nd a new input", "research article\nhuman inferences about sequences: a\nminimal transition probability model\n\nflorent meyniel1*, maxime maheu1,2, stanislas dehaene1,3\n\n1 cognitive neuroimaging unit, cea drf/i2bm, inserm, universite\u00b4 paris-sud, universite\u00b4 paris-saclay,\nneurospin center, gif-sur-yvette, france, 2 universite\u00b4 paris descartes, sorbonne paris cite\u00b4, paris, france,\n3 coll\u00e8ge de france, paris, france\n\na11111\n\n* florent.meyniel@cea.fr\n\nabstract\n\nthe brain constantly infers the causes of the inputs it rec", "exploiting compositionality to explore a large space of model structures\n\nroger b. grosse\n\nruslan salakhutdinov\n\ncomp. sci. & ai lab\n\nmit\n\ncambridge, ma 02139\n\ndept. of statistics\n\nuniversity of toronto\n\ntoronto, ontario, canada\n\nwilliam t. freeman\ncomp. sci. & ai lab\n\njoshua b. tenenbaum\n\nbrain and cognitive sciences\n\nmit\n\nmit\n\ncambridge, ma 02139\n\ncambridge, ma 02193\n\nabstract\n\nthe recent proliferation of richly structured prob-\nabilistic models raises the question of how to au-\ntomatically de", "neuron\n\nperspective\n\ndirect fit to nature: an evolutionary\nperspective on biological and\narti\ufb01cial neural networks\n\nuri hasson,1,2,* samuel a. nastase,1 and ariel goldstein1\n1princeton neuroscience institute, princeton university, princeton, nj, usa\n2department of psychology, princeton university, princeton, nj, usa\n*correspondence: hasson@princeton.edu\nhttps://doi.org/10.1016/j.neuron.2019.12.002\n\nevolution is a blind \ufb01tting process by which organisms become adapted to their environment. does t", "letter\n\ncommunicated by erkki oja\n\ncanonical correlation analysis: an overview with\napplication to learning methods\n\ndavid r. hardoon\ndrh@ecs.soton.ac.uk\nsandor szedmak\nss03v@ecs.soton.ac.uk\njohn shawe-taylor\njst@ecs.soton.ac.uk\nschool of electronics and computer science, image, speech and intelligent systems\nresearch group, university of southampton, southampton s017 1bj, u.k.\n\nwe present a general method using kernel canonical correlation analy-\nsis to learn a semantic representation to web im", "7\n1\n0\n2\n\n \nt\nc\no\n \n4\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n6\n4\n1\n1\n1\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nsuperspike: supervised learning in multi-layer spik-\ning neural networks\n\nfriedemann zenke1, 2 & surya ganguli1\n1department of applied physics\nstanford university\nstanford, ca 94305\nunited states of america\n\n2centre for neural circuits and behaviour\nuniversity of oxford\noxford\nunited kingdom\n\nkeywords: spiking neural networks, multi-layer networks, supervised learning, tempo-\nral coding, synaptic plasti", "mapreduce for parallel reinforcement learning\n\nyuxi li1 and dale schuurmans2\n\n1 college of computer science and engineering\n\nuniv. of electronic science and technology of china\n\nchengdu, china\n\n2 department of computing science\n\nuniversity of alberta\n\nedmonton, alberta, canada\n\nabstract. we investigate the parallelization of reinforcement learning\nalgorithms using mapreduce, a popular parallel computing framework.\nwe present parallel versions of several dynamic programming algorithms,\nincluding ", "bayesian learning and inference in\n\nrecurrent switching linear dynamical systems\n\nscott w. linderman\u2217\ncolumbia university\n\nmatthew j. johnson\u2217\nharvard and google brain\n\nandrew c. miller\nharvard university\n\nryan p. adams\n\nharvard and google brain\n\ndavid m. blei\n\ncolumbia university\n\nliam paninski\n\ncolumbia university\n\nabstract\n\nmany natural systems, such as neurons \ufb01ring\nin the brain or basketball teams traversing a\ncourt, give rise to time series data with com-\nplex, nonlinear dynamics. we can g", "research article\n\nneuroscience\npsychological and cognitive sciences\n\na hierarchy of linguistic predictions during natural language\ncomprehension\n\nmicha heilbrona,b,1\n\n, kristijan armenia, jan-mathijs scho\ufb00elena\n\n, peter hagoorta,b\n\n, and floris p. de langea\n\nedited by stanislas dehaene, commissariat a l\u2019 \u00b4energie atomique et aux \u00b4energies alternatives, gif-sur-yvette, france; received february 11, 2022;\naccepted june 28, 2022\n\nunderstanding spoken language requires transforming ambiguous acousti", "information selection in noisy environments with large action spaces\n\npedro tsividis (tsividis@mit.edu), samuel j. gershman (sjgershm@mit.edu),\n\njoshua b. tenenbaum (jbt@mit.edu), laura schulz (lshulz@mit.edu)\n\nmassachusetts institute of technology, department of brain and cognitive sciences,\n\n77 massachusetts ave., cambridge, ma 02139 usa\n\nabstract\n\na critical aspect of human cognition is the ability to effec-\ntively query the environment for information. the \u2018real\u2019\nworld is large and noisy, an", "neuron\n\nprimer\n\nimaging calcium in neurons\n\nchristine grienberger1 and arthur konnerth1,*\n1institute of neuroscience, technical university munich, biedersteinerstr. 29, 80802 munich, germany\n*correspondence: arthur.konnerth@lrz.tum.de\ndoi 10.1016/j.neuron.2012.02.011\n\ncalcium ions generate versatile intracellular signals that control key functions in all types of neurons. imaging\ncalcium in neurons is particularly important because calcium signals exert their highly speci\ufb01c functions in\nwell-de\ufb01", "on the spectral bias of neural networks\n\nnasim rahaman * 1 2 aristide baratin * 1 devansh arpit 1 felix draxler 2 min lin 1 fred a. hamprecht 2\n\nyoshua bengio 1 aaron courville 1\n\n9\n1\n0\n2\n\n \n\ny\na\nm\n1\n3\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n4\n3\n7\n8\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nneural networks are known to be a class of highly\nexpressive functions able to \ufb01t even random input-\noutput mappings with 100% accuracy.\nin this\nwork we present properties of neural networks\nthat complement this aspec", "inferring spike-timing-dependent plasticity from\n\nspike train data\n\nian h. stevenson and konrad p. kording\n\ndepartment of physical medicine and rehabilitation\n{i-stevenson, kk}@northwestern.edu\n\nnorthwestern university\n\nabstract\n\nsynaptic plasticity underlies learning and is thus central for development, mem-\nory, and recovery from injury. however, it is often dif\ufb01cult to detect changes in\nsynaptic strength in vivo, since intracellular recordings are experimentally chal-\nlenging. here we present", "neurally plausible reinforcement learning of\n\nworking memory tasks\n\njaldert o. rombouts, sander m. bohte\n\ncwi, life sciences\n\namsterdam, the netherlands\n\n{j.o.rombouts, s.m.bohte}@cwi.nl\n\npieter r. roelfsema\n\nnetherlands institute for neuroscience\n\namsterdam, the netherlands\n\np.r.roelfsema@nin.knaw.nl\n\nabstract\n\na key function of brains is undoubtedly the abstraction and maintenance of in-\nformation from the environment for later use. neurons in association cortex play\nan important role in this ", "1\n2\n0\n2\n\n \nt\nc\no\n5\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n5\nv\n2\n6\n2\n8\n0\n\n.\n\n0\n1\n0\n2\n:\nv\ni\nx\nr\na\n\nlocal plasticity rules can learn deep representations\n\nusing self-supervised contrastive predictions\n\nbernd illing\n\nguillaume bellec\u2217\n\njean ventura\n\nwulfram gerstner\u2217\n\n{firstname.lastname}@epfl.ch\n\ndepartment of computer science & department of life sciences\n\n\u00e9cole polytechnique f\u00e9d\u00e9rale de lausanne\n\n1015 switzerland\n\nabstract\n\nlearning in the brain is poorly understood and learning rules that respect biologic", "european journal of neuroscience, vol. 18, pp. 2011\u00b12024, 2003\n\n\u00df federation of european neuroscience societies\n\nretrospective and prospective persistent activity induced\nby hebbian learning in a recurrent cortical network\n\ngianluigi mongillo,1 daniel j. amit2,3 and nicolas brunel4\n1dipartimento di fisiologia umana, universita\u00e1 di roma la sapienza, rome, italy\n2infm, dipartimento di fisica, universita\u00e1 di roma la sapienza, rome, italy\n3racah institute of physics, hebrew university, jerusalem, is", "research article\n\nopposite initialization to novel cues in\ndopamine signaling in ventral and\nposterior striatum in mice\nwilliam menegas, benedicte m babayan, naoshige uchida,\nmitsuko watabe-uchida*\n\ndepartment of molecular and cellular biology, center for brain science, harvard\nuniversity, cambridge, united states\n\nabstract dopamine neurons are thought to encode novelty in addition to reward prediction\nerror (the discrepancy between actual and predicted values). in this study, we compared dopami", "7\n1\n0\n2\n\n \n\nn\na\nj\n \n\n0\n3\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n9\nv\n0\n8\n9\n6\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\npublishedasaconferencepaperaticlr2015adam:amethodforstochasticoptimizationdiederikp.kingma*universityofamsterdam,openaidpkingma@openai.comjimmyleiba\u2217universityoftorontojimmy@psi.utoronto.caabstractweintroduceadam,analgorithmfor\ufb01rst-ordergradient-basedoptimizationofstochasticobjectivefunctions,basedonadaptiveestimatesoflower-ordermo-ments.themethodisstraightforwardtoimplement,iscomputationallyef\ufb01cient,haslittle", "clustering appearances of objects under varying illumination conditions\n\n\u2020\n\njeffrey ho\n\njongwoo lim\njho@cs.ucsd.edu myang@honda-ri.com jlim1@uiuc.edu\n\nming-hsuan yang(cid:1)\n\n\u2020\n\ncomputer science & engineering\nuniversity of california at san diego\n\n(cid:1) honda research institute\n\n\u2021\n\nkuang-chih lee\nklee10@uiuc.edu\n\u2021\n\n\u2021\n\ndavid kriegman\n\n\u2020\n\nkriegman@cs.ucsd.edu\n\ncomputer science\n\n800 california street\n\nuniversity of illinois at urbana-champaign\n\nla jolla, ca 92093\n\nmountain view, ca 94041\n\nurbana,", "invariant scattering convolution networks\n\njoan bruna and st\u00b4ephane mallat\n\ncmap, ecole polytechnique, palaiseau, france\n\n1\n\n2\n1\n0\n2\n\n \nr\na\n\nm\n8\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n3\n1\n5\n1\n\n.\n\n3\n0\n2\n1\n:\nv\ni\nx\nr\na\n\nabstract\u2014a wavelet scattering network computes a translation invari-\nant image representation, which is stable to deformations and preserves\nhigh frequency information for classi\ufb01cation. it cascades wavelet trans-\nform convolutions with non-linear modulus and averaging operators. the\n\ufb01rst ", "consistent cross-modal identification of cortical \nneurons with coupled autoencoders\n\nrohan gala\u200a\nanton arkhipov, gabe murphy, bosiljka tasic, hongkui zeng, michael hawrylycz and uygar s\u00fcmb\u00fcl\u200a\n\n\u200a, fahimeh baftizadeh, jeremy miller\u200a\n\n\u200a, nathan gouwens\u200a\n\n\u200a, \n\n\u200a\u2009\u2709, agata budzillo\u200a\n\n\u200a\u2009\u2709\n\nconsistent identification of neurons in different experimental \nmodalities is a key problem in neuroscience. although meth-\nods  to  perform  multimodal  measurements  in  the  same  set \nof  single  neurons  have  ", "5\n1\n0\n2\n\n \n\ng\nu\na\n9\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n8\n5\n4\n0\n\n.\n\n8\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nlearning to predict independent of span\n\nhado van hasselt \u2217\n\nrichard s. sutton \u2020\n\noctober 18, 2018\n\nabstract\n\nwe consider how to learn multi-step predictions e\ufb03ciently. conventional algo-\nrithms wait until observing actual outcomes before performing the computations to\nupdate their predictions.\nif predictions are made at a high rate or span over a\nlarge amount of time, substantial computation can be required", "j neurophysiol 97: 4235\u2013 4257, 2007.\nfirst published march 21, 2007; doi:10.1152/jn.00095.2007.\n\ntemporal complexity and heterogeneity of single-neuron activity in\npremotor and motor cortex\n\nmark m. churchland and krishna v. shenoy\nneurosciences program and department of electrical engineering, stanford university, stanford, california\n\nsubmitted 29 january 2007; accepted in \ufb01nal form 15 march 2007\n\nchurchland mm, shenoy kv. temporal complexity and heteroge-\nneity of single-neuron activity in pr", "194 \n\nieee  transactions on image  processing, vol. 4,  no.  2, february  1995 \n\nlikelihood calculation for a  class of \nmultiscale stochastic models, with \napplication to texture discrimination \n\nmark  r.  luettgen,  member, ieee, and alan  s. willsky,  fellow, ieee \n\nabstruct-  a  class  of  multiscale  stochastic  models  based  on \nscale-recursive dynamics on trees has recently been  introduced. \ntheoretical and experimental results have shown that these mod- \nels provide  an extremely rich ", "ieee transactions on pattern analysis and machine intelligence, vol. 43, no. 11, november 2021\n\n4037\n\nself-supervised visual feature learning with\n\ndeep neural networks: a survey\n\nlonglong jing , student member, ieee and yingli tian , fellow, ieee\n\nabstract\u2014large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual\nfeature learning from images or videos for computer vision applications. to avoid extensive cost of collecting and a", "neuron\n\nreport\n\nlearning by the dendritic\nprediction of somatic spiking\n\nrobert urbanczik1 and walter senn1,*\n1department of physiology, and center for learning, cognition and memory, university of bern, bu\u00a8 hlplatz 5, ch-3012 bern, switzerland\n*correspondence: senn@pyl.unibe.ch\nhttp://dx.doi.org/10.1016/j.neuron.2013.11.030\n\nsummary\n\nrecent modeling of spike-timing-dependent plas-\nticity indicates that plasticity involves as a third\nfactor a local dendritic potential, besides pre- and\npostsynap", "synaptic plasticity rules with physiological\ncalcium levels\n\nyanis ingleberta,1\ue840, johnatan aljadeffb,c,d,1, nicolas brunelb,c,e,f,2\ue840, and dominique debannea,2\n\naunit\u00e9 de neurobiologie des canaux ionique et de la synapse, umr1072, inserm, aix-marseille universit\u00e9, 13015 marseille, france; bdepartment of\nneurobiology, university of chicago, chicago, il 60637; cdepartment of statistics, university of chicago, chicago, il 60637; dneurobiology section, division\nof biological sciences, university of c", "theoretical aspects of group\nequivariant neural networks\n\ncarlos esteves\n\ndepartment of computer and information science\n\nuniversity of pennsylvania\n\nphiladelphia, pa\n\nabstract\n\ngroup equivariant neural networks have been explored in the past few years\nand are interesting from theoretical and practical standpoints. they lever-\nage concepts from group representation theory, non-commutative harmonic\nanalysis and differential geometry that do not often appear in machine\nlearning. in practice, they ", "8\n1\n0\n2\n\n \n\nn\na\nj\n \n7\n1\n\n \n \n]\n\n.\n\no\nh\nh\nt\na\nm\n\n[\n \n \n\n1\nv\n4\n9\n8\n5\n0\n\n.\n\n1\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ndeep learning: an introduction for applied\n\nmathematicians\n\ncatherine f. higham\u2217\n\ndesmond j. higham\u2020\n\njanuary 19, 2018\n\nabstract\n\nmultilayered arti\ufb01cial neural networks are becoming a pervasive tool\nin a host of application \ufb01elds. at the heart of this deep learning revolution\nare familiar concepts from applied and computational mathematics; no-\ntably, in calculus, approximation theory, optimization and ", "deep linear neural networks:\n\na theory of learning in the brain and mind\n\na dissertation\n\nsubmitted to the department of electrical\n\nengineering\n\nand the committee on graduate studies\n\nof stanford university\n\nin partial fulfillment of the requirements\n\nfor the degree of\n\ndoctor of philosophy\n\nandrew michael saxe\n\njune 2015\n\n\f", "a direct adaptive method for faster backpropagation learning: \n\nthe rprop algorithm \n\nmartin  riedmiller \n\nheinrich braun \n\ninstitut fur logik, komplexitat  und  deduktionssyteme \n\nuniversity of  karlsruhe \n\nw-7500  karlsruhe \n\nfrg \n\nriedml@ira.uka.de \n\nabstract-  a  new learning algorithm for multi- \nlayer feedforward networks, rprop, is proposed. \nto  overcome  the  inherent  disadvantages of  pure \ngradient-descent, rprop  performs  a local  adap- \ntation of the weight-updates according  to t", "5\n1\n0\n2\n\n \nr\na\n\n \n\nm\n0\n2\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n2\n7\n5\n6\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2015\n\nexplaining and harnessing\nadversarial examples\n\nian j. goodfellow, jonathon shlens & christian szegedy\ngoogle inc., mountain view, ca\n{goodfellow,shlens,szegedy}@google.com\n\nabstract\n\nseveral machine learning models, including neural networks, consistently mis-\nclassify adversarial examples\u2014inputs formed by applying small but intentionally\nworst-case perturbati", "which neural net architectures give rise to\n\nexploding and vanishing gradients?\n\nboris hanin\n\ndepartment of mathematics\n\ntexas a& m university\ncollege station, tx, usa\nbhanin@math.tamu.edu\n\nabstract\n\nwe give a rigorous analysis of the statistical behavior of gradients in a randomly\ninitialized fully connected network n with relu activations. our results show\nthat the empirical variance of the squares of the entries in the input-output jacobian\nof n is exponential in a simple architecture-depende", "original research\npublished: 18 february 2021\ndoi: 10.3389/fnins.2021.633674\n\nscaling equilibrium propagation to\ndeep convnets by drastically\nreducing its gradient estimator bias\n\naxel laborieux 1*, maxence ernoult 1,2,3*, benjamin scellier 3\u2020, yoshua bengio 3,4,\njulie grollier 2 and damien querlioz 1\n\n1 universit\u00e9 paris-saclay, cnrs, centre de nanosciences et de nanotechnologies, palaiseau, france, 2 unit\u00e9 mixte de\nphysique, cnrs, thales, universit\u00e9 paris-saclay, palaiseau, france, 3 mila, univ", "feedforward and feedback connections between areas v1 and v2\nof the monkey have similar rapid conduction velocities\n\nrapid communication\n\np. girard, j. m. hupe\u00b4 , and j. bullier\ninstitut national de la sante\u00b4 et de la recherche me\u00b4dicale u371, 69500 bron, france\n\nreceived 7 august 2000; accepted in \ufb01nal form 2 november 2000\n\ngirard, p., j. m. hupe\u00b4, and j. bullier. feedforward and feedback\nconnections between areas v1 and v2 of the monkey have similar\nrapid conduction velocities. j neurophysiol ", "a hebbian/anti-hebbian network derived from \nonline non-negative matrix factorization can \n\ncluster and discover sparse features \n\ncengiz pehlevan1,2 and dmitri b. chklovskii2 \n\n1 janelia farm research campus \nhoward hughes medical institute \n\nashburn, va 20147 \n\npehlevanc@janelia.hhmi.org \n\n \n\n2 simons center for data analysis  \n\nsimons foundation \nnew york, ny 10010 \n\nmitya@simonsfoundation.org \n\nabstract:  despite  our  extensive  knowledge  of  biophysical \nproperties of neurons, there is no", "opinion\n\nstability of the \ufb01ttest: organizing\nlearning through retroaxonal signals\nkenneth d. harris1,2\n\n1 center for molecular and behavioral neuroscience, rutgers university, 197 university avenue, newark, nj 07102, usa\n2 smilow neuroscience program and department of otolaryngology, new york university school of medicine, 550 first avenue,\nnew york, ny 10016, usa\n\nclassically, neurons communicate by anterograde\nconduction of action potentials. however, information\ncan also pass backward along a", "computational neuroscience\ninternal representation of task rules by recurrent dynamics: \nthe importance of the diversity of neural responses\n\noriginal research article\npublished: 04 october 2010\ndoi: 10.3389/fncom.2010.00024\n\nmattia rigotti1,2, daniel ben dayan rubin1,2, xiao-jing wang 3 and stefano fusi1,2*\n\n1  center for theoretical neuroscience, college of physicians and surgeons, columbia university, new york, ny, usa\n2  institute of neuroinformatics, university of zurich and swiss federal i", "research article\n\ndemixed principal component analysis of\nneural population data\ndmitry kobak1\u2020, wieland brendel1,2,3\u2020, christos constantinidis4,\nclaudia e feierstein1, adam kepecs5, zachary f mainen1, xue-lian qi4,\nranulfo romo6,7, naoshige uchida8, christian k machens1*\n\n1champalimaud neuroscience program, champalimaud centre for the unknown,\nlisbon, portugal; 2e\u00b4 cole normale supe\u00b4 rieure, paris, france; 3centre for integrative\nneuroscience, university of tu\u00a8 bingen, tu\u00a8 bingen, germany; 4wak", "rapid adaptation with conditionally shifted neurons\n\ntsendsuren munkhdalai 1 xingdi yuan 1 soroush mehri 1 adam trischler 1\n\n8\n1\n0\n2\n\n \nl\nu\nj\n \n\n3\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n6\n2\n9\n9\n0\n\n.\n\n2\n1\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe describe a mechanism by which arti\ufb01cial neu-\nral networks can learn rapid adaptation \u2013 the abil-\nity to adapt on the \ufb02y, with little data, to new\ntasks \u2013 that we call conditionally shifted neurons.\nwe apply this mechanism in the framework of\nmetalearning, where the aim is to ", "5\n1\n0\n2\n\n \n\nb\ne\nf\n6\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n2\n5\n8\n1\n0\n\n.\n\n2\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nsurpassing human-level performance on imagenet classi\ufb01cation\n\ndelving deep into recti\ufb01ers:\n\nkaiming he\n\nxiangyu zhang\n\nshaoqing ren\n\njian sun\n\nmicrosoft research\n\n{kahe, v-xiangz, v-shren, jiansun}@microsoft.com\n\nabstract\n\nrecti\ufb01ed activation units (recti\ufb01ers) are essential for\nstate-of-the-art neural networks.\nin this work, we study\nrecti\ufb01er neural networks for image classi\ufb01cation from two\naspects. first, we ", "int j comput vis (2015) 113:54\u201366\ndoi 10.1007/s11263-014-0788-3\n\nspiking deep convolutional neural networks for energy-ef\ufb01cient\nobject recognition\nyongqiang cao \u00b7 yang chen \u00b7 deepak khosla\n\nreceived: 9 february 2014 / accepted: 10 november 2014 / published online: 23 november 2014\n\u00a9 springer science+business media new york 2014\n\nabstract deep-learning neural networks such as convo-\nlutional neural network (cnn) have shown great potential\nas a solution for dif\ufb01cult vision problems, such as object", "evidence that recurrent circuits are critical to \nthe ventral stream\u2019s execution of core object \nrecognition behavior\n\nkohitij kar\u200a\n\n\u200a1,2*, jonas kubilius1,3, kailyn schmidt1, elias b. issa1,4 and james j. dicarlo\u200a\n\n\u200a1,2\n\nnon-recurrent deep convolutional neural networks (cnns) are currently the best at modeling core object recognition, a behavior \nthat is supported by the densely recurrent primate ventral stream, culminating in the inferior temporal (it) cortex. if recurrence \nis critical to thi", "review\n\ndialogues on prediction errors\nyael niv1 and geoffrey schoenbaum2\n\n1 center for the study of brain, mind and behavior and department of psychology, green hall, princeton university, princeton,\nnj 08544, usa\n2 departments of anatomy and neurobiology, and psychiatry, university of maryland school of medicine, 20 penn street,\nbaltimore, md 21201, usa\n\nideas\n\ncomputational\n\nrecognition that\n\nthe\nfrom\nreinforcement learning are relevant to the study of\nneural circuits has taken the cognitive ", "special  issue:  time  in  the  brain\nopinion\nspace\nand\nsequence\n\n \n\n \n\n \n\ntime:\nthe\ngenerator\n\n \n\nhippocampus\n\n \n\nas\n\n \n\n \n\na\n\ngy\u00f6rgy  buzs\u00e1ki1,2,3,* and  david  tingley1\n\nneural  computations  are  often  compared  to  instrument-measured  distance  or\nduration,  and  such  relationships  are  interpreted  by  a  human  observer.  how-\never,  neural  circuits  do  not  depend  on  human-made  instruments  but  perform\ncomputations  relative  to  an  internally  de\ufb01ned  rate-of-change.  while  ", "letter\nhuman-level control through deep reinforcement\nlearning\n\ndoi:10.1038/nature14236\n\nvolodymyr mnih1*, koray kavukcuoglu1*, david silver1*, andrei a. rusu1, joel veness1, marc g. bellemare1, alex graves1,\nmartin riedmiller1, andreas k. fidjeland1, georg ostrovski1, stig petersen1, charles beattie1, amir sadik1, ioannis antonoglou1,\nhelen king1, dharshan kumaran1, daan wierstra1, shane legg1 & demis hassabis1\n\nthe theory of reinforcement learning provides a normative account1,\ndeeply rooted i", "m\no\nc\n\n.\n\ne\nr\nu\nt\na\nn\n\n.\ni\n\nc\ns\no\nr\nu\ne\nn\n\n/\n/\n:\np\nt\nt\nh\n\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n1\n0\n0\n2\n\u00a9\n\n \n\n\u00a9 2001 nature publishing group  http://neurosci.nature.com\n\narticles\n\ndendritic spine geometry is critical\nfor ampa receptor expression in\nhippocampal ca1 pyramidal neurons\n\nmasanori matsuzaki1,2,3, graham c. r. ellis-davies4, tomomi nemoto1,2,3, yasushi miyashita3,\nmasamitsu iino2,3 and haruo kasai1,2\n\n1 department of cell physiology, national institute for physiologic", "a computational model of limb impedance control\nbased on principles of internal model uncertainty\n\ndjordje mitrovic1*, stefan klanke1, rieko osu2,3, mitsuo kawato2, sethu vijayakumar1\n\n1 ipab, school of informatics, university of edinburgh, edinburgh, united kingdom, 2 atr computational neuroscience laboratories, keihanna science city, kyoto, japan,\n3 national institute of information and communications technology, kyoto, japan\n\nabstract\n\nefficient human motor control is characterized by an exte", "ieee transactions on information theory, vol. 67, no. 5, may 2021\n\n2581\n\ndeep neural network approximation theory\n\ndennis elbr\u00e4chter, dmytro perekrestenko , philipp grohs, and helmut b\u00f6lcskei\n\n, fellow, ieee\n\n(invited paper)\n\nabstract\u2014 this paper develops fundamental\n\nlimits of deep\nneural network learning by characterizing what is possible if\nno constraints are imposed on the learning algorithm and on the\namount of training data. concretely, we consider kolmogorov-\noptimal approximation through", "the journal of neuroscience, november 24, 2010 \u2022 30(47):15747\u201315759 \u2022 15747\n\nbehavioral/systems/cognitive\n\ncaudate encodes multiple computations for perceptual\ndecisions\n\nlong ding and joshua i. gold\ndepartment of neuroscience, university of pennsylvania, philadelphia, pennsylvania 19104-6074\n\nperceptual decision making is a complex process that requires multiple computations, including the accumulation of sensory evidence\nand an ongoing evaluation of the accumulation process to use for predicti", "neuron, vol. 39, 991\u20131004, september 11, 2003, copyright \uf8e92003 by cell press\n\nintensity versus identity coding\nin an olfactory system\n\nmark stopfer,1,2 vivek jayaraman,1\nand gilles laurent*\ncalifornia institute of technology\ndivision of biology\ncomputation and neural systems program\npasadena, california 91125\n\nsummary\n\nwe examined the encoding and decoding of odor iden-\ntity and intensity by neurons in the antennal lobe and\nthe mushroom body, first and second relays, respec-\ntively, of the locus", "downloaded from \n\nhttp://cshperspectives.cshlp.org/\n\n on october 2, 2023 - published by cold spring harbor laboratory press \n\nsynapses and memory storage\n\nmark mayford1, steven a. siegelbaum2, and eric r. kandel2\n\n1the scripps research institute, department of cell biology, la jolla, california 92037\n2columbia university, kavli institute for brain science, howard hughes medical institute,\ndepartment of neuroscience, new york, new york 10032\n\ncorrespondence: erk5@columbia.edu\n\nthe synapse is the ", "review\npublished: 31 july 2018\ndoi: 10.3389/fncir.2018.00053\n\neligibility traces and plasticity on\nbehavioral time scales:\nexperimental support of neohebbian\nthree-factor learning rules\n\nwulfram gerstner*, marco lehmann, vasiliki liakoni, dane corneil and johanni brea\n\nschool of computer science and school of life sciences, \u00e9cole polytechnique f\u00e9d\u00e9rale de lausanne, lausanne, switzerland\n\nmost elementary behaviors such as moving the arm to grasp an object or walking into\nthe next room to explore ", "the journal of neuroscience, october 15, 1998, 18(20):8455\u20138466\n\nspatial firing properties of hippocampal ca1 populations in an\nenvironment containing two visually identical regions\n\nwilliam e. skaggs and bruce l. mcnaughton\narizona research laboratories, division of neural systems, memory and aging, university of arizona, tucson,\narizona 85724\n\npopulations of 10-39 ca1 pyramidal cells were recorded from\nfour rats foraging for food reward in an environment consisting\nof two nearly identical boxe", "4\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n5\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n5\nv\n0\n5\n8\n0\n\n.\n\n8\n0\n3\n1\n:\nv\ni\nx\nr\na\n\ngenerating sequences with\nrecurrent neural networks\n\nalex graves\n\ndepartment of computer science\n\nuniversity of toronto\n\ngraves@cs.toronto.edu\n\nabstract\n\nthis paper shows how long short-term memory recurrent neural net-\nworks can be used to generate complex sequences with long-range struc-\nture, simply by predicting one data point at a time. the approach is\ndemonstrated for text (where the data are discrete) and ", "0\n2\n0\n2\n\n \n\nb\ne\nf\n7\n1\n\n \n\n \n \n]\nt\ns\nh\nt\na\nm\n\n.\n\n[\n \n \n\n3\nv\n1\n9\n1\n2\n1\n\n.\n\n4\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nlinearized two-layers neural networks in high dimension\n\nbehrooz ghorbani\u2217, song mei\u2020, theodor misiakiewicz\u2021, andrea montanari\u00a7\n\nfebruary 18, 2020\n\nabstract\n\nwe consider the problem of learning an unknown function f(cid:63) on the d-dimensional sphere\nwith respect to the square loss, given i.i.d. samples {(yi, xi)}i\u2264n where xi is a feature vector\nuniformly distributed on the sphere and yi = f(cid:63)(xi", "neural tuning and representational \ngeometry\n\nnikolaus\u00a0kriegeskorte \n\n  and xue- xin\u00a0wei \n\n \n\nabstract | a central goal of neuroscience is to understand the representations \nformed by brain activity patterns and their connection to behaviour. the classic \napproach is to investigate how individual neurons encode stimuli and how their \ntuning determines the fidelity of the neural representation. tuning analyses often \nuse the fisher information to characterize the sensitivity of neural responses t", "new types of deep neural network learning for speech recognition \n\nand related applications: an overview  \n\n \n\nli deng1, geoffrey hinton2, and brian kingsbury3 \n\n1microsoft research, redmond, wa, usa  \n2university of toronto, ontario, canada  \n\n3ibm t. j. watson research center, yorktown heights, ny, usa \n\nabstract \n\n \nin  this  paper,  we  provide  an  overview  of  the  invited  and \ncontributed  papers  presented  at  the  special  session  at  icassp-\n2013,  entitled  \u201cnew  types  of  deep  ", "vision research 49 (2009) 1295\u20131306\n\ncontents lists available at sciencedirect\n\nvision research\n\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / v i s r e s\n\nbayesian surprise attracts human attention\nlaurent itti a,*, pierre baldi b,1\na computer science department and neuroscience graduate program, university of southern california, hedco neuroscience building, 3641 watt way,\nhnb-30a, los angeles, ca 90089, usa\nb computer science department and institute for geno", "elifesciences.org\n\nresearch article\n\nreconceiving the hippocampal map as a \ntopological template\nyuri dabaghian1,2*, vicky l brandt1,2, loren m frank3,4\n\n1the jan and dan duncan neurological research institute at texas children's \nhospital, houston, united states; 2baylor college of medicine, houston, united \nstates; 3sloan-swartz center for theoretical neurobiology, w.m. keck center for \nintegrative neuroscience, university of california, san francisco, san francisco, \nunited states; 4departmen", "0\n2\n0\n2\n\n \n\nb\ne\nf\n0\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n2\n7\n5\n7\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nneural tangent kernel:\n\nconvergence and generalization in neural networks\n\narthur jacot\n\n\u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\narthur.jacot@netopera.net\n\nimperial college london and \u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\nfranck gabriel\n\nfranckrgabriel@gmail.com\n\ncl\u00b4ement hongler\n\n\u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\nclement.hongler@gmail.com\n\nabstract\n\nat initialization, arti\ufb01cial neural", "article\n\nreceived 7 jan 2016 | accepted 16 sep 2016 | published 8 nov 2016\n\ndoi: 10.1038/ncomms13276\n\nopen\n\nrandom synaptic feedback weights support error\nbackpropagation for deep learning\ntimothy p. lillicrap1,2, daniel cownden3, douglas b. tweed4,5 & colin j. akerman1\n\nthe brain processes information through multiple layers of neurons. this deep architecture is\nrepresentationally powerful, but complicates learning because it is dif\ufb01cult to identify the\nresponsible neurons when a mistake is mad", "biorxiv preprint \nthe copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\nhttps://doi.org/10.1101/2021.11.08.467806\n; \n\nthis version posted september 25, 2022. \n\ndoi: \n\navailable under a\n\ncc-by-nc-nd 4.0 international license\n.\n\nparametric control of \ufb02exible timing through low-dimensional neural\nmanifolds\nmanuel beiran1, 2,\u2021, nicolas meirhaeghe3, 4,\u2021, hansem sohn5, ", "predictive coding: a fresh view of inhibition in the retina \nauthor(s): m. v. srinivasan, s. b. laughlin and a. dubs \nsource: proceedings of the royal society of london. series b, biological sciences, nov. \n22, 1982, vol. 216, no. 1205 (nov. 22, 1982), pp. 427-459\npublished by: royal society \n\n \n\nstable url: https://www.jstor.org/stable/35861\n\njstor is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide \nrange of content in a trusted digit", "multi-manifold clustering\n\nyong wang1,2, yuan jiang2, yi wu1, and zhi-hua zhou2\n\n1 department of mathematics and systems science\n\nnational university of defense technology, changsha 410073, china\n\nyongwang82@gmail.com, wuyi work@sina.com\n\n2 national key laboratory for novel software technology\n\nnanjing university, nanjing 210093, china\n\n{jiangy, zhouzh}@lamda.nju.edu.cn\n\nabstract. manifold clustering, which regards clusters as groups of points around\ncompact manifolds, has been realized as a pro", "the journal of neuroscience, 2000, vol. 20 rc61 1 of 6\n\nselectivity for complex shapes in primate visual area v2\n\njay hegde\u00b4 and david c. van essen\ndepartment of anatomy and neurobiology, washington university school of medicine, st. louis, missouri 63110\n\nto explore the role of visual area v2 in shape analysis, we\nstudied the responses of neurons in area v2 of the alert ma-\ncaque using a set of 128 grating and geometric line stimuli that\nvaried in their shape characteristics and geometric compl", "m\no\nc\n\n.\n\ne\nr\nu\nt\na\nn\n\n.\ni\n\nc\ns\no\nr\nu\ne\nn\n\n/\n/\n:\np\nt\nt\nh\n\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n1\n0\n0\n2\n\u00a9\n\n \n\narticles\n\n\u00a9 2001 nature publishing group  http://neurosci.nature.com\n\nin vivo intracellular recording and\nperturbation of persistent activity\nin a neural integrator\n\ne. aksay1,2, g. gamkrelidze1,2, h. s. seung1,3, r. baker2 and d. w. tank1\n\n1 biological computation research department, bell laboratories, lucent technologies, 700 mountain avenue, murray hill, new jersey ", "research article summary \u25e5\n\nneural computation\n\nspiking neurons can discover\npredictive features by\naggregate-label learning\n\nrobert g\u00fctig\n\nintroduction: opportunities and dangers\ncan often be predicted on the basis of sensory\nclues. the attack of a predator, for example,\nmay be preceded by the sounds of breaking\ntwigs or whiffs of odor. life is easier if one\nlearns these clues. however, this is difficult\nwhen clues are hidden within distracting\nstreams of unrelated sensory activity. even\nworse,", "journal of computational neuroscience (2021) 49:107\u2013127\nhttps://doi.org/10.1007/s10827-021-00780-x\n\noriginal article\n\npredictive coding models  for\u00a0pain perception\n\nyuru\u00a0song1,2\u00a0\u00b7 mingchen\u00a0yao1,3 \namrita\u00a0singh6\u00a0\u00b7 jing\u00a0wang6,7,8 \n\n\u00a0\u00b7 helen\u00a0kemprecos4\u00a0\u00b7 aine\u00a0byrne5 \n\u00a0\u00b7 zhe\u00a0s.\u00a0chen1,7,8 \n\n\u00a0\u00b7 zhengdong\u00a0xiao1\u00a0\u00b7 qiaosheng\u00a0zhang6 \n\n\u00a0\u00b7 \n\nreceived: 1 september 2020 / revised: 14 december 2020 / accepted: 29 january 2021 \n\u00a9 the author(s), under exclusive licence to springer science+business media, llc par", "article\n\nreceived 17 mar 2016 | accepted 6 jan 2017 | published 20 feb 2017\n\ndoi: 10.1038/ncomms14531\n\nopen\n\nplace cells are more strongly tied to landmarks\nin deep than in super\ufb01cial ca1\ntristan geiller1,2, mohammad fattahi1,3, june-seek choi2 & se\u00b4bastien royer1,3\n\nenvironmental cues affect place cells responses, but whether this information is integrated\nversus segregated in distinct hippocampal cell populations is unclear. here, we show that, in\nmice running on a treadmill enriched with visu", "minimum sharpness: scale-invariant parameter-robustness of neural networks\n\n1\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n6\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n2\n1\n6\n2\n1\n\n.\n\n6\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nhikaru ibayashi 1 takuo hamaguchi 2 masaaki imaizumi 2\n\nabstract\n\ntoward achieving robust and defensive neural\nnetworks, the robustness against the weight pa-\nrameters perturbations, i.e., sharpness, attracts\nattention in recent years (sun et al., 2020). how-\never, sharpness is known to remain a critical is-\nsue, \u201cscale-sensitivity.\u201d", "12978 \u2022 the journal of neuroscience, september 29, 2010 \u2022 30(39):12978 \u201312995\n\nbehavioral/systems/cognitive\n\nselectivity and tolerance (\u201cinvariance\u201d) both increase as\nvisual information propagates from cortical area v4 to it\n\nnicole c. rust1,2,3 and james j. dicarlo1,2\n1mcgovern institute for brain research and 2department brain and cognitive, sciences, massachusetts institute of technology, cambridge, massachusetts\n02139, and 3department psychology, university of pennsylvania, philadelphia, pen", "managing competing goals \u2014 a key \nrole for the frontopolar cortex\n\nfarshad alizadeh mansouri1\u20133*, etienne koechlin4*, marcello g.\u00a0p.\u00a0rosa1,2  \nand mark j.\u00a0buckley5\nabstract | humans are set apart from other animals by many elements of advanced cognition and \nbehaviour, including language, judgement and reasoning. what is special about the human brain \nthat gives rise to these abilities? could the foremost part of the prefrontal cortex (the frontopolar \ncortex), which has become considerably enla", "learning and planning in average-reward markov decision processes\n\nyi wan * 1 abhishek naik * 1 richard s. sutton 1 2\n\nabstract\n\nwe introduce learning and planning algorithms\nfor average-reward mdps, including 1) the \ufb01rst\ngeneral proven-convergent off-policy model-free\ncontrol algorithm without reference states, 2) the\n\ufb01rst proven-convergent off-policy model-free pre-\ndiction algorithm, and 3) the \ufb01rst off-policy learn-\ning algorithm that converges to the actual value\nfunction rather than to the", "article\n\nreceived 31 may 2016 | accepted 3 aug 2016 | published 20 sep 2016\n\ndoi: 10.1038/ncomms12815\n\nopen\n\na dendritic disinhibitory circuit mechanism for\npathway-speci\ufb01c gating\nguangyu robert yang1, john d. murray1,2 & xiao-jing wang1,3\n\nwhile reading a book in a noisy cafe\u00b4, how does your brain \u2018gate in\u2019 visual information while\n\ufb01ltering out auditory stimuli? here we propose a mechanism for such \ufb02exible routing of\ninformation \ufb02ow in a complex brain network (pathway-speci\ufb01c gating), tested us", "the functional organization of cortical feedback \ninputs to primary visual cortex\n\ntiago marques\u200a\n\n\u200a1,2, julia nguyen\u200a\n\n\u200a1,2, gabriela fioreze1 and leopoldo petreanu\u200a\n\n\u200a1*\n\ncortical feedback is thought to mediate cognitive processes like attention, prediction, and awareness. understanding its function \nrequires identifying the organizational logic of feedback axons relaying different signals. we measured retinotopic specificity \nin inputs from the lateromedial visual area in mouse primary visual", "learning universal policies via text-guided video generation\n\nyilun du * 1 2 mengjiao yang * 3 2 bo dai 2 hanjun dai 2 o\ufb01r nachum 2\n\njoshua b. tenenbaum 1 dale schuurmans 2 4 pieter abbeel 3\n\n3\n2\n0\n2\n\n \n\nb\ne\nf\n2\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n2\nv\n1\n1\n1\n0\n0\n\n.\n\n2\n0\n3\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\na goal of arti\ufb01cial intelligence is to construct\nan agent that can solve a wide variety of tasks.\nrecent progress in text-guided image synthesis\nhas yielded models with an impressive ability to\ngenerate complex ", "article\n\ncommunicated by christian machens\n\nsequential optimal design of neurophysiology experiments\n\njeremy lewi\njeremy@lewi.us\nbioengineering graduate program, wallace h. coulter department of biomedical\nengineering, laboratory for neuroengineering, georgia institute of technology,\natlanta, ga 30332, u.s.a. http://www.lewilab.org\n\nrobert butera\nrbutera@ece.gatech.edu\nschool of electrical and computer engineering, laboratory for neuroengineering,\ngeorgia institute of technology, atlanta, ga 303", "functional network reorganization during learning in\na brain-computer interface paradigm\n\nbeata jarosiewicza,b,1,2, steven m. chasea,b,c,1, george w. frasera,b, meel vellistea,b, robert e. kassb,c,\nand andrew b. schwartza,b,3\n\nadepartment of neurobiology, university of pittsburgh, pittsburgh, pa 15213; cdepartment of statistics, carnegie mellon university, pittsburgh, pa 15213;\nand bcenter for the neural basis of cognition, university of pittsburgh and carnegie mellon university\n\nedited by j. an", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/083857\n; \n\nthis version posted august 9, 2017. \n\nthe copyright holder for this preprint (which was not\n\ncertified by peer review) is the author/funder. all rights reserved. no reuse allowed without permission. \n\n1\t\n\n\t\n\n \n\n \n \n \n \n \npredictive  representations  can  link  model-based  reinforcement  learning  to  model-free \nmechanisms \n \nevan  m.  russek1*\u00b6,  ida  momennejad2\u00b6,  matthew  m.  botvinick3,  samuel  j.  gershman4, \nnathaniel d. daw2 ", "recurrent orthogonal networks and long-memory tasks\n\nmikael henaff\nnew york university, facebook ai research\narthur szlam\nfacebook ai research\nyann lecun\nnew york university, facebook ai research\n\nabstract\n\nalthough rnns have been shown to be power-\nful tools for processing sequential data, \ufb01nding\narchitectures or optimization strategies that al-\nlow them to model very long term dependencies\nis still an active area of research. in this work,\nwe carefully analyze two synthetic datasets orig-\ninal", "on the di\ufb03culty of training recurrent neural networks\n\nrazvan pascanu\nuniversit\u00b4e de montr\u00b4eal, 2920, chemin de la tour, montr\u00b4eal, qu\u00b4ebec, canada, h3t 1j8\n\npascanur@iro.umontreal.ca\n\ntomas mikolov\nspeech@fit, brno university of technology, brno, czech republic\n\nt.mikolov@gmail.com\n\nyoshua bengio\nuniversit\u00b4e de montr\u00b4eal, 2920, chemin de la tour, montr\u00b4eal, qu\u00b4ebec, canada, h3t 1j8\n\nyoshua.bengio@umontreal.ca\n\nabstract\n\nthere are two widely known issues with prop-\nerly training recurrent neural", "understanding contrastive representation learning through\n\nalignment and uniformity on the hypersphere\n\ntongzhou wang 1 phillip isola 1\n\nabstract\n\ncontrastive representation learning has been out-\nstandingly successful in practice. in this work,\nwe identify two key properties related to the con-\ntrastive loss: (1) alignment (closeness) of features\nfrom positive pairs, and (2) uniformity of the in-\nduced distribution of the (normalized) features on\nthe hypersphere. we prove that, asymptotically,\n", "6\n1\n0\n2\n\n \nr\np\na\n3\n2\n\n \n\n \n \n]\n\n.\n\nc\no\nh\nt\na\nm\n\n[\n \n \n\n2\nv\n6\n9\n0\n6\n0\n\n.\n\n0\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nwhen are nonconvex problems not scary?\n\nju sun, qing qu, and john wright\n\n{js4038, qq2105, jw2966}@columbia.edu\n\ndepartment of electrical engineering, columbia university, new york, usa\n\noctober 20, 2015 revised: april 26, 2016\n\nabstract\n\nin this note, we focus on smooth nonconvex optimization problems that obey: (1) all local\nminimizers are also global; and (2) around any saddle point or local maximize", "contractive auto-encoders:\n\nexplicit invariance during feature extraction\n\nsalah rifai(1)\npascal vincent(1)\nxavier muller(1)\nxavier glorot(1)\nyoshua bengio(1)\n(1) dept. iro, universit\u00b4e de montr\u00b4eal. montr\u00b4eal (qc), h3c 3j7, canada\n\nrifaisal@iro.umontreal.ca\nvincentp@iro.umontreal.ca\nmullerx@iro.umontreal.ca\nglorotxa@iro.umontreal.ca\nbengioy@iro.umontreal.ca\n\nabstract\n\n1. introduction\n\nwe present in this paper a novel approach\nfor training deterministic auto-encoders. we\nshow that by adding a we", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/266254052\n\na critical time window for dopamine actions on the structural plasticity of\ndendritic spines\n\narticle\u00a0\u00a0in\u00a0\u00a0science \u00b7 september 2014\n\ndoi: 10.1126/science.1255514\u00a0\u00b7\u00a0source: pubmed\n\ncitations\n462\n\n6 authors, including:\n\nakiko hayashi-takagi\nriken\n\n63 publications\u00a0\u00a0\u00a04,698 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n930\n\nhidetoshi urakubo\nkyoto university\n\n34 publications\u00a0\u00a0\u00a0809 citations\u00a0\u00a0\u00a0\n", "supplementary information for \u201cthe hippocampus as a predictive map\u201d\n\nkimberly l. stachenfeld1,2,*, matthew m. botvinick1,3, samuel j. gershman4\n1deepmind, london, uk\n2princeton neuroscience institute, princeton university, princeton, nj, usa\n3gatsby computational neuroscience unit, university college london, london, uk\n4department of psychology and center for brain science, harvard university, cambridge, ma, usa\n*stachenfeld@google.com\n\n1 predictive temporal codes and the sr\nmany models of prosp", "large-scale neural recordings call for new insights \nto link brain and behavior\n\nanne e. urai\u200a\n\n\u200a1,2, brent doiron3, andrew m. leifer\u200a\n\n\u200a4 and anne k. churchland\u200a\n\n\u200a1,5\u2009\u2709\n\nneuroscientists today can measure activity from more neurons than ever before, and are facing the challenge of connecting \nthese brain-wide neural recordings to computation and behavior. in the present review, we first describe emerging tools and \ntechnologies being used to probe large-scale brain activity and new approaches t", "taming transformers for high-resolution image synthesis\n\npatrick esser*\n\nrobin rombach*\n\nbj\u00a8orn ommer\n\nheidelberg collaboratory for image processing, iwr, heidelberg university, germany\n\n*both authors contributed equally to this work\n\nfigure 1. our approach enables transformers to synthesize high-resolution images like this one, which contains 1280x460 pixels.\n\nabstract\n\ndesigned to learn long-range interactions on sequential\ndata, transformers continue to show state-of-the-art results\non a wide", "neuron, vol. 36, 955\u2013968, december 5, 2002, copyright \uf8e92002 by cell press\n\nprobabilistic decision making\nby slow reverberation in cortical circuits\n\nxiao-jing wang1\nvolen center for complex systems\nbrandeis university\nwaltham, massachusetts 02254\n\nsummary\n\nrecent physiological studies of alert primates have\nrevealed cortical neural correlates of key steps in a\nperceptual decision-making process. to elucidate\nsynaptic mechanisms of decision making, i investi-\ngated a biophysically realistic corti", "1\n2\n0\n2\n\n \nc\ne\nd\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n4\n3\n0\n0\n0\n\n.\n\n1\n1\n1\n2\n:\nv\ni\nx\nr\na\n\npreprint\n\nneural networks as kernel learners: the\nsilent alignment effect\n\nalexander atanasov\u2217\u00a7\u2021, blake bordelon\u2217\u2020\u2021 & cengiz pehlevan\u2020\u2021\n\u00a7department of physics\n\u2020john a. paulson school of engineering and applied sciences\n\u2021center for brain science\nharvard university\ncambridge, ma 02138, usa\n{atanasov,blake bordelon,cpehlevan}@g.harvard.edu\n\nabstract\n\nneural networks in the lazy training regime converge to kern", "0\n2\n0\n2\n\n \n\np\ne\ns\n8\n1\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n6\n2\n4\n3\n0\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\numap: uniform manifold\n\napproximation and projection for\n\ndimension reduction\n\nleland mcinnes\n\ntu(cid:138)e institute for mathematics and computing\n\nleland.mcinnes@gmail.com\n\njohn healy\n\ntu(cid:138)e institute for mathematics and computing\n\njchealy@gmail.com\n\njames melville\n\njlmelville@gmail.com\n\nseptember 21, 2020\n\nabstract\n\numap (uniform manifold approximation and projection) is a novel\nmanifold learni", "7\n1\n0\n2\n\n \nr\np\na\n \n6\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n3\n6\n4\n6\n0\n\n.\n\n2\n0\n7\n1\n:\nv\ni\nx\nr\na\n\npredicting non-linear dynamics by stable local learning in\n\na recurrent spiking neural network\n\naditya gilra1,*\n\nwulfram gerstner1\n\n1school of computer and communication sciences, and brain-mind institute, school of life\nsciences, \u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne (epfl), lausanne 1015, switzerland.\n\n\u2217correspondence: aditya.gilra@ep\ufb02.ch\n\nabstract\n\nbrains need to predict how the body reacts to ", "r e v i e w s\n\n*department of brain and \ncognitive sciences and center \nfor visual science, university \nof rochester, rochester, \nnew york 14627, usa. \n\u2021gatsby computational \nneuroscience unit, \nuniversity college london, \n17 queen square, \nlondon wc1n 3ar, uk. \ncorrespondence to a.p. \ne-mail: \nalex@bcs.rochester.edu\ndoi:10.1038/nrn1888\n\n358 | may 2006 | volume 7 \n\nneural correlations, population coding \nand computation\n\nbruno b. averbeck*, peter e. latham\u2021 and alexandre pouget*\nabstract | how t", "improved denoising diffusion probabilistic models\n\nalex nichol * 1 prafulla dhariwal * 1\n\n1\n2\n0\n2\n\n \n\nb\ne\nf\n8\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n2\n7\n6\n9\n0\n\n.\n\n2\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\ndenoising diffusion probabilistic models (ddpm)\nare a class of generative models which have re-\ncently been shown to produce excellent sam-\nples. we show that with a few simple modi\ufb01-\ncations, ddpms can also achieve competitive log-\nlikelihoods while maintaining high sample quality.\nadditionally, we \ufb01nd that l", "integrated morphoelectric and transcriptomic\nclassi\ufb01cation of cortical gabaergic cells\n\narticle\n\ngraphical abstract\n\nauthors\nnathan w. gouwens, staci a. sorensen,\nfahimeh baftizadeh, ..., jim berg,\ngabe j. murphy, hongkui zeng\n\ncorrespondence\nnathang@alleninstitute.org (n.w.g.),\nstacis@alleninstitute.org (s.a.s.),\ngabem@alleninstitute.org (g.j.m.)\n\nin brief\ngabaergic cortical interneurons of the\nmouse visual cortex can be de\ufb01ned into\n28 types based on their morphological,\nelectrophysiological, a", "a r t i c l e s\n\n , matthew m botvinick1\u20133 \n\ndorsal hippocampus contributes to model-based \nplanning\nkevin j miller1 \nplanning can be defined as action selection that leverages an internal model of the outcomes likely to follow each possible action. \nits neural mechanisms remain poorly understood. here we adapt recent advances from human research for rats, presenting for \nthe first time an animal task that produces many trials of planned behavior per session, making multitrial rodent experimenta", "8\n1\n0\n2\n\n \nt\nc\no\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n1\n6\n2\n1\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nrelational inductive biases, deep learning, and graph networks\n\npeter w. battaglia1\u2217, jessica b. hamrick1, victor bapst1,\n\nalvaro sanchez-gonzalez1, vinicius zambaldi1, mateusz malinowski1,\nandrea tacchetti1, david raposo1, adam santoro1, ryan faulkner1,\ncaglar gulcehre1, francis song1, andrew ballard1, justin gilmer2,\n\ngeorge dahl2, ashish vaswani2, kelsey allen3, charles nash4,\n\nvictoria langston1, chris dy", "research article\n\nextracting grid cell characteristics from\nplace cell inputs using non-negative\nprincipal component analysis\nyedidyah dordek1,2\u2020, daniel soudry3,4*\u2020, ron meir1, dori derdikman2*\n\n1faculty of electrical engineering, technion \u2013 israel institute of technology, haifa,\nisrael; 2rappaport faculty of medicine and research institute, technion \u2013 israel\ninstitute of technology, haifa, israel; 3department of statistics, columbia\nuniversity, new york, united states; 4center for theoretical ", "original article\ntheory\nwhatdoesthefreeenergyprincipletellusabout\nthebrain?\nsamuelj.gershman1\u2217\n1departmentofpsychologyandcenterfor\nbrainscience,harvarduniversity,\ncambridge,ma,02138,usa\ncorrespondence\nnorthwestlaboratories,52oxfordst.,\nroom295.05,cambridge,ma,02138,usa\nemail: gershman@fas.harvard.edu\nfundinginformation\nthisworkwassupportedbyaresearch\nfellowshipfromthealfredp.sloan\nfoundation.\n\nthefreeenergyprinciplehasbeenproposedasaunifying\naccountofbrainfunction. itiscloselyrelated,andinsome\nc", "1002 \u2022 the journal of neuroscience, january 26, 2005 \u2022 25(4):1002\u20131014\n\nbehavioral/systems/cognitive\n\nangular path integration by moving \u201chill of activity\u201d: a\nspiking neuron model without recurrent excitation of the\nhead-direction system\n\npengcheng song and xiao-jing wang\nvolen center for complex systems, brandeis university, waltham, massachusetts 02454\n\nduring spatial navigation, the head orientation of an animal is encoded internally by neural persistent activity in the head-direction (hd)\nsy", "journal of computer and system sciences (cid:21) ss1504\n\njournal of computer and system sciences 55, 119(cid:21)139 (1997)\narticle no. ss971504\n\na decision-theoretic generalization of on-line learning\n\nand an application to boosting*\n\nyoav freund and robert e. schapire-\n\nat6t labs, 180 park avenue, florham park, new jersey 07932\n\nreceived december 19, 1996\n\nin the first part of the paper we consider the problem of dynamically\napportioning resources among a set of options in a worst-case on-line\n", "on linear identi\ufb01ability of learned representations\n\ngeoffrey roeder\u2217\nprinceton university\n\ngoogle brain\n\nroeder@princeton.edu\n\nluke metz\ngoogle brain\n\nlmetz@google.com\n\n0\n2\n0\n2\n\n \nl\nu\nj\n \n\n8\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n0\n1\n8\n0\n0\n\n.\n\n7\n0\n0\n2\n:\nv\ni\nx\nr\na\n\ndiederik p. kingma\u2217\n\ngoogle brain\n\ndurk@google.com\n\nabstract\n\nidenti\ufb01ability is a desirable property of a statistical model: it implies that the\ntrue model parameters may be estimated to any desired precision, given suf\ufb01cient\ncomputational ", "\f", "phil. trans. r. soc. b (2009) 364, 1211\u20131221\ndoi:10.1098/rstb.2008.0300\n\npredictive coding under the free-energy principle\n\nkarl friston* and stefan kiebel\n\nthe wellcome trust centre of neuroimaging, institute of neurology, university college london,\n\nqueen square, london wc1n 3bg, uk\n\nthis paper considers prediction and perceptual categorization as an inference problem that is solved\nby the brain. we assume that the brain models the world as a hierarchy or cascade of dynamical\nsystems that enco", "physical review x 5, 021028 (2015)\n\nmacroscopic description for networks of spiking neurons\n\n1center for brain and cognition, department of information and communication technologies,\n\nernest montbri\u00f3,1 diego paz\u00f3,2 and alex roxin3\n\nuniversitat pompeu fabra, 08018 barcelona, spain\n\n2instituto de f\u00edsica de cantabria (ifca), csic-universidad de cantabria, 39005 santander, spain\n\n3centre de recerca matem\u00e0tica, campus de bellaterra, edifici c, 08193 bellaterra, spain\n\n(received 30 december 2014; pub", "7\n1\n0\n2\n\n \nr\na\n\nm\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n7\n8\n8\n0\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nhow to escape saddle points e\ufb03ciently\n\nchi jin\u2217\n\nrong ge\u2020\n\npraneeth netrapalli\u2021\n\nsham m. kakade\u00a7\n\nmichael i. jordan\u00b6\n\nmarch 3, 2017\n\nabstract\n\nthis paper shows that a perturbed form of gradient descent converges to a second-order\nstationary point in a number iterations which depends only poly-logarithmically on dimension\n(i.e., it is almost \u201cdimension-free\u201d). the convergence rate of this procedure matches the ", "journal of machine learning research 3 (2003) 993-1022\n\nsubmitted 2/02; published 1/03\n\nlatent dirichlet allocation\n\ndavid m. blei\ncomputer science division\nuniversity of california\nberkeley, ca 94720, usa\nandrew y. ng\ncomputer science department\nstanford university\nstanford, ca 94305, usa\n\nmichael i. jordan\ncomputer science division and department of statistics\nuniversity of california\nberkeley, ca 94720, usa\n\neditor: john lafferty\n\nblei@cs.berkeley.edu\n\nang@cs.stanford.edu\n\njordan@cs.berkeley.", "1\n\ntraining deep architectures without end-to-end\n\nbackpropagation:\n\na survey on the provably optimal methods\n\nshiyu duan, jos\u00b4e c. pr\u00b4\u0131ncipe\n\n2\n2\n0\n2\n\n \n\ng\nu\na\n9\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n9\n1\n4\n3\n0\n\n.\n\n1\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\u2014this tutorial paper surveys provably optimal al-\nternatives to end-to-end backpropagation (e2ebp) \u2014 the de\nfacto standard for training deep architectures. modular training\nrefers to strictly local training without both the forward and\nthe backward pass, i.e., d", "arti\ufb01cialintelligence112(1999)181\u2013211betweenmdpsandsemi-mdps:aframeworkfortemporalabstractioninreinforcementlearningrichards.suttona,\u2217,doinaprecupb,satindersinghaaat&tlabs.-research,180parkavenue,florhampark,nj07932,usabcomputersciencedepartment,universityofmassachusetts,amherst,ma01003,usareceived1december1998abstractlearning,planning,andrepresentingknowledgeatmultiplelevelsoftemporalabstractionarekey,longstandingchallengesforai.inthispaperweconsiderhowthesechallengescanbeaddressedwithinthemath", "6\n1\n0\n2\n\n \nt\nc\no\n8\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n2\nv\n4\n4\n1\n8\n0\n\n.\n\n9\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ngoogle\u2019s neural machine translation system: bridging the gap\n\nbetween human and machine translation\n\nyonghui wu, mike schuster, zhifeng chen, quoc v. le, mohammad norouzi\n\nyonghui,schuster,zhifengc,qvl,mnorouzi@google.com\n\nwolfgang macherey, maxim krikun, yuan cao, qin gao, klaus macherey,\nje\ufb00 klingner, apurva shah, melvin johnson, xiaobing liu, \u0142ukasz kaiser,\nstephan gouws, yoshikiyo kato, taku kudo, hideto kaz", "developmental brain research 107 1998 159\u2013163\n\n(cid:14)\n\n.\n\nshort communication\n\nquantified distribution of serotonin transporter and receptors during the\n\npostnatal development of the rat barrel field cortex\n\nsonia mansour-robaey, naguib mechawar, fatiha radja, clermont beaulieu,\n\nlaurent descarries )\n\ndepartement de pathologie et biologie cellulaire and centre de recherche en sciences neurologiques, uni\u02ddersite de montreal, montreal,\n\n\u00b4\n\n\u00b4\n\n\u00b4\n\n\u00b4\n\nquebec, canada h3c 3j7\n\n\u00b4\n\naccepted 16 december ", "proceedings of the thirtieth aaai conference on artificial intelligence (aaai-16)\n\ndeep reinforcement learning with double q-learning\n\nhado van hasselt , arthur guez, and david silver\n\ngoogle deepmind\n\nabstract\n\nthe popular q-learning algorithm is known to overestimate\naction values under certain conditions. it was not previously\nknown whether, in practice, such overestimations are com-\nmon, whether they harm performance, and whether they can\ngenerally be prevented. in this paper, we answer all ", "2\n2\n0\n2\n\n \nc\ne\nd\n0\n1\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\n1\nv\n4\n1\n1\n6\n\n.\n\n2\n1\n3\n1\n:\nv\ni\nx\nr\na\n\nauto-encoding variational bayes\n\ndiederik p. kingma\n\nmachine learning group\nuniversiteit van amsterdam\ndpkingma@gmail.com\n\nmax welling\n\nmachine learning group\nuniversiteit van amsterdam\n\nwelling.max@gmail.com\n\nabstract\n\nhow can we perform ef\ufb01cient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable posterior\ndistributions, and large data", "what does dopamine mean?\n\njoshua d. berke\n\ndopamine is a critical modulator of both learning and motivation. this presents a problem: how can target cells know whether \nincreased dopamine is a signal to learn or to move? it is often presumed that motivation involves slow (\u2018tonic\u2019) dopamine \nchanges,  while  fast  (\u2018phasic\u2019)  dopamine  fluctuations  convey  reward  prediction  errors  for  learning.  yet  recent  studies  have \nshown that dopamine conveys motivational value and promotes movement ", "journal of machine learning research 5 (2004) 1471\u20131530\n\nsubmitted 10/03; revised 10/04; published 11/04\n\nvariance reduction techniques for gradient estimates in\n\nreinforcement learning\n\nevan greensmith\nresearch school of information sciences and engineering\naustralian national university\ncanberra 0200, australia\npeter l. bartlett\ncomputer science division & department of statistics\nuc berkeley\nberkeley, ca 94720, usa\n\njonathan baxter\npanscient pty. ltd.\n10 gawler terrace\nwalkerville, sa 5081, a", "r e v i e w s\n\nthe origin of extracellular fields and \ncurrents \u2014 eeg, ecog, lfp and spikes\n\ngy\u00f6rgy buzs\u00e1ki1,2,3, costas a. anastassiou4 and christof koch4,5\n\nabstract | neuronal activity in the brain gives rise to transmembrane currents that can be \nmeasured in the extracellular medium. although the major contributor of the extracellular \nsignal is the synaptic transmembrane current, other sources \u2014 including na+ and ca2+ \nspikes, ionic fluxes through voltage- and ligand-gated channels, and int", "t e c h n i c a l   r e p o r t s\n\n \n\n  & john p cunningham1,3,4 \n\nstructure in neural population recordings: an expected \nbyproduct of simpler phenomena?\ngamaleldin f elsayed1\u20133 \nneuroscientists increasingly analyze the joint activity  \nof multineuron recordings to identify population-level \nstructures believed to be significant and scientifically novel. \nclaims of significant population structure support hypotheses \nin many brain areas. however, these claims require first \ninvestigating the po", "optimal architectures in a solvable model of deep\n\nnetworks\n\njonathan kadmon\n\nthe racah institute of physics and elsc\n\nthe hebrew university, israel\n\njonathan.kadmon@mail.huji.ac.il\n\nhaim sompolinsky\n\nthe racah institute of physics and elsc\n\nthe hebrew university, israel\n\nand\n\ncenter for brain science\n\nharvard university\n\nabstract\n\ndeep neural networks have received a considerable attention due to the success\nof their training for real world machine learning applications. they are also\nof great ", "r e v i e w s\n\nmultiple reward signals \nin the brain\n\nwolfram schultz\n\nthe fundamental biological importance of rewards has created an increasing interest in the\nneuronal processing of reward information. the suggestion that the mechanisms underlying\ndrug addiction might involve natural reward systems has also stimulated interest. this article\nfocuses on recent neurophysiological studies in primates that have revealed that neurons in a\nlimited number of brain structures carry specific signals ab", "8\n1\n0\n2\n\n \nt\nc\no\n3\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n9\n5\n7\n5\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ninsights on representational similarity in neural\n\nnetworks with canonical correlation\n\nari s. morcos\u2217\u2021\n\ndeepmind\u2020\n\narimorcos@gmail.com\n\nmaithra raghu\u2217\u2021\n\ngoogle brain, cornell university\n\nmaithrar@gmail.com\n\nsamy bengio\ngoogle brain\n\nbengio@google.com\n\nabstract\n\ncomparing different neural network representations and determining how repre-\nsentations evolve over time remain challenging open questions in ou", "assessing the scalability of biologically-motivated\n\ndeep learning algorithms and architectures\n\nsergey bartunov\n\ndeepmind\n\nadam santoro\n\ndeepmind\n\nblake a. richards\nuniversity of toronto\n\nluke marris\ndeepmind\n\ngeoffrey e. hinton\n\ngoogle brain\n\ntimothy p. lillicrap\n\ndeepmind, university college london\n\nabstract\n\nthe backpropagation of error algorithm (bp) is impossible to implement in a\nreal brain. the recent success of deep networks in machine learning and ai,\nhowever, has inspired proposals fo", "article\n\nreceived 21 mar 2016 | accepted 12 jul 2016 | published 13 sep 2016 | updated 23 nov 2017\n\ndoi: 10.1038/ncomms12554 open\n\na dynamic code for economic object valuation\nin prefrontal cortex neurons\nken-ichiro tsutsui1,*,w\n\n, fabian grabenhorst1,*, shunsuke kobayashi1,w\n\n& wolfram schultz1\n\nneuronal reward valuations provide the physiological basis for economic behaviour. yet, how\nsuch valuations are converted to economic decisions remains unclear. here we show that the\ndorsolateral prefro", "research\n\nresearch article summary \u25e5\n\nneuroscience\n\nneural population control via deep\nimage synthesis\n\npouya bashivan*, kohitij kar*, james j. dicarlo\u2020\n\nintroduction: the pattern of light that\nstrikes the eyes is processed and re-represented\nvia patterns of neural activity in a \u201cdeep\u201d series\nof six interconnected cortical brain areas called\nthe ventral visual stream. visual neuroscience\nresearch has revealed that these patterns of\nneural activity underlie our ability to recog-\nnize objects and ", "8\n1\n0\n2\n\n \n\nv\no\nn\n0\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n7\n8\n5\n4\n0\n\n.\n\n7\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nassessing the scalability of biologically-motivated\n\ndeep learning algorithms and architectures\n\nsergey bartunov\n\ndeepmind\n\nadam santoro\n\ndeepmind\n\nblake a. richards\nuniversity of toronto\n\nluke marris\ndeepmind\n\ngeoffrey e. hinton\n\ngoogle brain\n\ntimothy p. lillicrap\n\ndeepmind, university college london\n\nabstract\n\nthe backpropagation of error algorithm (bp) is impossible to implement in a real\nbrain. the recen", "limitations of lazy training of\ntwo-layers neural networks\n\nbehrooz ghorbani\n\ndepartment of electrical engineering\n\nstanford university\n\nsong mei\n\nicme\n\nstanford university\n\nghorbani@stanford.edu\n\nsongmei@stanford.edu\n\ntheodor misiakiewicz\ndepartment of statistics\n\nstanford university\n\nmisiakie@stanford.edu\n\nandrea montanari\n\ndepartment of electrical engineering\n\nand department of statistics\n\nstanford university\n\nmontanar@stanford.edu\n\nabstract\n\nwe study the supervised learning problem under eit", "vision research 51 (2011) 1484\u20131525\n\ncontents lists available at sciencedirect\n\nvision research\n\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / v i s r e s\n\nreview\nvisual attention: the past 25 years\n\nmarisa carrasco\n\npsychology and neural science, new york university, united states\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\nthis review focuses on covert attention and how it alters early vision. i explain why attention is consid-\nered a selective process, the cons", "article\n\ncommunicated by manuel beiran\n\nheterogeneity in neuronal dynamics is learned by gradient\ndescent for temporal processing tasks\n\nchloe n. winston\nwincnw@gmail.com\ndepartments of neuroscience and computer science, university of washington,\nseattle, wa 98195, u.s.a., and university of washington computational\nneuroscience center, seattle, wa 98195, u.s.a.\n\ndana mastrovito\ndana.mastrovito@alleninstitute.org\nallen institute for brain science, seattle, wa 98109, u.s.a.\n\neric shea-brown\netsb@u", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nphasic  dopamine  signals:  from  subjective  reward  value\nto  formal  economic  utility\nwolfram  schultz1,  regina  m  carelli2 and  r  mark  wightman3\n\nalthough  rewards  are  physical  stimuli  and  objects,  their  value\nfor  survival  and  reproduction  is  subjective.  the  phasic,\nneurophysiological  and  voltammetric  dopamine  reward\nprediction  error  response  signals  subjective  reward  value.\nthe  signal  incorporates  c", "chain-of-thought prompting elicits reasoning\n\nin large language models\n\njason wei\nbrian ichter\n\nxuezhi wang\n\ndale schuurmans\n\nfei xia\n\ned h. chi\n\nquoc v. le\n\nmaarten bosma\ndenny zhou\n\ngoogle research, brain team\n\n{jasonwei,dennyzhou}@google.com\n\nabstract\n\nwe explore how generating a chain of thought\u2014a series of intermediate reasoning\nsteps\u2014signi\ufb01cantly improves the ability of large language models to perform\ncomplex reasoning. in particular, we show how such reasoning abilities emerge\nnaturally ", "a r t i c l e s\n\ncomputational principles of synaptic memory \nconsolidation\nmarcus k benna1 & stefano fusi1,2\nmemories are stored and retained through complex, coupled processes operating on multiple timescales. to understand the \ncomputational principles behind these intricate networks of interactions, we construct a broad class of synaptic models that \nefficiently harness biological complexity to preserve numerous memories by protecting them against the adverse effects of \noverwriting. the mem", "\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\n\n\u0003\u0006\u0005\t\u0005\b\u0001\u000b\n\r\f\t\u000e\u0010\u000f\n\n\u0003\u0012\u0011\u0013\u000e\u0014\n\u0016\u0015\u0018\u0017\u0019\u0011\u001a\n\n\n\u001b\u0016\u001c\u001d\n\u0018\u001e\u001f\u001c \u0017\u0013\u000e\"! \u0017#\n\u0016\u0015\n\n\u001c$\u000f\n\n%&\u001c$\u001e'\u001e(\u001c$\u001e\n\n\u0003)\u0015\t\u000e+*,\n\u0016-.\u001e\u001f\u0017\n\n/1032547698:2<;\u0013=?>@8a0b;\rced\b0b;gfh8jie0lk3mn4hc\n\noqpqr\u0013;ts\tuwvx4n;hyqz[p\\2<f\rchoqpq=]re^w4_6(u\u0013`\u00128a;h8a;cbd4\\^\u0013egf\n\nh(8aij8aekbl8a2\u001fmnnhclmnnqnqn\n\npcqsr\u0013t\u0013u\u001fvxwxy\u001dr\u0013z{u|q\n\n}\u007f~\u0081\u0080\u0083\u0082{\u0084\u000b\u0085\u0004\u0086{~a\u0086{~j\u0087\u0089\u00885\u008a\u008c\u008b(\u008d\u0089\u008e\u008c\u0086\u0006\u008fa\u0090\u0092\u0091\u0092\u0093@\u0088\u0094\u00885~a\u008e'\u0095c\u00885~a\u00965\u0097\u0099\u0098,\u009a+\u008e\u0018\u009b\u0010\u0098,~\u008c\u009c\u009d\u0090\u009e\u008e\u008c\u0086\u0010\u009a\u00a0\u009f\u00a2\u00a1\u008c\u0086\u00a4\u00a3\u008c\u009a\u009d\u0098,\u0087\u008c\u0091\u0092\u0086{\u008b\u00a5\u00985\u0097\u0004~\u008c\u0098,~a\u00a6\u00a7\u0091\u009e\u0090\u009e~a\u0086?\u00885\u009a\n\u008e\u008c\u0090\u009e\u008b|\u0086{~\u008c\u009c\u009d\u0090\u009e\u0098,~\u008c\u00885\u0091\u009e\u0090\u00a8\u009f\u007f\u00a9\r\u009a+\u0086{\u008ea\u008a\u008c\u009b\u0010\u009f+\u0090\u009e\u0098,~q\u00aa\u000b\u008ea\u0090\u009e\u009c+\u009b\u0010\u0098\u00ab\u0093,\u0086{\u009a\u009d\u0090\u009e~\u008c\u0096\r\u0090\u0092~j\u009f+\u009a+\u0090\u0092~\u008c\u009c+\u0090\u0092\u009b?\u00885\u0091\u0092\u0091\u0092\u00a9\r\u0091", "published as a conference paper at iclr 2018\n\ndeep neural networks as gaussian processes\n\njaehoon lee\u2217\u2020, yasaman bahri\u2217\u2020, roman novak , samuel s. schoenholz,\njeffrey pennington, jascha sohl-dickstein\n\ngoogle brain\n{jaehlee, yasamanb, romann, schsam, jpennin, jaschasd}@google.com\n\nabstract\n\nit has long been known that a single-layer fully-connected neural network with an\ni.i.d. prior over its parameters is equivalent to a gaussian process (gp), in the limit\nof in\ufb01nite network width. this correspo", "letter\n\ncommunicated by bard ermentrout\n\nsynchrony of neuronal oscillations controlled by gabaergic\nreversal potentials\n\nho young jeong\njeonghy@cns.nyu.edu\ncenter for neural science, new york university, new york, ny 10003, u.s.a.\n\nboris gutkin\nboris.gutkin@ens.fr\ngroup for neural theory, dec, ens-paris; coll\u00b4ege de france, and ura 2169\nrecepteurs et cognition, institut pasteur, paris, france\n\ngabaergic synapse reversal potential is controlled by the concentration\nof chloride. this concentration", "neural networks 61 (2015) 85\u2013117\n\ncontents lists available at sciencedirect\n\nneural networks\n\njournal homepage: www.elsevier.com/locate/neunet\n\nreview\ndeep learning in neural networks: an overview\nj\u00fcrgen schmidhuber\nthe swiss ai lab idsia, istituto dalle molle di studi sull\u2019intelligenza artificiale, university of lugano & supsi, galleria 2, 6928 manno-lugano, switzerland\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\nin recent years, deep artificial neural networks (including recurrent ones) have won", "neuron\n\narticle\n\nsparseness and expansion\nin sensory representations\n\nbaktash babadi1 and haim sompolinsky1,2,*\n1swartz program in theoretical neuroscience, center for brain science, harvard university, cambridge, ma 02138, usa\n2edmond and lily safra center for brain sciences, hebrew university, jerusalem 91904, israel\n*correspondence: haim@\ufb01z.huji.ac.il\nhttp://dx.doi.org/10.1016/j.neuron.2014.07.035\n\nsummary\n\nin several sensory pathways, input stimuli project\nto sparsely active downstream popul", "6\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n4\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n0\n6\n4\n4\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nmodel-free episodic control\n\ncharles blundell\ngoogle deepmind\n\ncblundell@google.com\n\nbenigno uria\n\ngoogle deepmind\nburia@google.com\n\nalexander pritzel\ngoogle deepmind\n\napritzel@google.com\n\nyazhe li\n\ngoogle deepmind\nyazhe@google.com\n\navraham ruderman\n\ngoogle deepmind\n\naruderman@google.com\n\njoel z leibo\n\ngoogle deepmind\njzl@google.com\n\njack rae\n\ngoogle deepmind\njwrae@google.com\n\ndaan wierstra\ngoogle deepmi", "distinct eligibility traces for ltp and ltd in cortical\nsynapses\n\narticle\n\nhighlights\nd hebbian conditioning induces eligibility traces for ltp and\n\nltd in cortical synapses\n\nd b2ars and 5-ht2crs convert the traces into ltp and ltd,\n\nrespectively\n\nd anchoring of b2ars and 5-ht2c is key for trace conversion\n\nd temporal properties of the ltp/d traces allow reward-timing\n\nprediction\n\nauthors\n\nkaiwen he, marco huertas, su z. hong,\nxiaoxiu tie, johannes w. hell, harel\nshouval, alfredo kirkwood\n\ncorre", "4\n1\n0\n2\n\n \nc\ne\nd\n0\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n1\n0\n4\n5\n\n.\n\n0\n1\n4\n1\n:\nv\ni\nx\nr\na\n\nneural turing machines\n\nalex graves\ngreg wayne\nivo danihelka\n\ngravesa@google.com\ngregwayne@google.com\ndanihelka@google.com\n\ngoogle deepmind, london, uk\n\nabstract\n\nwe extend the capabilities of neural networks by coupling them to external memory re-\nsources, which they can interact with by attentional processes. the combined system is\nanalogous to a turing machine or von neumann architecture but is differentiable", "the journal of neuroscience, may 15, 1998, 18(10):3870\u20133896\n\nthe variable discharge of cortical neurons: implications for\nconnectivity, computation, and information coding\n\nmichael n. shadlen1 and william t. newsome2\n1department of physiology and biophysics and regional primate research center, university of washington, seattle,\nwashington 98195-7290, and 2howard hughes medical institute and department of neurobiology, stanford university\nschool of medicine, stanford, california 94305\n\ncortical ", "learnable latent embeddings for joint \nbehavioural and neural analysis\n\nin the format provided by the \nauthors and unedited\n\nnature | www.nature.com/naturesupplementary informationhttps://doi.org/10.1038/s41586-023-06031-6\f", "8\n1\n0\n2\n\n \n\ng\nu\na\n7\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n5\nv\n6\n3\n1\n5\n0\n\n.\n\n1\n1\n7\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2018\n\ndeep rewiring: training very sparse deep net-\nworks\n\nguillaume bellec, david kappel, wolfgang maass & robert legenstein\ninstitute for theoretical computer science\ngraz university of technology\naustria\n{bellec,kappel,maass,legenstein}@igi.tugraz.at\n\nabstract\n\nneuromorphic hardware tends to pose limits on the connectivity of deep networks\nthat one can run on them. but", "decoding the organization of spinal \ncircuits that control locomotion\n\nole kiehn\nabstract | unravelling the functional operation of neuronal networks and linking cellular activity \nto specific behavioural outcomes are among the biggest challenges in neuroscience. in this \nbroad\u00a0field of research, substantial progress has been made in studies of the spinal networks \nthat\u00a0control locomotion. through united efforts using electrophysiological and molecular \ngenetic network approaches and behavioural", "neuropsychologia 51 (2013) 2371\u20132388\n\ncontents lists available at sciencedirect\n\nneuropsychologia\n\njournal homepage: www.elsevier.com/locate/neuropsychologia\n\nmoderate levels of activation lead to forgetting\nin the think/no-think paradigm\n\ngreg j. detre 1, annamalai natarajan 1, samuel j. gershman, kenneth a. norman n\n\ndepartment of psychology and princeton neuroscience institute, princeton university, princeton, nj 08540, usa\n\na r t i c l e i n f o\n\na b s t r a c t\n\navailable online 7 march 201", "control of synaptic plasticity in deep \ncortical networks\n\npieter r.\u00a0roelfsema1,2,3* and anthony holtmaat4\nabstract | humans and many other animals have an enormous capacity to learn about sensory \nstimuli and to master new skills. however, many of the mechanisms that enable us to learn remain \nto be understood. one of the greatest challenges of systems neuroscience is to explain how \nsynaptic connections change to support maximally adaptive behaviour. here, we provide an \noverview of factors th", "b\ni\no\nl\n.\n \nc\ny\nb\ne\nr\nn\n.\n \n6\n6\n,\n \n2\n4\n1\n-\n2\n5\n1\n \n(\n1\n9\n9\n2\n)\n \nb\ni\no\nl\no\ng\ni\nc\na\nl\n \nc\ny\nb\ne\nr\nn\ne\nt\ni\nc\ns\n \n(cid:14)\n9\n \ns\np\nr\ni\nn\ng\ne\nr\n-\nv\ne\nr\nl\na\ng\n \n1\n9\n9\n2\n \no\nn\n \nt\nh\ne\n \nc\no\nm\np\nu\nt\na\nt\ni\no\nn\na\nl\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n \no\nf\n \nt\nh\ne\n \nn\ne\no\nc\no\nr\nt\ne\nx\n \ni\ni\n \nt\nh\ne\n \nr\no\nl\ne\n \no\nf\n \nc\no\nr\nt\ni\nc\no\n-\nc\no\nr\nt\ni\nc\na\nl\n \nl\no\no\np\ns\n \nd\n.\n \nm\nu\nm\nf\no\nr\nd\n \nm\na\nt\nh\ne\nm\na\nt\ni\nc\ns\n \nd\ne\np\na\nr\nt\nm\ne\nn\nt\n,\n \nh\na\nr\nv\na\nr\nd\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n,\n \n1\n \no\nx\nf\no\nr\nd\n \ns\nt\nr\ne\ne\nt\n,\n \nc\na\nm\nb\nr\ni", "research advance\n\nsequential neuromodulation of hebbian\nplasticity offers mechanism for effective\nreward-based navigation\nzuzanna brzosko1\u2020, sara zannone2\u2020, wolfram schultz1, claudia clopath2*\u2021,\nole paulsen1*\u2021\n\n1department of physiology, development and neuroscience, physiological\nlaboratory, cambridge, united kingdom; 2department of bioengineering, imperial\ncollege london, south kensington campus, london, united kingdom\n\nabstract spike timing-dependent plasticity (stdp) is under neuromodulatory", "dueling network architectures for deep reinforcement learning\n\n6\n1\n0\n2\n\n \nr\np\na\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n1\n8\n5\n6\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nziyu wang\ntom schaul\nmatteo hessel\nhado van hasselt\nmarc lanctot\nnando de freitas\ngoogle deepmind, london, uk\n\nabstract\n\nin recent years there have been many successes\nof using deep representations in reinforcement\nlearning. still, many of these applications use\nconventional architectures, such as convolutional\nnetworks, lstms, or auto-encoders. in ", "neuron\n\nreview\n\norienting and reorienting: the locus coeruleus\nmediates cognition through arousal\n\nsusan j. sara1,* and sebastien bouret2,*\n1laboratoire de physiologie de la perception et de l\u2019action, cnrs, umr-7152, colle` ge de france, 11 pl marcelin berthelot, 75005 paris,\nfrance and memolife laboratory of excellence, paris science & lettres research university, 75005 paris, france\n2team motivation brain & behavior, institut du cerveau et de la moelle e\u00b4 pinie` re, ho\u02c6 pital pitie\u00b4 -salpe\u02c6 tr", "reports\n\nfor helpful discussions and comments on the manu-\nscript. supported by nih grants ns09482 (j.c.m.)\nand ns11535, mh44754, and mh48432 (d.j.).\n\n17 september 1996; accepted 5 november 1996\n\nwas further depolarized by current injection\nto produce a burst of aps during the epsps,\nthen a persistent increase (\u2b0e20%) was ob-\nserved in 8 of 11 connections (fig. 1, c and\nd; 94 \u2afe 23% increase) (8, 9).\n\nto establish whether the occurrence of\npostsynaptic aps during epsps was indeed\ncritical for the ", "communicated by michael jordan \n\nthe helmholtz machine \n\npeter dayan \ngeoffrey e.  hinton \nradford m.  neal \ndepartment  of  computer science, university of  toronto, \n6 king\u2019s college road, toronto, ontario m5s 1a4, canada \n\nrichard s. zemel \ncnl, the salk institute, po box 85800, san diego, c a  92186-5800 usa \n\ndiscovering the structure inherent in a set of  patterns is a fundamen- \ntal aim of  statistical inference or learning.  one fruitful approach is to \nbuild a parameterized stochastic g", "biorxiv preprint \nthe copyright holder for this preprint (which was\nnot certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available \n\nthis version posted november 5, 2017. \n\nhttps://doi.org/10.1101/214262\n; \n\ndoi: \n\nunder a\n\ncc-by-nc-nd 4.0 international license\n.\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\na theory of multineuronal dimensionality,", "6\n1\n0\n2\n\n \nt\nc\no\n \n5\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n4\n5\n4\n8\n0\n\n.\n\n5\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nlinear dynamical neural population models through\n\nnonlinear embeddings\n\nyuanjun gao\u2217 1 , evan archer\u221712, liam paninski12, john p. cunningham12\n\ndepartment of statistics1 and grossman center2\n\ncolumbia university\n\nnew york, ny, united states\n\nyg2312@columbia.edu, evan@stat.columbia.edu,\nliam@stat.columbia.edu, jpc2181@columbia.edu\n\nabstract\n\na body of recent work in modeling neural activity focuses on r", "on fast deep nets for agi vision\n\nj\u00a8urgen schmidhuber, dan cires\u00b8an, ueli meier, jonathan masci, alex graves\n\nthe swiss ai lab idsia\n\nuniversity of lugano & supsi, switzerland\n\nmay 5, 2011\n\nabstract\n\narti\ufb01cial general intelligence will not be general without computer vision.\nbiologically inspired adaptive vision models have started to outperform traditional\npre-programmed methods: our fast deep / recurrent neural networks recently col-\nlected a string of 1st ranks in many important visual patter", "extending the effects of spike-timing-dependent\nplasticity to behavioral timescales\n\npatrick j. drew*\u2020 and l. f. abbott\u2021\n\n*neurobiology section, division of biology, university of california at san diego, 9500 gilman drive, la jolla, ca 92093-0357; and \u2021center of neurobiology\nand behavior, department of physiology and cellular biophysics, columbia university college of physicians and surgeons, kolb research annex,\n1051 riverside drive, new york, ny 10032-2695\n\nedited by charles f. stevens, the s", "denoising diffusion probabilistic models\n\njonathan ho\nuc berkeley\n\njonathanho@berkeley.edu\n\najay jain\nuc berkeley\n\najayj@berkeley.edu\n\npieter abbeel\nuc berkeley\n\npabbeel@cs.berkeley.edu\n\nabstract\n\nwe present high quality image synthesis results using diffusion probabilistic models,\na class of latent variable models inspired by considerations from nonequilibrium\nthermodynamics. our best results are obtained by training on a weighted variational\nbound designed according to a novel connection betwe", "neuroresource\n\nunsupervised discovery of demixed, low-\ndimensional neural dynamics across multiple\ntimescales through tensor component analysis\n\ngraphical abstract\n\nauthors\n\nalex h. williams, tony hyun kim,\nforea wang, ..., mark schnitzer,\ntamara g. kolda, surya ganguli\n\ncorrespondence\nahwillia@stanford.edu (a.h.w.),\nsganguli@stanford.edu (s.g.)\n\nin brief\nwilliams et al. describe an unsupervised\nmethod to uncover simple structure in\nlarge-scale recordings by extracting\ndistinct cell assemblies w", "\f", "\f", "reinforcement learning improves \nbehaviour from evaluative feedback\n\nmichael l. littman1\n\nreinforcement learning is a branch of machine learning concerned with using experience gained through interacting \nwith the world and evaluative feedback to improve a system\u2019s ability to make behavioural decisions. it has been called the \nartificial intelligence problem in a microcosm because learning algorithms must act autonomously to perform well and \nachieve their goals. partly driven by the increasing ", "articles\n\nvirtual reality for freely moving animals\njohn r stowers1,2, maximilian hofbauer1\u20134, renaud bastien5,6, johannes griessner1 \nsarfarazhussain farooqui3,4,7, ruth m fischer3, karin nowikovsky7, wulf haubensak1 \nkristin tessmar-raible3,4 \n\n  & andrew d straw1,8 \n\n \n\n , peter higgins1, \n\n , iain d couzin5,6, \n\nstandard animal behavior paradigms incompletely mimic \nnature and thus limit our understanding of behavior and brain \nfunction. virtual reality (vr) can help, but it poses challenges", "research article\npredictive representations can link model-\nbased reinforcement learning to model-free\nmechanisms\n\nevan m. russek1\u262f*, ida momennejad2\u262f, matthew m. botvinick3, samuel j. gershman4,\nnathaniel d. daw2\n\n1 center for neural science, new york university, new york, ny, united states of america, 2 princeton\nneuroscience institute and department of psychology, princeton university, princeton, nj, united states of\namerica, 3 deepmind, london, united kingdom and gatsby computational neurosc", "a r t i c l e s\n\nregulation of neuronal input transformations by \ntunable dendritic inhibition\nmatthew lovett-barron1, gergely f turi1, patrick kaifosh1, peter h lee2, fr\u00e9d\u00e9ric bolze3, xiao-hua sun3,  \njean-fran\u00e7ois nicoud3, boris v zemelman4, scott m sternson2 & attila losonczy1\n\ntransforming synaptic input into action potential output is a fundamental function of neurons. the pattern of action potential \noutput from principal cells of the mammalian hippocampus encodes spatial and nonspatial in", "7\n1\n0\n2\n\n \nl\nu\nj\n \n\n1\n1\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n4\n1\n9\n7\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nwhy do similarity matching objectives lead to\n\nhebbian/anti-hebbian networks?\n\ncengiz pehlevan1, anirvan m. sengupta1,2, and dmitri b.\n\nchklovskii1,3\n\n1center for computational biology, flatiron institute, new york,\n\nny\n\n2physics and astronomy department, rutgers university, new\n\nbrunswick, nj\n\n3nyu langone medical center, new york, ny\n\nabstract\n\nmodeling self-organization of neural networks for unsuper", "7\n1\n0\n2\n\n \n\nn\na\nj\n \n\n1\n2\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n6\n9\n5\n5\n0\n\n.\n\n2\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nneuromorphic deep learning machines\n\nemre neftci1, charles augustine3, somnath paul3, and georgios detorakis1\n\n1department of cognitive sciences, uc irvine, irvine, ca, usa,\n3circuit research lab, intel corporation, hilsboro, or, usa,\n\njanuary 24, 2017\n\nabstract\n\nan ongoing challenge in neuromorphic computing is to devise general and computationally e\ufb03cient\nmodels of inference and learning which are compatibl", " \n\na hebbian/anti-hebbian network for online sparse \ndictionary learning derived from symmetric matrix \n\nfactorization \n\ntao hu1, cengiz pehlevan2,3, and dmitri b. chklovskii3 \n\n1 texas a&m university \n\nms 3128 tamus  \n\ncollege station, tx 77843 \n\ntaohu@tees.tamus.edu \n\n2 janelia farm research campus \nhoward hughes medical institute \n\nashburn, va 20147 \n\npehlevanc@janelia.hhmi.org \n\n3 simons center for data analysis \n\nsimons foundation \nnew york, ny 10010 \n\nmitya@simonsfoundation.org \n\nimage  en", "insight review articles\n\ncomputational roles for dopamine \nin behavioural control\n\np. read montague1,2, steven e. hyman3 & jonathan d. cohen4,5\n\n1department of neuroscience and 2menninger department of psychiatry and behavioral sciences, baylor college of medicine, 1 baylor plaza,\nhouston, texas 77030, usa (e-mail: read@bcm.tmc.edu)\n3harvard university, cambridge, massachusetts 02138, usa (e-mail: seh@harvard.edu) \n4department of psychiatry, university of pittsburgh and 5department of psychology", "replay, the default mode \nnetwork and the cascaded memory \nsystems model\n\nkarola\u00a0kaefer \nfrancesco\u00a0p.\u00a0battaglia \n\n \n\n , federico\u00a0stella \n\n , bruce\u00a0l.\u00a0mcnaughton \n\n  and \n\nabstract | the spontaneous replay of patterns of activity related to past experiences \nand memories is a striking feature of brain activity, as is the coherent activation of \nsets of brain areas \u2014 particularly those comprising the default mode network \n(dmn) \u2014 during rest. we propose that these two phenomena are strongly \ninter", "letter\n\ncommunicated by douglas tweed\n\nequivalence of equilibrium propagation\nand recurrent backpropagation\n\nbenjamin scellier\nbenjamin.scellier@polytechnique.edu\nuniversity of montreal, montreal, quebec, h3t 1n8, canada\n\nyoshua bengio\nyoshua.bengio@mila.quebec\nuniversity of montreal, montreal, quebec, h3t 1n8, canada, and cifar\n\nrecurrent backpropagation and equilibrium propagation are supervised\nlearning algorithms for fixed-point recurrent neural networks, which dif-\nfer in their second phase", "16494 \u2022 the journal of neuroscience, november 16, 2011 \u2022 31(46):16494 \u201316506\n\nbehavioral/systems/cognitive\n\nsynaptic properties of corticocortical connections between\nthe primary and secondary visual cortical areas in the\nmouse\n\nroberto de pasquale and s. murray sherman\ndepartment of neurobiology, university of chicago, chicago, illinois 60637\n\ndespite the importance of corticocortical connections, few published studies have investigated the functional, synaptic properties of such\nconnections in", "7\n1\n0\n2\n\n \n\nn\na\nj\n \n\n3\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n3\n6\n7\n5\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nlearning to reinforcement learn\n\njx wang1, z kurth-nelson1, d tirumala1, h soyer1, jz leibo1,\nr munos1, c blundell1, d kumaran1,3, m botvinick1,2\n1deepmind, london, uk\n2gatsby computational neuroscience unit, ucl, london, uk\n3institute of cognitive neuroscience, ucl, london, uk\n\n{wangjane, zebk, dhruvat, soyer, jzl, munos, cblundell,\ndkumaran, botvinick} @google.com\n\nabstract\n\nin recent years deep reinforcem", "neurobiology of learning and memory 115 (2014) 68\u201377\n\ncontents lists available at sciencedirect\n\nneurobiology of learning and memory\n\nj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / y n l m e\n\nreview\nmodulation of learning and memory by cytokines: signaling\nmechanisms and long term consequences\n\nelissa j. donzis, natalie c. tronson\n\n\u21d1\n\ndepartment of psychology, university of michigan, ann arbor, mi 48109, usa\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle histo", "psychological  review\n2000, vol.  107, no.  2, 358-367\n\ncopyright 2000 by the american  psychological  association, inc.\n0033-295x/00/s5.0q  doi:  10.1037//0033-295x.107.2.358\n\ntheoretical notes\n\nhow  persuasive  is  a good  fit?  a comment on theory testing\n\nseth roberts\n\nuniversity  of  california,  berkeley\n\nharold pashler\n\nuniversity  of  california,  san  diego\n\nquantitative  theories  with  free  parameters  often  gain  credence  when they  closely  fit  data.  this  is  a\nmistake.  a  go", "communicated by david  haussler \n\nbayesian interpolation \n\ndavid j. c. mackay\u2019 \ncomputation and neural systems, california institute of  technology 139-74, \npasadena, ca 91225 usa \n\nalthough bayesian analysis has been in use since laplace, the bayesian \nmethod  of  model-comparison  has  only  recently  been  developed  in \ndepth.  in this  paper,  the  bayesian  approach  to  regularization  and \nmodel-comparison  is demonstrated  by  studying  the  inference  prob- \nlem of  interpolating noisy", "article\n\ncommunicated by jonathan victor\n\nestimation of entropy and mutual information\n\nliam paninski\nliam@cns.nyu.edu\ncenter for neural science, new york university, new york, ny 10003, u.s.a.\n\nwe present some new results on the nonparametric estimation of entropy\nand mutual information. first, we use an exact local expansion of the\nentropy function to prove almost sure consistency and central limit the-\norems for three of the most commonly used discretized information esti-\nmators. the setup i", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/9042578\n\nindependent rate and temporal coding in hippocampal pyramidal cells\n\narticle\u00a0\u00a0in\u00a0\u00a0nature \u00b7 november 2003\n\ndoi: 10.1038/nature02058\u00a0\u00b7\u00a0source: pubmed\n\ncitations\n536\n\n3 authors, including:\n\nneil burgess\nuniversity college london\n\n363 publications\u00a0\u00a0\u00a033,711 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n393\n\njohn o'keefe\nuniversity of toronto\n\n86 publications\u00a0\u00a0\u00a017,217 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nal", "gtm: the generative\ntopographic mapping\n\nchristopher m. bishop,\n\nmarkus svens(cid:19)en\n\nmicrosoft research\n\n7 j j thomson avenue\n\ncambridge, cb3 0fb, u.k.\n\nfcmbishop,markussvg@microsoft.com\n\nhttp://research.microsoft.com/f(cid:24)cmbishop,(cid:24)markussvg\n\nchristopher k. i. williams\n\ninstitute for adaptive and neural computation\ndivision of informatics, university of edinburgh\n\n5 forrest hill, edinburgh, eh1 2ql, scotland, u.k.\n\nckiw@dai.ed.ac.uk\n\npublished as: \"the generative topographic mapp", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\ntowards  the  neural  population  doctrine\nshreya  saxena  and  john  p  cunningham\n\nacross  neuroscience,  large-scale  data  recording  and\npopulation-level  analysis  methods  have  experienced  explosive\ngrowth.  while  the  underlying  hardware  and  computational\ntechniques  have  been  well  reviewed,  we  focus  here  on  the\nnovel  science  that  these  technologies  have  enabled.  we  detail\nfour  areas  of  the  \ufb01el", "journal of machine learning research 3 (2003) 1137\u20131155\n\nsubmitted 4/02; published 2/03\n\na neural probabilistic language model\n\nyoshua bengio\nr\u00e9jean ducharme\npascal vincent\nchristian jauvin\nd\u00e9partement d\u2019informatique et recherche op\u00e9rationnelle\ncentre de recherche math\u00e9matiques\nuniversit\u00e9 de montr\u00e9al, montr\u00e9al, qu\u00e9bec, canada\n\nbengioy@iro.umontreal.ca\nducharme@iro.umontreal.ca\nvincentp@iro.umontreal.ca\njauvinc@iro.umontreal.ca\n\neditors: jaz kandola, thomas hofmann, tomaso poggio and john shawe-t", "generation of stable heading \nrepresentations in diverse visual scenes\n\nhttps://doi.org/10.1038/s41586-019-1767-1\nreceived: 29 december 2018\naccepted: 7 october 2019\npublished online: 20 november 2019\n\nsung soo kim1,3,4*, ann m. hermundstad1, sandro romani1, l. f. abbott1,2 & vivek jayaraman1*\n\nmany animals rely on an internal heading representation when navigating in varied \nenvironments1\u201310. how this representation is linked to the sensory cues that define \ndifferent surroundings is unclear. i", "journal of physiology - paris 97 (2003) 683\u2013694\n\nwww.elsevier.com/locate/jphysparis\n\noptimal computation with attractor networks\n\npeter e. latham a,*, sophie deneve b, alexandre pouget c\n\na department of neurobiology, university of california at los angeles, los angeles, ca 90095-1763, usa\n\nb gatsby computational neuroscience unit, university college london, london wc1 3ar, uk\n\nc department of brain and cognitive sciences, university of rochester, rochester, ny 14627, usa\n\nabstract\n\nwe investiga", "_na~tu_r_e_v_o_l_. 3_2_3 _9_0_ct_o_b_e_r_1_98_6 __ ____ __ _  letterstonature - - - - - - - - - - - - - - - - - -=533  \n\ndelineating  the  absolute  indigeneity  of amino  acids  in  fossils. \nas  ams  techniques  are  refined  to  handle  smaller  samples,  it \nmay also become possible to date individual amino acid enan \ntiomers by the 14c method. if one enantiomer is entirely derived \nfrom the other by racemization during diagenesis, the individual \nd- and  l-enantiomers  for  a  given  amino ", "understanding batch normalization\n\njohan bjorck, carla gomes, bart selman, kilian q. weinberger\n\ncornell university\n\n{njb225,gomes,selman,kqw4} @cornell.edu\n\nabstract\n\nbatch normalization (bn) is a technique to normalize activations in intermediate\nlayers of deep neural networks.\nits tendency to improve accuracy and speed\nup training have established bn as a favorite technique in deep learning. yet,\ndespite its enormous success, there remains little consensus on the exact reason\nand mechanism be", "physical review x 8, 031003 (2018)\n\nclassification and geometry of general perceptual manifolds\n\nsueyeon chung,1,2,6 daniel d. lee,2,3 and haim sompolinsky2,4,5\n\n1program in applied physics, school of engineering and applied sciences, harvard university,\n\ncambridge, massachusetts 02138, usa\n\n2center for brain science, harvard university, cambridge, massachusetts 02138, usa\n\n3school of engineering and applied science, university of pennsylvania,\n\nphiladelphia, pennsylvania 19104, usa\n\n4racah inst", "the journal of neuroscience, november 1, 2002, 22(21):9475\u20139489\n\nresponse of neurons in the lateral intraparietal area during a\ncombined visual discrimination reaction time task\n\njamie d. roitman1 and michael n. shadlen2\n1program in neurobiology and behavior, and 2howard hughes medical institute, department of physiology and\nbiophysics, and regional primate research center, university of washington, seattle, washington 98195-7290\n\ndecisions about the visual world can take time to form, espe-\ncia", "original research\npublished: 31 august 2018\ndoi: 10.3389/fnins.2018.00608\n\ndeep supervised learning using\nlocal errors\n\nhesham mostafa 1*, vishwajith ramesh 2 and gert cauwenberghs 1,2\n\n1 institute for neural computation, university of california, san diego, san diego, ca, united states, 2 department of\nbioengineering, university of california, san diego, san diego, ca, united states\n\nerror backpropagation is a highly effective mechanism for\nlearning high-quality\nhierarchical features in deep ne", "10\n\ndistributedoptimizationofdeeplynestedsystemsmiguel\u00b4a.carreira-perpi\u02dcn\u00b4anweiranwangelectricalengineeringandcomputerscience,schoolofengineering,universityofcalifornia,mercedabstractintelligentprocessingofcomplexsignalssuchasimagesisoftenperformedbyahierarchyofnon-linearprocessinglayers,suchasadeepnetoranobjectrecognitioncascade.jointestimationoftheparametersofallthelayersisadif\ufb01cultnonconvexoptimization.wedescribeageneralstrategytolearntheparametersand,tosomeex-tent,thearchitectureofnestedsyst", "\f", "supplementary figure 1 \n\nlow-dimensional structure in the head direction circuit \n\neach column shows results from a different animal. (a) waking manifold visualized using isomap1. (b) waking manifold visualized using \na  variational  autoencoder2,3.  (c)  betti  barcodes  on  data  without  outlier  removal.  top  row  shows  betti-0  (number  of  connected \ncomponents); second row shows betti-1 (number of rings); third row shows betti-2 (number of three-dimensional holes or voids). there\nare a ", "gaussian-process factor analysis for low-dimensional\n\nsingle-trial analysis of neural population activity\n\nbyron m. yu1,2,4, john p. cunningham1, gopal santhanam1,\n\nstephen i. ryu1,3, krishna v. shenoy1,2\n\n1department of electrical engineering, 2neurosciences program,\n\n3department of neurosurgery, stanford university, stanford, ca 94305\n\n{byronyu,jcunnin,gopals,seoulman,shenoy}@stanford.edu\n\nmaneesh sahani4\n\n4gatsby computational neuroscience unit, ucl\n\nlondon, wc1n 3ar, uk\n\nmaneesh@gatsby.ucl.a", "0\n2\n0\n2\n\n \n\nn\na\nj\n \n\n9\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n7\n1\n0\n2\n0\n\n.\n\n1\n1\n8\n1\n:\nv\ni\nx\nr\na\n\na general theory of equivariant cnns on\n\nhomogeneous spaces\n\ntaco s. cohen\n\nqualcomm ai research\u2217\n\nmario geiger\n\npcsl research group\n\nqualcomm technologies netherlands b.v.\n\nepfl\n\ntacos@qti.qualcomm.com\n\nmario.geiger@epfl.ch\n\nmaurice weiler\n\nquva lab\n\nu. of amsterdam\nm.weiler@uva.nl\n\nabstract\n\nwe present a general theory of group equivariant convolutional neural networks\n(g-cnns) on homogeneous spaces such as ", "review\ntheories  of  error  back-propagation  in\nthe  brain\n\njames  c.r.  whittington1,2 and  rafal  bogacz1,*\n\nthis  review  article  summarises  recently  proposed  theories  on  how  neural\ncircuits  in  the  brain  could  approximate  the  error  back-propagation  algorithm\nused  by  arti\ufb01cial  neural  networks.  computational  models  implementing  these\ntheories  achieve  learning  as  ef\ufb01cient  as  arti\ufb01cial  neural  networks,  but  they  use\nsimple  synaptic  plasticity  rules  based  on", "neuropharmacology 52 (2007) 24e40\n\nwww.elsevier.com/locate/neuropharm\n\nthe late maintenance of hippocampal ltp: requirements, phases,\n\n\u2018synaptic tagging\u2019, \u2018late-associativity\u2019 and implications\n\nklaus g. reymann, julietta u. frey*\n\ndepartment for neurophysiology, leibniz institute for neurobiology, brenneckestrasse 6, d-39118 magdeburg, germany\n\nreceived 23 june 2006; received in revised form 14 july 2006; accepted 17 july 2006\n\nabstract\n\nour review focuses on the mechanisms which enable the late", "article \n\ncommunicated by peter dayan \n\nbiologically plausible error-driven learning \nusing local activation differences: \nthe generalized recirculation algorithm \n\nrandall c.  o\u2019reilly \ndepartment of  psychology, carnegie mellon university, pittsburgh, pa  15213 usa \n\nl\n\nd\no\nw\nn\no\na\nd\ne\nd\n\nthe error backpropagation  learning  algorithm  (bp) is generally  con- \nsidered biologically implausible because it does not use locally avail- \nable,  activation-based  variables.  a  version  of  bp  that ", "report\n\ngrid cells form a global representation of\nconnected environments\n\ngraphical abstract\n\nauthors\n\nfrancis carpenter, daniel manson, ...,\nneil burgess, caswell barry\n\ncorrespondence\nfrancis.carpenter.12@ucl.ac.uk (f.c.),\ncaswell.barry@ucl.ac.uk (c.b.)\n\nin brief\ngrid cells are thought to provide an\nef\ufb01cient metric for spatial navigation.\nhowever, this requires regular,\ncontinuous grid \ufb01ring patterns across the\nenvironment. carpenter et al. show that\ngrid patterns are initially determined by\n", "received: 2 january 2020 \ndoi: 10.1111/ejn.14745  \n\n|  revised: 23 march 2020 \n\n|  accepted: 25 march 2020\n\ns p e c i a l   i s s u e   r e v i e w\n\nthe credit assignment problem in cortico-basal ganglia-thalamic \nnetworks: a review, a problem and a possible solution\n\n \n\n|   catalina\u00a0vich2 \n\n|   matthew\u00a0clapp3 \n\njonathan e.\u00a0rubin1\ntimothy\u00a0verstynen3,5\n1department of mathematics, center for the neural basis of cognition, university of pittsburgh, pittsburgh, pa, usa\n2department de matem\u00e0tiques i ", "a. g. barto, r. s. sutton, and c. w. anderson, \u201cneuronlike adaptive elements that \ncan solve difficult learning control problems,\u201d   \nieee transactions on systems, man, and cybernetics, vol. smc-13, pp. 834\u2013846, \nsept./oct. 1983.\u00a0\n\n\f", "differentiable plasticity: training plastic neural networks with\n\nbackpropagation\n\nthomas miconi 1 jeff clune 1 kenneth o. stanley 1\n\n8\n1\n0\n2\n\n \nl\nu\nj\n \n\n1\n3\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n4\n6\n4\n2\n0\n\n.\n\n4\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nhow can we build agents that keep learning from\nexperience, quickly and ef\ufb01ciently, after their ini-\ntial training? here we take inspiration from the\nmain mechanism of learning in biological brains:\nsynaptic plasticity, carefully tuned by evolution\nto produce ef\ufb01cient ", "a unified model of nmda receptor-dependent\nbidirectional synaptic plasticity\n\nharel z. shouval*\u2020, mark f. bear*\u2021\u00a7, and leon n cooper*\u2021\u00b6\n\n*institute for brain and neural systems, departments of \u00b6physics and \u2021neuroscience, and \u00a7howard hughes medical institute, brown university,\nprovidence, ri 02912\n\ncontributed by leon n cooper, june 7, 2002\n\nsynapses in the brain are bidirectionally modi\ufb01able, but the routes of\ninduction are diverse. in various experimental paradigms, n-methyl-\nd-aspartate recept", "sn computer science (2021) 2:420 \nhttps://doi.org/10.1007/s42979-021-00815-1\n\nreview article\n\ndeep learning: a\u00a0comprehensive overview on\u00a0techniques, taxonomy, \napplications and\u00a0research directions\n\niqbal\u00a0h.\u00a0sarker1,2 \n\nreceived: 29 may 2021 / accepted: 7 august 2021 / published online: 18 august 2021 \n\u00a9 the author(s), under exclusive licence to springer nature singapore pte ltd 2021\n\nabstract\ndeep learning (dl), a branch of machine learning (ml) and artificial intelligence (ai) is nowadays consi", "5\n1\n0\n2\n\n \nr\np\na\n6\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n4\n1\n6\n6\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\naccepted as a workshop contribution at iclr 2015\n\nin search of the real inductive bias:\non the role of implicit regularization in deep\nlearning\n\nbehnam neyshabur, ryota tomioka & nathan srebro\ntoyota technological institute at chicago\nchicago, il 60637, usa\n{bneyshabur,tomioka,nati}@ttic.edu\n\nabstract\n\nwe present experiments demonstrating that some other form of capacity control,\ndifferent from network size, pl", "backpropagation and the brain\ntimothy\u00a0p.\u00a0lillicrap   , adam\u00a0santoro, luke\u00a0marris, colin\u00a0j.\u00a0akerman and geoffrey\u00a0hinton \nhttps://doi.org/10.1038/s41583-020-0277-3\n\nperspectivesnature reviews | neurosciencein format as provided by the authorssupplementary information\f", "review\n\n\u2018activity-silent\u2019  working  memory  in\nprefrontal  cortex:  a  dynamic  coding\nframework\n\nmark  g.  stokes\n\noxford  centre  for  human  brain  activity,  university  of  oxford,  oxford,  uk\n\nworking  memory  (wm)  provides  the  functional  back-\nbone  to  high-level  cognition.  maintenance  in  wm  is\noften  assumed  to  depend  on  the  stationary  persistence\nof  neural  activity  patterns  that  represent  memory  con-\ntent.  however,  accumulating  evidence  suggests  that\npersist", "spontaneous travelling cortical waves gate \nperception in behaving primates\n\nhttps://doi.org/10.1038/s41586-020-2802-y\nreceived: 19 february 2019\naccepted: 10 july 2020\npublished online: 7 october 2020\n\n check for updates\n\nzachary w. davis1,7\u2009\u2709, lyle muller1,2,3,4,7, julio martinez-trujillo3,5,6, terrence sejnowski1 &  \njohn h. reynolds1\u2009\u2709\n\nperceptual sensitivity varies from moment to moment. one potential source of this \nvariability is spontaneous fluctuations in cortical activity that can trav", "6\n1\n0\n2\n\n \nc\ne\nd\n \n3\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n9\n5\n0\n9\n0\n\n.\n\n9\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ntraining recurrent networks to generate hypotheses\nabout how the brain solves hard navigation problems\n\ningmar kanitscheider & ila fiete\n\ndepartment of neuroscience\n\nthe university of texas\n\naustin, tx 78712\n\nikanitscheider, ilafiete @mail.clm.utexas.edu\n\nabstract\n\nself-localization during navigation with noisy sensors in an ambiguous world is\ncomputationally challenging, yet animals and humans excel at", "communicated by garrison cottrell\n\ndimension reduction by local principal component analysis\n\nnandakishore kambhatla\ntodd k. leen\ndepartment of computer science and engineering, oregon graduate institute\nof science and technology, portland, oregon 97291-1000, u.s.a.\n\nreducing or eliminating statistical redundancy between the components\nof high-dimensional vector data enables a lower-dimensional represen-\ntation without signi\ufb01cant loss of information. recognizing the limita-\ntions of principal co", "article\nmastering the game of go with deep \nneural networks and tree search\n\ndavid silver1*, aja huang1*, chris j. maddison1, arthur guez1, laurent sifre1, george van den driessche1,  \njulian schrittwieser1, ioannis antonoglou1, veda panneershelvam1, marc lanctot1, sander dieleman1, dominik grewe1, \njohn nham2, nal kalchbrenner1, ilya sutskever2, timothy lillicrap1, madeleine leach1, koray kavukcuoglu1,  \nthore graepel1 & demis hassabis1\n\ndoi:10.1038/nature16961\n\nthe game of go has long been vie", "journal of machine learning research 6 (2005) 695{709\n\nsubmitted 11/04; revised 3/05; published 4/05\n\nestimation of non-normalized statistical models\n\nby score matching\n\naapo hyv(cid:127)arinen\nhelsinki institute for information technology (bru)\ndepartment of computer science\nfin-00014 university of helsinki, finland\n\neditor: peter dayan\n\naapo.hyvarinen@helsinki.fi\n\nabstract\n\none often wants to estimate statistical models where the probability density function is\nknown only up to a multiplicativ", "9\n1\n0\n2\n\n \n\nb\ne\nf\n6\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n2\n6\n1\n6\n0\n\n.\n\n2\n0\n9\n1\n:\nv\ni\nx\nr\na\n\n1\n\nself-supervised visual feature learning with\n\ndeep neural networks: a survey\n\nlonglong jing and yingli tian\u2217, fellow, ieee\n\nabstract\u2014large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual\nfeature learning from images or videos for computer vision applications. to avoid extensive cost of collecting and annotating\nlarge-scale datasets, a", "lettercommunicatedbytomheskeslearningcurvesforstochasticgradientdescentinlinearfeedforwardnetworksjustinwerfeljkwerfel@mit.edudepartmentofelectricalengineeringandcomputerscience,massachusettsinstituteoftechnology,cambridge,ma02139,u.s.a.xiaohuixiexhxie@mit.edubroadinstituteofmassachusettsinstituteoftechnologyandharvarduniversity,cambridge,ma02141,u.s.a.h.sebastianseungseung@mit.eduhowardhughesmedicalinstitute,departmentofbrainandcognitivesciences,massachusettsinstituteoftechnology,cambridge,ma02", "j neurophysiol 97: 4296 \u2013 4309, 2007.\nfirst published april 11, 2007; doi:10.1152/jn.00024.2007.\n\nobject category structure in response patterns of neuronal population in\nmonkey inferior temporal cortex\n\nroozbeh kiani,1,3 hossein esteky,1,2 koorosh mirpour,2 and keiji tanaka4,5\n1research group for brain and cognitive sciences, school of medicine, shaheed beheshti university, tehran, iran; 2school of cognitive\nsciences, institute for studies in theoretical physics and mathematics, niavaran, tehra", "6\n1\n0\n2\n\n \n\nb\ne\nf\n8\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n8\n0\n9\n5\n0\n\n.\n\n2\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ne\ufb03cient approaches for escaping higher order saddle\n\npoints in non-convex optimization\n\nanima anandkumar\u2217\n\nrong ge\u2020\n\nfebruary 19, 2016\n\nabstract\n\nlocal search heuristics for non-convex optimizations are popular in applied machine\nlearning. however, in general it is hard to guarantee that such algorithms even converge\nto a local minimum, due to the existence of complicated saddle point structures in high\ndime", "stochastic neighbor embedding\n\ngeoffrey hinton and sam roweis\n\ndepartment of computer science, university of toronto\n\n10 king\u2019s college road, toronto, m5s 3g5 canada\n\n\u0000 hinton,roweis\n\n@cs.toronto.edu\n\nabstract\n\nwe describe a probabilistic approach to the task of placing objects, de-\nscribed by high-dimensional vectors or by pairwise dissimilarities, in a\nlow-dimensional space in a way that preserves neighbor identities. a\ngaussian is centered on each object in the high-dimensional space and\nthe ", "neuron\n\narticle\n\nabrupt transitions between prefrontal neural\nensemble states accompany behavioral\ntransitions during rule learning\n\ndaniel durstewitz,1,4,* nicole m. vittoz,2,4 stan b. floresco,3 and jeremy k. seamans2,*\n1rg computational neuroscience, central institute of mental health and interdisciplinary center for neurosciences,\nuniversity of heidelberg, j 5, 68159 mannheim, germany\n2brain research centre, psychiatry, faculty of medicine, university of british columbia, vancouver, bc v6t 2", "j neurophysiol 129: 552\u2013580, 2023.\nfirst published february 8, 2023; doi:10.1152/jn.00454.2022\n\nreview\n\nnow and then\n\nhow our understanding of memory replay evolves\n\nzhe sage chen1,2,3,4 and matthew a. wilson5,6\n\n1department of psychiatry, new york university grossman school of medicine, new york, new york, united states;\n2department of neuroscience and physiology, new york university grossman school of medicine, new york, new york,\nunited states; 3neuroscience institute, new york university gro", "a r t i c l e s\n\nstimulus onset quenches neural variability:  \na widespread cortical phenomenon\n\nmark m churchland1,2,16, byron m yu1\u20133,16, john p cunningham1, leo p sugrue2,4, marlene r cohen2,4,  \ngreg s corrado2,4, william t newsome2,4,5, andrew m clark6, paymon hosseini6, benjamin b scott6,  \ndavid c bradley6, matthew a smith7, adam kohn8,9, j anthony movshon9, katherine m armstrong2,5,  \ntirin moore2,5, steve w chang10, lawrence h snyder10, stephen g lisberger11, nicholas j priebe12, ian m ", "neuroresource\n\nmapping sub-second structure in mouse behavior\n\nhighlights\nd computational modeling reveals structure in mouse behavior\n\nwithout observer bias\n\nd mouse behavior appears to be composed of stereotyped,\n\nsub-second modules\n\nauthors\n\nalexander b. wiltschko,\nmatthew j. johnson, giuliano iurilli, ...,\nvictoria e. abraira, ryan p. adams,\nsandeep robert datta\n\nd from this perspective, new behaviors result from altering\n\nboth modules and transitions\n\ncorrespondence\nsrdatta@hms.harvard.edu\n", "5\n1\n0\n2\n\n \nr\np\na\n9\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n9\n8\n9\n2\n\n.\n\n6\n0\n4\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2015\n\ntechniques for learning binary\nstochastic feedforward neural networks\n\ntapani raiko & mathias berglund\ndepartment of information and computer science\naalto university\nespoo, finland\n{tapani.raiko,mathias.berglund}@aalto.fi\n\nguillaume alain & laurent dinh\ndepartment of computer science and operations research\nuniversit\u00b4e de montr\u00b4eal\nmontr\u00b4eal, canada\nguillaume.ala", "torcs: the open racing car simulator\n\nbernhard wymann\n\ny\nandrew sumner\n\n(cid:3)\n\ny\nchristos dimitrakakis\nz\neric espi(cid:19)e\n\nchristophe guionneau\n\nz\n\nmarch 12, 2015\n\n1\n\nintroduction\n\nthe open racing car simulator (torcs [14]), is a modern, modular, highly-\nportable multi-player, multi-agent car simulator. its high degree of modularity\nand portability render it ideal for arti(cid:12)cial intelligence research. indeed, a num-\nber of research-oriented competitions and papers have already appeared", "journal \n\nof  complexity \n\n4,  216-245 \n\n(1988) \n\ndynamics  and architecture \n\nfor  neural  computation* \n\napplied \n\nphysics \n\nlaboratory, \n\nfernando \n\nj.  pineda \n\njohns \nlaurel, \n\nhopkins \nmaryland \n\nuniversity, \n20707 \n\nreceived  april,  1987 \n\njohns \n\nhopkins \n\nroad, \n\ntransformation \n\nthe  architectural \n\nand  autoassociative \n\nrecall.  backpropagation \n\nrecurrent  backpropagation \n\ncomponents  are  presented \n\ncomponents  which  perform \n\nfor  a  collective  nonlinear  dynamical \n\nuseful  ", "place cells may simply be memory cells: memory\ncompression leads to spatial tuning and history\ndependence\n\nmarcus k. bennaa,b,c,1,2\n\nand stefano fusia,b,d,1,2\n\nacenter for theoretical neuroscience, columbia university, new york, ny 10027; bmortimer b. zuckerman mind brain behavior institute, columbia\nuniversity, new york, ny 10027; cneurobiology section, division of biological sciences, university of california san diego, la jolla, ca 92093; and dkavli\ninstitute for brain sciences, columbia univ", "research article\n\ntowards deep learning with segregated\ndendrites\njordan guerguiev1,2, timothy p lillicrap3, blake a richards1,2,4*\n\n1department of biological sciences, university of toronto scarborough, toronto,\ncanada; 2department of cell and systems biology, university of toronto, toronto,\ncanada; 3deepmind, london, united kingdom; 4learning in machines and brains\nprogram, canadian institute for advanced research, toronto, canada\n\nabstract deep learning has led to significant advances in arti", "supporting information\nkirkpatrick et al. 10.1073/pnas.1611835114\nrandom patterns\nin this section we show that using ewc it is possible to recover\na power-law decay for the snr of random patterns. the task\nconsists of associating random n-dimensional binary vectors xt\nto a random binary output yt by learning a weight vector w . the\ncontinual-learning aspect of the problem arises from the fact that\nat time step i, only the ith pattern is accessible to the learning\nalgorithm. before providing a de", "6\n1\n0\n2\n\n \nt\nc\no\n1\n3\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n5\n3\n0\n0\n0\n\n.\n\n1\n1\n6\n1\n:\nv\ni\nx\nr\na\n\nfull-capacity unitary recurrent neural networks\n\nscott wisdom1\u2217, thomas powers1\u2217, john r. hershey2, jonathan le roux2, and les atlas1\n\n1 department of electrical engineering, university of washington\n\n{swisdom, tcpowers, atlas}@uw.edu\n\n2 mitsubishi electric research laboratories (merl)\n\n{hershey, leroux}@merl.com\n\nabstract\n\nrecurrent neural networks are powerful models for processing sequential data,\nbut ", "internally generated predictions enhance neural\nand behavioral detection of sensory stimuli in an\nelectric fish\n\narticle\n\nhighlights\nd negative images enhance neural coding of prey-like stimuli\n\nd negative images enhance behavioral detection of prey-like\n\nstimuli\n\nd disrupting synaptic plasticity impairs neural and behavioral\n\ndetection performance\n\nauthors\n\narmen g. enikolopov, l.f. abbott,\nnathaniel b. sawtell\n\ncorrespondence\nns2635@columbia.edu\n\nin brief\nstable and accurate perception require", "tensor programs iv:\n\nfeature learning in in\ufb01nite-width neural networks\n\ngreg yang 1 edward j. hu 2 3\n\nabstract\n\nas its width tends to in\ufb01nity, a deep neural\nnetwork\u2019s behavior under gradient descent can\nbecome simpli\ufb01ed and predictable (e.g. given\nby the neural tangent kernel (ntk)), if it\nis parametrized appropriately (e.g.\nthe ntk\nparametrization). however, we show that the stan-\ndard and ntk parametrizations of a neural net-\nwork do not admit in\ufb01nite-width limits that can\nlearn features, whic", "learning transferable visual models from natural language supervision\n\nalec radford * 1 jong wook kim * 1 chris hallacy 1 aditya ramesh 1 gabriel goh 1 sandhini agarwal 1\n\ngirish sastry 1 amanda askell 1 pamela mishkin 1 jack clark 1 gretchen krueger 1 ilya sutskever 1\n\n1\n2\n0\n2\n\n \n\nb\ne\nf\n6\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n0\n2\n0\n0\n0\n\n.\n\n3\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nstate-of-the-art computer vision systems are\ntrained to predict a \ufb01xed set of predetermined\nobject categories. this restricted for", "sliced score matching: a scalable approach to\n\ndensity and score estimation\n\n9\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n7\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n8\n8\n0\n7\n0\n\n.\n\n5\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nyang song\u2217\n\nstanford university\n\nsahaj garg\u2217\n\nstanford university\n\njiaxin shi\n\ntsinghua university\n\nstefano ermon\n\nstanford university\n\nabstract\n\nscore matching is a popular method for esti-\nmating unnormalized statistical models. how-\never, it has been so far limited to simple, shal-\nlow models or low-dimensional data, due to\nthe d", "9\n1\n0\n2\n\n \nl\nu\nj\n \n\n9\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n5\n9\n2\n0\n1\n\n.\n\n6\n0\n7\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2018\n\nnoisy networks for exploration\n\nmeire fortunato\u2217 mohammad gheshlaghi azar\u2217 bilal piot \u2217\n\njacob menick matteo hessel\n\nian osband alex graves volodymyr mnih\n\nremi munos demis hassabis olivier pietquin charles blundell\n\nshane legg\n\ndeepmind {meirefortunato,mazar,piot,\njmenick,mtthss,iosband,gravesa,vmnih,\nmunos,dhcontact,pietquin,cblundell,legg}@google.com\n\nabstract\n\nw", "opinion\n\ntrends in cognitive sciences vol.11 no.8\n\nuntangling invariant object\nrecognition\n\njames j. dicarlo and david d. cox\n\nmcgovern institute for brain research, and department of brain and cognitive sciences, massachusetts institute of technology,\ncambridge, ma 02139, usa\n\ndespite tremendous variation in the appearance of visual\nobjects, primates can recognize a multitude of objects,\neach in a fraction of a second, with no apparent effort.\nhowever, the brain mechanisms that enable this fund", "r e v i e w s\n\nhomeostatic plasticity in the\ndeveloping nervous system\n\ngina g. turrigiano and sacha b. nelson\n\nactivity has an important role in refining synaptic connectivity during development, in part through\n\u2018hebbian\u2019 mechanisms such as long-term potentiation and long-term depression. however,\nhebbian plasticity is probably insufficient to explain activity-dependent development because it\ntends to destabilize the activity of neural circuits. how can complex circuits maintain stable activity", "unsupervised learning of video representations using lstms\n\nnitish srivastava\nelman mansimov\nruslan salakhutdinov\nuniversity of toronto, 6 kings college road, toronto, on m5s 3g4 canada\n\nnitish@cs.toronto.edu\nemansim@cs.toronto.edu\nrsalakhu@cs.toronto.edu\n\n6\n1\n0\n2\n\n \n\nn\na\nj\n \n\n4\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n1\n8\n6\n4\n0\n\n.\n\n2\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe use multilayer long short term memory\n(lstm) networks to learn representations of\nvideo sequences. our model uses an encoder\nlstm to map an inp", "a uni\ufb01ed theory for the origin of grid cells through\n\nthe lens of pattern formation\n\nben sorscher*1, gabriel c. mel*2, surya ganguli1, samuel a. ocko1\n\n1department of applied physics, stanford university\n2neurosciences phd program, stanford university\n\nabstract\n\ngrid cells in the brain \ufb01re in strikingly regular hexagonal patterns across space.\nthere are currently two seemingly unrelated frameworks for understanding these\npatterns. mechanistic models account for hexagonal \ufb01ring \ufb01elds as the resul", "distributed  hierarchical  processing\nin the  primate  cerebral  cortex\n\ndaniel j.  felleman1 and  david c. van  essen2\n\n1 department  of neurobiology and anatomy,\nuniversity of texas medical  school, houston,  texas\n77030, and  2 division of biology,  california\ninstitute  of technology,  pasadena,  california  91125\n\nin  recent  years,  many  new  cortical  areas  have  been\nidentified in the macaque monkey. the number of iden-\ntified  connections  between  areas  has  increased  even\nmore dra", "8\n1\n0\n2\n\n \nr\na\n\n \n\nm\n8\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n0\n6\n7\n0\n1\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nunsupervised predictive memory in a\n\ngoal-directed agent\n\ngreg wayne\u2217,1, chia-chun hung\u2217,1, david amos\u2217,1, mehdi mirza1,\n\narun ahuja1, agnieszka grabska-barwi\u00b4nska1, jack rae1, piotr mirowski1,\n\njoel z. leibo1, adam santoro1, mevlana gemici1, malcolm reynolds1,\n\ntim harley1, josh abramson1, shakir mohamed1, danilo rezende1,\n\ndavid saxton1, adam cain1, chloe hillier1, david silver1,\n\nkoray kavukcuoglu1, matt ", "neural episodic control\n\n7\n1\n0\n2\n\n \nr\na\n\nm\n6\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n8\n8\n9\n1\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nalexander pritzel\nbenigno uria\nsriram srinivasan\nadri`a puigdom`enech\noriol vinyals\ndemis hassabis\ndaan wierstra\ncharles blundell\ndeepmind, london uk\n\napritzel@google.com\nburia@google.com\nsrsrinivasan@google.com\nadriap@google.com\nvinyals@google.com\ndemishassabis@google.com\nwierstra@google.com\ncblundell@google.com\n\nabstract\n\ndeep reinforcement\nlearning methods attain\nsuper-human perform", "3\n2\n0\n2\n\n \nr\na\n\nm\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n0\n1\n3\n3\n0\n\n.\n\n0\n1\n2\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2023\n\nscaling forward gradient with local losses\n\nmengye ren1\u2217, simon kornblith2, renjie liao3, geoffrey hinton2,4\n1nyu, 2google, 3ubc, 4vector institute\n\nabstract\n\nforward gradient learning computes a noisy directional gradient and is a biologi-\ncally plausible alternative to backprop for learning deep neural networks. however,\nthe standard forward gradient algorithm, w", "reliable evaluation of adversarial robustness with an ensemble of diverse \n\nparameter-free attacks \n\nfrancesco croce 1  matthias hein 1 \n\nabstract \n\nthe feld of defense strategies against adversarial \nattacks has signifcantly grown over the last years, \nbut progress is hampered as the evaluation of ad-\nversarial defenses is often insuffcient and thus \ngives a wrong impression of robustness.  many \npromising defenses could be broken later on, mak-\ning it diffcult to identify the state-of-the-art.", "published as a conference paper at iclr 2017\n\nrevisiting classifier two-sample tests\n\ndavid lopez-paz1, maxime oquab1,2\n1facebook ai research, 2willow project team, inria / ens / cnrs\ndlp@fb.com, maxime.oquab@inria.fr\n\nabstract\n\nthe goal of two-sample tests is to assess whether two samples, sp \u223c p n and sq \u223c\nqm, are drawn from the same distribution. perhaps intriguingly, one relatively\nunexplored method to build two-sample tests is the use of binary classi\ufb01ers. in\nparticular, construct a dataset", "letter\n\nhttps://doi.org/10.1038/s41586-018-0102-6\n\nvector-based navigation using grid-like \nrepresentations in artificial agents\n\nandrea banino1,2,3,5*, caswell barry2,5*, benigno uria1, charles blundell1, timothy lillicrap1, piotr mirowski1, alexander pritzel1, \nmartin j. chadwick1, thomas degris1, joseph modayil1, greg wayne1, hubert soyer1, fabio viola1, brian zhang1, ross goroshin1, \nneil rabinowitz1, razvan pascanu1, charlie beattie1, stig petersen1, amir sadik1, stephen gaffney1, helen kin", "journal of machine learning research 13 (2012) 281-305\n\nsubmitted 3/11; revised 9/11; published 2/12\n\nrandom search for hyper-parameter optimization\n\njames.bergstra@umontreal.ca\nyoshua.bengio@umontreal.ca\n\njames bergstra\nyoshua bengio\nd\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle\nuniversit\u00b4e de montr\u00b4eal\nmontr\u00b4eal, qc, h3c 3j7, canada\n\neditor: leon bottou\n\nabstract\n\ngrid search and manual search are the most widely used strategies for hyper-parameter optimiza-\ntion. this paper show", "the recurrent neural tangent kernel\n\nsina alemohammad, zichao wang, randall balestriero, richard g. baraniuk\n\ndepartment of electrical and computer engineering\n\nrice university\n\n{sa86,zw16,rb42,richb}@rice.edu\n\n1\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n5\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n6\n4\n2\n0\n1\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nthe study of deep neural networks (dnns) in the in\ufb01nite-width limit, via the\nso-called neural tangent kernel (ntk) approach, has provided new insights into the\ndynamics of learning, generali", "leading edge\n\nreview\n\nthe self-tuning neuron:  \nsynaptic scaling of excitatory synapses\n\ngina g. turrigiano1,*\n1department of biology, volen center for complex systems, and national center for behavioral genomics, brandeis university, waltham, \nma 02454, usa\n*correspondence: turrigiano@brandeis.edu\ndoi 10.1016/j.cell.2008.10.008\n\nhomeostatic  synaptic  scaling  is  a  form  of  synaptic  plasticity  that  adjusts  the  strength  of  all  of \na  neuron\u2019s  excitatory  synapses  up  or  down  to  s", "article\n\nhttps://doi.org/10.1038/s41467-019-08931-6\n\nopen\n\nhumans can decipher adversarial images\n\nzhenglong zhou\n\n1 & chaz firestone1\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\ndoes the human mind resemble the machine-learning systems that mirror its performance?\nconvolutional neural networks (cnns) have achieved human-level benchmarks in classifying\nnovel\nimages. these advances support technologies such as autonomous vehicles and\nmachine diagnosis; but beyond this, they serve as candidate models for huma", "research article\nscaling properties of dimensionality\nreduction for neural populations and\nnetwork models\n\nryan c. williamson1,2,3, benjamin r. cowley1,3, ashok litwin-kumar4, brent doiron1,5,\nadam kohn6,7,8, matthew a. smith1,9,10,11\u262f, byron m. yu1,12,13\u262f*\n\n1 center for the neural basis of cognition, carnegie mellon university, pittsburgh, pennsylvania, united\nstates of america, 2 school of medicine, university of pittsburgh, pittsburgh, pennsylvania, united states of\namerica, 3 department of m", "9\n1\n0\n2\n\n \n\nv\no\nn\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n6\n8\n6\n1\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\non extended long short-term memory and dependent\n\nbidirectional recurrent neural network\n\nyuanhang sua,\u2217, c.-c. jay kuoa\n\nauniversity of southern california, ming hsieh department of electrical engineering, 3740\n\nmcclintock avenue, los angeles, ca, united states\n\nabstract\n\nin this work, we \ufb01rst analyze the memory behavior in three recurrent neural\n\nnetworks (rnn) cells; namely, the simple rnn (srn), the long", "letter\ngrid cell symmetry is shaped by environmental\ngeometry\n\ndoi:10.1038/nature14153\n\njulija krupic1*, marius bauza1*, stephen burton1, caswell barry1 & john o\u2019keefe1,2\n\ngrid cells represent an animal\u2019s location by firing in multiple fields\narranged in a striking hexagonal array1. such an impressive and con-\nstant regularity prompted suggestions that grid cells represent a\nuniversal and environmental-invariant metric for navigation1,2. orig-\ninally the properties of grid patterns were believed", "high-resolution image synthesis with latent diffusion models\n\nrobin rombach1 \u2217\n\nandreas blattmann1 \u2217\n\ndominik lorenz1\n\npatrick esser\n\nbj\u00a8orn ommer1\n\n1ludwig maximilian university of munich & iwr, heidelberg university, germany\n\nrunway ml\n\nhttps://github.com/compvis/latent-diffusion\n\nabstract\n\ninput\n\nours (f = 4)\n\ndall-e (f = 8)\n\npsnr: 27.4 r-fid: 0.58\n\npsnr: 22.8 r-fid: 32.01\n\nvqgan (f = 16)\npsnr: 19.9 r-fid: 4.98\n\nby decomposing the image formation process into a se-\nquential application of den", "chicco biodata mining  (2017) 10:35 \ndoi 10.1186/s13040-017-0155-3\n\nreview\n\nopen access\n\nten quick tips for machine learning in\ncomputational biology\n\ndavide chicco\n\ncorrespondence:\ndavide.chicco@davidechicco.it\nprincess margaret cancer centre,\npmcr tower 11-401, 101 college\nstreet, m5g 1l7 toronto, ontario,\ncanada\n\nabstract\nmachine learning has become a pivotal tool for many projects in computational\nbiology, bioinformatics, and health informatics. nevertheless, beginners and\nbiomedical researc", "an introduction to locally linear embedding\n\nlawrence k. saul\n\nat&t labs \u2013 research\n\n180 park ave, florham park, nj 07932 usa\n\nlsaul@research.att.com\n\nsam t. roweis\n\ngatsby computational neuroscience unit, ucl\n\n17 queen square, london wc1n 3ar, uk\n\nroweis@gatsby.ucl.ac.uk\n\nabstract\n\nmany problems in information processing involve some form of dimension-\nality reduction. here we describe locally linear embedding (lle), an unsu-\npervised learning algorithm that computes low dimensional, neighborho", "spectrum dependent learning curves in kernel regression and wide neural\n\nnetworks\n\nblake bordelon 1 abdulkadir canatar 2 cengiz pehlevan 1 3\n\nabstract\n\nwe derive analytical expressions for the gener-\nalization performance of kernel regression as a\nfunction of the number of training samples us-\ning theoretical methods from gaussian processes\nand statistical physics. our expressions apply to\nwide neural networks due to an equivalence be-\ntween training them and kernel regression with\nthe neural ta", "neuron\n\narticle\n\ninhibitory stabilization of the cortical network\nunderlies visual surround suppression\n\nhirofumi ozeki,1 ian m. finn,1 evan s. schaffer,2 kenneth d. miller,2,3,* and david ferster1,3,*\n1department of neurobiology and physiology, northwestern university, evanston, il 60208, usa\n2center for theoretical neuroscience and department of neuroscience, columbia university, college of physicians and surgeons,\nnew york, ny 10032, usa\n3these authors contributed equally to this work\n*corres", "inferring synaptic conductances from spike trains\nunder a biophysically inspired point process model\n\nkenneth w. latimer\n\nthe institute for neuroscience\n\nthe university of texas at austin\n\nlatimerk@utexas.edu\n\ne. j. chichilnisky\n\ndepartment of neurosurgery\n\nhansen experimental physics laboratory\n\nstanford university\nej@stanford.edu\n\nfred rieke\n\ndepartment of physiology and biophysics\n\nhoward hughes medical institute\n\nuniversity of washington\n\nrieke@u.washington.edu\n\njonathan w. pillow\n\nprinceton", "remote explainability faces the bouncer problem\n\nerwan le merrer\u200a\n\n\u200a1\u2009\u2709 and gilles tr\u00e9dan2\u2009\u2709\n\nthe concept of explainability is envisioned to satisfy society\u2019s demands for transparency about machine learning decisions. \nthe concept is simple: like humans, algorithms should explain the rationale behind their decisions so that their fairness can be \nassessed. although this approach is promising in a local context (for example, the model creator explains it during debugging at \nthe time of training)", "intrinsic dimension of data representations in deep\n\nneural networks\n\ninternational school for advanced studies\n\ninternational school for advanced studies\n\nalessio ansuini\n\nalessioansuini@gmail.com\n\njakob h. macke\n\nmacke@tum.de\n\nalessandro laio\n\nlaio@sissa.it\n\ndavide zoccolan\n\nzoccolan@sissa.it\n\ntechnical university of munich\n\ninternational school for advanced studies\n\nabstract\n\ndeep neural networks progressively transform their inputs across multiple pro-\ncessing layers. what are the geometrica", "rstb.royalsocietypublishing.org\n\nreview\n\ncite this article: lisman j. 2017\nglutamatergic synapses are structurally and\nbiochemically complex because of multiple\nplasticity processes: long-term potentiation,\nlong-term depression, short-term potentiation\nand scaling. phil. trans. r. soc. b 372:\n20160260.\nhttp://dx.doi.org/10.1098/rstb.2016.0260\n\naccepted: 29 june 2016\n\none contribution of 16 to a discussion meeting\nissue \u2018integrating hebbian and homeostatic\nplasticity\u2019.\n\nsubject areas:\nneuroscienc", "r e v i e w s\n\nstate-dependent computations: \nspatiotemporal processing in  \ncortical networks\n\ndean v. buonomano* and wolfgang maass\u2021\n\nabstract | a conspicuous ability of the brain is to seamlessly assimilate and process spatial \nand temporal features of sensory stimuli. this ability is indispensable for the recognition of \nnatural stimuli. yet, a general computational framework for processing spatiotemporal \nstimuli remains elusive. recent theoretical and experimental work suggests that \nspati", "solving the problem of negative synaptic weights in\n\ncortical models\n\nchristopher parisien,1 charles h. anderson,2 chris eliasmith3\u2217\n\n1dept. of computer science, university of toronto,\n\ntoronto, on m5s 3g4, canada\n\n2dept. of anatomy and neurobiology, washington university school of medicine,\n\nst. louis, mo 63110, u.s.a.\n\n3centre for theoretical neuroscience, university of waterloo,\n\nwaterloo, on n2l 3g1, canada\n\n\u2217\n\nto whom correspondence should be addressed; e-mail: celiasmith@uwaterloo.ca\n\nin c", "visualizing and understanding convolutional networks\n\n3\n1\n0\n2\n\n \n\nv\no\nn\n8\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n3\nv\n1\n0\n9\n2\n\n.\n\n1\n1\n3\n1\n:\nv\ni\nx\nr\na\n\nmatthew d. zeiler\ndept. of computer science, courant institute, new york university\n\nrob fergus\ndept. of computer science, courant institute, new york university\n\nzeiler@cs.nyu.edu\n\nfergus@cs.nyu.edu\n\nabstract\n\nlarge convolutional network models have\nrecently demonstrated impressive classi\ufb01ca-\ntion performance on the imagenet bench-\nmark (krizhevsky et al.,", "this is the author version of the following paper published by nature on 27 may, 2015:\n\nghahramani, z. (2015) probabilistic machine learning and arti\ufb01cial intelligence. nature\n521:452\u2013459.\n\nprobabilistic machine learning and arti\ufb01cial intelligence\n\nzoubin ghahramani\n\nuniversity of cambridge\n\nmay 28, 2015\n\nhow can a machine learn from experience? probabilistic modelling provides a frame-\n\nwork for understanding what learning is, and has therefore emerged as one of the\n\nprincipal theoretical and p", "original research article\npublished: 22 march 2013\ndoi: 10.3389/fncir.2013.00037\n\nlearning and exploration in action-perception loops\ndaniel y. little 1 and friedrich t. sommer 2*\n\n1 department of molecular and cell biology, redwood center for theoretical neuroscience, university of california, berkeley, ca, usa\n2 redwood center for theoretical neuroscience, helen wills neuroscience institute, university of california, berkeley, ca, usa\n\nedited by:\nahmed el hady, max planck\ninstitute for dynamic", "12368 \u2022 the journal of neuroscience, december 7, 2016 \u2022 36(49):12368 \u201312384\n\nsystems/circuits\n\norientation selectivity from very sparse lgn inputs in a\ncomprehensive model of macaque v1 cortex\n\nx logan chariker,1 robert shapley,2 and lai-sang young1,2\n1courant institute of mathematical sciences, and 2center for neural science, new york university, new york, new york 10003\n\na new computational model of the primary visual cortex (v1) of the macaque monkey was constructed to reconcile the visual fu", "5\n1\n0\n2\n\n \nl\nu\nj\n \n\n2\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n4\n4\n5\n4\n0\n\n.\n\n5\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nsynapticsampling:aconnectionbetweenpspvariabilityanduncertaintyexplainsneurophysiologicalobservationslaurenceaitchisonandpetere.lathamjuly12,2015abstractwhenanactionpotentialistransmittedtoapostsynapticneuron,asmallchangeinthepostsynapticneuron\u2019smembranepotentialoccurs.thesesmallchanges,knownasapostsynapticpotentials(psps),arehighlyvariable,andcurrentmodelsassumethatthisvariabilityiscor-ruptingnoise.in", "o\ufb00-policy actor-critic\n\nthomas degris\nflowers team, inria, talence, ensta-paristech, paris, france\n\nthomas.degris@inria.fr\n\nmartha white\nrichard s. sutton\nrlai laboratory, department of computing science, university of alberta, edmonton, canada\n\nwhitem@cs.ualberta.ca\nsutton@cs.ualberta.ca\n\n3\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n9\n3\n8\n4\n\n.\n\n5\n0\n2\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nthis paper presents the \ufb01rst actor-critic al-\ngorithm for o\ufb00-policy reinforcement learning.\nour algorithm is onl", "ef\ufb01cientnet: rethinking model scaling for convolutional neural networks\n\nmingxing tan 1 quoc v. le 1\n\nabstract\n\nconvolutional neural networks (convnets) are\ncommonly developed at a \ufb01xed resource budget,\nand then scaled up for better accuracy if more\nresources are available.\nin this paper, we sys-\ntematically study model scaling and identify that\ncarefully balancing network depth, width, and res-\nolution can lead to better performance. based\non this observation, we propose a new scaling\nmethod th", "mathematics  of  computation,  volume  35,  number  151\njuly  1980,  pages  773-782\n\nupdating quasi-newton matrices\n\nwith limited storage\n\nby jorge nocedal\n\n  we  study  how  to  use  the  bfgs  quasi-newton  matrices \n\nabstract. \nminimization  methods \nformula  which  generates  matrices  using  information \nm  is  any  number  supplied  by  the  user.    the  quasi-newton  matrix \niteration \ntion.    it  is  shown \n\nfor  problems  where  the  storage  is  critical.    we  give  an  update\nthe ", "3\n1\n0\n2\n\n \n\nv\no\nn\n1\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n3\n6\n6\n6\n\n.\n\n5\n0\n3\n1\n:\nv\ni\nx\nr\na\n\ngeneralized denoising auto-encoders as generative\n\nmodels\n\nyoshua bengio, li yao, guillaume alain, and pascal vincent\n\nd\u00b4epartement d\u2019informatique et recherche op\u00b4erationnelle, universit\u00b4e de montr\u00b4eal\n\nabstract\n\nrecent work has shown how denoising and contractive autoencoders implicitly\ncapture the structure of the data-generating density, in the case where the cor-\nruption noise is gaussian, the reconstructi", "\f", "neural networks 16 (2003) 1353\u20131371\n\n2003 special issue\n\nwww.elsevier.com/locate/neunet\n\nmodeling the adaptive visual system: a survey of principled approaches\n\nlars schwabe*, klaus obermayer\n\ndepartment of computer science and electrical engineering, berlin university of technology, fr2-1, franklinstrasse 28/29, berlin 10587, germany\n\nreceived 2 december 2002; revised 28 july 2003; accepted 28 july 2003\n\nabstract\n\nmodeling the visual system can be done at multiple levels of description ranging ", "chaos in neuronal networks with balanced\n\nexcitatory and inhibitory activity\n\nc. van vreeswijk and h. sompolinsky\n\nneurons in the cortex of behaving animals show temporally irregular spiking patterns.\nthe origin of this irregularity and its implications for neural processing are unknown. the\nhypothesis that the temporal variability in the firing of a neuron results from an approx-\nimate balance between its excitatory and inhibitory inputs was investigated theoretically.\nsuch a balance emerges na", "corrected: publisher correction\n\npredictive and reactive reward signals conveyed \nby climbing fiber inputs to cerebellar purkinje cells\n\ndimitar kostadinov\u200a\n\n\u200a1*, maxime beau\u200a\n\n\u200a1, marta blanco-pozo\u200a\n\n\u200a1,2 and michael h\u00e4usser\u200a\n\n\u200a1*\n\nthere is increasing evidence for a cerebellar contribution to cognitive processing, but the specific input pathways conveying \nthis information remain unclear. we probed the role of climbing fiber inputs to purkinje cells in generating and evaluating \npredictions abo", "progress  in  neurobiology  103  (2013)  214\u2013222\n\ncontents  lists  available  at  sciverse  sciencedirect\n\nprogress  in  neurobiology\n\nj o  u  r n  a l  h o  m  e p a g  e :  w w  w . e l s  e v i e r  . c  o m  / l o  c a t  e / p n  e u  r o  b  i o\n\nfrom  \ufb01xed  points  to  chaos:  three  models  of  delayed  discrimination\n\nomri  barak a,*,  david  sussillo b,  ranulfo  romo c,g,  misha  tsodyks d,a,  l.f.  abbott a,e,f\na center  for  theoretical  neuroscience,  columbia  university,  new  yo", "letters to nature \n\n(academic,  new york,  1975). \n\n17. tolhurst, 0. j. & thompson, i. d. proc.  r.  soc.  land.  b 2 13,  183- 199 (1981 ). \n18.  de valois, r.  l.  &  de valois, k.  k.  spatial vision  (oxford  university press, oxford, 1988). \n19.  bonhoeffer,  t.  &  grinvald, a.  nat11re 353, 429~431 {1991). \n20.  movshon, j. a., thompson, i. d.  & tolhurst, d.  j.]. physiol. (lond. ) 283,  101-120 ( 1978). \n2 1.  shoham, d.  israel]. med. sci.  (aijstr.) 32, s7 ( 1996). \n22. tootell, r.  b", "levels4. our analysis shows that it is appropriate and necessary to\nattempt restoration on a global scale, and provides a benchmark\na\nagainst which community recovery could be assessed.\n\nmethods\ndata selection\nfor shelf communities, we compiled data from research trawl surveys from the southern\ngrand banks (43\u2013468 n, 49\u2013538 w) and saint pierre banks (45\u2013478 n, 55\u2013588 w) (ref. 28),\nthe gulf of thailand (9\u2013148 n, 100\u20131058 w) (ref. 29) and south georgia (53\u2013568 s,\n35\u2013408 w) (ref. 14). all other tra", "neuroimage 11, 805\u2013821 (2000)\ndoi:10.1006/nimg.2000.0582, available online at http://www.idealibrary.com on\n\nvoxel-based morphometry\u2014the methods\n\njohn ashburner and karl j. friston\n\nthe wellcome department of cognitive neurology, institute of neurology, queen square, london wc1n 3bg, united kingdom\n\nreceived october 22, 1999\n\nat its simplest, voxel-based morphometry (vbm) in-\nvolves a voxel-wise comparison of the local concentra-\ntion of gray matter between two groups of subjects.\nthe procedure ", "research | reports\n\nshore birds (22), and interdigital webbing has\nbeen reported in theropod dinosaurs (29).\n\nreduction of the pelvic girdle and hindlimb and\nthe concomitant enhancement of axial-powered\nlocomotion are common among semiaquatic\nvertebrates. the flexibility of the tail and the\nform of the neural spines in spinosaurus suggest\ntail-assisted swimming. like extinct and extant\nsemiaquatic reptiles, spinosaurus used lateral\nundulation of the tail, in contrast to the vertical\naxial undula", "nature  vol.  337  12  januar._.y_:c::l9\"'8'-9 _________  commentary----\n\n129 \n\nthe  recent  excitement  about  neural  networks \n\nfrancis  crick \n\nthe remarkable properties of some recent computer algorithms for neural networks seemed to promise \na fresh  approach  to  understanding  the  computational properties  of the  brain.  unfortunately  most  of \nthese  neural nets are  unrealistic  in  important respects. \n\nthere  has  been  a  lot  of  excitement \nrecently  about  neural  nets.  a  ne", "review article\n\nten simple rules for the computational\nmodeling of behavioral data\nrobert c wilson1,2\u2020*, anne ge collins3,4\u2020*\n\n1department of psychology, university of arizona, tucson, united states;\n2cognitive science program, university of arizona, tucson, united states;\n3department of psychology, university of california, berkeley, berkeley, united\nstates; 4helen wills neuroscience institute, university of california, berkeley,\nberkeley, united states\n\nabstract computational modeling of behav", "empirical models of spiking in neural populations\n\njakob h. macke\n\nlars b\u00a8using\n\ngatsby computational neuroscience unit\n\ngatsby computational neuroscience unit\n\nuniversity college london, uk\njakob@gatsby.ucl.ac.uk\n\nuniversity college london, uk\nlars@gatsby.ucl.ac.uk\n\njohn p. cunningham\n\ndepartment of engineering\nuniversity of cambridge, uk\n\njpc74@cam.ac.uk\n\nbyron m. yu\nece and bme\n\ncarnegie mellon university\n\nbyronyu@cmu.edu\n\nkrishna v. shenoy\n\ndepartment of electrical engineering\n\nstanford univ", "i an update to this article is included at the end\n\nreport\n\ntiming rules for synaptic plasticity matched to\nbehavioral function\n\nhighlights\nd synaptic plasticity rules are not uniform, but tuned to speci\ufb01c\n\ncircuit function\n\nauthors\n\naparna suvrathan, hannah l. payne,\njennifer l. raymond\n\nd different rules at different cerebellar parallel \ufb01ber-to-purkinje\n\ncell synapses\n\ncorrespondence\naparnasu@stanford.edu\n\nd synaptic plasticity can precisely compensate for circuit\n\ndelays of >100 ms\n\nd provide", "elifesciences.org\n\ntools and resources\n\nautomatic discovery of cell types and\nmicrocircuitry from neural connectomics\neric jonas1*, konrad kording2,3,4\n\n1department of electrical engineering and computer science, university of california,\nberkeley, berkeley, united states; 2department of physical medicine and rehabilitation,\nnorthwestern university, chicago, united states; 3department of physical medicine and\nrehabilitation, rehabilitation institute of chicago, chicago, united states; 4departmen", "neuroscience and biobehavioral reviews 96 (2019) 367\u2013400\n\ncontents lists available at sciencedirect\n\nneuroscience and biobehavioral reviews\n\njournal homepage: www.elsevier.com/locate/neubiorev\n\nreview article\ndo \u2018early\u2019 brain responses reveal word form prediction during language\ncomprehension? a critical review\nmante s. nieuwlanda,b,\u204e\na max planck institute for psycholinguistics, nijmegen, the netherlands\nb donders institute for brain, cognition and behaviour, nijmegen, the netherlands\n\nt\n\na b s", "emergence of a stable cortical map for neuroprosthetic\ncontrol\n\nkarunesh ganguly1,2,3, jose m. carmena1,2,4*\n\n1 department of electrical engineering and computer sciences, university of california berkeley, berkeley, california, united states of america, 2 helen wills neuroscience\ninstitute, university of california berkeley, berkeley, california, united states of america, 3 department of neurology, university of california san francisco, san francisco,\ncalifornia, united states of america, 4 pr", "neuroresource\n\nhigh-density, long-lasting, and multi-region\nelectrophysiological recordings using polymer\nelectrode arrays\n\nhighlights\nd modular polymer electrode-based system capable of\n\nrecording up to 1,024 channels\n\nd recording from 375 single units across multiple regions in\n\nfreely behaving rats\n\nd single-unit recording longevity for 160 or more days post-\n\nimplantation\n\nd system capable of tracking populations of single units\n\ncontinuously for over a week\n\nauthors\n\njason e. chung, hannah ", "neuron\n\nreview\n\ndecision making in recurrent neuronal circuits\n\nxiao-jing wang1,*\n1department of neurobiology and kavli institute for neuroscience, yale university school of medicine, 333 cedar street,\nnew haven, ct 06510, usa\n*correspondence: xjwang@yale.edu\ndoi 10.1016/j.neuron.2008.09.034\n\ndecision making has recently emerged as a central theme in neurophysiological studies of cognition, and\nexperimental and computational work has led to the proposal of a cortical circuit mechanism of element", "9\n1\n0\n2\n\n \n\np\ne\ns\n5\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n4\n0\n3\n1\n1\n\n.\n\n9\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nasymptotics of wide networks\n\nfrom feynman diagrams\n\nethan dyer\u2217\n\ngoogle\n\nguy gur-ari\u2217\n\ngoogle\n\nmountain view, ca\nedyer@google.com\n\nmountain view, ca\nguyga@google.com\n\nseptember 26, 2019\n\nabstract\n\nunderstanding the asymptotic behavior of wide networks is of considerable interest. in this work,\nwe present a general method for analyzing this large width behavior. the method is an adaptation\nof feynman diagram", "a convergence theory for deep learning via over-parameterization\n\nzeyuan allen-zhu * 1 yuanzhi li * 2 3 zhao song * 4 5 6\n\nabstract\n\ndeep neural networks (dnns) have demon-\nstrated dominating performance in many \ufb01elds;\nsince alexnet, networks used in practice are go-\ning wider and deeper. on the theoretical side, a\nlong line of works have been focusing on why\nwe can train neural networks when there is only\none hidden layer. the theory of multi-layer net-\nworks remains unsettled. in this work, we", "1\n2\n0\n2\n\n \n\np\ne\ns\n8\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n0\n3\n0\n4\n0\n\n.\n\n2\n1\n0\n2\n:\nv\ni\nx\nr\na\n\nstatistical mechanics of deep linear neural networks:\n\nthe back-propagating kernel renormalization\n\nqianyi li1,2and haim sompolinsky2,3,4\n\n1the biophysics program, harvard university, cambridge, ma 02138, usa\n2center for brain science, harvard university, cambridge, ma 02138, usa\n\n4racah institute of physics, hebrew university, jerusalem 91904, israel\n\n5edmond and lily safra center for brain sciences, hebrew", "ieee transactions on neural networks, vol. 22, no. 7, july 2011\n\n1149\n\nspectral clustering on multiple manifolds\n\nyong wang, yuan jiang, yi wu, and zhi-hua zhou, senior member, ieee\n\nabstract\u2014 spectral clustering (sc) is a large family of group-\ning methods that partition data using eigenvectors of an af\ufb01nity\nmatrix derived from the data. though sc methods have been\nsuccessfully applied to a large number of challenging clustering\nscenarios, it is noteworthy that they will fail when there are\nsig", "large-scale object classi\ufb01cation using label\n\nrelation graphs\n\njia deng\u2020\u2217, nan ding\u2217, yangqing jia\u2217, andrea frome\u2217, kevin murphy\u2217,\n\nsamy bengio\u2217, yuan li\u2217, hartmut neven\u2217, hartwig adam\u2217\n\nuniversity of michigan\u2020, google inc.\u2217\n\nabstract. in this paper we study how to perform object classi\ufb01cation in\na principled way that exploits the rich structure of real world labels. we\ndevelop a new model that allows encoding of \ufb02exible relations between\nlabels. we introduce hierarchy and exclusion (hex) graphs", "physiol rev 90: 1195\u20131268, 2010;\ndoi:10.1152/physrev.00035.2008.\n\nneurophysiological and computational principles of cortical\n\nrhythms in cognition\n\nxiao-jing wang\n\ndepartment of neurobiology and kavli institute of neuroscience, yale university school of medicine,\n\nnew haven, connecticut\n\ni. introduction\n\na. synchronization and stochastic neuronal activity in the cerebral cortex\nb. cortical oscillations associated with cognitive behaviors\nc. interplay between neuronal and synaptic dynamics\nd. or", "t e c h n i c a l   r e p o r t s\n\nrevealed no correlation between place field location and a cell\u2019s ana-\ntomical location8. by contrast, ieg studies have provided evidence for \nmicro-clustering of cells that were active in restricted places within \na larger environment5.\n\nfunctional imaging of hippocampal place cells at \ncellular resolution during virtual navigation\ndaniel a dombeck1, christopher d harvey1, lin tian2, loren l looger2 & david w tank1\nspatial navigation is often used as a behavio", "review\n\ncomputational rationality: a\nconverging paradigm for intelligence\nin brains, minds, and machines\n\nsamuel j. gershman,1* eric j. horvitz,2* joshua b. tenenbaum3*\n\nafter growing up together, and mostly growing apart in the second half of the 20th century,\nthe fields of artificial intelligence (ai), cognitive science, and neuroscience are\nreconverging on a shared view of the computational foundations of intelligence that\npromotes valuable cross-disciplinary exchanges on questions, methods, ", "letter\n\nhttps://doi.org/10.1038/s41586-019-1261-9\n\nspecialized coding of sensory, motor and cognitive \nvariables in vta dopamine neurons\n\nben engelhard1,2, joel finkelstein1,3, julia cox1, weston fleming1, hee jae jang1, sharon ornelas1, sue ann koay1,  \nstephan y. thiberge1,2, nathaniel d. daw1,3, david w. tank1,2 & ilana b. witten1,2,3*\n\nthere is increased appreciation that dopamine neurons in the \nmidbrain respond not only to reward1 and reward-predicting \ncues1,2, but also to other variables", "j neurophysiol 92: 780 \u2013789, 2004;\n10.1152/jn.01171.2003.\n\nprecision of spike trains in primate retinal ganglion cells\n\nv. j. uzzell1,2 and e. j. chichilnisky1,2\n1the salk institute, la jolla 92037; and 2university of california, san diego, california 92037\n\nsubmitted 8 december 2003; accepted in \ufb01nal form 22 february 2004\n\nuzzell, v. j. and e. j. chichilnisky. precision of spike trains in\nprimate retinal ganglion cells. j neurophysiol 92: 780 \u2013789, 2004;\n10.1152/jn.01171.2003. recent studies ha", "bayesian statistics and modelling\n\n 5, \n\n 1\u2009\u2709, sarah\u00a0depaoli2, ruth\u00a0king \n\n 3,4, bianca\u00a0kramer \n\n 7, marina\u00a0vannucci \n\n 8, andrew\u00a0gelman9, \n\n 1, joukje\u00a0willemsen \n\n 6, mahlet\u00a0g.\u00a0tadesse \n\n 1 and christopher\u00a0yau4,10\n\nrens\u00a0van de schoot \nkaspar\u00a0m\u00e4rtens \nduco\u00a0veen \nabstract | bayesian statistics is an approach to data analysis based on bayes\u2019 theorem, where \navailable knowledge about parameters in a statistical model is updated with the information in \nobserved data. the background knowledge is exp", "learning to learn with feedback and local plasticity\n\njack lindsey, ashok litwin-kumar\n\ncolumbia university, department of neuroscience\n\n{j.lindsey, a.litwin-kumar}@columbia.edu\n\nabstract\n\ninterest in biologically inspired alternatives to backpropagation is driven by the\ndesire to both advance connections between deep learning and neuroscience and\naddress backpropagation\u2019s shortcomings on tasks such as online, continual learn-\ning. however, local synaptic learning rules like those employed by th", "st01ch10-blei\n\nari\n\n4 december 2013\n\n17:0\n\nbuild, compute, critique,\nrepeat: data analysis with\nlatent variable models\ndavid m. blei\ncomputer science department, princeton university, princeton, new jersey 08540;\nemail: blei@cs.princeton.edu\n\nannu. rev. stat. appl. 2014. 1:203\u201332\n\nthe annual review of statistics and its application is\nonline at statistics.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev-statistics-022513-115657\ncopyright c(cid:2) 2014 by annual reviews.\nall rights reserved", "distributed representations of words and phrases\n\nand their compositionality\n\ntomas mikolov\n\ngoogle inc.\n\nmountain view\n\nilya sutskever\n\ngoogle inc.\n\nmountain view\n\nkai chen\ngoogle inc.\n\nmountain view\n\nmikolov@google.com\n\nilyasu@google.com\n\nkai@google.com\n\ngreg corrado\n\ngoogle inc.\n\nmountain view\n\njeffrey dean\ngoogle inc.\n\nmountain view\n\ngcorrado@google.com\n\njeff@google.com\n\nabstract\n\nthe recently introduced continuous skip-gram model is an ef\ufb01cient method for\nlearning high-quality distributed v", "this is the accepted manuscript made available via chorus. the article has been\n\npublished as:\n\nmachine learning conservation laws from trajectories\n\nziming liu and max tegmark\n\nphys. rev. lett. 126, 180604 \u2014 published  6 may 2021\n\ndoi: 10.1103/physrevlett.126.180604\n\n\f", "biologically plausible error-driven learning using local activation differences:\n\nthe generalized recirculation algorithm\n\nrandall c. o\u2019reilly\n\ndepartment of psychology\ncarnegie mellon university\n\npittsburgh, pa 15213\n\noreilly+@cmu.edu\n\njuly 1, 1996\n\nneural computation, 8:5, 895-938, 1996\n\n\f", "march  1994 \n\nlids-p-2237\n\nincremental  least  squares  methodsi\n\nand  the  extended  kalman  filter\n\nby\n\ndimitri p.  bertsekas2\n\nabstract\n\nin  this  paper  we  propose  and  analyze  nonlinear  least  squares  methods,  which  process  the  data  incre-\nmentally,  one  data  block  at  a  time.  such  methods  are  well  suited  for  large  data sets  and  real time  operation,\nand  have  received  much  attention  in  the  context  of  neural  network  training  problems.  we  focus  on  the\ne", "9\n1\n0\n2\n\n \nc\ne\nd\n8\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n4\nv\n0\n2\n7\n6\n0\n\n.\n\n2\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nwide neural networks of any depth evolve as\n\nlinear models under gradient descent\n\njaehoon lee\u2217, lechao xiao\u2217, samuel s. schoenholz, yasaman bahri\n\nroman novak, jascha sohl-dickstein, jeffrey pennington\n\ngoogle brain\n\n{jaehlee, xlc, schsam, yasamanb, romann, jaschasd, jpennin}@google.com\n\nabstract\n\na longstanding goal in deep learning research has been to precisely characterize\ntraining and generalization. ", "exact learning dynamics of deep linear networks with\n\nprior knowledge\n\nlukas braun \u2020,1\n\nlukas.braun@psy.ox.ac.uk\n\ncl\u00e9mentine c. j. domin\u00e9 \u2020,2\n\nclementine.domine.20@ucl.ac.uk\n\njames e. fitzgerald 3\n\nfitzgeraldj@janelia.hhmi.org\n\nandrew m. saxe 2,4,5\na.saxe@ucl.ac.uk\n\nabstract\n\nlearning in deep neural networks is known to depend critically on the knowledge\nembedded in the initial network weights. however, few theoretical results have\nprecisely linked prior knowledge to learning dynamics. here we d", "o p i n i o n\n\ndopamine reward prediction-\nerror signalling: a two-component \nresponse\n\nwolfram schultz\n\nabstract | environmental stimuli and objects, including rewards, are often \nprocessed sequentially in the brain. recent work suggests that the phasic \ndopamine reward prediction-error response follows a similar sequential pattern. \nan initial brief, unselective and highly sensitive increase in activity unspecifically \ndetects a wide range of environmental stimuli, then quickly evolves into th", "amplification of trial-to-trial\nresponse variability by neurons in visual cortex\n\nopen access, freely available online plos biology\n\nmatteo carandini*\n\nsmith-kettlewell eye research institute, san francisco, california, united states of america\n\nthe visual cortex responds to repeated presentations of the same stimulus with high variability. because the firing\nmechanism is remarkably noiseless, the source of this variability is thought to lie in the membrane potential\nfluctuations that result fro", "letters  to  nature \n\nrn \n+1 \n\nthe \n\nfig.  3  percentage  virus \ninduced  mortality  in  wild \ntype  (shaded  bars)  and \nrecombinant  (black  bars) \ntreatments  at  three  virus \ndoses,  sampled  on  four \noccasions:  a,  2  days;  b,  7 \ndays; c, 11 days, and d,  16 \ndays  after  release.  error \nbars  are  the  least signifi-\ncant  difference  between \ntreatments.  each \ntime \npoint was analysed separ \nately.  for \nfirst  two \ntimepoints,  mortality  var \nied  between  doses  (day \n2;  f = 15", " \n\nreview\n\nfeature\nwhat  learning  systems  do\nintelligent  agents  need?\ncomplementary  learning\nsystems  theory  updated\ndharshan  kumaran,1,2,*  demis  hassabis,1,3,*  and\njames  l.  mcclelland4,*\n\nwe  update  complementary  learning  systems  (cls)  theory,  which  holds  that\nintelligent  agents  must  possess  two  learning  systems,  instantiated  in  mamma-\nlians  in  neocortex  and  hippocampus.  the \n\ufb01rst  gradually  acquires  structured\nknowledge  representations  while  the  second  ", "predictive reward signal of dopamine neurons\n\nwolfram schultz\ninstitute of physiology and program in neuroscience, university of fribourg, ch-1700 fribourg, switzerland\n\ninvited review\n\nis called rewards, which elicit and reinforce approach behav-\nior. the functions of rewards were developed further during\nthe evolution of higher mammals to support more sophisti-\ncated forms of individual and social behavior. thus biologi-\ncal and cognitive needs de\ufb01ne the nature of rewards, and\nthe availability", "letter\n\ncommunicated by gal chechik\n\nreinforcement learning through modulation of\nspike-timing-dependent synaptic plasticity\n\nr\u02d8azvan v. florian\n\ufb02orian@coneural.org\ncenter for cognitive and neural studies (coneural), 400504 cluj-napoca, romania,\nand babes\u00b8-bolyai university, institute for interdisciplinary experimental research,\n400271 cluj-napoca, romania\n\nthe persistent modi\ufb01cation of synaptic ef\ufb01cacy as a function of the rela-\ntive timing of pre- and postsynaptic spikes is a phenomenon known ", "neural ordinary differential equations\n\nricky t. q. chen*, yulia rubanova*, jesse bettencourt*, david duvenaud\n\n{rtqichen, rubanova, jessebett, duvenaud}@cs.toronto.edu\n\nuniversity of toronto, vector institute\n\n9\n1\n0\n2\n\n \nc\ne\nd\n4\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n6\n6\n3\n7\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe introduce a new family of deep neural network models. instead of specifying a\ndiscrete sequence of hidden layers, we parameterize the derivative of the hidden\nstate using a neural network.", "a r t i c l e s\n\nhistory-dependent variability in population dynamics \nduring evidence accumulation in cortex\nari s morcos & christopher d harvey\nwe studied how the posterior parietal cortex combines new information with ongoing activity dynamics as mice accumulate \nevidence during a virtual navigation task. using new methods to analyze population activity on single trials, we found that activity \ntransitioned rapidly between different sets of active neurons. each event in a trial, whether an ev", "the forward-forward algorithm: some preliminary\n\ninvestigations\n\ngeoffrey hinton\n\ngoogle brain\n\ngeoffhinton@google.com\n\nabstract\n\nthe aim of this paper is to introduce a new learning procedure for neural networks\nand to demonstrate that it works well enough on a few small problems to be worth\nserious investigation. the forward-forward algorithm replaces the forward and\nbackward passes of backpropagation by two forward passes, one with positive\n(i.e. real) data and the other with negative data wh", "published as a conference paper at iclr 2018\n\nspherical cnns\n\ntaco s. cohen\u2217\nuniversity of amsterdam\n\nmario geiger\u2217\nepfl\n\njonas k\u00f6hler\u2217\nuniversity of amsterdam\n\nmax welling\nuniversity of amsterdam & cifar\n\nabstract\n\nconvolutional neural networks (cnns) have become the method of choice for\nlearning problems involving 2d planar images. however, a number of problems of\nrecent interest have created a demand for models that can analyze spherical images.\nexamples include omnidirectional vision for dro", "under review as a conference paper at iclr 2016\n\nunsupervised representation learning\nwith deep convolutional\ngenerative adversarial networks\n\nalec radford & luke metz\nindico research\nboston, ma\n{alec,luke}@indico.io\n\nsoumith chintala\nfacebook ai research\nnew york, ny\nsoumith@fb.com\n\nabstract\n\nin recent years, supervised learning with convolutional networks (cnns) has\nseen huge adoption in computer vision applications. comparatively, unsupervised\nlearning with cnns has received less attention.\ni", "manifold-tiling localized receptive fields are\n\noptimal in similarity-preserving neural networks\n\nanirvan m. sengupta\u2020\u2021\n\nmariano tepper\u2021\u21e4\n\ncengiz pehlevan\u2021\u21e4\n\nalexander genkin\u00a7\n\ndmitri b. chklovskii\u2021\u00a7\n\n\u2020rutgers university\n\n\u2021flatiron institute\n\n\u00a7nyu langone medical center\n\nanirvans@physics.rutgers.edu, alexander.genkin@gmail.com\n{mtepper,cpehlevan,dchklovskii}@flatironinstitute.org\n\nabstract\n\nmany neurons in the brain, such as place cells in the rodent hippocampus, have lo-\ncalized receptive \ufb01elds", "article\n\ndynamic control of response criterion in premotor\ncortex during perceptual detection under temporal\nuncertainty\n\nhighlights\nd a template-matching algorithm detects neural correlates of\n\nfalse alarm events\n\nd the subject\u2019s response criterion modulates over the course\n\nof a trial\n\nd the response criterion is represented by the dynamics of a\n\nneural population\n\nd a trained recurrent network unveils a mechanism for \ufb02exible\n\nresponse criterion\n\nauthors\n\nfederico carnevale,\nvictor de lafuente", "hypothesis and theory\npublished: 24 march 2016\ndoi: 10.3389/fnins.2016.00106\n\nbeliever-skeptic meets actor-critic:\nrethinking the role of basal ganglia\npathways during decision-making\nand reinforcement learning\n\nkyle dunovan 1, 2 and timothy verstynen 2, 3*\n\n1 department of psychology, university of pittsburgh, pittsburgh, pa, usa, 2 center for the neural basis of cognition,\nuniversity of pittsburgh and carnegie mellon university, pittsburgh, pa, usa, 3 department of psychology, carnegie mellon\n", "research article\n\nreward-based training of recurrent neural\nnetworks for cognitive and value-based\ntasks\nh francis song1, guangyu r yang1, xiao-jing wang1,2*\n\n1center for neural science, new york university, new york, united states; 2nyu-\necnu institute of brain and cognitive science, nyu shanghai, shanghai, china\n\nabstract trained neural network models, which exhibit features of neural activity recorded from\nbehaving animals, may provide insights into the circuit mechanisms of cognitive functio", ".\n\nd\ne\nv\nr\ne\ns\ne\nr\n \n\ns\nt\nh\ng\ni\nr\n \nl\nl\n\na\n\n \n.\n\nc\nn\n\ni\n \n,\n\na\nc\n\n \n\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n6\n1\n0\n2\n\u00a9\n\n \n\n \n\ng\np\nn\n\np e r s p e c t i v e  \n\nf o c u s   o n   n e u r a l   c o m p u t a t i o n   a n d   t h e o r y\n\nusing goal-driven deep learning models to understand \nsensory cortex\n\ndaniel l k yamins1,2 & james j dicarlo1,2\n\nfueled by innovation in the computer vision and artificial \nintelligence communities, recent developments in \ncomputational neuroscience have used goal-driven hierarchica", "b r i e f c o m m u n i c at i o n s\n\nreinforcement learning in\npopulations of spiking neurons\n\nrobert urbanczik & walter senn\n\npopulation coding is widely regarded as an important\nmechanism for achieving reliable behavioral responses despite\nneuronal variability. however, standard reinforcement learning\nslows down with increasing population size, as the global\nreward signal becomes less and less related to the performance\nof any single neuron. we found that learning speeds up with\nincreasing po", "on the importance of normalisation layers in deep learning with piecewise\n\nlinear activation units\n\narc centre of excellence for robotic vision, university of adelaide, australia \u2217\n\nzhibin liao and gustavo carneiro\n\n{zhibin.liao,gustavo.carneiro}@adelaide.edu.au\n\n5\n1\n0\n2\n\n \n\nv\no\nn\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n0\n3\n3\n0\n0\n\n.\n\n8\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\ndeep feedforward neural networks with piecewise lin-\near activations are currently producing the state-of-the-art\nresults in several public", "megatron-lm: training multi-billion parameter language models using\n\nmodel parallelism\n\nmohammad shoeybi 1 2 mostofa patwary 1 2 raul puri 1 2 patrick legresley 2 jared casper 2\n\nbryan catanzaro 2\n\n0\n2\n0\n2\n\n \nr\na\n\n \n\nm\n3\n1\n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n4\nv\n3\n5\n0\n8\n0\n\n.\n\n9\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nrecent work in language modeling demonstrates\nthat training large transformer models advances\nthe state of the art in natural language processing\napplications. however, very large models can be\nquite dif\ufb01", "cortical microcircuitry of performance monitoring\n\namirsaman\u00a0sajad, david\u00a0c.\u00a0godlove and jeffrey\u00a0d.\u00a0schall\u200a\n\n\u200a*\n\nthe medial frontal cortex enables performance monitoring, indexed by the error-related negativity (ern) and manifested by \nperformance adaptations. we recorded electroencephalogram over and neural spiking across all layers of the supplementary \neye field, an agranular cortical area, in monkeys performing a saccade-countermanding (stop signal) task. neurons signaling \nerror production,", "getting aligned on representational alignment\n\nilia sucholutsky\u2217\nprinceton university\n\nis2961@princeton.edu\n\nlukas muttenthaler\u2217\n\ngoogle deepmind; technische universit\u00e4t berlin\n\nlukas.muttenthaler@tu-berlin.de\n\n3\n2\n0\n2\n\n \nt\nc\no\n \n8\n1\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n8\n1\n0\n3\n1\n\n.\n\n0\n1\n3\n2\n:\nv\ni\nx\nr\na\n\nadrian weller\n\nuniversity of cambridge\n\nandi peng\n\nmit\n\nandreea bobu\nuc berkeley\n\nbeen kim\n\ngoogle deepmind\n\nbradley c. love\n\nucl\n\nerin grant\n\nucl\n\njascha achterberg\n\nuniversity of cambridge\n\njos", "matconvnet\n\nconvolutional neural networks for matlab\n\nandrea vedaldi\n\nkarel lenc\n\n6\n1\n0\n2\n\n \n\ny\na\nm\n5\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n3\nv\n4\n6\n5\n4\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\ni\n\n\f", "learning a latent manifold of odor representations\n\nfrom neural responses in piriform cortex\n\nanqi wu1\n\nstan l. pashkovski2\n\nsandeep robert datta2\n\njonathan w. pillow1\n\n1 princeton neuroscience institute, princeton university,\n\n{anqiw, pillow}@princeton.edu\n\n2 department of neurobiology, harvard medical school,\n\n{pashkovs, srdatta}@hms.harvard.edu\n\nabstract\n\na major dif\ufb01culty in studying the neural mechanisms underlying olfactory percep-\ntion is the lack of obvious structure in the relationship ", "journal of mathematical psychology 53 (2009) 139\u2013154\n\ncontents lists available at sciencedirect\n\njournal of mathematical psychology\n\njournal homepage: www.elsevier.com/locate/jmp\n\nreinforcement learning in the brain\nyael niv\u2217\n\npsychology department & princeton neuroscience institute, princeton university, united states\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\narticle history:\nreceived 9 june 2008\nreceived in revised form\n3 december 2008\navailable online 10 february 2009\n\na wealth of research fo", "[geoffrey hinton, li deng, dong yu, george e. dahl, abdel-rahman mohamed, navdeep jaitly, \nandrew senior, vincent vanhoucke, patrick nguyen, tara n. sainath, and brian kingsbury]\n\n[the shared views of four research groups]\n\ni\n\n \n\nt\na\np\nd\na\nt\nr\ne\nl\na\no\nh\nc\nu\ns\nm\no\nc\no\nt\no\nh\np\nk\nc\no\nt\ns\n\n.\n\n/\n\ni\n \n\n\u00a9\n\nfundamental technologies \nin modern speech recognition\n\nmost  current  speech  recognition  systems  use \n\nhidden  markov  models  (hmms)  to  deal  with \nthe  temporal  variability  of  speech  and ", "journal of mathematical psychology 56 (2012) 1\u201312\n\ncontents lists available at sciverse sciencedirect\n\njournal of mathematical psychology\n\njournal homepage: www.elsevier.com/locate/jmp\n\ntutorial\na tutorial on bayesian nonparametric models\nsamuel j. gershman a,\u2217, david m. blei b\n\na department of psychology and princeton neuroscience institute, princeton university, princeton nj 08540, usa\nb department of computer science, princeton university, princeton nj 08540, usa\n\na r t i c l e\n\ni n f o\n\na b ", "a r t i c l e s\n\ninformation-limiting correlations\nrub\u00e9n moreno-bote1,2, jeffrey beck3, ingmar kanitscheider4, xaq pitkow3,5,6, peter latham7 &  \nalexandre pouget3,4,7\ncomputational strategies used by the brain strongly depend on the amount of information that can be stored in population \nactivity, which in turn strongly depends on the pattern of noise correlations. in vivo, noise correlations tend to be positive and \nproportional to the similarity in tuning properties. such correlations are tho", "5\n1\n0\n2\n\n \nl\nu\nj\n \n7\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n0\n8\n5\n7\n0\n\n.\n\n7\n0\n5\n1\n:\nv\ni\nx\nr\na\n\ncomputational principles of biological memory\n\nmarcus k. benna1, stefano fusi1\u2217\n\n1center for theoretical neuroscience, columbia university,\n\ncollege of physicians and surgeons, new york, ny 10032, usa\n\n\u2217to whom correspondence should be addressed; e-mail: sf2237@columbia.edu.\n\njuly 29, 2015\n\nmemories are stored, retained, and recollected through complex, coupled processes\noperating on multiple timescales", "0\n2\n0\n2\n\n \n\ny\na\nm\n0\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n4\nv\n6\n6\n7\n0\n1\n\n.\n\n1\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nsynaptic plasticity dynamics for deep continuous local learning\n\n(decolle)\n\nfzi research center for information technology\n\njacques kaiser,\n\nkarlsruhe, germany\n\nhesham mostafa,\n\ndepartment of bioengineering\n\nuniversity of california san diego\n\nla jolla, usa\n\nemre neftci,\n\ndepartment of cognitive sciences\ndepartment of computer science\n\nuniversity of california irvine, irvine, usa\n\nmay 22, 2020\n\nabstract\n\na gr", "research article\n\na general decoding strategy explains \nthe relationship between behavior and \ncorrelated\u00a0variability\namy m ni1,2*, chengcheng huang1,2,3, brent doiron2,3, marlene r cohen1,2\n\n1department of neuroscience,university of pittsburgh, pittsburgh, united states; \n2center for the neural basis of cognition, pittsburgh, united states; 3department of \nmathematics, university of pittsburgh, pittsburgh, united states\n\nabstract improvements in perception are frequently accompanied by decrease", "the code for facial identity in the primate brain\n\narticle\n\ngraphical abstract\n\nauthors\nle chang, doris y. tsao\n\ncorrespondence\nlechang@caltech.edu (l.c.),\ndortsao@caltech.edu (d.y.t.)\n\nin brief\nfacial identity is encoded via a\nremarkably simple neural code that relies\non the ability of neurons to distinguish\nfacial features along speci\ufb01c axes in face\nspace, disavowing the long-standing\nassumption that single face cells encode\nindividual faces.\n\nhighlights\nd facial images can be linearly reconst", "article\n\nuncovering spatial representations from\nspatiotemporal patterns of rodent hippocampal \ufb01eld\npotentials\n\ngraphical abstract\n\nauthors\n\na lfp feature extraction methods\n\nb spatiotemporal patterns estimation\n\nraw lfp \ntraces\n\n4-12 hz\nbandpass \n\n300 hz\nhighpass \n\ndemodulation\n\noptical flow\n\nhilbert tranform\n\noptical flow\n\nra w tra ce s\n\nc position dependent\nspatiotemporal patterns\n\nattern s\n\nw  p\n\nptic al flo\n\no\n\nliang cao, viktor varga, zhe s. chen\n\ncorrespondence\nzhe.chen@nyulangone.org\n\nin", "journal of machine learning research 8 (2007) 2169-2231\n\nsubmitted 6/06; revised 3/07; published 10/07\n\nproto-value functions: a laplacian framework for learning\n\nrepresentation and control in markov decision processes\n\nsridhar mahadevan\ndepartment of computer science\nuniversity of massachusetts\namherst, ma 01003, usa\n\nmauro maggioni\ndepartment of mathematics and computer science\nduke university\ndurham, nc 27708\n\neditor: carlos guestrin\n\nmahadeva@cs.umass.edu\n\nmauro.maggioni@duke.edu\n\nabstract\n\n", "volume 65, number 13\n\nphysical review letters\n\n24 september 1990\n\nlearning from examples in large neural networks\n\nh. sompolinsky\n\n' and n. tishby\n\nat& t bell laboratories, murray hill,\n\n/ver jersey 07974\n\ndepartment of physics, harvard university, cambridge, massachusetts 02t38\n\n(received 29 may 1990)\n\nh. s. seung\n\na statistical mechanical\n\ntheory of learning from examples\n\nin layered networks\n\nat finite temperature\n\nis\n\nstudied. when the training error is a smooth function of continuously\nerro", "r e v i e w s\n\noptimal feedback control and\nthe neural basis of volitional\nmotor control\n\nstephen h. scott\n\nskilled motor behaviour, from the graceful leap of a ballerina to a precise pitch by a baseball\nplayer, appears effortless but reflects an intimate interaction between the complex mechanical\nproperties of the body and control by a highly distributed circuit in the cns. an important\nchallenge for understanding motor function is to connect these three levels of the motor system\n\u2014 motor behav", "the journal of neuroscience, july 8, 2015 \u2022 35(27):10005\u201310014 \u2022 10005\n\nbehavioral/cognitive\n\ndeep neural networks reveal a gradient in the complexity\nof neural representations across the ventral stream\n\numut gu\u00a8c\u00b8lu\u00a8 and marcel a. j. van gerven\nradboud university, donders institute for brain, cognition and behaviour, nijmegen, the netherlands\n\nconverging evidence suggests that the primate ventral visual pathway encodes increasingly complex stimulus features in downstream\nareas. we quantitativel", "towards biologically plausible deep learning\n\nyoshua bengio1, dong-hyun lee, jorg bornschein, thomas mesnard and zhouhan lin\nmontreal institute for learning algorithms, university of montreal, montreal, qc, h3c 3j7\n1cifar senior fellow\n\n6\n1\n0\n2\n\n \n\ng\nu\na\n9\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n6\n5\n1\n4\n0\n\n.\n\n2\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nneuroscientists have long criticised deep learn-\ning algorithms as incompatible with current\nknowledge of neurobiology. we explore more bi-\nologically plausible versi", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nwith  or  without  you:  predictive  coding  and  bayesian\ninference  in  the  brain\nlaurence  aitchison1 and  ma\u00b4 te\u00b4 lengyel1,2\n\ntwo  theoretical  ideas  have  emerged  recently  with  the  ambition\nto  provide  a  unifying  functional  explanation  of  neural  population\ncoding  and  dynamics:  predictive  coding  and  bayesian\ninference.  here,  we  describe  the  two  theories  and  their\ncombination  into  a  single  fram", "inception loops discover what excites neurons \nmost using deep predictive models\n\nedgar y. walker\u200a\nemmanouil froudarakis\u200a\nxaq pitkow\u200a\n\n\u200a1,2,7 and andreas s. tolias\u200a\n\n\u200a1,2,7*\n\n\u200a1,2,8*, fabian h. sinz\u200a\n\n\u200a1,2,3,4,8*, erick cobos1,2, taliah muhammad1,2, \n\n\u200a1,2, paul g. fahey1,2, alexander s. ecker\u200a\n\n\u200a1,3,5,6, jacob reimer1,2, \n\nfinding sensory stimuli that drive neurons optimally is central to understanding information processing in the brain. however, \noptimizing sensory input is difficult due to t", "gradient-based learning drives robust \nrepresentations in recurrent neural networks by \nbalancing compression and expansion\n\nmatthew farrell\u200a\neric shea-brown1,2,5\n\n\u200a1,2,6\u2009\u2709, stefano recanatesi2, timothy moore2, guillaume lajoie3,4 and \n\nneural networks need the right representations of input data to learn. here we ask how gradient-based learning shapes a fun-\ndamental property of representations in recurrent neural networks (rnns)\u2014their dimensionality. through simulations and \nmathematical analy", "ieee transactions on automatic control, vol. 46, no. 4, april 2001\n\n613\n\n[12]\n\n, \u201cfrequency response of sampled-data systems ii: closed-loop\n\nconsiderations,\u201d in proc. ifac world congr., 1993, pp. 263\u2013266.\n\n[13] y. yamamoto and p. p. khargonekar, \u201cfrequency response of sam-\npled-data systems,\u201d ieee trans. automat. contr., vol. 41, pp. 166\u2013176,\nfeb. 1996.\n\n[14] p. gahinet, a. nemirovski, a. j. laub, and m. chilali, \u201cthe lmi control\ntoolbox,\u201d in proc. third european control conf., 1995, pp. 3206\u20133", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/299416173\n\ndiversity in neural \ufb01ring dynamics supports both rigid and learned\nhippocampal sequences\n\narticle\u00a0\u00a0in\u00a0\u00a0science \u00b7 march 2016\n\ndoi: 10.1126/science.aad1935\n\ncitations\n243\n\n2 authors:\n\nandres grosmark\nuconn health center\n\n29 publications\u00a0\u00a0\u00a01,786 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n195\n\ngyorgy buzs\u00e1ki\nnyu langone medical center\n\n510 publications\u00a0\u00a0\u00a0106,444 citations\u00a0\u00a0\u00a0\n\nsee profile\n\ns", "article\n\ndoi: 10.1038/s41467-017-02717-4\n\nopen\n\ngeneralized leaky integrate-and-\ufb01re models\nclassify multiple neuron types\n\ncorinne teeter\naaron szafer1, nicholas cain\n\n1, ramakrishnan iyer\n\n1, vilas menon1,2, nathan gouwens1, david feng\n\n1, jim berg1,\n\n1, hongkui zeng1, michael hawrylycz1, christof koch\n\n1 & stefan mihalas\n\n1\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\nthere is a high diversity of neuronal types in the mammalian neocortex. to facilitate con-\nstruction of system models with multiple cell typ", "2\n2\n0\n2\n\n \nt\nc\no\n1\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n4\n6\n3\n0\n0\n\n.\n\n6\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nelucidating the design space of diffusion-based\n\ngenerative models\n\ntero karras\n\nnvidia\n\nmiika aittala\n\nnvidia\n\ntimo aila\nnvidia\n\nsamuli laine\n\nnvidia\n\nabstract\n\nwe argue that the theory and practice of diffusion-based generative models are\ncurrently unnecessarily convoluted and seek to remedy the situation by presenting\na design space that clearly separates the concrete design choices. this lets us\nidentify ", "6\n1\n0\n2\n\n \nc\ne\nd\n7\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n0\n1\n1\n7\n0\n\n.\n\n5\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ndeep learning without poor local minima\n\nkenji kawaguchi\n\nmassachusetts institute of technology\n\nkawaguch@mit.edu\n\nabstract\n\nin this paper, we prove a conjecture published in 1989 and also partially address\nan open problem announced at the conference on learning theory (colt) 2015.\nwith no unrealistic assumption, we \ufb01rst prove the following statements for the\nsquared loss function of deep linear neural ne", "open source tools and methods\n\nnovel tools and methods\n\npsychrnn: an accessible and flexible python\npackage for training recurrent neural network\nmodels on cognitive tasks\n\ndaniel b. ehrlich,1,p jasmine t. stone,2,p david brandfonbrener,2,3 alexander atanasov,4,5 and\njohn d. murray1,4,6\n\nhttps://doi.org/10.1523/eneuro.0427-20.2020\n\n1interdepartmental neuroscience program, yale university, new haven, ct 06520-8074, 2department of computer\nscience, yale university, new haven, ct 06520-8285, 3depar", "j.  phys. a:  math. gen. 22  (1989) 2181-2190.  printed in the u k  \n\nthe space of  interactions in neural networks: gardner\u2019s \ncomputation with the cavity method \n\nmarc mczard \nlaboratoire  de physique  theorique de  i\u2019ecole  normale  superieuret, 24  rue  lhomond, \n75231 paris cedex 05, france \n\nreceived  16 january  1989 \n\nabstract.  gardner\u2019s computation of the number of  n-bit patterns  which can be stored in \nan optimal neural network used as an associative memory is derived without replic", "packnet: adding multiple tasks to a single network by iterative pruning\n\narun mallya and svetlana lazebnik\n\nuniversity of illinois at urbana-champaign\n\n{amallya2,slazebni}@illinois.edu\n\n8\n1\n0\n2\n\n \n\ny\na\nm\n3\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n9\n6\n7\n5\n0\n\n.\n\n1\n1\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nthis paper presents a method for adding multiple tasks\nto a single deep neural network while avoiding catastrophic\nforgetting. inspired by network pruning techniques, we ex-\nploit redundancies in large deep networks", "the journal of neuroscience, march 13, 2013 \u2022 33(11):4693\u2013 4709 \u2022 4693\n\nsystems/circuits\n\ndiversity and homogeneity in responses of midbrain\ndopamine neurons\n\nchristopher d. fiorillo,1,2 sora r. yun,1 and minryung r. song1\n1department of bio and brain engineering, kaist (korea advanced institute of science and technology), daejeon 305-701, republic of korea, and\n2department of neurobiology, stanford university, stanford, california 94305\n\ndopamine neurons of the ventral midbrain have been found ", "supplementary information for \u201cseparability and geometry\n\nof object manifolds in deep neural networks\u201d\n\ndecember 22, 2019\n\ncontents\n\n1 figures\n\n1.1 object manifolds in deep convolutional networks (alexnet, vgg-16, resnet-50) . . . . . . . . . . .\n1.1.1 capacity for point-cloud manifolds in alexnet, vgg-16, resnet-50 . . . . . . . . . . . . . .\n1.1.2 capacity for smooth manifolds in alexnet, vgg-16, resnet-50\n. . . . . . . . . . . . . . . .\n1.1.3 geometry of point-cloud and smooth manifolds in al", "and ti2ni incoherently precipitate in the b2\nmatrix upon cooling from 700\u00b0c to 87\u00b0c. the\nb2 matrix transforms completely to b19 upon\nfurther cooling to 22\u00b0c, aided by the simulta-\nneous ti2cu/b19 epitaxies (figs. 3 and 4, table 1,\nand table s3). the b19/ti2cu epitaxy provides an\ninternal stress pattern, which stabilizes the b19\nphase at low temperatures. during stress cycling,\nthe equivalent epitaxy alternatingly stabilizes the\nb2 phase. at each temperature and stress, the\ntransforming phases at", "a deep learning framework for neuroscience\n\n\u200a5,6,42, philippe beaudoin7, yoshua bengio1,4,8,  \n\nblake a. richards1,2,3,4,42*, timothy p. lillicrap\u200a\nrafal bogacz9, amelia christensen10, claudia clopath\u200a\nsurya ganguli14,15, colleen j. gillon\u200a\nnikolaus kriegeskorte21,22, peter latham\u200a\nrichard naud\u200a\njo\u00e3o sacramento30, andrew saxe31, benjamin scellier1,8, anna c. schapiro\u200a\n\u200a32, walter senn13, \ngreg wayne5, daniel yamins33,34,35, friedemann zenke36,37, joel zylberberg4,38,39, denis therien\u200a\nand konrad", "2005 american control conference \njune 8-10, 2005. portland, or, usa \n\nwea10.2 \n\na  generalized iterative lqg method for locally-optimal feedback \n\ncontrol of constrained nonlinear stochastic systems \n\nemanuel todorov and weiwei li \n\nabstract-we \n\npresent  an \n\niterative  linear-quadratic- \ngaussian  method  for  locally-optimal  feedback  control  of \nnonlinear  stochastic  systems  subject  to  control  constraints. \npreviously, similar methods have been restricted to determin- \nistic  unconst", "successful reconstruction of a physiological circuit with\nknown connectivity from spiking activity alone\n\nfelipe gerhard1*, tilman kispersky2, gabrielle j. gutierrez2,3, eve marder2, mark kramer4, uri eden4\n\n1 brain mind institute, ecole polytechnique fe\u00b4de\u00b4rale de lausanne (epfl), lausanne, switzerland, 2 biology department and volen center, brandeis university, waltham,\nmassachusetts, united states of america, 3 group for neural theory, e\u00b4cole normale supe\u00b4rieure, paris, france, 4 department o", "3\n2\n0\n2\n\n \n\nb\ne\nf\n \n4\n2\n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n4\nv\n8\n8\n0\n8\n0\n\n.\n\n1\n1\n0\n2\n:\nv\ni\nx\nr\na\n\noriginal article\ncommentary\nstrongandweakprinciplesofneuraldimension\nreduction\nmarkd.humphries1\n1schoolofpsychology,universityof\nnottingham,uk\ncorrespondence\nschoolofpsychology,universityof\nnottingham,uk\nemail: mark.humphries@nottingham.ac.uk\nfundinginformation\nthetimeneededtothinkthesethoughts\nwasmadeavailablethankstothegenerous\nsupportofthemedicalresearchcouncil\ngrantsmr/j008648/1,mr/p005659/1,\nandmr", "48. h. s. mayberg et al., ann. neurol. 28, 57 (1990).\n49. r. m. cohen et al., neuropsychopharmacology 2,\n\n241 (1989).\n\n50. j. e. ledoux, sci. am. 6, 50 (june 1994); m. davis,\n\nannu. rev. neurosci. 15, 353 (1992).\n\n51. j. e. ledoux, curr. opin. neurobiol. 2, 191 (1992); l.\nm. romanski and j. e. ledoux, j. neurosci. 12, 4501\n(1992); j. l. armony, j. d. cohen, d. servan-schre-\niber, j. e. ledoux, behav. neurosci. 109, 246 (1995).\n52. k. p. corodimas and j. e. ledoux, behav. neurosci.\n\n109, 613 (199", "capturing the dynamical repertoire of single neurons\nwith generalized linear models\nalison i. weber1 & jonathan w. pillow2,3\n1graduate program in neuroscience, university of washington, seattle, wa, usa.\n2princeton neuroscience institute, princeton university, princeton, nj, usa.\n3dept. of psychology, princeton university, princeton, nj, usa.\n\nkeywords: point process; generalized linear model (glm); izhikevich model; spike timing;\nvariability\n\nabstract\n\na key problem in computational neuroscienc", "true online td(\u03bb)\n\nharm van seijen\nrichard s. sutton\ndepartment of computing science, university of alberta, edmonton, alberta, t6g 2e8, canada\n\nharm.vanseijen@ualberta.ca\nsutton@cs.ualberta.ca\n\nabstract\n\ntd(\u03bb) is a core algorithm of modern reinforce-\nment learning. its appeal comes from its equiv-\nalence to a clear and conceptually simple for-\nward view, and the fact that it can be imple-\nmented online in an inexpensive manner. how-\never, the equivalence between td(\u03bb) and the for-\nward view is ", "ella: an e\ufb03cient lifelong learning algorithm\n\npaul ruvolo\neric eaton\nbryn mawr college, computer science department, 101 north merion avenue, bryn mawr, pa 19010 usa\n\npruvolo@cs.brynmawr.edu\neeaton@cs.brynmawr.edu\n\nabstract\n\nthe problem of learning multiple consecu-\ntive tasks, known as lifelong learning, is of\ngreat importance to the creation of intelli-\ngent, general-purpose, and \ufb02exible machines.\nin this paper, we develop a method for on-\nline multi-task learning in the lifelong learn-\ning se", "jmlr: workshop and conference proceedings 27:17\u201337, 2012 workshop on unsupervised and transfer learning\n\ndeep learning of representations for unsupervised and\n\ntransfer learning\n\nyoshua bengio\ndept. iro, universit\u00b4e de montr\u00b4eal. montr\u00b4eal (qc), h2c 3j7, canada\n\nyoshua.bengio@umontreal.ca\n\neditor: i. guyon, g. dror, v. lemaire, g. taylor, and d. silver\n\nabstract\n\ndeep learning algorithms seek to exploit the unknown structure in the input distribution\nin order to discover good representations, of", "mach learn (2012) 87:159\u2013182\ndoi 10.1007/s10994-012-5278-7\n\noptimal control as a graphical model inference problem\n\nhilbert j. kappen \u00b7 vicen\u00e7 g\u00f3mez \u00b7 manfred opper\n\nreceived: 3 december 2010 / accepted: 11 january 2012 / published online: 1 february 2012\n\u00a9 the author(s) 2012. this article is published with open access at springerlink.com\n\nabstract we reformulate a class of non-linear stochastic optimal control problems in-\ntroduced by todorov (in advances in neural information processing system", "\f", "biorxiv preprint \n\ndoi: \n\nhttps://doi.org/10.1101/2023.07.09.548255\n; \n\nthis version posted july 9, 2023. \n\nthe copyright holder for this preprint (which\n\nwas not certified by peer review) is the author/funder. all rights reserved. no reuse allowed without permission. \n\na biochemical description of postsynaptic\nplasticity \u2013 with timescales ranging from\nmilliseconds to seconds\n\nguanchun lia, david w. mclaughlina,b,c,2, and charles s. peskina\n\nacourant institute of mathematical sciences and center", "1\n2\n0\n2\n\n \nr\np\na\n8\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n7\n1\n2\n2\n0\n\n.\n\n8\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nhopfield networks is all you need\n\nhubert ramsauer\u2217 bernhard sch\u00e4\ufb02\u2217 johannes lehner\u2217 philipp seidl\u2217\nmichael widrich\u2217 thomas adler\u2217 lukas gruber\u2217 markus holzleitner\u2217\nmilena pavlovi\u00b4c\u2021 ,\u00a7 geir kjetil sandve\u00a7 victor greiff\u2021 david kreil\u2020\nmichael kopp\u2020 g\u00fcnter klambauer\u2217 johannes brandstetter\u2217 sepp hochreiter\u2217 ,\u2020\n\u2217ellis unit linz, lit ai lab, institute for machine learning,\njohannes kepler university linz, austria\n\u2020", "statistical mechanics of generalization in kernel \nregression and wide neural networks\n\ncitation\ncanatar, abdulkadir. 2022. statistical mechanics of generalization in kernel regression and \nwide neural networks. doctoral dissertation, harvard university graduate school of arts and \nsciences.\n\npermanent link\nhttps://nrs.harvard.edu/urn-3:hul.instrepos:37373673\n\nterms of use\nthis article was downloaded from harvard university\u2019s dash repository, and is made available \nunder the terms and conditions", "7\n1\n0\n2\n\n \n\ny\na\nm\n3\n2\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n3\n4\n0\n5\n0\n\n.\n\n2\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nunbiased online recurrent optimization\n\ncorentin tallec, yann ollivier\n\nabstract\n\nthe novel unbiased online recurrent optimization (uoro) algorithm allows\nfor online learning of general recurrent computational graphs such as recurrent\nnetwork models. it works in a streaming fashion and avoids backtracking through\npast activations and inputs. uoro is computationally as costly as truncated\nbackpropagation throu", "mation in the liver (6). thus, we investigated the\neffect of manipulating mir-33 levels in vivo in\nmice using lentiviruses encoding pre-mir-33,\nanti-mir-33, or control. efficient lentiviral deliv-\nery was confirmed by measuring green fluores-\ncent protein in the liver (fig. s9d). consistent\nwith our in vitro results, mir-33 significantly\nreduced hepatic abca1 expression (fig. 4d;\nquantification in fig. s9e). it also modestly de-\ncreased abcg1 and npc1 protein levels, al-\nthough the effect was no", "spike-based learning rules and stabilization of \n\npersistent neural activity \n\nxiaohui xie and h. sebastian seung \n\ndept.  of brain &  cog.  sci., mit, cambridge, ma 02139 \n\n{xhxie, seung}@mit.edu \n\nabstract \n\nwe  analyze  the  conditions  under  which  synaptic  learning  rules  based \non  action  potential timing can  be approximated by  learning rules  based \non  firing  rates.  in  particular, we  consider a form  of plasticity in  which \nsynapses depress when a presynaptic spike is followed", "communicated by fernando pineda \n\na learning algorithm for  continually running fully \nrecurrent neural networks \n\nronald j.  williams \ncollege of  computer science, northeastern  university, \nboston, ma  02115,  usa \n\ndavid zipser \ninstitute for  cognitive science, university of california, \nla  jolla, ca  92093, usa \n\nthe  exact  form  of  a gradient-following  learning  algorithm for com- \npletely recurrent networks running in continually sampled time is de- \nrived and used as the basis for p", "institute of mathematical statistics is collaborating with jstor to digitize, preserve, and extend access to\nthe annals of statistics.\n\nwww.jstor.org\n\n\u00ae\n\n\f", "5\n1\n0\n2\n\n \nr\na\n\n \n\nm\n3\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n1\n7\n5\n5\n0\n\n.\n\n3\n0\n5\n1\n:\nv\ni\nx\nr\na\n\ngsns: generative stochastic networks\n\nguillaume alain\u2217+, yoshua bengio\u2217+, li yao\u2217, jason yosinski\u2020, \u00b4eric thibodeau-\nlaufer\u2217, saizheng zhang\u2217 and pascal vincent\u2217\n\u2217 department of computer science and operations research\nuniversity of montreal\nmontreal, h3c 3j7, quebec, canada\n\u2020 department of computer science, cornell university\n\neditor:\n\nabstract\n\nwe introduce a novel training principle for generative probabi", "article\n\nserotonin neurons modulate learning rate through\nuncertainty\n\nhighlights\nd mice demonstrate variable behavioral \ufb02exibility during\n\ndecision making\n\nauthors\n\ncooper d. grossman, bilal a. bari,\njeremiah y. cohen\n\nd flexible behavior can be characterized as meta-learning\n\nguided by uncertainty\n\ncorrespondence\njeremiah.cohen@jhmi.edu\n\nd serotonin neuron activity correlates with expected and\n\nunexpected uncertainty\n\nd reversible inhibition of serotonin neuron activity impairs\n\nmeta-learning\n", "d\ny\nn\na\n,\n \na\nn\n \ni\nn\nt\ne\ng\nr\na\nt\ne\nd\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n \nf\no\nr\n \nl\ne\na\nr\nn\ni\nn\ng\n,\n \np\nl\na\nn\nn\ni\nn\ng\n,\n \na\nn\nd\n \nr\ne\na\nc\nt\ni\nn\ng\n \nr\ni\nc\nh\na\nr\nd\n \ns\n.\n \ns\nu\nt\nt\no\nn\n \ng\nt\ne\n \nl\na\nb\no\nr\na\nt\no\nr\ni\ne\ns\n \ni\nn\nc\no\nr\np\no\nr\na\nt\ne\nd\n \nw\na\nl\nt\nh\na\nm\n,\n \nm\na\n \n0\n2\n2\n5\n4\n \ng\nu\nt\nt\no\nn\n~\ng\nt\ne\n.\nc\no\nm\n \na\nb\ns\nt\nr\na\nc\nt\n \nd\ny\nn\na\n \ni\ns\n \na\nn\n \na\ni\n \na\nr\nc\nh\ni\nt\ne\nc\nt\nu\nr\ne\n \nt\nh\na\nt\n \ni\nn\nt\ne\ng\nr\na\nt\ne\ns\n \nl\ne\na\nr\nn\ni\nn\ng\n,\n \np\nl\na\nn\nn\ni\nn\ng\n,\n \na\nn\nd\n \nr\ne\na\nc\nt\ni\nv\ne\n \ne\nx\ne\nc\nu\nt\ni\no\nn\n.\n \nl\ne\na\n", "leee transactions  on  neural  networks,  vol.  5, no.  2,  march  1994 \n\n213 \n\nback propagation through adjoints for \nthe identification of  nonlinear dynamic \nsystems using recurrent neural models \n\nb.  srinivasan,  u.  r.  prasad, member, ieee,  and  n.  j.  rao \n\nabstract-in \n\nthis paper, back propagation  is reinvestigated for \nan efficient  evaluation of  the  gradient in  arbitrary interconnec- \ntions of  recurrent subsystems. it is shown that the error has to \nbe  back-propagated through", "research article\n\ndissociable components of\nrule-guided behavior depend on\ndistinct medial and prefrontal regions\nmark j. buckley,1*\u2020 farshad a. mansouri,2*\u2020 hassan hoda,2 majid mahboubi,2\nphilip g. f. browning,1 sze c. kwok,1 adam phillips,2 keiji tanaka2\n\nmuch of our behavior is guided by rules. although human prefrontal cortex (pfc) and anterior\ncingulate cortex (acc) are implicated in implementing rule-guided behavior, the crucial\ncontributions made by different regions within these areas ar", "1342\n\nieee transactions on pattern analysis and machine intelligence,  vol.  20,  no.  12,  december  1998\n\nbayesian classification\n\nwith gaussian processes\n\nchristopher k.i. williams, member, ieee computer society, and david barber\n\nabstract\u2014we consider the problem of assigning an input vector to one of m classes by predicting p(c|x) for c = 1, \u2026, m. for a two-\nclass problem, the probability of class one given x is estimated by s(y(x)), where s(y) = 1/(1 + e-y\n). a gaussian process prior is\npla", "rainbow: combining improvements in deep reinforcement learning\n\nmatteo hessel\n\ndeepmind\n\njoseph modayil\n\ndeepmind\n\nhado van hasselt\n\ndeepmind\n\ntom schaul\n\ndeepmind\n\ngeorg ostrovski\n\ndeepmind\n\nwill dabney\n\ndeepmind\n\ndan horgan\n\ndeepmind\n\nbilal piot\ndeepmind\n\nmohammad azar\n\ndeepmind\n\ndavid silver\n\ndeepmind\n\n7\n1\n0\n2\n\n \nt\nc\no\n6\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\nv\n8\n9\n2\n2\n0\n\n.\n\n0\n1\n7\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nthe deep reinforcement learning community has made sev-\neral independent improvements to the dqn ", "vol 451 | 3 january 2008 | doi:10.1038/nature06445\n\nletters\n\nsparse optical microstimulation in barrel cortex\ndrives learned behaviour in freely moving mice\ndaniel huber1,2, leopoldo petreanu1,2, nima ghitani1, sachin ranade2, toma\u00b4s\u02c7 hroma\u00b4dka2, zach mainen2\n& karel svoboda1,2\n\nelectrical microstimulation can establish causal links between the\nactivity of groups of neurons and perceptual and cognitive\nfunctions1\u20136. however, the number and identities of neurons\nmicrostimulated, as well as the nu", "350 \u2022 the journal of neuroscience, january 6, 2010 \u2022 30(1):350 \u2013360\n\nbehavioral/systems/cognitive\n\nfunctional, but not anatomical, separation of \u201cwhat\u201d\nand \u201cwhen\u201d in prefrontal cortex\n\nchristian k. machens,1 ranulfo romo,2 and carlos d. brody3,4,5\n1group for neural theory, inserm u960, de\u00b4partement d\u2019e\u00b4tudes cognitives, e\u00b4cole normale supe\u00b4rieure, 75005 paris, france, 2instituto de fisiolog\u00eda celular\u2013\nneurociencias, universidad nacional auto\u00b4noma de me\u00b4xico, 04510 me\u00b4xico d.f., me\u00b4xico, and 3how", "neuron, vol. 29, 33\u201344, january, 2001, copyright \u00aa 2001 by cell press\n\ntraveling electrical waves in cortex:\ninsights from phase dynamics\nand speculation on a computational role\n\nviewpoint\n\ng. bard ermentrout*\u2020k and david kleinfeld\u2021\u00a7k\n* department of mathematics\n\u2020 department of neurobiology\nuniversity of pittsburgh\npittsburgh, pennsylvania 15260\n\u2021 department of physics\n\u00a7 neurosciences graduate program\nuniversity of california, san diego\nla jolla, california 92093\n\nsummary\n\nthe theory of coupled ", "available online at www.sciencedirect.com\n\nthe case for and against muscle synergies\nmatthew c tresch1,2,3 and anthony jarc1\n\na long standing goal in motor control is to determine the\nfundamental output controlled by the cns: does the cns\ncontrol the activation of individual motor units, individual\nmuscles, groups of muscles, kinematic or dynamic features of\nmovement, or does it simply care about accomplishing a task?\nof course, the output controlled by the cns might not be\nexclusive but instead", "neighborhood preserving embedding\n\nxiaofei he1\n\ndeng cai2\n\nshuicheng yan3\n\nhong-jiang zhang4\n\n1 department of computer science, university of chicago, chicago, il 60637\n\n2 department of computer science, university of illinois at urbana-champaign, urbana, il 61081\n\n3 department of information engineering, chinese university of hong kong, hong kong\n\n4 microsoft research asia, beijing, p.r. china\ncontact: xiaofei@cs.uchicago.edu\n\nabstract\n\ninterest\n\nrecently there has been a lot of\nin geometri-\nca", "5\n1\n0\n2\n\n \n\nv\no\nn\n8\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n1\nv\n3\n4\n5\n2\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nsandwiching the marginal likelihood using\n\nbidirectional monte carlo\n\nroger b. grosse\nuniversity of toronto\n\nzoubin ghahramani\nuniversity of cambridge\n\nryan p. adams\n\ntwitter and harvard university\n\nrgrosse@cs.toronto.edu\n\nzoubin@eng.cam.ac.uk\n\nrpa@seas.harvard.edu\n\nabstract\n\ncomputing the marginal likelihood (ml) of a model requires marginalizing out all of the\nparameters and latent variables, a di\ufb03cult h", ".\n\nd\ne\nv\nr\ne\ns\ne\nr\n \n\ns\nt\nh\ng\ni\nr\n \nl\nl\n\na\n\n \n \n.\n\nc\nn\n\ni\n \n,\n\na\nc\n\n \n\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n9\n0\n0\n2\n\u00a9\n\n \n\nb r i e f   c o m m u n i c at i o n s\n\nthe timing of external input \ncontrols the sign of plasticity  \nat local synapses\n\njeehyun kwag & ole paulsen\n\nthe method by which local networks in the brain store \ninformation from extrinsic afferent inputs is not well \nunderstood. we found that the timing of afferent input can \nbidirectionally control the sign of spike timing\u2013dependent \nplasticity ", "a simple framework for contrastive learning of visual representations\n\nting chen 1 simon kornblith 1 mohammad norouzi 1 geoffrey hinton 1\n\n0\n2\n0\n2\n\n \nl\nu\nj\n \n\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n9\n0\n7\n5\n0\n\n.\n\n2\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nthis paper presents simclr: a simple framework\nfor contrastive learning of visual representations.\nwe simplify recently proposed contrastive self-\nsupervised learning algorithms without requiring\nspecialized architectures or a memory bank. in\norder to understand wh", "neuron\n\narticle\n\nspatial attention decorrelates intrinsic\nactivity fluctuations in macaque area v4\n\njude f. mitchell,1 kristy a. sundberg,1 and john h. reynolds1,*\n1systems neurobiology lab, the salk institute, la jolla, ca 92037-1099, usa\n*correspondence: reynolds@salk.edu\ndoi 10.1016/j.neuron.2009.09.013\n\nsummary\n\nattention typically ampli\ufb01es neuronal\nresponses\nevoked by task-relevant stimuli while attenuating\nresponses to task-irrelevant distracters. in this con-\ntext, visual distracters cons", "research article\n\nnonlinear hebbian learning as a unifying\nprinciple in receptive field formation\ncarlos s. n. brito1,2*, wulfram gerstner1\n\n1 school of computer and communication sciences and school of life science, brain mind institute, ecole\npolytechnique federale de lausanne, lausanne epfl, switzerland, 2 gatsby computational neuroscience\nunit, university college london, london, united kingdom\n\na11111\n\n* c.brito@ucl.ac.uk\n\nabstract\n\nthe development of sensory receptive fields has been modele", "journal of machine learning research 12 (2011) 2121-2159\n\nsubmitted 3/10; revised 3/11; published 7/11\n\nadaptive subgradient methods for\n\nonline learning and stochastic optimization\u2217\n\njohn duchi\ncomputer science division\nuniversity of california, berkeley\nberkeley, ca 94720 usa\nelad hazan\ntechnion - israel institute of technology\ntechnion city\nhaifa, 32000, israel\n\nyoram singer\ngoogle\n1600 amphitheatre parkway\nmountain view, ca 94043 usa\n\neditor: tong zhang\n\njduchi@cs.berkeley.edu\n\nehazan@ie.tec", "a r t i c l e s\n\nattention improves performance primarily by reducing \ninterneuronal correlations\n\nmarlene r cohen & john h r maunsell\n\nvisual attention can improve behavioral performance by allowing observers to focus on the important information in a complex \nscene. attention also typically increases the firing rates of cortical sensory neurons. rate increases improve the signal-to-noise \nratio of individual neurons, and this improvement has been assumed to underlie attention-related improveme", "credit assignment through\n\nbroadcasting a global error vector\n\ndavid g. clark, l.f. abbott, sueyeon chung\n\ncenter for theoretical neuroscience\n\ncolumbia university\n\nnew york, ny\n\n{david.clark, lfabbott, sueyeon.chung}@columbia.edu\n\nabstract\n\nbackpropagation (bp) uses detailed, unit-speci\ufb01c feedback to train deep neural\nnetworks (dnns) with remarkable success. that biological neural circuits appear\nto perform credit assignment, but cannot implement bp, implies the existence of\nother powerful lear", "biorxiv preprint \nthe copyright holder for this preprint (which\nwas not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\nthis version posted november 11, 2016. \n\nhttps://doi.org/10.1101/085886\n; \n\ndoi: \n\navailable under a\n\ncc-by-nc-nd 4.0 international license\n.\n\ncause for pause before leaping to conclusions about stepping \n\nariel zylberberg*, michael n. shadlen* \n\nhoward hughes medical institute, kavli institut", "learning hierarchical category structure in deep neural networks\n\nandrew m. saxe (asaxe@stanford.edu)\n\ndepartment of electrical engineering\n\njames l. mcclelland (mcclelland@stanford.edu)\n\ndepartment of psychology\n\nsurya ganguli (sganguli@stanford.edu)\n\ndepartment of applied physics\n\nstanford university, stanford, ca 94305 usa\n\nabstract\n\npsychological experiments have revealed remarkable regulari-\nties in the developmental time course of cognition. infants gen-\nerally acquire broad categorical di", "6\n1\n0\n2\n\n \n\ny\na\nm\n9\n1\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n7\nv\n3\n7\n4\n0\n\n.\n\n9\n0\n4\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2015\n\nneural machine translation\nby jointly learning to align and translate\n\ndzmitry bahdanau\njacobs university bremen, germany\n\nkyunghyun cho\nuniversit\u00b4e de montr\u00b4eal\n\nyoshua bengio\u2217\n\nabstract\n\nneural machine translation is a recently proposed approach to machine transla-\ntion. unlike the traditional statistical machine translation, the neural machine\ntranslation aims at", "r. e. kalman \nresearch institute for advanced study,2 \nbaltimore, md. \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n\n \n\na new approach to linear filtering  \nand prediction problems1\n \nthe  classical  filtering  and  prediction  problem  is  re-examined  using  the  bode-\nshannon  representation  of  random  processes  and  the  \u201cstate  transition\u201d  method  of \nanalysis of dynamic systems.  new results are: \n(1) the formulation and methods of solution of the problem apply without modifica- \ntion  to  stationary", "a r t i c l e s\n\nlearning enhances the relative impact of top-down \nprocessing in the visual cortex\nhiroshi makino1 & takaki komiyama1,2\ntheories have proposed that, in sensory cortices, learning can enhance top-down modulation by higher brain areas while reducing \nbottom-up sensory drives. to address circuit mechanisms underlying this process, we examined the activity of layer 2/3 (l2/3) \nexcitatory neurons in the mouse primary visual cortex (v1) as well as l4 excitatory neurons, the main botto", "the journal of comparative neurology 285:54-72  (1989) \n\nterminal arbors of individual \u201cfeedback\u201d \naxons projecting from area v2 to v l  in the \n\nmacaque monkey: a study using \n\nimmunohistochemistry of anterogradely \n\ntransported phaseoh \nvulgaris-leucoagglutinin \n\neye research institute, boston, massachusetts 02115; enrm v.a. hospital, bedford, \n\nkathleen s. rockland and agnes virga \n\nmassachusetts 01730 \n\nabstract \n\nin the present study, the anterograde tracer phaseolus vulgaris-leucoag- \nglut", "unsupervised learning by competing hidden units\n\ndmitry krotova,b,1,2 and john j. hop\ufb01eldc,1,2\n\namassachusetts institute of technology\u2013international business machines (ibm) watson arti\ufb01cial intelligence laboratory, ibm research, cambridge, ma\n02142; binstitute for advanced study, princeton, nj 08540; and cprinceton neuroscience institute, princeton university, princeton, nj 08544\n\ncontributed by john j. hop\ufb01eld, february 11, 2019 (sent for review november 30, 2018; reviewed by dmitri b. chklovsk", "a map of object space in primate \ninferotemporal cortex\n\nhttps://doi.org/10.1038/s41586-020-2350-5\nreceived: 21 january 2019\naccepted: 17 march 2020\npublished online: 3 june 2020\n\n check for updates\n\npinglei bao1,2\u2009\u2709, liang she1, mason mcgill3 & doris y. tsao1,2,3\u2009\u2709\n\nthe inferotemporal (it) cortex is responsible for object recognition, but it is unclear \nhow the representation of visual objects is organized in this part of the brain. areas \nthat are selective for categories such as faces, bodies", "www.elsevier.com/locate/ynimg\nneuroimage 26 (2005) 839 \u2013 851\n\nunified segmentation\njohn ashburnert and karl j. friston\n\nwellcome department of imaging neuroscience, functional imaging labolatory 12 queen square, london, wc1n 3bg, uk\n\nreceived 10 november 2004; revised 2 february 2005; accepted 10 february 2005\navailable online 1 april 2005\n\na probabilistic framework is presented that enables image registration,\ntissue classification, and bias correction to be combined within the\nsame generative ", "on the expressive power of deep neural networks\n\nmaithra raghu 1 2 ben poole 3 jon kleinberg 1 surya ganguli 3 jascha sohl dickstein 2\n\n7\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n8\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n6\nv\n6\n3\n3\n5\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe propose a new approach to the problem of\nneural network expressivity, which seeks to char-\nacterize how structural properties of a neural net-\nwork family affect the functions it is able to com-\npute. our approach is based on an interrelated\nset of measures ", "a multimodal cell census and atlas of the \nmammalian primary motor cortex\n\n       \nhttps://doi.org/10.1038/s41586-021-03950-0\nreceived: 4 october 2020\naccepted: 25 august 2021\npublished online: 6 october 2021\nopen access\n\n check for updates\n\nbrain initiative cell census network (biccn)1*\n\nhere we report the generation of a multimodal cell census and atlas of the mammalian \nprimary motor cortex as the initial product of the brain initiative cell census \nnetwork (biccn). this was achieved by coord", "gaussian processes in machine learning\n\ncarl edward rasmussen\n\nmax planck institute for biological cybernetics, 72076 t\u00a8ubingen, germany\n\nwww home page: http://www.tuebingen.mpg.de/\u223ccarl\n\ncarl@tuebingen.mpg.de\n\nabstract. we give a basic introduction to gaussian process regression\nmodels. we focus on understanding the role of the stochastic process\nand how it is used to de\ufb01ne a distribution over functions. we present\nthe simple equations for incorporating training data and examine how\nto learn th", "reports\n\nthe high affinity of carbon for certain metals (11),\nmolten metal nanoparticles are able to act as\ncatalytic sites for the uptake of carbon with sub-\nsequent carbon nanotube outgrowth (12).\n\nunder continuing heating, we observed nano-\nwire growth over the course of 10 min (fig. 3).\nmovie s2, taken at a higher resolution, better shows\nthe initial stages of growth at high temperature (10).\nthe faceted end of the emerging wire indicates\nthat the free end of the wire is solid during the\ngro", "a r t i c l e s\n\nsubtype-specific plasticity of inhibitory circuits  \nin motor cortex during motor learning\nsimon x chen1\u20133, an na kim1\u20133, andrew j peters1\u20133 & takaki komiyama1\u20134\nmotor skill learning induces long-lasting reorganization of dendritic spines, principal sites of excitatory synapses, in the motor \ncortex. however, mechanisms that regulate these excitatory synaptic changes remain poorly understood. here, using in vivo  \ntwo-photon imaging in awake mice, we found that learning-induced ", "math. program., ser. a 108, 177\u2013205 (2006)\n\ndigital object identi\ufb01er (doi) 10.1007/s10107-006-0706-8\n\nyurii nesterov \u00b7 b.t. polyak\ncubic regularization of newton method and its global\nperformance(cid:1)\n\nreceived: august 31, 2005 / accepted: january 27, 2006\npublished online: april 25, 2006 \u2013 \u00a9 springer-verlag 2006\n\nabstract. in this paper, we provide theoretical analysis for a cubic regularization of newton method as\napplied to unconstrained minimization problem. for this scheme, we prove gener", "concepts in a probabilistic language of thought\n\nnoah d. goodman\u2217, stanford university\n\njoshua b. tenenbaum, mit\ntobias gerstenberg, mit\n\nfebruary 15, 2014\n\nto appear in concepts: new directions, eds. margolis and laurence, mit press.\n\n1 introduction\n\nknowledge organizes our understanding of the world, determining what we expect\ngiven what we have already seen. our predictive representations have two key prop-\nerties: they are productive, and they are graded. productive generalization is possibl", "signsgd: compressed optimisation for non-convex problems\n\njeremy bernstein 1 2 yu-xiang wang 2 3 kamyar azizzadenesheli 4 anima anandkumar 1 2\n\nabstract\n\ntraining large neural networks requires distribut-\ning learning across multiple workers, where the\ncost of communicating gradients can be a signif-\nicant bottleneck. signsgd alleviates this prob-\nlem by transmitting just the sign of each minibatch\nstochastic gradient. we prove that it can get the\nbest of both worlds: compressed gradients and\nsg", "article\n\nthe stabilized supralinear network: a unifying\ncircuit motif underlying multi-input integration in\nsensory cortex\n\nhighlights\nd a simple, uni\ufb01ed circuit model of contextual modulation and\n\nnormalization\n\nauthors\n\ndaniel b. rubin, stephen d. van hooser,\nkenneth d. miller\n\nd explains transition from facilitation to suppression w/\n\nincreasing stimulus strength\n\ncorrespondence\nken@neurotheory.columbia.edu\n\nd both excitatory and inhibitory neurons show normalization or\n\nsuppression\n\nd new ex", "understanding image representations\n\nby measuring their equivariance and equivalence\n\nkarel lenc\n\nandrea vedaldi\n\ndepartment of engineering science, university of oxford\n\n5\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n2\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n8\n0\n9\n5\n\n.\n\n1\n1\n4\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\ndespite the importance of image representations such as\nhistograms of oriented gradients and deep convolutional\nneural networks (cnn), our theoretical understanding of\nthem remains limited. aiming at \ufb01lling this gap, we inves-\ntigat", "ne40ch25-foster\n\nari\n\n8 july 2017\n\n8:23\n\nreplay comes of age\ndavid j. foster\ndepartment of psychology and helen wills neuroscience institute, university of california,\nberkeley, california 94720; email: davidfoster@berkeley.edu\n\nannual \n\n reviews further\n\nclick here to view this article's \nonline features:\n\u2022 download \ufb01gures as ppt slides\n\u2022 navigate linked references\n\u2022 download citations\n\u2022 explore related articles\n\u2022 search keywords\n\nannu. rev. neurosci. 2017. 40:581\u2013602\n\nthe annual review of neur", "66\n\nieeeproc. conf. oncomp. vis. patt. recog. (cvpr'97)\n\ngradient vector flow: a new external force for snakes\n\nchenyang xu and jerry l. prince\n\ndepartment of electrical and computer engineering\nthe johns hopkins university, baltimore, md 21218\n\nabstract\n\nsnakes, or active contours, are used extensively in com-\nputer vision and image processing applications, particu-\nlarly to locate object boundaries. problems associated with\ninitialization and poor convergence to concave boundaries,\nhowever, ha", "identifying and attacking the saddle point\n\nproblem in high-dimensional non-convex optimization\n\nyann n. dauphin razvan pascanu caglar gulcehre kyunghyun cho\n\nuniversit\u00b4e de montr\u00b4eal\n\ndauphiya@iro.umontreal.ca, r.pascanu@gmail.com,\n\ngulcehrc@iro.umontreal.ca, kyunghyun.cho@umontreal.ca\n\nsurya ganguli\n\nstanford university\n\nsganguli@standford.edu\n\nyoshua bengio\n\nuniversit\u00b4e de montr\u00b4eal, cifar fellow\nyoshua.bengio@umontreal.ca\n\nabstract\n\na central challenge to many fields of science and engineeri", "taking the human out of the loop:\na review of bayesian optimization\n\nbobak shahriari, kevin swersky, ziyu wang, ryan p. adams and nando de freitas\n\n1\n\nabstract\u2014big data applications are typically associated with\nsystems involving large numbers of users, massive complex\nsoftware systems, and large-scale heterogeneous computing and\nstorage architectures. the construction of such systems involves\nmany distributed design choices. the end products (e.g., rec-\nommendation systems, medical analysis too", "bipartite expander hop\ufb01eld networks as\n\nself-decoding high-capacity error correcting codes\n\nrishidev chaudhuri\n\ncenter for neuroscience,\n\ndepartments of mathematics and\n\nneurobiology, physiology and behavior,\n\nuniversity of california, davis,\n\ndavis, ca 95616\n\nrchaudhuri@ucdavis.edu\n\nila fiete\n\nbrain and cognitive sciences,\n\nmassachusetts institute of technology,\n\ncambridge, ma 02139\n\nfiete@mit.edu\n\nabstract\n\nneural network models of memory and error correction famously include the hop-\n\ufb01eld net", "the journal of neuroscience, december 1, 1998, 18(23):9870\u20139895\n\ncorrelation-based development of ocularly matched orientation\nand ocular dominance maps: determination of required\ninput activities\n\ned erwin1,4 and kenneth d. miller1,2,3,4,5\ndepartments of 1physiology and 2otolaryngology, 3neuroscience graduate program, 4w. m. keck center for integrative\nneuroscience, and 5sloan center for theoretical neurobiology, university of california, san francisco,\ncalifornia 94143-0444\n\nwe extend previous", "6\n1\n0\n2\n\n \nr\na\n\n \n\nm\n9\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n6\n4\n9\n5\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2016\n\nacdc: a structured efficient linear layer\n\nmarcin moczulski1 misha denil1\n\njeremy appleyard2 nando de freitas1,3\n\n1university of oxford\n2nvidia 3cifar\nmarcin.moczulski@stcatz.ox.ac.uk\nmisha.denil@gmail.com\njappleyard@nvidia.com\nnando.de.freitas@cs.ox.ac.uk\n\nabstract\n\nthe linear layer is one of the most pervasive modules in deep learning representa-\ntions. however", "corrected: author correction\n\nperceptual straightening of natural videos\n\nolivier j. h\u00e9naff\u200a\n\n\u200a1,6*, robbe l. t. goris2 and eero p. simoncelli1,3,4,5\n\nmany behaviors rely on predictions derived from recent visual input, but the temporal evolution of those inputs is generally \ncomplex and difficult to extrapolate. we propose that the visual system transforms these inputs to follow straighter temporal \ntrajectories. to test this \u2018temporal straightening\u2019 hypothesis, we develop a methodology for est", "6\n1\n0\n2\n\n \n\nb\ne\nf\n5\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n2\n5\n9\n5\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2016\n\nprioritized experience replay\n\ntom schaul, john quan, ioannis antonoglou and david silver\ngoogle deepmind\n{schaul,johnquan,ioannisa,davidsilver}@google.com\n\nabstract\n\nexperience replay lets online reinforcement learning agents remember and reuse\nexperiences from the past. in prior work, experience transitions were uniformly\nsampled from a replay memory. however, ", "research article\n\ncan sleep protect memories from\ncatastrophic forgetting?\noscar c gonza\u00b4 lez1\u2020, yury sokolov1\u2020, giri p krishnan1, jean erik delanois1,2,\nmaxim bazhenov1*\n\n1department of medicine, university of california, san diego, la jolla, united\nstates; 2department of computer science and engineering, university of california,\nsan diego, la jolla, united states\n\nabstract continual learning remains an unsolved problem in artificial neural networks. the brain\nhas evolved mechanisms to prevent", "proc. natl. acad. sci. usa\nvol. 81, pp. 3088-3092, may 1984\nbiophysics\n\nneurons with graded response have collective computational\nproperties like those of two-state neurons\n\n(associative memory/neural network/stability/action potentials)\n\nj. j. hopfield\ndivisions of chemistry and biology, california institute of technology, pasadena, ca 91125; and bell laboratories, murray hill, nj 07974\n\ncontributed by j. j. hopfield, february 13, 1984\n\na model for a large network of \"neurons\"\nabstract\nwith a ", "available online at www.sciencedirect.com\n\nsciencedirect\n\ncurrent opinion in\n\nneurobiology\n\nneural population geometry: an approach for\nunderstanding biological and artificial neural networks\nsueyeon chung and l. f. abbott\n\nabstract\nadvances in experimental neuroscience have transformed our\nability to explore the structure and function of neural circuits. at\nthe same time, advances in machine learning have unleashed\nthe remarkable computational power of artificial neural net-\nworks (anns). while", "measuring invariances in deep networks\n\nian j. goodfellow, quoc v. le, andrew m. saxe, honglak lee, andrew y. ng\n\ncomputer science department\n\nstanford university\nstanford, ca 94305\n\n{ia3n,quocle,asaxe,hllee,ang}@cs.stanford.edu\n\nabstract\n\nfor many pattern recognition tasks, the ideal input feature would be invariant to\nmultiple confounding properties (such as illumination and viewing angle, in com-\nputer vision applications). recently, deep architectures trained in an unsupervised\nmanner have b", "further\nannual\nreviews\nclick here for quick links to \nannual reviews content online, \nincluding:\n\u2022 other articles in this volume\n\u2022 top cited articles\n\u2022 top downloaded articles\n\u2022 our comprehensive search\n\nplace cells, grid cells,\nand the brain\u2019s spatial\nrepresentation system\n\nedvard i. moser,1 emilio kropff,1,2\nand may-britt moser1\n1kavli institute for systems neuroscience and centre for the biology of memory,\nnorwegian university of science and technology, 7489 trondheim, norway\n2cognitive neuro", "ef\ufb01cient neural architecture search via parameter sharing\n\n8\n1\n0\n2\n\n \n\nb\ne\nf\n2\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n8\n6\n2\n3\n0\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nhieu pham * 1 2 melody y. guan * 3 barret zoph 1 quoc v. le 1 jeff dean 1\n\nabstract\n\nwe propose ef\ufb01cient neural architecture search\n(enas), a fast and inexpensive approach for au-\ntomatic model design. in enas, a controller dis-\ncovers neural network architectures by searching\nfor an optimal subgraph within a large computa-\ntional graph. the controll", "neuroscience 282 (2014) 248\u2013257\n\nreview\n\nthe place of dopamine in the cortico-basal ganglia circuit\n\ns. n. haber *\n\nsignificance of\n\ndepartment of pharmacology and physiology, university of\nrochester school of medicine, 601 elmwood avenue, rochester,\nny 14642, united states\n\nfunctions:\n\nabstract\u2014the midbrain dopamine (da) neurons play a cen-\ntral role in developing appropriate goal-directed behaviors,\nincluding the motivation and cognition to develop appropri-\nate actions to obtain a speci\ufb01c out", "j neurophysiol 102: 1315\u20131330, 2009.\nfirst published march 18, 2009; doi:10.1152/jn.00097.2009.\n\ninnovative methodolgy\n\nfactor-analysis methods for higher-performance neural prostheses\n\ngopal santhanam,1 byron m. yu,1,2,6 vikash gilja,3 stephen i. ryu,1,4 afsheen afshar,1,5 maneesh sahani,6\nand krishna v. shenoy1,2\n1department of electrical engineering, 2neurosciences program, 3department of computer science, 4department of neurosurgery,\n5medical scientist training program, stanford university, ", "ne33ch05-shadmehr\n\nari\n\n11 march 2010\n\n14:32\n\ne\n\nr\n\nv\n\ni e\n\nw\n\ns\n\ni\n\nn\n\na d v\n\ne\nc\n\nn\n\na\n\nannu. rev. neurosci. 2010. 33:89\u2013108\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nthis article\u2019s doi:\n10.1146/annurev-neuro-060909-153135\ncopyright c(cid:2) 2010 by annual reviews.\nall rights reserved\n\n0147-006x/10/0721-0089$20.00\n\nerror correction, sensory\nprediction, and adaptation\nin motor control\nreza shadmehr,1 maurice a. smith,2\nand john w. krakauer3\n1department of biomedica", "8\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n8\n2\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n2\n2\n8\n1\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nrelational recurrent neural networks\n\nadam santoro*\u03b1, ryan faulkner*\u03b1, david raposo*\u03b1, jack rae\u03b1\u03b2, mike chrzanowski\u03b1,\n\nth\u00e9ophane weber\u03b1, daan wierstra\u03b1, oriol vinyals\u03b1, razvan pascanu\u03b1, timothy lillicrap\u03b1\u03b2\n\n*equal contribution\n\n\u03b1deepmind\n\nlondon, united kingdom\n\n\u03b2complex, computer science, university college london\n\nlondon, united kingdom\n\n{adamsantoro; rfaulk; draposo; jwrae; chrzanowskim;\n\ntheophane; weirs", "letters\n\nvol 466 | 29 july 2010 | doi:10.1038/nature09159\n\nregulation of parkinsonian motor behaviours by\noptogenetic control of basal ganglia circuitry\nalexxai v. kravitz1, benjamin s. freeze1,4,5, philip r. l. parker1,3, kenneth kay1,5, myo t. thwin1, karl deisseroth6\n& anatol c. kreitzer1,2,3,4,5\n\nneural circuits of the basal ganglia are critical for motor planning\nand action selection1\u20133. two parallel basal ganglia pathways have\nbeen described4, and have been proposed to exert opposing influ", "research article\n\nsingle-cell transcriptomic evidence for\ndense intracortical neuropeptide\nnetworks\nstephen j smith1*, uygar su\u00a8 mbu\u00a8 l1, lucas t graybuck1, forrest collman1,\nsharmishtaa seshamani1, rohan gala1, olga gliko1, leila elabbady1,\njeremy a miller1, trygve e bakken1, jean rossier2, zizhen yao1, ed lein1,\nhongkui zeng1, bosiljka tasic1, michael hawrylycz1*\n\n1allen institute for brain science, seattle, united states; 2neuroscience paris seine,\nsorbonne universite\u00b4 , paris, france\n\nabstra", "e\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\ne\nr\nu\nt\na\nn\nm\no\nc\n.\ne\nr\nu\nt\na\nn\nw\nw\nw\n\n/\n\n.\n\n/\n/\n:\np\nt\nt\nh\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n5\n0\n0\n2\n\u00a9\n\n \n\na r t i c l e s\n\ncomputation and systems\n\nuncertainty-based competition between prefrontal and\ndorsolateral striatal systems for behavioral control\n\nnathaniel d daw1, yael niv1,2 & peter dayan1\n\na broad range of neural and behavioral data suggests that the brain contains multiple systems for behavioral choice, including\none associated with prefr", "research\n\nresearch articles \u25e5\n\ncognitive science\n\nhuman-level concept learning\nthrough probabilistic\nprogram induction\n\nbrenden m. lake,1* ruslan salakhutdinov,2 joshua b. tenenbaum3\n\nnew concept, and even children can make mean-\ningful generalizations via \u201cone-shot learning\u201d\n(1\u20133). in contrast, many of the leading approaches\nin machine learning are also the most data-hungry,\nespecially \u201cdeep learning\u201d models that have\nachieved new levels of performance on object\nand speech recognition benchmark", "5\n1\n0\n2\n\n \nr\np\na\n5\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n6\n5\n8\n6\n\n.\n\n2\n1\n4\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2015\n\nobject detectors emerge in deep scene cnns\n\nbolei zhou, aditya khosla, agata lapedriza, aude oliva, antonio torralba\ncomputer science and arti\ufb01cial intelligence laboratory, mit\n{bolei,khosla,agata,oliva,torralba}@mit.edu\n\nabstract\n\nwith the success of new computational architectures for visual processing, such as\nconvolutional neural networks (cnn) and access to ima", "can the brain do backpropagation?\n\n\u2014 exact implementation of backpropagation in\n\npredictive coding networks\n\nyuhang song1, thomas lukasiewicz1, zhenghua xu2,\u2217, rafal bogacz3\n\n1department of computer science, university of oxford, uk\n\n2state key laboratory of reliability and intelligence of electrical equipment,\n\nhebei university of technology, tianjin, china\n\n3mrc brain network dynamics unit, university of oxford, uk\n\nyuhang.song@some.ox.ac.uk, thomas.lukasiewicz@cs.ox.ac.uk,\n\nzhenghua.xu@hebut.", "information sciences 328 (2016) 26\u201341\n\ncontents lists available at sciencedirect\n\ninformation sciences\n\njournal homepage: www.elsevier.com/locate/ins\n\nintrinsic dimension estimation: advances and open problems\n\n\u2217\nfrancesco camastra\n, antonino staiano\n\ndepartment of science and technology, university of naples parthenope, centro direzionale isola c4 - 80143 napoli, italy\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\ndimensionality reduction methods are preprocessing techniques used for coping with hi", "a framework for studying synaptic plasticity\n\nwith neural spike train data\n\nscott w. linderman\nharvard university\n\ncambridge, ma 02138\n\nchristopher h. stock\n\nharvard college\n\ncambridge, ma 02138\n\nryan p. adams\nharvard university\n\ncambridge, ma 02138\n\nswl@seas.harvard.edu\n\ncstock@post.harvard.edu\n\nrpa@seas.harvard.edu\n\nabstract\n\nlearning and memory in the brain are implemented by complex, time-varying\nchanges in neural circuitry. the computational rules according to which synaptic\nweights change ", "published as a conference paper at iclr 2017\n\nmultilayer recurrent network models of pri-\nmate retinal ganglion cell responses\n\neleanor batty, josh merel \u2217\ndoctoral program in neurobiology & behavior, columbia university\nerb2180@columbia.edu,jsmerel@gmail.com\n\nnora brackbill *\ndepartment of physics, stanford university\nnbrack@stanford.edu\n\nalexander heitman\nneurosciences graduate program, university of california, san diego\nalexkenheitman@gmail.com\n\nalexander sher & alan litke\nsanta cruz institu", "7\n1\n0\n2\n\n \nr\np\na\n7\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n4\nv\n3\n4\n7\n6\n0\n\n.\n\n5\n0\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\ninductive bias of deep convolutional\nnetworks through pooling geometry\n\nnadav cohen & amnon shashua\n{cohennadav,shashua}@cs.huji.ac.il\n\nabstract\n\nour formal understanding of the inductive bias that drives the success of convo-\nlutional networks on computer vision tasks is limited. in particular, it is unclear\nwhat makes hypotheses spaces born from convolution and p", "neural path features and neural path kernel :\nunderstanding the role of gates in deep learning\n\nchandrashekar lakshminarayanan\u21e4 and amit vikram singh\u21e4,\n\nindian institute of technology palakkad\n\nchandru@iitpkd.ac.in, amitkvikram@gmail.com\n\nabstract\n\nrecti\ufb01ed linear unit (relu) activations can also be thought of as gates, which,\neither pass or stop their pre-activation input when they are on (when the pre-\nactivation input is positive) or off (when the pre-activation input is negative)\nrespectivel", "ly charged air. several clouds growing over\nthe negative charge source developed exten-\nsive regions containing positive charge in\ntheir lower portions. an example is shown\nin fig. 2. the measurements reported for\n1985 were made with ground-based equip-\nment because the instrumented airplane and\nballoon systems used in 1984 were not\navailable.\n\nit is conceivable that the abnormal polari-\nexhibited by these\nty of electrification\nstorms may have been an unusual natural\noccurrence that was not rela", "the role of population structure in computations \nthrough neural dynamics\n\nalexis dubreuil\u200a\nand srdjan ostojic\u200a\n\n\u200a1\u2009\u2709\n\n\u200a1,2,6\u2009\u2709, adrian valente\u200a\n\n\u200a1,6\u2009\u2709, manuel beiran1,3, francesca mastrogiuseppe\u200a\n\n\u200a4,5  \n\nneural computations are currently investigated using two separate approaches: sorting neurons into functional subpopula-\ntions or examining the low-dimensional dynamics of collective activity. whether and how these two aspects interact to shape \ncomputations is currently unclear. using a nove", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\ndendritic  solutions  to  the  credit  assignment  problem\nblake  a  richards1,2,3 and  timothy  p  lillicrap4\n\nguaranteeing  that  synaptic  plasticity  leads  to  effective  learning\nrequires  a  means  for  assigning  credit  to  each  neuron  for  its\ncontribution  to  behavior.  the  \u2018credit  assignment  problem\u2019\nrefers  to  the  fact  that  credit  assignment  is  non-trivial  in\nhierarchical  networks  with  multiple  st", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nwhy  neurons  mix:  high  dimensionality  for  higher\ncognition\nstefano  fusi1,  earl  k  miller2 and  mattia  rigotti3\n\nneurons  often  respond  to  diverse  combinations  of  task-\nrelevant  variables.  this  form  of  mixed  selectivity  plays  an\nimportant  computational  role  which  is  related  to  the\ndimensionality  of  the  neural  representations:  high-dimensional\nrepresentations  with  mixed  selectivity  allow  a  simple ", "de\u00a0novo design of protein structure and \nfunction with rfdiffusion\n\nhttps://doi.org/10.1038/s41586-023-06415-8\nreceived: 14 december 2022\naccepted: 7 july 2023\npublished online: 11 july 2023\nopen access\n\njoseph l. watson1,2,15, david juergens1,2,3,15, nathaniel r. bennett1,2,3,15, brian l. trippe2,4,5,15, \njason yim2,6,15, helen e. eisenach1,2,15, woody ahern1,2,7,15, andrew j. borst1,2, robert j. ragotte1,2, \nlukas f. milles1,2, basile i. m. wicky1,2, nikita hanikel1,2, samuel j. pellock1,2, al", "1\n2\n0\n2\n\n \n\nb\ne\nf\n0\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n6\n5\n4\n3\n1\n\n.\n\n1\n1\n0\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2021\n\nscore-based generative modeling through\nstochastic differential equations\n\nyang song\u02da\nstanford university\nyangsong@cs.stanford.edu\n\njascha sohl-dickstein\ngoogle brain\njaschasd@google.com\n\ndiederik p. kingma\ngoogle brain\ndurk@google.com\n\nabhishek kumar\ngoogle brain\nabhishk@google.com\n\nstefano ermon\nstanford university\nermon@cs.stanford.edu\n\nben poole\ngoogle brain\n", "neuron\n\nperspective\n\nthe brain as an ef\ufb01cient\nand robust adaptive learner\n\nsophie dene` ve,1,* alireza alemi,1 and ralph bourdoukan1\n1group for neural theory, de\u00b4 partement d\u2019etudes cognitives, ecole normale supe\u00b4 rieure, 75005 paris, france\n*correspondence: sophie.deneve@ens.fr\nhttp://dx.doi.org/10.1016/j.neuron.2017.05.016\n\nunderstanding how the brain learns to compute functions reliably, ef\ufb01ciently, and robustly with noisy spiking\nactivity is a fundamental challenge in neuroscience. most sens", "5\n1\n0\n2\n \nc\ne\nd\n1\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n3\nv\n7\n6\n5\n0\n0\n\n.\n\n2\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nrethinking the inception architecture for computer vision\n\nchristian szegedy\n\ngoogle inc.\n\nszegedy@google.com\n\nvincent vanhoucke\n\nsergey ioffe\n\nvanhoucke@google.com\n\nsioffe@google.com\n\njonathon shlens\nshlens@google.com\n\nzbigniew wojna\n\nuniversity college london\nzbigniewwojna@gmail.com\n\nabstract\n\nconvolutional networks are at the core of most state-\nof-the-art computer vision solutions for a wide variety of\ntas", "adversarial robustness through local linearization\n\nchongli qin\ndeepmind\n\njames martens\n\ndeepmind\n\nsven gowal\ndeepmind\n\ndilip krishnan\n\ngoogle\n\nkrishnamurthy (dj) dvijotham\n\ndeepmind\n\nalhussein fawzi\n\ndeepmind\n\nsoham de\ndeepmind\n\nrobert stanforth\n\ndeepmind\n\npushmeet kohli\n\ndeepmind\n\nchongliqin@google.com\n\nabstract\n\nadversarial training is an effective methodology to train deep neural networks\nwhich are robust against adversarial, norm-bounded perturbations. however, the\ncomputational cost of adv", "lecture notes in computer science\ncommenced publication in 1973\nfounding and former series editors:\ngerhard goos, juris hartmanis, and jan van leeuwen\n\n7700\n\neditorial board\n\ndavid hutchison\n\nlancaster university, uk\n\ntakeo kanade\n\ncarnegie mellon university, pittsburgh, pa, usa\n\njosef kittler\n\nuniversity of surrey, guildford, uk\n\njon m. kleinberg\n\ncornell university, ithaca, ny, usa\n\nalfred kobsa\n\nuniversity of california, irvine, ca, usa\n\nfriedemann mattern\n\neth zurich, switzerland\n\njohn c. mi", "7\n1\n0\n2\n\n \n\nb\ne\nf\n7\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n0\n6\n3\n8\n0\n\n.\n\n2\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nneural map: structured memory for deep re-\ninforcement learning\n\nemilio parisotto & ruslan salakhutdinov\ndepartment of machine learning\ncarnegie mellon university\npittsburgh, pa 15213, usa\n{eparisot,rsalakhu}@cs.cmu.edu\n\nabstract\n\na critical component to enabling intelligent reasoning in partially observable en-\nvironments is memory. despite this importance, deep reinforcement learning\n(drl) agents have so f", "a biologically plausible 3-factor learning rule for expectation\nmaximization in reinforcement learning and decision making\n\nmohammad javad faraji\n\nschool of life sciences, brain mind institute\n\nschool of computer and communication sciences\n\u00b4ecole polytechnique f\u00b4ed\u00b4eral de lausanne (epfl)\n\nlausanne, ch-1015\n\nmohammadjavad.faraji@epfl.ch\n\nkerstin preuschoff\n\ngeneva \ufb01nance research institute\n\nschool of economics and management\n\nuniversity of geneva\n\ngeneva, ch-1211\n\nkerstin.preuschoff@unige.ch\n\nwu", "lyapunov spectra of chaotic recurrent neural networks\n\nrainer engelken\n\ndepartment of neuroscience, zuckerman institute,\n\ncolumbia university, new york, ny, united states of america\n\nfred wolf\n\nmax planck institute for dynamics and self-organization, g\u00f6ttingen, germany\n\nbernstein center for computational neuroscience, g\u00f6ttingen, germany\n\nbernstein focus for neurotechnology, g\u00f6ttingen, germany and\nfaculty of physics, university of g\u00f6ttingen, g\u00f6ttingen, germany\n\n0\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n3\n\n \n \n]\n\n.\n\nd", "https://doi.org/10.1038/s41583-022-00642-0\n\ncheck for updates\n\nattractor and integrator \nnetworks in the brain\n\nmikail khona1,2,3,4 & ila r. fiete\u2009\nabstract\n\n \u20091,2,3 \n\nin this review, we describe the singular success of attractor neural \nnetwork models in describing how the brain maintains persistent \nactivity states for working memory, corrects errors and integrates noisy \ncues. we consider the mechanisms by which simple and forgetful units \ncan organize to collectively generate dynamics on the", "original research article\npublished: 19 november 2014\ndoi: 10.3389/fnhum.2014.00825\n\nuncertainty in perception and the hierarchical gaussian\nfilter\nchristoph d. mathys 1,2,3,4*, ekaterina i. lomakina 3,4,5, jean daunizeau 6, sandra iglesias 3,4,\nkay h. brodersen 3,4, karl j. friston 1 and klaas e. stephan 1,3,4\n\n1 wellcome trust centre for neuroimaging, institute of neurology, university college london, london, uk\n2 max planck ucl centre for computational psychiatry and ageing research, london, ", "article\n\nneural trajectories in the supplementary motor area\nand motor cortex exhibit distinct geometries,\ncompatible with different classes of computation\n\nhighlights\nd guiding action across time necessitates population activity\n\nwith \u2018\u2018low divergence\u2019\u2019\n\nd the supplementary motor area, but not motor cortex, exhibits\n\nlow divergence\n\nd low divergence explains diverse single-neuron and\n\npopulation-level features\n\nauthors\n\nabigail a. russo, ramin khajeh,\nsean r. bittner, sean m. perkins,\njohn p. c", "neuron\n\narticle\n\nstates versus rewards: dissociable neural\nprediction error signals underlying model-based\nand model-free reinforcement learning\n\njan gla\u00a8 scher,1,3,* nathaniel daw,4 peter dayan,5 and john p. o\u2019doherty1,2,6\n1division of humanities and social sciences\n2computation and neural systems program\ncalifornia institute of technology, pasadena, ca 91101, usa\n3neuroimage nord, department of systems neuroscience, university medical center hamburg-eppendorf, 20246 hamburg, germany\n4center fo", "volume 61, number 3\n\nphysical review letters\n\n18 july 1988\n\nchaos in random neural networks\n\nh. sompolinsky\n\n' and a. crisanti\n\natd'c t bell laboratories, murray hill, new jersey 07974, and\n\nracah institute of physics, the hebrew university, 91904 jerusalem,\n\nisrael\n\nand\n\nh. j. sommers\n\n'~\n\nfachbereich physik, universitat gesa-mthochschule\n\nessen, d-4300 essen, federal republic of germany\n\n(received 30 march 1988)\n\na continuous-time\n\nmetric couplings\ntransition\nthe autocorrelations\n\nis studied. ", "ieee signal processing letters, vol. 9, no. 6, june 2002\n\n177\n\nconditions for nonnegative independent\n\ncomponent analysis\n\nmark plumbley, member, ieee\n\nabstract\u2014 we consider the noiseless linear independent\ncomponent analysis problem,\nin the case where the hid-\nden sources s are non-negative. we assume that the ran-\ndom variables sis are well-grounded in that they have a non-\nvanishing pdf in the (positive) neighbourhood of zero. for\nan orthonormal rotation y = wx of pre-whitened observa-\ntions ", "vol 436|7 july 2005|doi:10.1038/nature03689\n\narticles\n\ndynamic predictive coding by the retina\n\ntoshihiko hosoya1\u2020, stephen a. baccus1\u2020 & markus meister1\n\nretinal ganglion cells convey the visual image from the eye to the brain. they generally encode local differences in space\nand changes in time rather than the raw image intensity. this can be seen as a strategy of predictive coding, adapted\nthrough evolution to the average image statistics of the natural environment. yet animals encounter many", "article\n\ncommunicated by ilya sutskever\n\nopening the black box: low-dimensional dynamics\nin high-dimensional recurrent neural networks\n\ndavid sussillo\nsussillo@stanford.edu\ndepartment of electrical engineering, neurosciences program,\nstanford university, stanford, ca 94305-9505, u.s.a.\n\nomri barak\nomri.barak@gmail.com\ndepartment of neuroscience, columbia university college of\nphysicians and surgeons, new york, ny 10032-2695, u.s.a.\n\nrecurrent neural networks (rnns) are useful tools for learning ", "a r t i c l e s\n\nthe reorganization and reactivation of hippocampal \nmaps predict spatial memory performance\ndavid dupret, joseph o\u2019neill, barty pleydell-bouverie & jozsef csicsvari\nthe hippocampus is an important brain circuit for spatial memory and the spatially selective spiking of hippocampal neuronal \nassemblies is thought to provide a mnemonic representation of space. we found that remembering newly learnt goal locations \nrequired nmda receptor\u2013dependent stabilization and enhanced reactiva", "unsupervised learning of invariant feature hierarchies\n\nwith applications to object recognition\n\nmarc\u2019aurelio ranzato, fu-jie huang, y-lan boureau, yann lecun\n\ncourant institute of mathematical sciences, new york university, new york, ny, usa\n\n{ranzato,jhuangfu,ylan,yann}@cs.nyu.edu, http://www.cs.nyu.edu/\u02dcyann\n\nabstract\n\nwe present an unsupervised method for learning a hier-\narchy of sparse feature detectors that are invariant to small\nshifts and distortions. the resulting feature extractor con", "research\n\nneuroscience\n\nbehavioral time scale synaptic\nplasticity underlies ca1 place fields\n\nkatie c. bittner,1* aaron d. milstein,1,2* christine grienberger,1\nsandro romani,1 jeffrey c. magee1\u2020\n\nlearning is primarily mediated by activity-dependent modifications of synaptic strength within\nneuronal circuits. we discovered that place fields in hippocampal area ca1 are produced by a\nsynaptic potentiation notably different from hebbian plasticity. place fields could be produced\nin vivo in a single", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\non  simplicity  and  complexity  in  the  brave  new  world\nof  large-scale  neuroscience\npeiran  gao1 and  surya  ganguli2\n\ntechnological  advances  have  dramatically  expanded  our\nability  to  probe  multi-neuronal  dynamics  and  connectivity  in  the\nbrain.  however,  our  ability  to  extract  a  simple  conceptual\nunderstanding  from  complex  data  is  increasingly  hampered  by\nthe  lack  of  theoretically  principled  data  ", "8\n1\n0\n2\n\n \nt\nc\no\n9\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n6\nv\n0\n7\n0\n6\n0\n\n.\n\n2\n0\n8\n1\n:\nv\ni\nx\nr\na\n\ndiversity is all you need:\nlearning skills without a reward function\n\nbenjamin eysenbach\u2217\ncarnegie mellon university\nbeysenba@cs.cmu.edu\n\nabhishek gupta\nuc berkeley\n\njulian ibarz\ngoogle brain\n\nsergey levine\nuc berkeley\ngoogle brain\n\nabstract\n\nintelligent creatures can explore their environments and learn useful skills without\nsupervision. in this paper, we propose \u201cdiversity is all you need\u201d(diayn), a\nmethod fo", "topic models\n\ndavid m. blei\n\nprinceton university\n\njohn d. lafferty\n\ncarnegie mellon university\n\n1. introduction\n\nscientists need new tools to explore and browse large collections of schol-\narly literature. thanks to organizations such as jstor, which scan and\nindex the original bound archives of many journals, modern scientists can\nsearch digital libraries spanning hundreds of years. a scientist, suddenly\nfaced with access to millions of articles in her \ufb01eld, is not satis\ufb01ed with\nsimple search.", "learning enhances sensory and multiple non-\nsensory representations in primary visual cortex\n\narticle\n\nhighlights\nd v1 neurons increasingly discriminate task-relevant stimuli\n\nwith learning\n\nd chronic imaging reveals single cell changes underlying this\n\npopulation effect\n\nd learning-related changes are reduced when animals ignore\n\ntask-relevant stimuli\n\nd anticipatory and behavioral choice-related signals emerge in\n\nreward-predicting cells\n\nauthors\n\njasper poort, adil g. khan,\nmarius pachitariu,", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nneuromodulation  of  neurons  and  synapses\nfarzan  nadim  and  dirk  bucher\n\nneuromodulation  underlies  the  \ufb02exibility  of  neural  circuit\noperation  and  behavior.  individual  neuromodulators  can  have\ndivergent  actions  in  a  neuron  by  targeting  multiple\nphysiological  mechanisms.  conversely,  multiple\nneuromodulators  may  have  convergent  actions  through\noverlapping  targets.  the  divergent  and  convergent\nneuromodu", "published as a conference paper at iclr 2022\n\nneural collapse under mse loss: proximity\nto and dynamics on the central path\n\nx.y. han\u2217\ncornell university\nxh332@cornell.edu\n\nvardan papyan\u2217\nuniversity of toronto\nvardan.papyan@utoronto.ca\n\ndavid l. donoho\nstanford university\ndonoho@stanford.edu\n\nabstract\n\nthe recently discovered neural collapse (nc) phenomenon occurs pervasively\nin today\u2019s deep net training paradigm of driving cross-entropy (ce) loss towards\nzero. during nc, last-layer features col", "article\n\ndoi:10.1038/nature14446\n\nneural dynamics for landmark\norientation and angular path integration\n\njohannes d. seelig1 & vivek jayaraman1\n\nmany animals navigate using a combination of visual landmarks and path integration. in mammalian brains, head\ndirection cells integrate these two streams of information by representing an animal\u2019s heading relative to landmarks,\nyet maintaining their directional tuning in darkness based on self-motion cues. here we use two-photon calcium\nimaging in head-", "4\n1\n0\n2\n\n \n\nb\ne\nf\n9\n1\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n4\nv\n9\n9\n1\n6\n\n.\n\n2\n1\n3\n1\n:\nv\ni\nx\nr\na\n\nintriguing properties of neural networks\n\nchristian szegedy\n\ngoogle inc.\n\nwojciech zaremba\nnew york university\n\nilya sutskever\n\njoan bruna\n\ngoogle inc.\n\nnew york university\n\ndumitru erhan\n\ngoogle inc.\n\nian goodfellow\n\nrob fergus\n\nuniversity of montreal\n\nnew york university\n\nfacebook inc.\n\nabstract\n\ndeep neural networks are highly expressive models that have recently achieved\nstate of the art performance on spe", "\f", "journal of computational neuroscience 11, 207\u2013215, 2001\nc(cid:2) 2002 kluwer academic publishers. manufactured in the netherlands.\n\nsupervised and unsupervised learning with two sites\n\nof synaptic integration\n\ninstitute of neuroinformatics, eth/uni z\u00a8urich, winterthurerstr. 190, 8057 z\u00a8urich, switzerland\n\nkonrad p. k \u00a8ording and peter k \u00a8onig\n\nkoerding@ini.phys.ethz.ch\n\nreceived june 1, 2000; revised august 31, 2001; accepted august 31, 2001\n\naction editor: terrence sejnowski\n\nabstract. many lea", "a r t i c l e s\n\nexpectancy-related changes in firing of dopamine \nneurons depend on orbitofrontal cortex\nyuji k takahashi1, matthew r roesch2,3, robert c wilson4,5, kathy toreson1, patricio o\u2019donnell1,6, yael niv4,5,8 & \ngeoffrey schoenbaum1,6\u20138\n\nthe orbitofrontal cortex has been hypothesized to carry information regarding the value of expected rewards. such information \nis essential for associative learning, which relies on comparisons between expected and obtained reward for generating \ninstr", "biorxiv preprint \ncertified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made available under \n\nthe copyright holder for this preprint (which was not\n\nthis version posted march 1, 2019. \n\nhttps://doi.org/10.1101/564476\n; \n\ndoi: \n\na\n\ncc-by-nc-nd 4.0 international license\n.\n\ndynamic compression and expansion in a classifying recurrent\n\nnetwork\n\nmatthew farrell1-2, stefano recanatesi1, guillaume lajoie3-4, and eric\n\nshea-brown1", "kickback cuts backprop\u2019s red-tape:\n\nbiologically plausible credit assignment in neural networks\n\ndavid balduzzi\n\ndavid.balduzzi@vuw.ac.nz\nvictoria university of wellington\n\nhastagiri vanchinathan\nhastagiri@inf.ethz.ch\n\neth zurich\n\njoachim buhmann\n\njbuhmann@inf.ethz.ch\n\neth zurich\n\n4\n1\n0\n2\n\n \n\nv\no\nn\n3\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n1\n9\n1\n6\n\n.\n\n1\n1\n4\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nerror backpropagation is an extremely effective algorithm\nfor assigning credit in arti\ufb01cial neural networks. however,\nwei", "article\n\ncomputing by robust transience: how the fronto-\nparietal network performs sequential, category-\nbased decisions\n\nhighlights\nd recurrent networks trained to perform dmc tasks exhibit\n\nrobust transience dynamics\n\nd dynamics consist of stable and slow states connected by\n\nrobust trajectory tunnels\n\nd models\u2019 neural activities are remarkably similar to recordings\n\nfrom lip and pfc\n\nd trained rnns replicate categorization studies with multiple\n\ncategories\n\nauthors\n\nwarasinee chaisangmongkon,", "taylorized training: towards better approximation of\n\nneural network training at finite width\n\nyu bai 1 ben krause 1 huan wang 1 caiming xiong 1 richard socher 1\n\n0\n2\n0\n2\n\n \n\nb\ne\nf\n4\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n0\n1\n0\n4\n0\n\n.\n\n2\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe propose taylorized training as an initiative to-\nwards better understanding neural network train-\ning at \ufb01nite width. taylorized training involves\ntraining the k-th order taylor expansion of the\nneural network at initialization, and is", "www.neuroscience-ibro.com\n\npii: s 0 3 0 6 - 4 5 2 2 ( 0 2 ) 0 0 0 2 6 - x\n\nneuroscience vol. 111, no. 4, pp. 815^835, 2002\n\u00df 2002 ibro. published by elsevier science ltd\nall rights reserved. printed in great britain\n0306-4522 / 02 $22.00+0.00\n\nneuromodulatory transmitter systems in the cortex and\n\ntheir role in cortical plasticity\n\nbrain research center, and department of ophthalmology, university of british columbia, and vancouver hospital and health\n\nsciences center, 2550 willow street, vancou", "1684 \u2022 the journal of neuroscience, january 23, 2013 \u2022 33(4):1684 \u20131695\n\nsystems/circuits\n\ngating of sensory input by spontaneous cortical activity\n\nartur luczak,1,2 peter bartho,1,3 and kenneth d. harris1,4\n1center for molecular and behavioral neuroscience, rutgers university, newark, new jersey 07102, 2canadian centre for behavioural neuroscience,\nuniversity of lethbridge, lethbridge, alberta, canada t1k 3m4, 3institute of experimental medicine, hungarian academy of sciences, budapest 1083,\nhu", "single neurons in the human brain encode numbers\n\narticle\n\nhighlights\nd single neurons in the human medial temporal lobe (mtl)\n\nencode numerical information\n\nd numerosity and abstract numerals are encoded by distinct\n\nneuronal populations\n\nd numerosity representation shows a distance effect; numerals\n\nare encoded categorically\n\nd representation of symbolic numerals may evolve from\n\nnumerosity representations\n\nauthors\n\nesther f. kutter, jan bostroem,\nchristian e. elger, florian mormann,\nandreas n", "contrastive learning inverts the data generating process\n\nroland s. zimmermann * 1 2 yash sharma * 1 2 steffen schneider * 1 2 3 matthias bethge \u2020 1 wieland brendel \u2020 1\n\nabstract\n\ncontrastive learning has recently seen tremendous\nsuccess in self-supervised learning. so far, how-\never, it is largely unclear why the learned represen-\ntations generalize so effectively to a large variety\nof downstream tasks. we here prove that feed-\nforward models trained with objectives belonging\nto the commonly us", "if deep learning is the answer, what \nis the question?\n\nandrew\u00a0saxe \n\n , stephanie\u00a0nelli \n\n  and christopher\u00a0summerfield \n\n \n\nabstract | neuroscience research is undergoing a minor revolution. recent \nadvances in machine learning and artificial intelligence research have opened up \nnew ways of thinking about neural computation. many researchers are excited by \nthe possibility that deep neural networks may offer theories of perception, \ncognition and action for biological brains. this approach ha", "letter\nneural constraints on learning\n\ndoi:10.1038/nature13665\n\npatrick t. sadtler1,2,3, kristin m. quick1,2,3, matthew d. golub2,4, steven m. chase2,5, stephen i. ryu6,7,\nelizabeth c. tyler-kabara1,8,9, byron m. yu2,4,5* & aaron p. batista1,2,3*\n\nlearning, whether motor, sensory or cognitive, requires networks of\nneurons to generate new activity patterns. as some behaviours are\neasier to learn than others1,2, we asked if some neural activity patterns\nare easier to generate than others. here we ", "catalyzing next-generation artificial intelligence through neuroai\n\nby anthony zador*, sean escola*, blake richards, bence \u00f6lveczky, yoshua bengio, kwabena boahen,\n\nmatthew botvinick,  dmitri chklovskii, anne churchland, claudia clopath, james dicarlo, surya\nganguli, jeff hawkins, konrad k\u00f6rding, alexei koulakov, yann lecun, timothy lillicrap, adam\n\nmarblestone, bruno olshausen, alexandre pouget, cristina savin, terrence sejnowski, eero simoncelli,\n\nsara solla, david sussillo, andreas s. tolias,", "3\n2\n0\n2\n\n \nt\nc\no\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n7\n0\n2\n2\n0\n\n.\n\n0\n1\n3\n2\n:\nv\ni\nx\nr\na\n\npreprint\n\nlanguage models represent space and time\n\nwes gurnee & max tegmark\nmassachusetts institute of technology\n{wesg, tegmark}@mit.edu\n\nabstract\n\nthe capabilities of large language models (llms) have sparked debate over\nwhether such systems just learn an enormous collection of superficial statistics\nor a coherent model of the data generating process\u2014a world model. we find\nevidence for the latter by analyzin", "1798\n\nieee transactions on pattern analysis and machine intelligence, vol. 35, no. 8, august 2013\n\nrepresentation learning:\n\na review and new perspectives\n\nyoshua bengio, aaron courville, and pascal vincent\n\nabstract\u2014the success of machine learning algorithms generally depends on data representation, and we hypothesize that this is\nbecause different representations can entangle and hide more or less the different explanatory factors of variation behind the data.\nalthough specific domain knowledg", "article\n\nencoding of discriminative fear memory by input-\nspeci\ufb01c ltp in the amygdala\n\nhighlights\nd ltp is not induced globally in acx/mgn-la pathways in\n\nauthors\n\nwoong bin kim, jun-hyeong cho\n\ndiscriminative fear learning\n\nd ltp is induced in cs+, but not cs\u2013, pathways to la in\n\ndiscriminative fear learning\n\nd synapses in cs+ pathways to la remain potentiated after\n\nfear extinction\n\nd depotentiation of cs+, but not cs\u2013, pathways to la prevents\n\nthe recall of fear memory\n\ncorrespondence\njuncho@", "7\n1\n0\n2\n\n \n\nb\ne\nf\n7\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n3\n0\n8\n8\n0\n\n.\n\n5\n0\n6\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2017\n\ndensity estimation using real nvp\n\nlaurent dinh\u2217\nmontreal institute for learning algorithms\nuniversity of montreal\nmontreal, qc h3t1j4\n\njascha sohl-dickstein\ngoogle brain\n\nsamy bengio\ngoogle brain\n\nabstract\n\nunsupervised learning of probabilistic models is a central yet challenging problem\nin machine learning. speci\ufb01cally, designing models with tractable learning", "original research\npublished: 04 may 2017\ndoi: 10.3389/fncom.2017.00024\n\nequilibrium propagation: bridging\nthe gap between energy-based\nmodels and backpropagation\n\nbenjamin scellier * and yoshua bengio \u2020\n\nd\u00e9partement d\u2019informatique et de recherche op\u00e9rationnelle, montreal institute for learning algorithms, universit\u00e9 de\n\nmontr\u00e9al, montreal, qc, canada\n\nwe introduce equilibrium propagation, a learning framework for energy-based models.\nit involves only one kind of neural computation, performed in ", "article\n\ndoi:10.1038/nature12742\n\ncontext-dependent computation by\nrecurrent dynamics in prefrontal cortex\n\nvalerio mante1{*, david sussillo2*, krishna v. shenoy2,3 & william t. newsome1\n\nprefrontal cortex is thought to have a fundamental role in flexible, context-dependent behaviour, but the exact nature of\nthe computations underlying this role remains largely unknown. in particular, individual prefrontal neurons often\ngenerate remarkably complex responses that defy deep understanding of their ", "a r t i c l e s\n\nmembrane potential correlates of sensory perception \nin mouse barrel cortex\nshankar sachidhanandam, varun sreenivasan, alexandros kyriakatos, yves kremer & carl c h petersen\nneocortical activity can evoke sensory percepts, but the cellular mechanisms remain poorly understood. we trained mice to \ndetect single brief whisker stimuli and report perceived stimuli by licking to obtain a reward. pharmacological inactivation and \noptogenetic stimulation demonstrated a causal role for t", "6\n1\n0\n2\n\n \n\nn\na\nj\n \n\n7\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n9\n2\n6\n0\n\n.\n\n1\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nunder review as a conference paper at iclr 2016\n\npolicy distillation\n\nandrei a. rusu, sergio g\u00b4omez colmenarejo, c\u00b8 a\u02d8glar g\u00a8ulc\u00b8ehre\u2217, guillaume desjardins,\njames kirkpatrick, razvan pascanu, volodymyr mnih, koray kavukcuoglu & raia hadsell\ngoogle deepmind\nlondon, uk\n{andreirusu, sergomez, gdesjardins, kirkpatrick, razp, vmnih,\nkorayk, raia}@google.com, gulcehrc@iro.umontreal.ca\n\nabstract\n\npolicies for comple", "the discovery of structural form\n\ncharles kemp*\u2020 and joshua b. tenenbaum\u2021\n\n*department of psychology, carnegie mellon university, 5000 forbes avenue, pittsburgh, pa 15213; and \u2021department of brain and cognitive sciences,\nmassachusetts institute of technology, 77 massachusetts avenue, cambridge, ma 02139\n\nedited by richard m. shiffrin, indiana university, bloomington, in, and approved may 30, 2008 (received for review march 17, 2008)\n\nalgorithms for \ufb01nding structure in data have become increasing", "journal of computational neuroscience 11, 63\u201385, 2001\nc(cid:2) 2001 kluwer academic publishers. manufactured in the netherlands.\n\neffects of neuromodulation in a cortical network model of object working\n\nmemory dominated by recurrent inhibition\n\nlps, ecole normale sup\u00b4erieure, 24 rue lhomond, 75231 paris cedex 05, france\n\nnicolas brunel\n\nbrunel@lps.ens.fr\n\nxiao-jing wang\n\nvolen center for complex systems, brandeis university, 415 south st., waltham, ma 02254-9110, usa\n\nxjwang@volen.brandeis.edu\n", "physical review research 3, 013176 (2021)\n\nquality of internal representation shapes learning performance in feedback neural networks\n\nlee susman ,1,2,*,\u2020 francesca mastrogiuseppe,3,*,\u2021 naama brenner\nand omri barak 2,5,\u00a7,\u00b6\n1interdisciplinary program in applied mathematics, technion israel institute of technology, haifa 32000, israel\n\n,2,4,\u00a7,(cid:2)\n\n2network biology research laboratories, technion israel institute of technology, haifa 32000, israel\n\n3gatsby computational neuroscience unit, unive", "research article\n\ndynamic representation of partially\noccluded objects in primate prefrontal\nand visual cortex\namber m fyall1\u2020, yasmine el-shamayleh2\u2020, hannah choi3, eric shea-brown4,\nanitha pasupathy5*\n\n1department of biological structure, washington national primate research center,\nuniversity of washington, seattle, united states; 2physiology and biophysics,\nwashington national primate research center, university of washington, seattle,\nunited states; 3applied mathematics, university of washi", "predictive information in a sensory population\n\nstephanie e. palmera,b, olivier marrec,d, michael j. berry iic,d, and william bialeka,b,1\n\najoseph henry laboratories of physics and blewis\u2013sigler institute for integrative genomics, and cdepartment of molecular biology and dprinceton\nneuroscience institute, princeton university, princeton, nj 08544\n\ncontributed by william bialek, april 13, 2015 (sent for review january 19, 2014)\n\nguiding behavior requires the brain to make predictions about the\nfu", "deep neural networks rival the representation of\nprimate it cortex for core visual object recognition\n\ncharles f. cadieu1*, ha hong1,2, daniel l. k. yamins1, nicolas pinto1, diego ardila1, ethan a. solomon1,\nnajib j. majaj1, james j. dicarlo1\n\n1 department of brain and cognitive sciences and mcgovern institute for brain research, massachusetts institute of technology, cambridge, massachusetts, united states\nof america, 2 harvard\u2013mit division of health sciences and technology, institute for medic", "journal of machine learning research 9 (2008) 2579-2605\n\nsubmitted 5/08; revised 9/08; published 11/08\n\nvisualizing data using t-sne\n\nlaurens van der maaten\nticc\ntilburg university\np.o. box 90153, 5000 le tilburg, the netherlands\n\ngeoffrey hinton\ndepartment of computer science\nuniversity of toronto\n6 king\u2019s college road, m5s 3g4 toronto, on, canada\n\neditor: yoshua bengio\n\nlvdmaaten@gmail.com\n\nhinton@cs.toronto.edu\n\nabstract\n\nwe present a new technique called \u201ct-sne\u201d that visualizes high-dimensio", "nerf in the wild: neural radiance fields for unconstrained photo collections\n\nricardo martin-brualla\u2217, noha radwan\u2217, mehdi s. m. sajjadi\u2217,\njonathan t. barron, alexey dosovitskiy, and daniel duckworth\n\n{rmbrualla, noharadwan, msajjadi, barron, adosovitskiy, duckworthd}@google.com\n\ngoogle research\n\nabstract\n\nwe present a learning-based method for synthesizing\nnovel views of complex scenes using only unstructured col-\nlections of in-the-wild photographs. we build on neural\nradiance fields (nerf), w", "align, then memorise:\n\nthe dynamics of learning with feedback alignment\n\nmaria re\ufb01netti * 1 2 st\u00b4ephane d\u2019ascoli * 1 3 ruben ohana 1 4 sebastian goldt 5\n\n1\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n0\n1\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n8\n2\n4\n2\n1\n\n.\n\n1\n1\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\ndirect feedback alignment (dfa) is emerging\nas an ef\ufb01cient and biologically plausible alterna-\ntive to backpropagation for training deep neural\nnetworks. despite relying on random feedback\nweights for the backward pass, dfa successfully\ntrains", "r e v i e w\n\nthe neocortical circuit: themes and \nvariations\n\nkenneth d harris1 & gordon m g shepherd2\n\nsimilarities in neocortical circuit organization across areas and species suggest a common strategy to process diverse types of \ninformation, including sensation from diverse modalities, motor control and higher cognitive processes. cortical neurons belong to \na small number of main classes. the properties of these classes, including their local and long-range connectivity, developmental \nhist", "8\n1\n0\n2\n\n \nt\nc\no\n1\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n0\n0\n9\n0\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nalgorithmic regularization in learning deep homogeneous\n\nmodels: layers are automatically balanced\n\nsimon s. du\u2217\n\nwei hu\u2020\n\njason d. lee\u2021\n\nabstract\n\nwe study the implicit regularization imposed by gradient descent for learning multi-layer homogeneous\nfunctions including feed-forward fully connected and convolutional deep neural networks with linear,\nrelu or leaky relu activation. we rigorously prove that gradi", "published as a conference paper at iclr 2019\n\nkernel rnn learning (kernl)\n\nchristopher roth[1],[2], ingmar kanitscheider[2],[3], and ila fiete[2],[4]\n[1] department of physics, university of texas at austin, austin, tx, 78712\n[2] department of neuroscience, university of texas at austin, austin, tx, 78712\n[3] openai, san francisco ca, 94110\n[4] department of brain and cognitive sciences, massachusetts institute of technology, cambridge, ma, 02139\n\nabstract\n\nwe describe kernel rnn learning (kernl", "\u00a0\u00a0\u00a0\u00a0a\u00a0tutorial\u00a0on\n\nenergy\u00adbased\u00a0learning\n\n\u00a0yann\u00a0lecun,\u00a0sumit\u00a0chopra,\u00a0raia\u00a0hadsell,\u00a0\n\nmarc'aurelio\u00a0ranzato,\u00a0fu\u00adjie\u00a0huang\n\n\u00a0\u00a0\u00a0\u00a0the\u00a0courant\u00a0institute\u00a0of\u00a0mathematical\u00a0sciences\n\nnew\u00a0york\u00a0university\n\nhttp://yann.lecun.com\nhttp://www.cs.nyu.edu/~yann\n\nyann\u00a0lecun\n\n\f", "human neuroscience\na bayesian foundation for individual learning under \nuncertainty\n\noriginal research article\npublished: 02 may 2011\ndoi: 10.3389/fnhum.2011.00039\n\nchristoph mathys1,2*, jean daunizeau1,3, karl j. friston3 and klaas e. stephan1,3\n\n1  laboratory for social and neural systems research, department of economics, university of zurich, zurich, switzerland\n2  institute for biomedical engineering, eidgen\u00f6ssische technische hochschule zurich, zurich, switzerland\n3  wellcome trust centre ", "2\n2\n0\n2\n\n \n\nb\ne\nf\n7\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n4\n8\n3\n8\n0\n\n.\n\n2\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nlimitations of neural collapse\n\nfor understanding generalization in deep learning\n\nlike hui\n\nlhui@ucsd.edu\n\nmikhail belkin\n\nmbelkin@ucsd.edu\n\npreetum nakkiran\npreetum@ucsd.edu\n\nhal\u0131c\u0131o\u02d8glu data science institute\nuniversity of california san diego\n\nabstract\n\n(cid:145)e recent work of papyan, han, and donoho (2020) presented an intriguing \u201cneural collapse\u201d phenomenon,\nshowing a structural property of interpola", "neuron\n\nreview\n\ndopamine in motivational control:\nrewarding, aversive, and alerting\n\nethan s. bromberg-martin,1 masayuki matsumoto,1,2 and okihide hikosaka1,*\n1laboratory of sensorimotor research, national eye institute, national institutes of health, bethesda, maryland 20892, usa\n2primate research institute, kyoto university, inuyama, aichi 484-8506, japan\n*correspondence: oh@lsr.nei.nih.gov\ndoi 10.1016/j.neuron.2010.11.022\n\nmidbrain dopamine neurons are well known for their strong responses to", "9\n1\n0\n2\n\n \n\ny\na\nm\n8\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n4\n0\n8\n3\n0\n\n.\n\n1\n1\n8\n1\n:\nv\ni\nx\nr\na\n\ngradient descent finds global minima of deep neural networks\n\nsimon s. du * 1 jason d. lee * 2 haochuan li * 3 4 liwei wang * 5 4 xiyu zhai * 6\n\nabstract\n\ngradient descent \ufb01nds a global minimum in train-\ning deep neural networks despite the objective\nfunction being non-convex. the current pa-\nper proves gradient descent achieves zero train-\ning loss in polynomial time for a deep over-\nparameterized neural ne", "5314 \u2022 the journal of neuroscience, may 11, 2016 \u2022 36(19):5314 \u20135327\n\nsystems/circuits\n\nselectivity of neuromodulatory projections from the basal\nforebrain and locus ceruleus to primary sensory cortices\n\njae-hyun kim,1 a-hyun jung,1 daun jeong,1 ilsong choi,1 kwansoo kim,1 soowon shin,2 sung june kim,2\nand x seung-hee lee1\n1department of biological sciences, korea advanced institute of science and technology, daejeon 34141, korea, and 2department of electrical and\ncomputer engineering, seoul nat", "a r t i c l e s\n\nconjunctive input processing drives feature selectivity \nin hippocampal ca1 neurons\nkatie c bittner1, christine grienberger1, sachin p vaidya1, aaron d milstein1, john j macklin1, junghyup suh2,3, \nsusumu tonegawa2,3 & jeffrey c magee1\n\nfeature-selective firing allows networks to produce representations of the external and internal environments. despite its \nimportance, the mechanisms generating neuronal feature selectivity are incompletely understood. in many cortical microcirc", "understanding the dif\ufb01culty of training deep feedforward neural networks\n\nxavier glorot\n\nyoshua bengio\n\ndiro, universit\u00b4e de montr\u00b4eal, montr\u00b4eal, qu\u00b4ebec, canada\n\nabstract\n\nwhereas before 2006 it appears that deep multi-\nlayer neural networks were not successfully\ntrained, since then several algorithms have been\nshown to successfully train them, with experi-\nmental results showing the superiority of deeper\nvs less deep architectures. all these experimen-\ntal results were obtained with new initi", "convolutional networks for fast, energy-efficient\nneuromorphic computing\n\nsteven k. essera,1, paul a. merollaa, john v. arthura, andrew s. cassidya, rathinakumar appuswamya,\nalexander andreopoulosa, david j. berga, jeffrey l. mckinstrya, timothy melanoa, davis r. barcha, carmelo di nolfoa,\npallab dattaa, arnon amira, brian tabaa, myron d. flicknera, and dharmendra s. modhaa\n\nabrain-inspired computing, ibm research\u2013almaden, san jose, ca 95120\n\nedited by karlheinz meier, university of heidelberg, ", "a r t i c l e s\n\nproperties of basal dendrites of layer 5 pyramidal\nneurons: a direct patch-clamp recording study\n\nthomas nevian1,2,4, matthew e larkum1,4, alon polsky3 & jackie schiller3\n\nbasal dendrites receive the majority of synapses that contact neocortical pyramidal neurons, yet our knowledge of synaptic\nprocessing in these dendrites has been hampered by their inaccessibility for electrical recordings. a new approach to patch-clamp\nrecordings enabled us to characterize the integrative prop", "9\n1\n0\n2\n\n \n\np\ne\ns\n8\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n6\n5\n1\n8\n0\n\n.\n\n9\n0\n9\n1\n:\nv\ni\nx\nr\na\n\ndynamics of deep neural networks and neural tangent hierarchy\n\njiaoyang huang\n\nias\n\ne-mail: jiaoyang@ias.edu\n\nhorng-tzer yau\n\nharvard university\n\ne-mail: htyau@math.harvard.edu\n\nabstract\n\nthe evolution of a deep neural network trained by the gradient descent can be described by its neural\ntangent kernel (ntk) as introduced in [20], where it was proven that in the in\ufb01nite width limit the\nntk converges to an e", "9\n1\n0\n2\n\n \n\nb\ne\nf\n2\n2\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n5\nv\n5\n8\n8\n0\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nessentially no barriers in neural network energy landscape\n\nfelix draxler 1 2 kambis veschgini 2 manfred salmhofer 2 fred a. hamprecht 1\n\nabstract\n\ntraining neural networks involves \ufb01nding min-\nima of a high-dimensional non-convex loss func-\ntion. relaxing from linear interpolations, we con-\nstruct continuous paths between minima of re-\ncent neural network architectures on cifar10\nand cifar100. surprisin", "a fast pairwise heuristic for planning under uncertainty\n\nkoosha khalvati and alan k. mackworth\n\n{kooshakh, mack}@cs.ubc.ca\ndepartment of computer science\nuniversity of british columbia\n\nvancouver, b.c. v6t 1z4 canada\n\nabstract\n\npomdp (partially observable markov decision process) is\na mathematical framework that models planning under un-\ncertainty. solving a pomdp is an intractable problem and\neven the state of the art pomdp solvers are too computation-\nally expensive for large domains. this is", "ll\n\nopen access\n\nperspective\nplanning in the brain\n\nmarcelo g. mattar1,2,* and ma\u00b4 te\u00b4 lengyel2,3\n1department of cognitive science, university of california, san diego, san diego, ca, usa\n2computational and biological learning lab, department of engineering, university of cambridge, cambridge, uk\n3center for cognitive computation, department of cognitive science, central european university, budapest, hungary\n*correspondence: mmattar@ucsd.edu\nhttps://doi.org/10.1016/j.neuron.2021.12.018\n\nsummary", "independent component\nanalysis: recent advances\naapo hyv\u00e4rinen\n\nrsta.royalsocietypublishing.org\n\ndepartment of computer science, department of mathematics and\nstatistics, and hiit, university of helsinki, helsinki, finland\n\nreview\ncite this article: hyv\u00e4rinen a. 2013\nindependent component analysis: recent\nadvances. phil trans r soc a 371: 20110534.\nhttp://dx.doi.org/10.1098/rsta.2011.0534\n\none contribution of 17 to a discussion meeting\nissue \u2018signal processing and inference for the\nphysical scie", "stochastic backpropagation and approximate inference\n\nin deep generative models\n\ndanilo j. rezende, shakir mohamed, daan wierstra\n\n{danilor, shakir, daanw}@google.com\n\ngoogle deepmind, london\n\n4\n1\n0\n2\n\n \n\ny\na\nm\n0\n3\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n2\n8\n0\n4\n\n.\n\n1\n0\n4\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe marry ideas from deep neural networks\nand approximate bayesian inference to derive\na generalised class of deep, directed genera-\ntive models, endowed with a new algorithm\nfor scalable inference and learn", "continual lifelong learning with neural networks:\n\na review\n\ngerman i. parisi1, ronald kemker2, jose l. part3, christopher kanan2, stefan wermter1\n\n1knowledge technology, department of informatics, universit\u00a8at hamburg, germany\n\n2chester f. carlson center for imaging science, rochester institute of technology, ny, usa\n\n3department of computer science, heriot-watt university, edinburgh centre for robotics, scotland, uk\n\n9\n1\n0\n2\n\n \n\nb\ne\nf\n1\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n9\n6\n5\n7\n0\n\n.\n\n2\n0\n8\n1\n:", "2\n2\n0\n2\n\n \nr\np\na\n9\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n5\nv\n6\n8\n6\n2\n0\n\n.\n\n7\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nmeta-learning through hebbian plasticity in\n\nrandom networks\n\nelias najarro and sebastian risi\n\nit university of copenhagen\n2300 copenhagen, denmark\nenaj@itu.dk, sebr@itu.dk\n\nabstract\n\nlifelong learning and adaptability are two de\ufb01ning aspects of biological agents.\nmodern reinforcement learning (rl) approaches have shown signi\ufb01cant progress\nin solving complex tasks, however once training is concluded, the foun", "a biophysical model of bidirectional synaptic\nplasticity: dependence on ampa\nand nmda receptors\n\ngastone c. castellani*, elizabeth m. quinlan\u2020, leon n cooper\u2021\u00a7\u00b6, and harel z. shouval\u2021i\n\n*physics department, cig and dimor\ufb01pa bologna university, bologna 40121, italy; \u2020department of biology, university of maryland, college park, md 20742;\nand \u2021institute for brain and neural systems, \u00a7department of neuroscience, and \u00b6department of physics, brown university, providence, ri 02912\n\ncontributed by leon ", "neuronal codes for arithmetic rule processing in the\nhuman brain\n\narticle\n\ngraphical abstract\n\nauthors\n\nesther f. kutter, jan bostro\u00a8 m,\nchristian e. elger, andreas nieder,\nflorian mormann\n\ncorrespondence\nandreas.nieder@uni-tuebingen.de (a.n.),\n\ufb02orian.mormann@ukbonn.de (f.m.)\n\nin brief\nkutter et al. demonstrate abstract and\nnotation-independent codes for addition\nand subtraction in neuronal populations\nin the human medial temporal lobe (mtl).\na dynamic code in the parahippocampal\ncortex contrast", "b r i e f c o m m u n i c at i o n s\n\nneural repetition suppression\nre\ufb02ects ful\ufb01lled perceptual\nexpectations\nchristopher summer\ufb01eld1,2, emily h trittschuh3,\njim m monti3, m-marsel mesulam3 & tobias egner3\n\nstimulus-evoked neural activity is attenuated on stimulus\nrepetition (repetition suppression), a phenomenon that is\nattributed to largely automatic processes in sensory neurons.\nby manipulating the likelihood of stimulus repetition, we found\nthat repetition suppression in the human brain was r", "article\n\ndoi:10.1038/nature11649\n\nthe entorhinal grid map is discretized\n\nhanne stensola1*, tor stensola1*, trygve solstad1, kristian fr\u00f8land1, may-britt moser1 & edvard i. moser1\n\nthe medial entorhinal cortex (mec) is part of the brain\u2019s circuit for dynamic representation of self-location. the metric\nof this representation is provided by grid cells, cells with spatial firing fields that tile environments in a periodic\nhexagonal pattern. limited anatomical sampling has obscured whether the grid ", "biorxiv preprint \n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\nhttps://doi.org/10.1101/2023.01.16.523429\n; \n\nthis version posted january 19, 2023. \n\nthe copyright holder for this preprint\n\ndoi: \n\navailable under a\n\ncc-by 4.0 international license\n.\n\na recurrent network model of planning explains hippocampal replay\n\nand human behavior\n\nkristopher t. jensen@1, guillaume hennequin\u22171, and marcelo ", "material on science online.\n\ns. j. morrison, cell 121, 1109 (2005).\n\n8. m. j. kiel, o. h. yilmaz, t. iwashita, c. terhorst,\n9. d. a. sipkins et al., nature 435, 969 (2005).\n10. materials and methods are available as supporting\n11. a. peled et al., science 283, 845 (1999).\n12. t. lapidot, o. kollet, leukemia 16, 1992 (2002).\n13. h. e. broxmeyer, curr. opin. hematol. 15, 49 (2008).\n14. d. j. ceradini et al., nat. med. 10, 858 (2004).\n15. c. hitchon et al., arthritis rheum. 46, 2587 (2002).\n16. b. ", "communicated by david haussler \n\na practical bayesian framework for backpropagation \nnetworks \n\ndavid j.  c. mackay\u2019 \ncomputation and neural systems, california lnstitute of technology 139-74, \npasadena, c a  91125 usa \n\na quantitative and practical bayesian framework is described for learn- \ning  of  mappings  in  feedforward  networks.  the  framework  makes \npossible (1) objective comparisons between solutions using alternative \nnetwork architectures, (2) objective stopping rules for network ", "letter\n\ncommunicated by alessandro treves\n\nmemory states and transitions between them\nin attractor neural networks\n\nstefano recanatesi\nstefano.recanatesi@gmail.com\nmikhail katkov\nmikhail.katkov@gmail.com\nmisha tsodyks\nmtsodyks@gmail.com\nneurobiology department, weizmann institute of science, rehovot 76100, israel\n\nhuman memory is capable of retrieving similar memories to a just re-\ntrieved one. this associative ability is at the base of our everyday pro-\ncessing of information. current models of", "p\ns\ny\nc\nh\no\nm\ne\nt\nr\ni\nk\na\n-\n-\nv\no\nl\n.\n \n3\n1\n~\n \nn\no\n,\n \n1\n \nm\na\na\nc\nh\n,\n \n1\n9\n6\n6\n \na\n \ng\ne\nn\ne\nr\na\nl\ni\nz\ne\nd\n \ns\no\nl\nu\nt\ni\no\nn\n \no\nf\n \nt\nh\ne\n \no\nr\nt\nh\no\ng\no\nn\na\nl\n \np\nr\no\nc\nr\nu\ns\nt\ne\ns\n \np\nr\no\nb\nl\ne\nm\n*\n \np\ne\nt\ne\nr\n \nh\n.\n \ns\nc\ni\n-\ni\n6\nn\ne\nm\na\nn\nn\n \np\ns\ny\nc\nh\no\nm\ne\nt\nr\ni\nc\n \nl\na\nb\no\nr\na\nt\no\nr\ny\n \nn\no\nr\nt\nh\n \nc\na\nr\no\nl\ni\nn\na\n \nt\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n \no\nf\n \na\n \ns\no\nl\nu\nt\ni\no\nn\n \nt\n \no\nf\n \nt\nh\ne\n \nl\ne\na\ns\nt\n-\ns\nq\nu\na\nr\ne\ns\n \np\nr\no\nb\nl\ne\nm\n \na\n \nt\n \n=\n \nb\n \n+\n \ne\n,\n \ng\ni\nv\ne\nn\n \na\n \na\nn\nd\n \nb\n \ns\n", "0\n2\n0\n2\n\n \n\ny\na\nm\n3\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n4\n5\n0\n2\n0\n\n.\n\n0\n1\n9\n1\n:\nv\ni\nx\nr\na\n\nzero: memory optimizations toward training trillion\n\nparameter models\n\nsamyam rajbhandari\u2217, je\ufb00 rasley\u2217, olatunji ruwase, yuxiong he\n\n{samyamr, jerasley, olruwase, yuxhe}@microsoft.com\n\nabstract\n\nlarge deep learning models o\ufb00er signi\ufb01cant accuracy gains, but training billions to trillions of\nparameters is challenging. existing solutions such as data and model parallelisms exhibit funda-\nmental limitations to", "optical control of neuronal ion \nchannels and receptors\n\n 3*\n\n 2* and alexandre\u00a0mourot \n\n 1*, graham\u00a0c.\u00a0r.\u00a0ellis- davies \n\npierre\u00a0paoletti \nabstract | light- controllable tools provide powerful means to manipulate and interrogate  \nbrain function with relatively low invasiveness and high spatiotemporal precision. although \noptogenetic approaches permit neuronal excitation or inhibition at the network level, other \ntechnologies, such as optopharmacology (also known as photopharmacology) have emer", "mach learn (2016) 102:349\u2013391\ndoi 10.1007/s10994-015-5528-6\n\nsupersparse linear integer models for optimized medical\nscoring systems\nberk ustun1 \u00b7 cynthia rudin2\n\nreceived: 1 february 2015 / accepted: 5 august 2015 / published online: 5 november 2015\n\u00a9 the author(s) 2015\n\nabstract scoring systems are linear classi\ufb01cation models that only require users to add,\nsubtract and multiply a few small numbers in order to make a prediction. these models are in\nwidespread use by the medical community, but ", "an empirical investigation of catastrophic forgetting in\n\ngradient-based neural networks\n\nian j. goodfellow\nmehdi mirza\nda xiao\naaron courville\nyoshua bengio\n\nabstract\n\ncatastrophic forgetting is a problem faced\nby many machine learning models and al-\ngorithms. when trained on one task, then\ntrained on a second task, many machine\nlearning models \u201cforget\u201d how to perform the\n\ufb01rst task. this is widely believed to be a\nserious problem for neural networks. here,\nwe investigate the extent to which the", "9\n1\n0\n2\n\n \nt\nc\no\n5\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n4\n0\n1\n7\n0\n\n.\n\n0\n1\n9\n1\n:\nv\ni\nx\nr\na\n\northogonal gradient descent for continual learning\n\nmehrdad farajtabar\n\ndeepmind\n\nnavid azizan1\n\ncaltech\n\nalex mott\ndeepmind\n\nang li\ndeepmind\n\nabstract\n\nneural networks are achieving state of the art\nand sometimes super-human performance on\nlearning tasks across a variety of domains.\nwhenever these problems require learning in\na continual or sequential manner, however,\nneural networks su\ufb00er from the problem o", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nrecurrent  neural  networks  as  versatile  tools  of\nneuroscience  research\nomri  barak\n\nrecurrent  neural  networks  (rnns)  are  a  class  of  computational\nmodels  that  are  often  used  as  a  tool  to  explain  neurobiological\nphenomena,  considering  anatomical,  electrophysiological  and\ncomputational  constraints.\nrnns  can  either  be  designed  to  implement  a  certain\ndynamical  principle,  or  they  can  be  trained  by ", "understanding black-box predictions via in\ufb02uence functions\n\npang wei koh 1 percy liang 1\n\nabstract\n\nhow can we explain the predictions of a black-\nbox model? in this paper, we use in\ufb02uence func-\ntions \u2014 a classic technique from robust statis-\ntics \u2014 to trace a model\u2019s prediction through the\nlearning algorithm and back to its training data,\nthereby identifying training points most respon-\nsible for a given prediction. to scale up in\ufb02uence\nfunctions to modern machine learning settings,\nwe develop ", "the journal of neuroscience, april 18, 2012 \u2022 32(16):5609 \u20135619 \u2022 5609\n\nbehavioral/systems/cognitive\n\nspatial profile of excitatory and inhibitory synaptic\nconnectivity in mouse primary auditory cortex\n\nrobert b. levy and alex d. reyes\ncenter for neural science, new york university, new york, new york 10003\n\nthe role of local cortical activity in shaping neuronal responses is controversial. among other questions, it is unknown how the diverse\nresponse patterns reported in vivo\u2014lateral inhibition", "neuronal cell-type classification: \nchallenges, opportunities and the  \npath forward\n\nhongkui zeng1 and joshua r.\u00a0sanes2\nabstract| neurons have diverse molecular, morphological, connectional and functional \nproperties. we believe that the only realistic way to manage this complexity \u2014 and thereby pave \nthe way for understanding the structure, function and development of brain circuits \u2014 is to \ngroup neurons into types, which can then be analysed systematically and reproducibly. however, \nneurona", "neural population dynamics underlying motor\nlearning transfer\n\narticle\n\nhighlights\nd covert learning via a brain-machine interface transfers to\n\novert reaching behavior\n\nd covert learning systematically changes motor cortical\n\npreparatory activity\n\nd covert and overt movements share preparatory neural states\n\nand facilitate transfer\n\nd covert and overt movements engage a similar neural\n\ndynamical system\n\nauthors\n\nsaurabh vyas, nir even-chen,\nsergey d. stavisky, stephen i. ryu,\npaul nuyujukian, k", "physiol rev 86: 1033\u20131048, 2006;\ndoi:10.1152/physrev.00030.2005.\n\nspike timing-dependent plasticity:\n\nfrom synapse to perception\n\nyang dan and mu-ming poo\n\ndivision of neurobiology, department of molecular and cell biology, and helen wills neuroscience institute,\n\nuniversity of california, berkeley, california\n\ni. introduction\nii. cellular mechanisms underlying synaptic spike timing-dependent plasticity\n\niii. spike timing-dependent plasticity with complex spatiotemporal activity patterns\n\na. ltp", "letter\nsensory-evoked ltp driven by dendritic plateau\npotentials in vivo\n\ndoi:10.1038/nature13664\n\nfre\u00b4de\u00b4ric gambino1*{, ste\u00b4phane page`s1*, vassilis kehayas1,2, daniela baptista1, roberta tatti1,2, alan carleton1\n& anthony holtmaat1\n\nlong-term synaptic potentiation (ltp) is thought to be a key pro-\ncess in cortical synaptic network plasticity and memory formation1.\nhebbian forms of ltp depend on strong postsynaptic depolarization,\nwhich in many models is generated by action potentials that pro", "deep learning and the information bottleneck principle\n\nnaftali tishby1,2\n\nnoga zaslavsky1\n\ninformation theoretic limits of\n\nabstract\u2014 deep neural networks (dnns) are analyzed via\nthe theoretical framework of the information bottleneck (ib)\nprinciple. we \ufb01rst show that any dnn can be quanti\ufb01ed by\nthe mutual information between the layers and the input and\noutput variables. using this representation we can calculate\nthe optimal\nthe dnn and\nobtain \ufb01nite sample generalization bounds. the advantage ", "relative flatness and generalization\n\nhenning petzka\u2217\n\nlund university, sweden\n\nhenning.petzka@math.lth.se\n\nmichael kamp\u2217\n\ncispa helmholtz center for information security,\n\ngermany and monash university, australia\n\nmichael.kamp@monash.edu\n\nlinara adilova\n\nruhr university bochum, germany\n\nand fraunhofer iais\n\ncristian sminchisescu\nlund university, sweden\n\nand google research, switzerland\n\nmario boley\n\nmonash university, australia\n\nabstract\n\nflatness of the loss curve is conjectured to be connecte", "letter\n\ncommunicated by mark mcdonnell\n\ndeep learning with dynamic spiking neurons and fixed\nfeedback weights\n\narash samadi\nars2023@med.cornell.edu\ndepartment of physiology, university of toronto, toronto, ontario,\nm5s 1a8, canada\n\ntimothy p. lillicrap\ntimothylillicrap@google.com\ngoogle deepmind, london, ec4a 3tw, u.k.\n\ndouglas b. tweed\ndouglas.tweed@utoronto.ca\ndepartment of physiology, university of toronto, toronto, ontario, m5s 1a8,\ncanada, and centre for vision research, york university, to", "article\n\nhttps://doi.org/10.1038/s41467-022-34452-w\n\nreal-time brain-machine interface in\nnon-human primates achieves high-velocity\nprosthetic \ufb01nger movements using a shallow\nfeedforward neural network decoder\n\nreceived: 16 march 2022\n\naccepted: 25 october 2022\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\n;\n,\n:\n)\n(\n\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\ncheck for updates\n\nmatthew s. willsey 1,2, samuel r. nason-tomaszewski\nhisham temmar\nparag g. patil1,2,4,5 & cynthia a. chestek 2,3,4,6,7\n\n2, matthew j. mender\n\n2, joseph t.", "the journal of neuroscience, march 5, 2008 \u2022 28(10):2435\u20132446 \u2022 2435\n\ncellular/molecular\n\ndopamine receptor activation is required for\ncorticostriatal spike-timing-dependent plasticity\n\nverena pawlak1,2 and jason n. d. kerr2\n1department of cell physiology, max planck institute for medical research, 69120 heidelberg, germany, and 2network imaging group, max planck\ninstitute for biological cybernetics, 72076 tu\u00a8bingen, germany\n\nsingle action potentials (aps) backpropagate into the higher-order den", "a large-scale standardized physiological survey \nreveals functional organization of the mouse \nvisual cortex\n\n\u200a1,5*, jerome a. lecoq\u200a\n\n\u200a1,5*, michael a. buice\u200a\n\u200a1, nicholas cain\u200a\n\n\u200a1, carol thompson1, wayne wakeman1, jack waters\u200a\n\n\u200a1,5*, peter a. groblewski1,  \nsaskia e. j. de vries\u200a\ngabriel k. ocker1, michael oliver1, david feng\u200a\n\u200a1, peter ledochowitsch1,  \ndaniel millman1, kate roll1, marina garrett1, tom keenan1, leonard kuan1, stefan mihalas\u200a\nshawn olsen\u200a\n\u200a1, derric williams1, \nchris barber1", "emerging properties in self-supervised vision transformers\n\nmathilde caron1,2\n\njulien mairal2\n\nhugo touvron1,3\n\nishan misra1\n\nherv\u00b4e jegou1\n\npiotr bojanowski1\n\narmand joulin1\n\n1\n2\n0\n2\n\n \n\ny\na\nm\n4\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n4\n9\n2\n4\n1\n\n.\n\n4\n0\n1\n2\n:\nv\ni\nx\nr\na\n\n1 facebook ai research\n\n2 inria\u2217\n\n3 sorbonne university\n\nfigure 1: self-attention from a vision transformer with 8 \u00d7 8 patches trained with no supervision. we look at the self-attention of\nthe [cls] token on the heads of the last layer", "9\n1\n0\n2\n\n \n\nv\no\nn\n4\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n5\n5\n9\n1\n1\n\n.\n\n4\n0\n9\n1\n:\nv\ni\nx\nr\na\n\non exact computation with an in\ufb01nitely wide\n\nneural net\u2217\n\nsanjeev arora\u2020\n\nsimon s. du\u2021\n\nwei hu\u00a7\n\nzhiyuan li\u00b6\n\nruslan salakhutdinovk\n\nruosong wang\u2217\u2217\n\nabstract\n\nhow well does a classic deep net architecture like alexnet or vgg19 classify\non a standard dataset such as cifar-10 when its \u201cwidth\u201d\u2014 namely, number of\nchannels in convolutional layers, and number of nodes in fully-connected internal\nlayers \u2014 is allowed ", "learning interactive real-world simulators\n\nmengjiao yang\u2020,\u22c4, yilun du\u266e, kamyar ghasemipour\u22c4\njonathan tompson\u22c4, dale schuurmans\u22c4,\u2021, pieter abbeel\u2020\n\u2020uc berkeley, \u22c4google deepmind, \u266emit, \u2021university of alberta\n\nsherryy@{berkeley.edu, google.com}\n\nuniversal-simulator.github.io\n\n3\n2\n0\n2\n\n \nt\nc\no\n9\n\n \n\n \n \n]\ni\n\na\n.\ns\nc\n[\n \n \n\n1\nv\n4\n1\n1\n6\n0\n\n.\n\n0\n1\n3\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\ngenerative models trained on internet data have revolutionized how text, image,\nand video content can be created. perhaps the ne", "neuron\n\nperspective\n\nneuroscience needs behavior:\ncorrecting a reductionist bias\n\njohn w. krakauer,1,* asif a. ghazanfar,2 alex gomez-marin,3 malcolm a. maciver,4 and david poeppel5,6\n1departments of neurology, and neuroscience, johns hopkins university, baltimore, md 21287, usa\n2princeton neuroscience institute, departments of psychology and ecology & evolutionary biology, princeton university, princeton,\nnj 08540 usa\n3instituto de neurociencias, consejo superior de investigaciones cient\u0131\u00b4\ufb01cas ", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nnew  light  on  cortical  neuropeptides  and  synaptic\nnetwork  plasticity\nstephen  j  smith1,  michael  hawrylycz1,  jean  rossier2 and\nuygar  su\u00a8 mbu\u00a8 l1\n\nneuropeptides,  members  of  a  large  and  evolutionarily  ancient\nfamily  of  proteinaceous  cell\u2013cell  signaling  molecules,  are\nwidely  recognized  as  extremely  potent  regulators  of  brain\nfunction  and  behavior.  at  the  cellular  level,  neuropeptides  are\nknow", "articles\n\nvol 452 | 27 march 2008 | doi:10.1038/nature06725\n\ncompartmentalized dendritic plasticity\nand input feature storage in neurons\n\nattila losonczy1*, judit k. makara1* & jeffrey c. magee1\n\nalthough information storage in the central nervous system is thought to be primarily mediated by various forms of synaptic\nplasticity, other mechanisms, such as modifications in membrane excitability, are available. local dendritic spikes are\nnonlinear voltage events that are initiated within dendritic", "a r t i c l e s\n\nrelease probability of hippocampal glutamatergic \nterminals scales with the size of the active zone\nnoemi holderith1, andrea lorincz1, gergely katona2, bal\u00e1zs r\u00f3zsa2, akos kulik3,4, masahiko watanabe5 & \nzoltan nusser1\n\n.\n\nd\ne\nv\nr\ne\ns\ne\nr\n \n\ns\nt\nh\ng\ni\nr\n \nl\nl\n\na\n\n \n \n.\n\nc\nn\n\ni\n \n,\n\na\nc\n\n \n\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n5\n1\n0\n2\n\u00a9\n\ncortical synapses have structural, molecular and functional heterogeneity; our knowledge regarding the relationship between \ntheir ultrastructural and function", "6266 \u2022 the journal of neuroscience, april 27, 2011 \u2022 31(17):6266 \u2013 6276\n\nbehavioral/systems/cognitive\n\nstimulus selectivity in dorsal and ventral prefrontal cortex\nafter training in working memory tasks\n\ntravis meyer,1,2 xue-lian qi,1 terrence r. stanford,1 and christos constantinidis1\n1department of neurobiology and anatomy, wake forest university school of medicine, winston-salem, north carolina 27157 and 2center for the neural\nbasis of cognition, carnegie mellon university, pittsburgh, pennsy", "bayesian latent structure discovery from\n\nmulti-neuron recordings\n\nscott w. linderman\ncolumbia university\n\nswl2133@columbia.edu\n\nryan p. adams\n\nharvard university and twitter\n\nrpa@seas.harvard.edu\n\njonathan w. pillow\nprinceton university\n\npillow@princeton.edu\n\nabstract\n\nneural circuits contain heterogeneous groups of neurons that differ in type, location,\nconnectivity, and basic response properties. however, traditional methods for\ndimensionality reduction and clustering are ill-suited to recove", "course 9 \n\nirregular  activity  in large  networks  of \n\nneurons \n\nc.  van vreeswijk i and h.  sompolinsky 2'3 \n\n1 cnrs umr 8119,  universit~ paris 5 ren~ descartes,  45 rue des saints pkres, \n\n2racah institute of physics and center for neural computation,  hebrew  university,  jerusalem, \n\n75270 paris cedex 06, \n\n91904, israel (permanent address) \n\n3department of molecular and cellular biology,  harvard university,  cambridge,  ma 02138,  usa \n\nc c. chow, b.  gutkin, d. hansel,  c  meunier and ", "a r t i c l e s\n\nbump attractor dynamics in prefrontal cortex explains \nbehavioral precision in spatial working memory\nklaus wimmer1, duane q nykamp1,2, christos constantinidis3 & albert compte1\nprefrontal persistent activity during the delay of spatial working memory tasks is thought to maintain spatial location in memory. \na \u2018bump attractor\u2019 computational model can account for this physiology and its relationship to behavior. however, direct \nexperimental evidence linking parameters of prefron", "probing the compositionality of intuitive functions\n\neric schulz\n\nuniversity college london\n\ne.schulz@cs.ucl.ac.uk\n\njoshua b. tenenbaum\n\nmit\n\njbt@mit.edu\n\ndavid duvenaud\n\nuniversity of toronto\n\nduvenaud@cs.toronto.edu\n\nmaarten speekenbrink\nuniversity college london\nm.speekenbrink@ucl.ac.uk\n\nsamuel j. gershman\nharvard university\n\ngershman@fas.harvard.edu\n\nabstract\n\nhow do people learn about complex functional structure? taking inspiration from\nother areas of cognitive science, we propose that thi", "recurrence is required to capture the representational\ndynamics of the human visual system\n\ntim c. kietzmanna,b,1, courtney j. spoerera, lynn k. a. s\u00f6rensenc, radoslaw m. cichyd, olaf hauka,\nand nikolaus kriegeskortee\n\namrc cognition and brain sciences unit, university of cambridge, cambridge cb2 7ef, united kingdom; bdonders institute for brain, cognition and\nbehaviour, radboud university, 6525 hr nijmegen, the netherlands; cdepartment of psychology, university of amsterdam, 1018 wd amsterdam,\n", "chu kiong loo   keem siah yap\nkok wai wong   andrew teoh\nkaizhu huang (eds.)\n\n \n\n4\n3\n8\n8\ns\nc\nn\nl\n\nneural\ninformation processing\n\n21st international conference, iconip 2014\nkuching, malaysia, november 3\u20136, 2014\nproceedings, part i\n\n \n1\n2\n3\n\f", "neural networks 115 (2019) 100\u2013123\n\ncontents lists available at sciencedirect\n\nneural networks\n\njournal homepage: www.elsevier.com/locate/neunet\n\nreview\nrecent advances in physical reservoir computing: a review\ngouhei tanaka a,b,\u2217, toshiyuki yamane c, jean benoit h\u00e9roux c, ryosho nakane a,b,\nnaoki kanazawa c, seiji takeda c, hidetoshi numata c, daiju nakano c, akira hirose a,b\na institute for innovation in international engineering education, graduate school of engineering, the university of tok", "b r i e f   co m m u n i c at i o n s\n\na hierarchy of intrinsic \ntimescales across primate cortex\njohn d murray1,2, alberto bernacchia2,3, david j freedman4, \nranulfo romo5,6, jonathan d wallis7,8, xinying cai9,10,  \ncamillo padoa-schioppa10, tatiana pasternak11,12, hyojung seo2, \ndaeyeol lee2 & xiao-jing wang1,2,9\n\nspecialization and hierarchy are organizing principles for \nprimate cortex, yet there is little direct evidence for how \ncortical areas are specialized in the temporal domain. we \nme", "cortical-like dynamics in recurrent circuits \noptimized for sampling-based probabilistic \ninference\n\nrodrigo echeveste\u200a\n\n\u200a1,2\u2009\u2709, laurence aitchison1, guillaume hennequin\u200a\n\n\u200a1,4 and m\u00e1t\u00e9 lengyel\u200a\n\n\u200a1,3,4\n\nsensory cortices display a suite of ubiquitous dynamical features, such as ongoing noise variability, transient overshoots and \noscillations,  that  have  so  far  escaped  a  common,  principled  theoretical  account.  we  developed  a  unifying  model  for  these \nphenomena  by  training  a  r", "alpa: automating inter- and intra-operator \nparallelism for distributed deep learning\nlianmin zheng, zhuohan li, and hao zhang, uc berkeley; yonghao zhuang, \nshanghai jiao tong university; zhifeng chen and yanping huang, google;  \nyida wang, amazon web services; yuanzhong xu, google; danyang zhuo,  \n\nduke university; eric p. xing, mbzuai and carnegie mellon university;  \n\njoseph e. gonzalez and ion stoica, uc berkeley\n\nhttps://www.usenix.org/conference/osdi22/presentation/zheng-lianmin\n\nthis pap", "how important is weight symmetry in backpropagation?\n\nqianli liao and joel z. leibo and tomaso poggio\n\ncenter for brains, minds and machines, mcgovern institute\n\nmassachusetts institute of technology\n\n77 massachusetts ave., cambridge, ma, 02139, usa\n\n6\n1\n0\n2\n\n \n\nb\ne\nf\n4\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n7\n6\n0\n5\n0\n\n.\n\n0\n1\n5\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\ngradient backpropagation (bp) requires symmetric feedfor-\nward and feedback connections\u2014the same weights must be\nused for forward and backward passes. t", "m\no\nc\n\n.\n\ne\nr\nu\nt\na\nn\n\n.\ni\n\nc\ns\no\nr\nu\ne\nn\n\n/\n/\n:\np\nt\nt\nh\n \n\u2022\n \n.\n\nc\nn\n\ni\n \n\na\nc\n\n \n\ni\nr\ne\nm\na\ne\nr\nu\nt\na\nn\n9\n9\n9\n1\n\u00a9\n\n \n\n \n\n\u00a9 1999 nature america inc. \u2022 http://neurosci.nature.com\n\narticles\n\npredictive coding in the visual cortex: \na functional interpretation of some\nextra-classical receptive-field effects\n\nrajesh p. n. rao1 and dana h. ballard2\n\n1 the salk institute, sloan center for theoretical neurobiology and computational neurobiology laboratory, 10010 n. torrey pines road, \n\nla jolla, calif", "c o r t e x 9 8 ( 2 0 1 8 ) 2 4 9 e2 6 1\n\navailable online at www.sciencedirect.com\n\nsciencedirect\n\njournal homepage: www.elsevier.com/locate/cortex\n\nspecial issue: review\n\nvisual pathways from the perspective of cost\nfunctions and multi-task deep neural networks\n\nh. steven scholte a,b,*,1, max m. losch a,b,c,1, kandan ramakrishnan c,\nedward h.f. de haan a,b and sander m. bohte d\na department of psychology, university of amsterdam, amsterdam, the netherlands\nb amsterdam brain and cognition, univ", "on the local behavior of spaces of natural images\n\ngunnar carlsson\ntigran ishkhanov\n\nvin de silva\n\nafra zomorodian\n\nabstract\n\nin this study we concentrate on qualitative topological analysis of the local behavior of the\nspace of natural images. to this end, we use a space of 3 by 3 high-contrast patches m studied by\nmumford et al. we develop a theoretical model for the high-density 2-dimensional submanifold\nof m showing that it has the topology of the klein bottle. using our topological software", "jcb: mini-review\n\n cytoplasmic diffusion: molecular motors mix it up \n\n  clifford p.   brangwynne ,  1    gijsje h.   koenderink ,  1,2,3    frederick c.   mackintosh ,  4   and  david a.   weitz   1,2   \n\n  1 school of engineering and applied sciences and  2 department of physics, harvard university, cambridge, ma 02138 \n  3 foundation for fundamental research on matter institute for atomic and molecular physics, 1098 sj amsterdam, netherlands \n  4 department of physics and astronomy, vrije uni", "2\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n3\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n6\n3\n0\n0\n1\n\n.\n\n3\n0\n2\n2\n:\nv\ni\nx\nr\na\n\non the generalization mystery in deep learning\n\nsatrajit chatterjee and piotr zielinski\n\nabstract. the generalization mystery in deep learning is the following: why do over-parameterized\nneural networks trained with gradient descent (gd) generalize well on real datasets even though\nthey are capable of \ufb01tting random datasets of comparable size? furthermore, from among all\nsolutions that \ufb01t the training data, ho", "the journal of neuroscience, may 1, 2003 \u2022 23(9):3697\u20133714 \u2022 3697\n\nlearning input correlations through nonlinear temporally\nasymmetric hebbian plasticity\n\nr. gu\u00a8tig,1* r. aharonov,2* s. rotter,1 and haim sompolinsky2,3\n1institute of biology iii, university of freiburg, 79104 freiburg, germany, and 2interdisciplinary center for neural computation and 3racah institute of\nphysics, hebrew university, jerusalem 91904, israel\n\ntriggered by recent experimental results, temporally asymmetric hebbian (ta", "article\n\ncommunicated by rodney douglas\n\nreal-time computing without stable states: a new\nframework for neural computation based on perturbations\n\nwolfgang maass\nmaass@igi.tu-graz.ac.at\nthomas natschl\u00a8ager\ntnatschl@igi.tu-graz.ac.at\ninstitute for theoretical computer science, technische universit\u00a8at graz;\na-8010 graz, austria\n\nhenry markram\nhenry.markram@ep\ufb02.ch\nbrain mind institute, ecole polytechnique federale de lausanne,\nch-1015 lausanne, switzerland\n\na key challenge for neural modeling is to", "6\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n3\n1\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n3\n1\n8\n3\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\ntowards an integration of deep learning and neuroscience\n\nadam h. marblestone\nmit media lab\ncambridge, ma 02139, usa\n\ngreg wayne\ngoogle deepmind\nlondon, ec4a 3tw, uk\n\nkonrad p. kording\nrehabilitation institute of chicago\nnorthwestern university\nchicago, il 60611, usa\n\neditor: tbn\n\namarbles@media.mit.edu\n\ngregwayne@google.com\n\nkoerding@gmail.com\n\nabstract\n\nneuroscience has focused on the detailed implem", "communicated by ronald williams \n\nrelating real-time backpropagation and \nbackpropagation-through-time: an application \nof  flow graph interreciprocity \n\nfransoise  beaufays \neric a.  wan \ndepartment  of  electrical engineering, stanford  university, \nstanford, c a  94305-4055 u s a  \n\nwe  show that  signal flow graph theory  provides  a  simple way  to  re- \nlate  two  popular  algorithms  used  for  adapting  dynamic  neural  net- \nworks,  real-time  backpropagation  and backpropagation-throug", "neural networks 132 (2020) 428\u2013446\n\ncontents lists available at sciencedirect\n\nneural networks\n\njournal homepage: www.elsevier.com/locate/neunet\n\nhigh-dimensional dynamics of generalization error in neural networks\nmadhu s. advani a,1, andrew m. saxe a,2,\u2217,1, haim sompolinsky a,b\n\na center for brain science, harvard university, cambridge, ma 02138, united states of america\nb edmond and lily safra center for brain sciences, hebrew university, jerusalem 91904, israel\n\na r t i c l e\n\ni n f o\n\na b s", "page 1 of 77\n\narticles in press. j neurophysiol (october 10, 2007). doi:10.1152/jn.00364.2007 \n\nreinforcement learning with modulated \nspike timing-dependent synaptic plasticity\n\nrunning head: reinforcement learning with stdp\n\nmichael a. farries1\nadrienne l. fairhall2\n\n1university of texas at san antonio\ndept. of biology\none utsa circle\nsan antonio, tx 78249\n\n2 university of washington\ndept. of physiology and biophysics\n1959 ne pacific st., box 357290\nseattle, wa 98195-7290\n\ncorresponding author", "how to start training:\n\nthe effect of initialization and architecture\n\nboris hanin\n\ndepartment of mathematics\n\ntexas a& m university\ncollege station, tx, usa\nbhanin@math.tamu.edu\n\ndavid rolnick\n\ndepartment of mathematics\n\nmassachusetts institute of technology\n\ncambridge, ma, usa\ndrolnick@mit.edu\n\nabstract\n\nwe identify and study two common failure modes for early training in deep relu\nnets. for each, we give a rigorous proof of when it occurs and how to avoid it, for\nfully connected, convolutiona", "9\n1\n0\n2\n\n \nc\ne\nd\n4\n\n \n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n2\nv\n9\n4\n5\n8\n0\n\n.\n\n7\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nuniversality and individuality in neural dynamics\nacross large populations of recurrent networks\n\nniru maheswaranathan\u2217\ngoogle brain, google inc.\n\nmountain view, ca\nnirum@google.com\n\nalex h. williams\u2217\nstanford university\n\nstanford, ca\n\nahwillia@stanford.edu\n\nmatthew d. golub\nstanford university\n\nstanford, ca\n\nmgolub@stanford.edu\n\nsurya ganguli\n\nstanford university and google brain\nstanford, ca and mounta", "www.nature.com/npjscilearn\n\nopen\n\nreview article\nis plasticity of synapses the mechanism of long-term memory\nstorage?\n\nwickliffe c. abraham 1, owen d. jones1 and david l. glanzman2\n\nit has been 70 years since donald hebb published his formalized theory of synaptic adaptation during learning. hebb\u2019s seminal\nwork foreshadowed some of the great neuroscienti\ufb01c discoveries of the following decades, including the discovery of long-term\npotentiation and other lasting forms of synaptic plasticity, and m", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nusing  computational  theory  to  constrain  statistical\nmodels  of  neural  data\nscott  w  linderman1 and  samuel  j  gershman2\n\ncomputational  neuroscience  is,  to  \ufb01rst  order,  dominated  by\ntwo  approaches:  the  \u2018bottom-up\u2019  approach,  which  searches\nfor  statistical  patterns  in  large-scale  neural  recordings,  and  the\n\u2018top-down\u2019  approach,  which  begins  with  a  theory  of\ncomputation  and  considers  plausible  neural ", "0\n2\n0\n2\n\n \nt\nc\no\n7\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n5\n6\n7\n4\n1\n\n.\n\n0\n1\n0\n2\n:\nv\ni\nx\nr\na\n\npreprint\n\ndeep networks from the principle of rate reduction\n\nkwan ho ryan chan\u2020 yaodong yu\u2020 chong you\u2020 haozhi qi\u2020 john wright\u2021(cid:5) yi ma\u2020\n\n\u2020department of eecs, university of california, berkeley\n\u2021department of electrical engineering and data science institute, columbia university\n(cid:5)department of applied physics and applied mathematics, columbia university\n\nabstract\n\nthis work attempts to interpret mo", "p\ne\nr\ng\na\nm\no\nn\n \np\ni\ni\n:\n \ns\n0\n0\n4\n2\n-\n6\n9\n8\n9\n(\n9\n7\n)\n0\n0\n1\n6\n9\n-\n7\n \nv\ni\ns\ni\no\nn\n \nr\ne\ns\n.\n,\n \nv\no\nl\n.\n \n3\n7\n,\n \nn\no\n.\n \n2\n3\n,\n \np\np\n.\n \n3\n3\n1\n1\n-\n3\n3\n2\n5\n,\n \n1\n9\n9\n7\n \n\u00a9\n \n1\n9\n9\n7\n \ne\nl\ns\ne\nv\ni\ne\nr\n \ns\nc\ni\ne\nn\nc\ne\n \nl\nt\nd\n.\n \na\nl\nl\n \nr\ni\ng\nh\nt\ns\n \nr\ne\ns\ne\nr\nv\ne\nd\n \np\nr\ni\nn\nt\ne\nd\n \ni\nn\n \ng\nr\ne\na\nt\n \nb\nr\ni\nt\na\ni\nn\n \n0\n0\n4\n2\n-\n6\n9\n8\n9\n/\n9\n7\n \n$\n1\n7\n.\n0\n0\n \n+\n \n0\n.\n0\n0\n \ns\np\na\nr\ns\ne\n \nc\no\nd\ni\nn\ng\n \nw\ni\nt\nh\n \na\nn\n \no\nv\ne\nr\nc\no\nm\np\nl\ne\nt\ne\n \nb\na\ns\ni\ns\n \ns\ne\nt\n:\n \na\n \ns\nt\nr\na\nt\ne\ng\ny\n \ne\nm\np\nl\no\ny\n", "the  journal \n\nof  neuroscience, \n\nmay \n\n15,  1996, \n\n16(10):3351-3362 \n\nefficient  coding  of  natural  scenes \nnucleus:  experimental \n\ntest  of  a  computational \n\nin  the  lateral  geniculate \n\ntheory \n\nyang  dan,\u2019 \nlaboratories \n\nj.  atick, \n\njoseph \nof  lneurobiology \n\nand  r.  clay  reid\u2019 \n\nand  2computational  neuroscience, \n\nthe  rockefeller  university,  new  york,  new  york  70021 \n\ncorrelation \n\ntheory  suggests \n\ntheoretic \ninformation \n\ninformation \ninformation \n\nthe  lateral  gen", "cell metabolism\n\nreview\n\nthe role of advanced glycation end\nproducts in aging and metabolic diseases:\nbridging association and causality\n\njyotiska chaudhuri,1,* yasmin bains,2 sanjib guha,1 arnold kahn,1,3 david hall,1 neelanjan bose,1,3\nalejandro gugliucci,2,* and pankaj kapahi1,3,*\n1the buck institute for research on aging, 8001 redwood boulevard, novato, ca 94945, usa\n2touro university college of osteopathic medicine, glycation oxidation and research laboratory, vallejo, ca, 94592, usa\n3unive", "supplementary figures and legends: \n \n \n \n \n \n \n \n \n\nsupplementary information\n\nprojections versus each other\n\n(250 ms of data) \n\ndoi:10.1038/nature11129\n\njpc2 projection versus time\n\n(all times)\n\ntarget\n\nmove onset\n\n200 ms\n\njpc1 projection versus time\n\n(all times)\n\ntarget\n\nmove onset\n\n200 ms\n\n \n \nsupplementary figure 1.  projection onto each jpca axis as a function of time.  data is shown for monkey j3.  the \nplot of the jpca plane uses the same format as in figure 3 of the main text.  the plot", "journal of machine learning research 4 (2003) 119-155\n\nsubmitted 6/02; published 6/03\n\nthink globally, fit locally:\n\nunsupervised learning of low dimensional manifolds\n\nlawrence k. saul\ndepartment of computer and information science\nuniversity of pennsylvania\n200 south 33rd street\n557 moore school - grw\nphiladelphia, pa 19104-6389, usa\n\nsam t. roweis\ndepartment of computer science\nuniversity of toronto\n6 king\u2019s college road\npratt building 283\ntoronto, ontario m5s 3g4, canada\n\neditor: yoram singe", "modeling behaviorally relevant neural dynamics \nenabled by preferential subspace identification\n\nomid g. sani\u200a\nmaryam m. shanechi\u200a\n\n\u200a1,3,4\u2009\u2709\n\n\u200a1, hamidreza abbaspourazad\u200a\n\n\u200a1, yan t. wong2,5, bijan pesaran\u200a\n\n\u200a2 and \n\nneural activity exhibits complex dynamics related to various brain functions, internal states and behaviors. understanding \nhow neural dynamics explain specific measured behaviors requires dissociating behaviorally relevant and irrelevant dynam-\nics, which is not achieved with curre", "8\n1\n0\n2\n\n \nr\na\n\nm\n3\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n5\n6\n1\n0\n0\n\n.\n\n1\n1\n7\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2018\n\ndeep neural networks as gaussian processes\n\njaehoon lee\u2217\u2020, yasaman bahri\u2217\u2020, roman novak , samuel s. schoenholz,\njeffrey pennington, jascha sohl-dickstein\n\ngoogle brain\n{jaehlee, yasamanb, romann, schsam, jpennin, jaschasd}@google.com\n\nabstract\n\nit has long been known that a single-layer fully-connected neural network with an\ni.i.d. prior over its parameters is ", "weight uncertainty in neural networks\n\ncharles blundell\njulien cornebise\nkoray kavukcuoglu\ndaan wierstra\ngoogle deepmind\n\ncblundell@google.com\njucor@google.com\nkorayk@google.com\nwierstra@google.com\n\nabstract\n\nwe introduce a new, ef\ufb01cient, principled and\nbackpropagation-compatible algorithm for learn-\ning a probability distribution on the weights of\na neural network, called bayes by backprop. it\nregularises the weights by minimising a com-\npression cost, known as the variational free en-\nergy or ", "available\n\nwww.sciencedirect.com\n\n \n\n \n\nat\n\nonline\nsciencedirect\n\n \n\nlinking  neural  responses  to  behavior  with\ninformation-preserving  population  vectors\ntatyana  o  sharpee  and  john  a  berkowitz\n\nall  systems  for  processing  signals,  both  arti\ufb01cial  and  within\nanimals,  must  obey  fundamental  statistical  laws  for  how\ninformation  can  be  processed.  we  discuss  here  recent  results\nusing  information  theory  that  provide  a  blueprint  for  building\ncircuits  where  sign", "line attractor dynamics in recurrent networks for sentiment classication\n\nniru maheswaranathan * 1 alex h. williams * 2 1 matthew d. golub 2 surya ganguli 2 1 david sussillo 1\n\nabstract\n\nrecurrent neural networks (rnns) are a pow-\nerful tool for modeling sequential data. despite\ntheir widespread usage, understanding how rnns\nsolve complex problems remains elusive. here,\nwe characterize how popular off-the-shelf archi-\ntectures (including lstms, grus, and vanilla\nrnns) perform document-level sent", "0\n2\n0\n2\n\n \nt\nc\no\n3\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n1\n1\n0\n9\n0\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nimproved techniques for training score-based\n\ngenerative models\n\nyang song\n\ncomputer science department\n\nstanford university\n\nyangsong@cs.stanford.edu\n\nstefano ermon\n\ncomputer science department\n\nstanford university\n\nermon@cs.stanford.edu\n\nabstract\n\nscore-based generative models can produce high quality image samples comparable\nto gans, without requiring adversarial optimization. however, existing training\npr", "rigorous dynamical mean field theory for stochastic gradient descent methods \u2217\nc\u00b4edric gerbelot\u2020 , emanuele troiani\u2021 , francesca mignacco\u00a7 , florent krzakala\u00b6, and lenka\n\nzdeborov\u00b4a\u2021\n\nabstract. we prove closed-form equations for the exact high-dimensional asymptotics of a family of first order\ngradient-based methods, learning an estimator (e.g. m-estimator, shallow neural network, ...) from\nobservations on gaussian data with empirical risk minimization. this includes widely used algo-\nrithms suc", "1\n2\n0\n2\n\n \n\nv\no\nn\n9\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n6\n4\n4\n0\n1\n\n.\n\n5\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nredunet: a white-box deep network from the principle of\n\nmaximizing rate reduction\u2217\n\nkwan ho ryan chan \u2020, (cid:5)\nyaodong yu \u2020, (cid:5)\nchong you \u2020, (cid:5)\nhaozhi qi \u2020\njohn wright \u2021\nyi ma \u2020\n\nryanchankh@berkeley.edu\n\nyyu@eecs.berkeley.edu\n\ncyou@berkeley.edu\n\nhqi@berkeley.edu\n\njohnwright@ee.columbia.edu\n\nyima@eecs.berkeley.edu\n\n\u2020 department of electrical engineering and computer sciences\n\nuniversity of califo", "functional organization of the \nhippocampal longitudinal axis\n\nbryan a.\u00a0strange1,2, menno p.\u00a0witter3, ed s.\u00a0lein4 and edvard i.\u00a0moser3\n\nabstract | the precise functional role of the hippocampus remains a topic of much \ndebate. the dominant view is that the dorsal (or posterior) hippocampus is implicated in \nmemory and spatial navigation and the ventral (or anterior) hippocampus mediates \nanxiety-related behaviours. however, this \u2018dichotomy view\u2019 may need revision. gene \nexpression studies demons", "learnable latent embeddings for joint \nbehavioural and neural analysis\n\nhttps://doi.org/10.1038/s41586-023-06031-6\nreceived: 30 march 2022\naccepted: 28 march 2023\npublished online: 3 may 2023\nopen access\n\n check for updates\n\nsteffen schneider1,2, jin hwa lee1,2 & mackenzie weygandt mathis1\u2009\u2709\n\nmapping behavioural actions to neural activity is a fundamental goal of neuroscience. \nas our ability to record large neural and behavioural data increases, there is growing \ninterest in modelling neural dy", "research article\n\na map of abstract relational knowledge in\nthe human hippocampal\u2013entorhinal cortex\nmona m garvert1,2*, raymond j dolan1,3, timothy ej behrens1,2\n\n1wellcome trust centre for neuroimaging, institute of neurology, university\ncollege london, london, united kingdom; 2oxford centre for functional mri of\nthe brain, nuffield department of clinical neurosciences, university of oxford,\noxford, united kingdom; 3max planck-ucl centre for computational psychiatry\nand ageing research, london,", "          \n\np1: ars/bta\ndecember 9, 1998\n\np2: ars/spd\n\nqc: ars\n\n17:38\n\nannual reviews\n\nar076-11\n\nannu. rev. neurosci. 1999. 22:241\u201359\n\ncopyright c(cid:176) 1999 by annual reviews. all rights reserved\n\nneural selection and\ncontrol of visually\nguided eye movements\n\njeffrey d. schall and kirk g. thompson\nvanderbilt vision research center, department of psychology, vanderbilt university,\nnashville, tennessee 37240; e-mail: jeffrey.d.schall@vanderbilt.edu\n\nkey words:\n\nsaccade, visual search, counterm", "open\n\nreceived: 21 august 2017\naccepted: 19 february 2018\npublished: xx xx xxxx\n\ndeep residual network predicts \ncortical representation and \norganization of visual features for \nrapid categorization\n\nhaiguang wen2,3, junxing shi \n\n 2,3, wei chen4 & zhongming liu \n\n 1,2,3\n\nthe brain represents visual objects with topographic cortical patterns. to address how distributed \nvisual representations enable object categorization, we established predictive encoding models based \non a deep residual netwo", "neuroimage 85 (2014) 656\u2013666\n\ncontents lists available at sciencedirect\n\nneuroimage\n\nj o u r n a l h o m e p a ge : w ww . e l s e v i e r . c o m/ l o c a t e / y n i m g\n\nreview\ntheta rhythm and the encoding and retrieval of space and time\nmichael e. hasselmo \u204e, chantal e. stern\ncenter for memory and brain, department of psychology and graduate program for neuroscience, boston university, 2 cummington mall, boston, ma, 02215, usa\n\na r t i c l e\n\ni n f o\n\na b s t r a c t\n\nphysiological data dem", "letters to nature\n\n(3.0\u20135.0 mm diameter) for chronic recording was drilled through the cranioplastic cap\nand the underlying skull.\n\n11. ono, t., nishijo, h. & uwano, t. amygdala role in conditioned associative learning. prog. neurobiol.\n\n46, 401\u2013422 (1995).\n\nbehavioural task\nthe rats were required to lick a spout to obtain rewards of 0.3 m sucrose solution or icss\n(0.5-s train of 100-hz, 0.3-ms capacitor-coupled negative square wave pulses). we\nmonitored licking behaviour with a photoelectric se", "elifesciences.org\n\nresearch advance\n\na cortical disinhibitory circuit for\nenhancing adult plasticity\nyu fu1*\u2020, megumi kaneko1\u2020, yunshuo tang2,3,4, arturo alvarez-buylla2,3,\nmichael p stryker1*\n\n1center for integrative neuroscience, department of physiology, university of\ncalifornia, san francisco, san francisco, united states; 2department of\nneurological surgery, university of california, san francisco, san francisco,\nunited states; 3the eli and edythe broad center of regeneration medicine, univ", "article\nshared and distinct transcriptomic cell \ntypes across neocortical areas\n\nhttps://doi.org/10.1038/s41586-018-0654-5\n\nbosiljka tasic1*, zizhen yao1,4, lucas t. graybuck1,4, kimberly a. smith1,4, thuc nghi nguyen1, darren bertagnolli1, jeff goldy1, \nemma garren1, michael n. economo2, sarada viswanathan2, osnat penn1, trygve bakken1, vilas menon1,2, jeremy miller1,  \nolivia fong1, karla e. hirokawa1, kanan lathia1, christine rimorin1, michael tieu1, rachael larsen1, tamara casper1,  \neliza b", "dynamics of blood flow and oxygenation changes \nduring brain activation: the balloon model \n\nrichard b. buxton, eric c. wong, lawrence r. frank \n\na biomechanical model is presented for the dynamic changes \nin  deoxyhemoglobin content  during  brain  activation.  the \nmodel  incorporates  the  conflicting  effects  of  dynamic \nchanges in both blood oxygenation and blood volume. calcu- \nlations based on the model show pronounced transients in the \ndeoxyhemoglobin content and  the  blood  oxygenat", "7\n1\n0\n2\n\n \nr\na\n\n \n\nm\n1\n2\n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n2\nv\n6\n0\n9\n3\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nmassive exploration of neural machine translation\n\narchitectures\n\ndenny britz\u2217\u2020, anna goldie\u2217, minh-thang luong, quoc le\n\n{dennybritz,agoldie,thangluong,qvl}@google.com\n\ngoogle brain\n\nabstract\n\nthe \ufb01rst\n\nneural machine translation (nmt) has\nshown remarkable progress over the past\nfew years with production systems now\nbeing deployed to end-users. one major\ndrawback of current architectures is that\nthey are exp", "expectation-induced modulation of metastable \nactivity underlies faster coding of sensory stimuli\n\nl.\u00a0mazzucato1,2, g.\u00a0la\u00a0camera\u200a\n\n\u200a1,3* and a.\u00a0fontanini\u200a\n\n\u200a1,3*\n\nsensory stimuli can be recognized more rapidly when they are expected. this phenomenon depends on expectation affect-\ning the cortical processing of sensory information. however, the mechanisms responsible for the effects of expectation on \nsensory circuits remain elusive. in the present study, we report a novel computational mechanism", "article\n\nreceived 27 jun 2016 | accepted 14 sep 2016 | published 27 oct 2016\n\ndoi: 10.1038/ncomms13239\n\nopen\n\nreorganization between preparatory and\nmovement population responses in motor cortex\ngamaleldin f. elsayed1,2,*, antonio h. lara2,*, matthew t. kaufman3, mark m. churchland2,4,5,6\n& john p. cunningham1,4,7\n\nthe underlying computational strategies at\n\nneural populations can change the computation they perform on very short timescales.\nalthough such \ufb02exibility is common,\nthe\npopulation lev", "manhattan rule training for memristive crossbar \n\ncircuit pattern classifiers \n\nelham zamanidoost, farnood m. bayat, dmitri strukov \n\nelectrical and computer engineering department \n\nuniversity of california santa barbara \n\nsanta barbara, ca, usa \n\n{elham, farnoodmb, strukov}@ece.ucsb.edu \n\n \n\nabstract\u2014we  investigated  batch  and  stochastic  manhattan \nrule  algorithms  for  training  multilayer  perceptron  classifiers \nimplemented  with  memristive  crossbar  circuits.  in  manhattan \nrule  ", "22. j. f. kelly, k. g. horton, glob. ecol. biogeogr. 25, 1159\u20131165 (2016).\n\nbiological sciences research council grant bb/j004286/1 to j.w.c.\ndata are available at dryad at http://dx.doi.org/10.5061/dryad.6kt29.\n\ntables s1 to s8\nreferences (23\u201349)\n\nacknowledgments\ng.h.\u2019s visiting scholarship was funded by the china scholarship\ncouncil. we acknowledge the support provided by cost\u2014european\ncooperation in science and technology through the action es1305\n\u201cenram.\u201d the project was supported by uk biot", "research article\nidentifying control ensembles for information\nprocessing within the cortico-basal ganglia-\nthalamic circuit\n\ncatalina vichid1,2, matthew clappid3,4, jonathan e. rubinid4,5\u262f*,\ntimothy verstynenid3,4\u262f*\n1 dept. de matem\u00e0tiques i inform\u00e0tica, universitat de les illes balears, palma, spain, 2 institute of applied\ncomputing and community code, palma, spain, 3 department of psychology & neuroscience institute,\ncarnegie mellon university, pittsburgh, pennsylvania, united states of ameri", "p\ne\nr\ng\na\nm\no\nn\n \np\ni\ni\n:\n \ns\n0\n0\n4\n2\n-\n6\n9\n8\n9\n(\n9\n7\n)\n0\n0\n1\n2\n1\n-\n1\n \nv\ni\ns\ni\no\nn\n \nr\ne\ns\n.\n,\n \nv\no\nl\n.\n \n3\n7\n,\n \nn\no\n.\n \n2\n3\n,\n \np\np\n.\n \n3\n3\n2\n7\n-\n3\n3\n3\n8\n,\n \n1\n9\n9\n7\n \n\u00a9\n \n1\n9\n9\n7\n \ne\nl\ns\ne\nv\ni\ne\nr\n \ns\nc\ni\ne\nn\nc\ne\n \nl\nt\nd\n.\n \na\nl\nl\n \nr\ni\ng\nh\nt\ns\n \nr\ne\ns\ne\nr\nv\ne\nd\n \np\nr\ni\nn\nt\ne\nd\n \ni\nn\n \ng\nr\ne\na\nt\n \nb\nr\ni\nt\na\ni\nn\n \n0\n0\n4\n2\n-\n6\n9\n8\n9\n/\n9\n7\n \n$\n1\n7\n.\n0\n0\n \n+\n \n0\n.\n0\n0\n \nt\nh\ne\n \n\"\ni\nn\nd\ne\np\ne\nn\nd\ne\nn\nt\n \nc\no\nm\np\no\nn\ne\nn\nt\ns\n\"\n \no\nf\n \ns\nc\ne\nn\ne\ns\n \na\nr\ne\n \ne\nd\ng\ne\n \nf\ni\nl\nt\ne\nr\ns\n \na\nn\nt\nh\no\nn\ny\n", "\f", "hybrid speech recognition with deep bidirectional lstm\n\nalex graves, navdeep jaitly and abdel-rahman mohamed\n\nuniversity of toronto\n\ndepartment of computer science\n\n6 king\u2019s college rd. toronto, m5s 3g4, canada\n\nabstract\n\ndeep bidirectional lstm (dblstm) recurrent neural net-\nworks have recently been shown to give state-of-the-art per-\nformance on the timit speech database. however, the re-\nsults in that work relied on recurrent-neural-network-speci\ufb01c\nobjective functions, which are dif\ufb01cult to i", "article\n\nhttps://doi.org/10.1038/s41467-020-19788-5\n\nopen\n\nremembrance of things practiced with fast and\nslow learning in cortical and subcortical pathways\n\njames m. murray\n\n1,2\u2709 & g. sean escola\n\n1,3\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\nthe learning of motor skills unfolds over multiple timescales, with rapid initial gains in\nperformance followed by a longer period in which the behavior becomes more re\ufb01ned,\nhabitual, and automatized. while recent lesion and inactivation experiments have provided\nhin", "article\n\nreceived 3 feb 2016 | accepted 29 oct 2016 | published 13 dec 2016\n\ndoi: 10.1038/ncomms13749\n\nopen\n\nmaking brain\u2013machine interfaces robust to\nfuture neural variability\ndavid sussillo1,2,*, sergey d. stavisky3,*, jonathan c. kao1,*, stephen i. ryu1,4 & krishna v. shenoy1,2,3,5,6,7\n\na major hurdle to clinical translation of brain\u2013machine interfaces (bmis) is that current\ndecoders, which are trained from a small quantity of recent data, become ineffective when\nneural recording conditions s", "neuron\n\nreview\n\nneuroscience-inspired arti\ufb01cial intelligence\n\ndemis hassabis,1,2,* dharshan kumaran,1,3 christopher summer\ufb01eld,1,4 and matthew botvinick1,2\n1deepmind, 5 new street square, london, uk\n2gatsby computational neuroscience unit, 25 howland street, london, uk\n3institute of cognitive neuroscience, university college london, 17 queen square, london, uk\n4department of experimental psychology, university of oxford, oxford, uk\n*correspondence: dhcontact@google.com\nhttp://dx.doi.org/10.1016/", "computational prediction of methylation status\nin human genomic sequences\n\nrajdeep das*, nevenka dimitrova\u2020, zhenyu xuan*, robert a. rollins\u2021, fatemah haghighi\u00a7, john r. edwards\u00a7\u00b6,\njingyue ju\u00a7\u00b6, timothy h. bestor\u2021, and michael q. zhang*\u50a8\n\n*cold spring harbor laboratory, cold spring harbor, ny 11724; \u2020philips research, 345 scarborough road, briarcliff manor, ny 10510; \u2021department\nof genetics and development, college of physicians and surgeons of columbia university, new york, ny 10032; and \u00a7colum", "article\n\ndoi:10.1038/nature14251\n\nbranch-specific dendritic ca21 spikes\ncause persistent synaptic plasticity\n\njoseph cichon1 & wen-biao gan1\n\nthe brain has an extraordinary capacity for memory storage, but how it stores new information without disrupting\npreviously acquired memories remains unknown. here we show that different motor learning tasks induce dendritic\nca21 spikes on different apical tuft branches of individual layer v pyramidal neurons in the mouse motor cortex. these\ntask-related, ", "asynchronous methods for deep reinforcement learning\n\nvolodymyr mnih1\nadri\u00e0 puigdom\u00e8nech badia1\nmehdi mirza1,2\nalex graves1\ntim harley1\ntimothy p. lillicrap1\ndavid silver1\nkoray kavukcuoglu 1\n1 google deepmind\n2 montreal institute for learning algorithms (mila), university of montreal\n\nvmnih@google.com\nadriap@google.com\nmirzamom@iro.umontreal.ca\ngravesa@google.com\ntharley@google.com\ncountzero@google.com\ndavidsilver@google.com\nkorayk@google.com\n\nabstract\nconceptually\n\na\n\nsimple\n\npropose\n\nand\nwe\nl", "the journal of neuroscience, march 1, 2001, 21(5):1635\u20131644\n\naccumulation of hippocampal place fields at the goal location in\nan annular watermaze task\n\nstig a. hollup,1 sturla molden,1 james g. donnett,2 may-britt moser,1 and edvard i. moser1\n1department of psychology, norwegian university of science and technology, 7491 trondheim, norway, and 2axona\nltd., st. albans, herts al3 6eu, united kingdom\n\nto explore the plastic representation of information in spatially\nselective hippocampal pyramidal", "j neurophysiol 124: 312\u2013329, 2020.\nfirst published june 24, 2020; doi:10.1152/jn.00158.2020.\n\nresearch article nervous system pathophysiology\n\ndelta oscillations are a robust biomarker of dopamine depletion severity and\nmotor dysfunction in awake mice\n\nx timothy c. whalen,1,2 x amanda m. willard,1,2,3 x jonathan e. rubin,1,4 and aryn h. gittis1,2\n1center for the neural basis of cognition, carnegie mellon university, pittsburgh, pennsylvania; 2neuroscience institute and\ndepartment of biological s", "neuron, volume 81 \n\nsupplemental information \n\nlearning by the dendritic \n\nprediction of somatic spiking \n\nrobert urbanczik and walter senn \n\n\f", "surrogate gradient learning  \nin spiking neural networks\nbringing the power of gradient-based optimization  \nto spiking neural networks\n\ns piking neural networks (snns) are nature\u2019s versatile solu-\n\ntion to fault-tolerant, energy-efficient signal processing. to \ntranslate these benefits into hardware, a growing number of \nneuromorphic spiking nn processors have attempted to emulate \nbiological nns. these developments have created an imminent \nneed  for  methods  and  tools  that  enable  such  s", "hebbian  synapses \n\nin  visual  cortex \n\nthe  journal \n\nof  neuroscience, \n\nmarch \n\n1994, \n\n14(3): \n\n1634-l \n\n645 \n\nalfred0  kirkwood \nbrown  university  department \nrhode \n\nisland  02912 \n\nand  mark  f.  bear \n\nof  neuroscience \n\nand  institute \n\nfor  brain  and  neural  systems,  brown  university,  providence, \n\n(ltp) \nburst \nby  theta \nof  the  cortical \niv.  this  synaptic \nfield  potentials \n\nin  slices  of  rat  visual  cortex \n\nthat  reliable \n\nof  synaptic \n\nresponses \n\nin \n\nlayer \n\nlon", "j neurophysiol 98: 2038 \u20132057, 2007.\nfirst published july 25, 2007; doi:10.1152/jn.01311.2006.\n\nmodel of birdsong learning based on gradient estimation by dynamic\nperturbation of neural conductances\n\nila r. fiete,1,2 michale s. fee,3,5 and h. sebastian seung4,5\n1kavli institute for theoretical physics, university of california, santa barbara, santa barbara; 2center for theoretical biological\nphysics, university of california, san diego, la jolla, california; and 3mcgovern institute for brain res", "research\n\nneuroscience\n\nlearning and attention reveal a\ngeneral relationship between\npopulation activity and behavior\n\na. m. ni, d. a. ruff, j. j. alberts, j. symmonds, m. r. cohen*\n\nprior studies have demonstrated that correlated variability changes with cognitive processes\nthat improve perceptual performance. we tested whether correlated variability covaries\nwith subjects\u2019 performance\u2014whether performance improves quickly with attention or slowly\nwith perceptual learning. we found a single, con", "unified rational protein engineering with \nsequence-based deep representation learning\n\nethan c. alley1,2,6, grigory khimulya6,7, surojit biswas1,3,6, mohammed alquraishi\u200a\ngeorge m. church\u200a\n\n\u200a1,5*\n\n\u200a4 and \n\nrational protein engineering requires a holistic understanding of protein function. here, we apply deep learning to unlabeled \namino-acid sequences to distill the fundamental features of a protein into a statistical representation that is semantically rich \nand structurally, evolutionarily an", "prevalence of neural collapse during the terminal\nphase of deep learning training\n\nvardan papyana,1\n\n, x. y. hanb,1\n\n, and david l. donohoa,2\n\nadepartment of statistics, stanford university, stanford, ca 94305-4065; and bschool of operations research and information engineering, cornell\nuniversity, ithaca, ny 14850\n\ncontributed by david l. donoho, august 18, 2020 (sent for review july 22, 2020; reviewed by helmut boelsckei and st \u00b4ephane mallat)\n\nmodern practice for training classi\ufb01cation deepne", "neurons learn by predicting future activity\n\nartur luczak\u200a\n\n\u200a1\u2009\u2709, bruce l. mcnaughton1,2 and yoshimasa kubo1\n\nunderstanding how the brain learns may lead to machines with human-like intellectual capacities. it was previously proposed \nthat the brain may operate on the principle of predictive coding. however, it is still not well understood how a predictive system \ncould be implemented in the brain. here we demonstrate that the ability of a single neuron to predict its future activity may \nprovid", "research article\ndeep convolutional models improve\npredictions of macaque v1 responses to\nnatural images\n\nsantiago a. cadenaid1,2,3*, george h. denfieldid3,4, edgar y. walkerid3,4, leon\na. gatys1,2, andreas s. toliasid2,3,4,5\u262f, matthias bethge1,2,3,6\u262f, alexander s. eckerid1,2,3\u262f\n\n1 centre for integrative neuroscience and institute for theoretical physics, university of tu\u00a8bingen,\ntu\u00a8bingen, germany, 2 bernstein center for computational neuroscience, tu\u00a8bingen, germany, 3 center\nfor neuroscience ", "direct voxel grid optimization:\n\nsuper-fast convergence for radiance fields reconstruction\n\ncheng sun*,\u2020\n\nmin sun*,\u2021\n\nchengsun@gapp.nthu.edu.tw\n\nsunmin@ee.nthu.edu.tw\n\nhwann-tzong chen*,\u00a7\nhtchen@cs.nthu.edu.tw\n\n2\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n3\n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n5\n1\n2\n1\n1\n\n.\n\n1\n1\n1\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe present a super-fast convergence approach to recon-\nstructing the per-scene radiance \ufb01eld from a set of images\nthat capture the scene with known poses. this task, which is\noften applied to ", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nbrain  structure  and  dynamics  across  scales:  in  search\nof  rules\nxiao-jing  wang1,2 and  henry  kennedy3,4\n\nlouis  henry  sullivan,  the  father  of  skyscrapers,  famously  stated\n\u2018form  ever  follows  function\u2019.  in  this  short  review,  we  will  focus\non  the  relationship  between  form  (structure)  and  function\n(dynamics)  in  the  brain.  we  summarize  recent  advances  on  the\nquanti\ufb01cation  of  directed-  and  weight", "0\n2\n0\n2\n\n \nl\nu\nj\n \n\n3\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n9\n0\n5\n2\n0\n\n.\n\n0\n1\n9\n1\n:\nv\ni\nx\nr\na\n\nremind your neural network to prevent\n\ncatastrophic forgetting\n\ntyler l. hayes1,(cid:63), kushal ka\ufb02e2,(cid:63), robik shrestha1,(cid:63),\n\nmanoj acharya1, and christopher kanan1,3,4\n\n1 rochester institute of technology, rochester ny 14623, usa\n\n{tlh6792,rss9369,ma7583,kanan}@rit.edu\n2 adobe research, san jose ca 95110, usa\n\nkkafle@adobe.com\n\n3 paige, new york ny 10036, usa\n\n4 cornell tech, new york ny 10044,", "long short-term memory and learning-to-learn in\n\nnetworks of spiking neurons\n\nguillaume bellec*, darjan salaj*, anand subramoney*, robert legenstein & wolfgang maass\n\ninstitute for theoretical computer science\n\n{bellec,salaj,subramoney,legenstein,maass}@igi.tugraz.at\n\ngraz university of technology, austria\n\n* equal contributions\n\nabstract\n\nrecurrent networks of spiking neurons (rsnns) underlie the astounding comput-\ning and learning capabilities of the brain. but computing and learning capabilit", "reverse engineering learned optimizers\nreveals known and novel mechanisms\n\nniru maheswaranathan\u2217\ngoogle research, brain team\n\nniru@hey.com\n\ndavid sussillo\u2217\n\ngoogle research, brain team\n\nluke metz\n\nruoxi sun\n\ngoogle research, brain team\n\ngoogle research, brain team\n\nlmetz@google.com\n\nruoxis@google.com\n\njascha sohl-dickstein\n\ngoogle research, brain team\n\njaschasd@google.com\n\nabstract\n\nlearned optimizers are parametric algorithms that can themselves be trained to\nsolve optimization problems. in con", "2\n2\n0\n2\n\n \nt\nc\no\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n2\n0\n5\n2\n0\n\n.\n\n0\n1\n0\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2021\n\ndenoising diffusion implicit models\n\njiaming song, chenlin meng & stefano ermon\nstanford university\n{tsong,chenlin,ermon}@cs.stanford.edu\n\nabstract\n\ndenoising diffusion probabilistic models (ddpms) have achieved high qual-\nity image generation without adversarial training, yet they require simulating a\nmarkov chain for many steps in order to produce a sample. to acc", "letter\n\ncommunicated by rajesh rao\n\noptimal spike-timing-dependent plasticity for precise action\npotential firing in supervised learning\n\njean-pascal p\ufb01ster\njean-pascal.p\ufb01ster@ep\ufb02.ch\ntaro toyoizumi\ntaro@sat.t.u-tokyo.ac.jp\ndavid barber\ndavid.barber@idiap.ch\nwulfram gerstner\nwulfram.gerstner@ep\ufb02.ch\nlaboratory of computational neuroscience, school of computer and communication\nsciences and brain-mind institute, ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n(epfl), ch-1015 lausanne, switzerland\n\nin ti", "e\nc\nn\ne\ni\nc\ns\no\nr\nu\ne\nn\ne\nr\nu\nt\na\nn\nm\no\nc\n.\ne\nr\nu\nt\na\nn\nw\nw\nw\n\n/\n\n.\n\n/\n/\n:\np\nt\nt\nh\n \n \n\np\nu\no\nr\ng\ng\nn\nh\ns\n\ni\n\n \n\ni\nl\n\n \n\nb\nu\np\ne\nr\nu\nt\na\nn\n2\n0\n0\n2\n\u00a9\n\n \n\narticles\n\noptimal feedback control as a\ntheory of motor coordination\n\nemanuel todorov1 and michael i. jordan2\n\n1 department of cognitive science, university of california, san diego, 9500 gilman dr., la jolla, california 92093-0515, usa\n2 division of computer science and department of statistics, university of california, berkeley, 731 soda hall", "vision research 41 (2001) 1409\u20131422\n\nwww.elsevier.com/locate/visres\n\nthe lateral occipital complex and its role in object recognition\n\nkalanit grill-spector a,*, zoe kourtzi a, nancy kanwisher a,b\n\na department of brain and cogniti6e sciences, massachusetts institute of technology, cambridge, ma 02139, usa\n\nb massachusetts general hospital, nmr center, charlestown, ma 02129, usa\n\nreceived 14 february 2001\n\nabstract\n\nhere we review recent \ufb01ndings that reveal the functional properties of extra-str", "physical review e 69, 066138 (2004)\n\nestimating mutual information\n\nalexander kraskov, harald st\u00f6gbauer, and peter grassberger\n\njohn-von-neumann institute for computing, forschungszentrum j\u00fclich, d-52425 j\u00fclich, germany\n\n(received 28 may 2003; published 23 june 2004)\n\nwe present two classes of improved estimators for mutual information msx,yd, from samples of random\npoints distributed according to some joint probability density msx, yd. in contrast to conventional estimators\nbased on binnings, t", "proc. natl. acad. sci. usa\nvol. 96, pp. 1083\u20131087, february 1999\nneurobiology\n\nthe role of presynaptic activity in monocular deprivation:\ncomparison of homosynaptic and heterosynaptic mechanisms\n\nbrian s. blais*, harel z. shouval, and leon n. cooper\ndepartments of physics and neuroscience and institute for brain and neural systems, brown university, providence ri 02912\n\ncontributed by leon n cooper, november 3, 1998\n\nabstract\nalthough investigations in computational\nneuroscience have been extens", "the journal of neuroscience, december 1, 2000, 20(23):8812\u20138821\n\nstable hebbian learning from spike timing-dependent plasticity\n\nm. c. w. van rossum,1 g. q. bi,2 and g. g. turrigiano1\n1brandeis university, department of biology, waltham, massachusetts 02454-9110, and 2university of california at san\ndiego, department of biology, la jolla, california 92093-0357\n\nwe explore a synaptic plasticity model that incorporates recent\n\ufb01ndings that potentiation and depression can be induced by\nprecisely tim", "8\n1\n0\n2\n\n \nt\nc\no\n0\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n6\nv\n8\n3\n4\n2\n0\n\n.\n\n6\n0\n5\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2016\n\nhigh-dimensional continuous control using\ngeneralized advantage estimation\n\njohn schulman, philipp moritz, sergey levine, michael i. jordan and pieter abbeel\ndepartment of electrical engineering and computer science\nuniversity of california, berkeley\n{joschu,pcmoritz,levine,jordan,pabbeel}@eecs.berkeley.edu\n\nabstract\n\npolicy gradient methods are an appealing appro", "9\n1\n0\n2\n\n \nr\na\n\nm\n3\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n3\nv\n4\n4\n1\n6\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nkernel machines that adapt to gpus for\n\neffective large batch training\n\nsiyuan ma 1 mikhail belkin 1\n\nabstract\n\nmodern machine learning models are typically trained using stochastic gradient descent (sgd) on massively\nparallel computing resources such as gpus. increasing mini-batch size is a simple and direct way to utilize the\nparallel computing capacity. for small batch an increase in batch size results ", "cerebral cortex, april 2018;28: 1396\u20131415\n\ndoi: 10.1093/cercor/bhx339\nadvance access publication date: 28 december 2017\noriginal article\n\no r i g i n a l a r t i c l e\nmulticontact co-operativity in spike-timing\u2013\ndependent structural plasticity stabilizes networks\nmoritz deger1,2, alexander seeholzer1 and wulfram gerstner1\n\n1school of computer and communication sciences and school of life sciences, brain mind institute, \u00e9cole\npolytechnique f\u00e9d\u00e9rale de lausanne, 1015 lausanne epfl, switzerland an", "a uni\ufb01ed architecture for natural language processing:\n\ndeep neural networks with multitask learning\n\nronan collobert\njason weston\nnec labs america, 4 independence way, princeton, nj 08540 usa\n\ncollober@nec-labs.com\njasonw@nec-labs.com\n\nabstract\n\nwe describe a single convolutional neural net-\nwork architecture that, given a sentence, out-\nputs a host of language processing predic-\ntions: part-of-speech tags, chunks, named en-\ntity tags, semantic roles, semantically similar\nwords and the likeliho", "nonlinear ica using auxiliary variables\n\nand generalized contrastive learning\n\naapo hyv\u00a8arinen 1,2\n\nhiroaki sasaki 3,1\n\nrichard e. turner 4\n\n1 the gatsby unit\n\nucl, uk\n\n2 dept. of cs and hiit\nuniv. helsinki, finland\n\n3div. of info. sci.\n\nnaist, japan\n\n4 univ. cambridge &\nmicrosoft research, uk\n\nabstract\n\nnonlinear ica is a fundamental problem\nfor unsupervised representation learning, em-\nphasizing the capacity to recover the underly-\ning latent variables generating the data (i.e.,\nidenti\ufb01ability", "letters to nature\n\nwith empty vector were used as controls. we isolated agp fractions from the culture\nmedium of transgenic by-2 cells by using the speci\ufb01c interaction of agps with b glcy3 and\nadded them into zinnia microbead cultures to monitor the activity of xylogen.\n\nanalysis of xylogen\nxylogen was puri\ufb01ed from the agp fraction, which was obtained from the conditioned\nmedium of the zinnia xylogenic culture by using the speci\ufb01c interaction of agps with\nb glcy3. the agp fraction was boiled for", "ps68ch05-gershman\n\nari\n\n4 november 2016\n\n10:31\n\nreinforcement learning and\nepisodic memory in humans\nand animals: an integrative\nframework\nsamuel j. gershman1 and nathaniel d. daw2\n1department of psychology and center for brain science, harvard university, cambridge,\nmassachusetts 02138; email: gershman@fas.harvard.edu\n2princeton neuroscience institute and department of psychology, princeton university,\nprinceton, new jersey 08544\n\nannu. rev. psychol. 2017. 68:101\u201328\n\nfirst published online as a", "published as a conference paper at iclr 2021\n\nscore-based generative modeling through\nstochastic differential equations\n\nyang song\u02da\nstanford university\nyangsong@cs.stanford.edu\n\njascha sohl-dickstein\ngoogle brain\njaschasd@google.com\n\ndiederik p. kingma\ngoogle brain\ndurk@google.com\n\nabhishek kumar\ngoogle brain\nabhishk@google.com\n\nstefano ermon\nstanford university\nermon@cs.stanford.edu\n\nben poole\ngoogle brain\npooleb@google.com\n\nabstract\n\ncreating noise from data is easy; creating data from noise i", "9\n1\n0\n2\n\n \nr\na\n\nm\n4\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n5\nv\n5\n3\n6\n3\n0\n\n.\n\n3\n0\n8\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2019\n\nthe lottery ticket hypothesis:\nfinding sparse, trainable neural networks\n\njonathan frankle\nmit csail\njfrankle@csail.mit.edu\n\nmichael carbin\nmit csail\nmcarbin@csail.mit.edu\n\nabstract\n\nneural network pruning techniques can reduce the parameter counts of trained net-\nworks by over 90%, decreasing storage requirements and improving computational\nperformance of inferenc", "optimal errors and phase transitions in\nhigh-dimensional generalized linear models\n\njean barbiera,b,1,2, florent krzakalab,1, nicolas macrisc,1, l \u00b4eo miolaned,1,2, and lenka zdeborov \u00b4ae,1\n\naquantitative life sciences, international center for theoretical physics, 34151 trieste, italy; blaboratoire de physique de l\u2019ecole normale sup \u00b4erieure,\nuniversit \u00b4e paris-sciences-et-lettres, centre national de la recherche scienti\ufb01que, sorbonne universit \u00b4e, universit \u00b4e paris-diderot, sorbonne paris cit", "the impact of traditional neuroimaging methods on\nthe spatial localization of cortical areas\n\ntimothy s. coalsona, david c. van essena,1, and matthew f. glassera,b,1\n\nadepartment of neuroscience, washington university school of medicine, st. louis, mo 63110; and bst. luke\u2019s hospital, st. louis, mo 63017\n\ncontributed by david c. van essen, may 17, 2018 (sent for review january 29, 2018; reviewed by alexander l. cohen, james v. haxby, and martin i. sereno)\n\nin\nlocalizing human brain functions is a", "maximum likelihood from incomplete data via the em algorithm \nauthor(s): a. p. dempster, n. m. laird and d. b. rubin \nsource: journal of the royal statistical society. series b (methodological), 1977, vol. 39, \nno. 1 (1977), pp. 1-38\n \npublished by: wiley for the royal statistical society \n\nstable url: https://www.jstor.org/stable/2984875\n\njstor is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide \nrange of content in a trusted digital a", "the journal of neuroscience, june 15, 1999, 19(12):5066\u20135073\n\ndifferential neural responses during performance of matching and\nnonmatching to sample tasks at two delay intervals\n\nrebecca elliott1 and raymond j. dolan1,2\n1wellcome department of cognitive neurology, institute of neurology, london wc1n 3bg, united kingdom, and\n2royal free hospital school of medicine, london nw3 2pf, united kingdom\n\nvisual short-term memory in humans and animals is frequently\nassessed using delayed matching to sampl", "european journal of neuroscience\n\neuropean journal of neuroscience, vol. 35, pp. 1024\u20131035, 2012\n\ndoi:10.1111/j.1460-9568.2011.07980.x\n\nhow much of reinforcement learning is working memory,\nnot reinforcement learning? a behavioral, computational,\nand neurogenetic analysis\n\nanne g. e. collins and michael j. frank\ndepartment of cognitive, linguistic and psychological sciences, brown institute for brain science, brown university, providence,\nri, usa\n\nkeywords: basal ganglia, capacity, human, prefro", "a r t i c l e s\n\nmultilaminar networks of cortical neurons integrate \ncommon inputs from sensory thalamus\nnicol\u00e1s a morgenstern, jacques bourg & leopoldo petreanu\nneurons in the thalamorecipient layers of sensory cortices integrate thalamic and recurrent cortical input. cortical neurons \nform fine-scale, functionally cotuned networks, but whether interconnected cortical neurons within a column process common \nthalamocortical inputs is unknown. we tested how local and thalamocortical connectivity", "kernel measures of conditional dependence\n\nkenji fukumizu\n\ninstitute of statistical mathematics\n4-6-7 minami-azabu, minato-ku\n\ntokyo 106-8569 japan\n\narthur gretton\n\nmax-planck institute for biological cybernetics\nspemannstra\u00dfe 38, 72076 t\u00a8ubingen, germany\narthur.gretton@tuebingen.mpg.de\n\nfukumizu@ism.ac.jp\n\nxiaohai sun\n\nmax-planck institute for biological cybernetics\nspemannstra\u00dfe 38, 72076 t\u00a8ubingen, germany\n\nxiaohi@tuebingen.mpg.de\n\nbernhard sch\u00a8olkopf\n\nmax-planck institute for biological cybe", "cerebral cortex, august 2020;30: 4361\u20134380\n\ndoi: 10.1093/cercor/bhaa023\nadvance access publication date: 3 april 2020\noriginal article\n\no r i g i n a l a r t i c l e\na domain-general cognitive core defined in\nmultimodally parcellated human cortex\nmoataz assem 1, matthew f. glasser2,3,4, david c. van essen2 and\njohn duncan1,5\n1mrc cognition and brain sciences unit, university of cambridge, cambridge, cb2 7ef, uk, 2department of\nneuroscience, washington university in st. louis, saint louis, mo 631", "stochastic solutions for linear inverse problems\n\nusing the prior implicit in a denoiser\n\nzahra kadkhodaie\n\ncenter for data science,\n\nnew york university\n\nzk388@nyu.edu\n\neero p. simoncelli\n\ncenter for neural science, and\n\ncourant inst. of mathematical sciences,\n\nnew york university\n\nflatiron institute, simons foundation\n\neero.simoncelli@nyu.edu\n\nabstract\n\ndeep neural networks have provided state-of-the-art solutions for problems such\nas image denoising, which implicitly rely on a prior probabili", "an oscillatory interference model of grid cell firing\n\nneil burgess,1,2* caswell barry,1\u20133 and john o\u2019keefe2,3\n\nhippocampus 17:801\u2013812 (2007)\n\nabstract: we expand upon our proposal that the oscillatory inter-\nference mechanism proposed for the phase precession effect in place\ncells underlies the grid-like \ufb01ring pattern of dorsomedial entorhinal grid\ncells (o\u2019keefe and burgess (2005) hippocampus 15:853\u2013866). the origi-\nnal one-dimensional interference model is generalized to an appropriate\ntwo-di", "manifold clustering\n\nrichard souvenir and robert pless\nwashington university in st. louis\n\ndepartment of computer science and engineering\n\ncampus box 1045, one brookings drive, st. louis, mo 63130\n\n{rms2, pless}@cse.wustl.edu\n\nabstract\n\nmanifold learning has become a vital tool in data driven\nmethods for interpretation of video, motion capture, and\nhandwritten character data when they lie on a low dimen-\nsional, non-linear manifold. this work extends manifold\nlearning to classify and parameteriz", "review\nreinforcement  learning,  fast  and  slow\n\nmatthew  botvinick,1,2,* sam  ritter,1,3 jane  x.  wang,1 zeb  kurth-nelson,1,2 charles  blundell,1 and\ndemis  hassabis1,2\n\ndeep  reinforcement  learning  (rl)  methods  have  driven  impressive  advances  in\narti\ufb01cial  intelligence  in  recent  years,  exceeding  human  performance  in  domains\nranging  from  atari  to  go  to  no-limit  poker.  this  progress  has  drawn  the  atten-\ntion  of  cognitive  scientists  interested  in  understandin", "overcoming catastrophic forgetting in\nneural networks\n\njames kirkpatricka,1, razvan pascanua, neil rabinowitza, joel venessa, guillaume desjardinsa, andrei a. rusua,\nkieran milana, john quana, tiago ramalhoa, agnieszka grabska-barwinskaa, demis hassabisa, claudia clopathb,\ndharshan kumarana, and raia hadsella\n\nadeepmind, london ec4 5tw, united kingdom; and bbioengineering department, imperial college london, london sw7 2az, united kingdom\n\nedited by james l. mcclelland, stanford university, stan", "neuron\n\narticle\n\nhippocampal swr activity predicts\ncorrect decisions during the initial learning\nof an alternation task\n\nannabelle c. singer,1,2 margaret f. carr,2,3 mattias p. karlsson,2,4 and loren m. frank2,*\n1mcgovern institute for brain research and mit media lab, mit, cambridge, ma 02139, usa\n2center for integrative neuroscience and department of physiology, university of california, san francisco, ca 94143-0444, usa\n3cnc program, stanford university, stanford, ca 94305, usa\n4janelia farms", "innovative methodology\n\nj neurophysiol 93: 1074 \u20131089, 2005.\nfirst published september 8, 2004; doi:10.1152/jn.00697.2004.\n\na point process framework for relating neural spiking activity to spiking\nhistory, neural ensemble, and extrinsic covariate effects\n\nwilson truccolo,1 uri t. eden, 2,3 matthew r. fellows,1 john p. donoghue,1 and emery n. brown2,3\n1neuroscience department, brown university, providence, rhode island; 2neuroscience statistics research laboratory, department of\nanesthesia and c", "in the format provided by the authors and unedited.\n\nprefrontal cortex as a meta-reinforcement \nlearning system\n\njane x. wang\u200a\njoel z. leibo1, demis hassabis1,4 and matthew botvinick\u200a\n\n\u200a1,5, zeb kurth-nelson1,2,5, dharshan kumaran1,3, dhruva tirumala1, hubert soyer1,  \n\n\u200a1,4*\n\n1deepmind, london, uk. 2max planck-ucl centre for computational psychiatry and ageing research, university college london, london, uk. 3institute \nof cognitive neuroscience, university college london, london, uk. 4gatsby c", "21st international conference on pattern recognition (icpr 2012)\nnovember 11-15, 2012. tsukuba, japan\n\n978-4-9906441-1-6 \u00a92012 iapr\n\n3288\n\nconvolutionalneuralnetworksappliedtohousenumbersdigitclassi\ufb01cationpierresermanet,soumithchintalaandyannlecunthecourantinstituteofmathematicalsciences-newyorkuniversity{sermanet,soumith,yann}@cs.nyu.eduabstractweclassifydigitsofreal-worldhousenumbersus-ingconvolutionalneuralnetworks(convnets).con-vnetsarehierarchicalfeaturelearningneuralnet-workswhosestructure", "0\n2\n0\n2\n\n \n\nb\ne\nf\n0\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n1\n6\n5\n3\n0\n\n.\n\n0\n1\n9\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2020\n\ndeep network classification by scattering\nand homotopy dictionary learning\n\njohn zarka, louis thiry, tom\u00e1s angles\nd\u00e9partement d\u2019informatique de l\u2019ens, ens, cnrs, psl university, paris, france\n{john.zarka,louis.thiry,tomas.angles}@ens.fr\n\nst\u00e9phane mallat\ncoll\u00e8ge de france, paris, france\nflatiron institute, new york, usa\n\nabstract\n\nwe introduce a sparse scattering ", "888\n\nieee transactions on pattern analysis and machine intelligence, vol. 22, no. 8, august 2000\n\nnormalized cuts and image segmentation\n\njianbo shi and jitendra malik, member, ieee\n\nabstract\u2014we propose a novel approach for solving the perceptual grouping problem in vision. rather than focusing on local features\nand their consistencies in the image data, our approach aims at extracting the global impression of an image. we treat image\nsegmentation as a graph partitioning problem and propose a no", "anatomical substrates for functional\ncolumns in macaque monkey primary\nvisual cortex\n\njennifer s. lund1, alessandra angelucci1 and paul c. bressloff2\n\n1moran eye center, university of utah, 50 north medical drive,\nsalt lake city, ut 84132, usa and 2department of\nmathematics, university of utah, 155 s. 1400 e. salt lake city,\nut 84112, usa\n\nin this review we re-examine the concept of a cortical column in\nmacaque primary visual cortex, and consider to what extent a\nfunctionally defined column refl", "j comput neurosci (2006) 21:35\u201349\ndoi 10.1007/s10827-006-7074-5\n\npredicting spike timing of neocortical pyramidal neurons\nby simple threshold models\nrenaud jolivet \u00b7 alexander rauch \u00b7\nhans-rudolf l\u00a8uscher \u00b7 wulfram gerstner\n\nreceived: 26 september 2005 / revised: 21 december 2005 / accepted: 11 january 2006 / published online: 22 april 2006\nc(cid:1) springer science + business media, llc 2006\n\nabstract neurons generate spikes reliably with millisec-\nond precision if driven by a \ufb02uctuating curren", "2\n2\n0\n2\n\n \n\nb\ne\nf\n2\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n1\n3\n1\n0\n0\n\n.\n\n8\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nsimple, fast, and flexible framework for matrix completion\n\nwith in\ufb01nite width neural networks\n\nadityanarayanan radhakrishnan1 george stefanakis1 mikhail belkin 2 caroline uhler1,3\n\nfebruary 23, 2022\n\nabstract\n\nmatrix completion problems arise in many applications including recommendation systems, computer\nvision, and genomics. increasingly larger neural networks have been successful in many of these appli-\n", "hypothesis and theory\npublished: 14 september 2016\ndoi: 10.3389/fncom.2016.00094\n\ntoward an integration of deep\nlearning and neuroscience\n\nadam h. marblestone 1*, greg wayne 2 and konrad p. kording 3\n\n1 synthetic neurobiology group, massachusetts institute of technology, media lab, cambridge, ma, usa, 2 google\ndeepmind, london, uk, 3 rehabilitation institute of chicago, northwestern university, chicago, il, usa\n\nneuroscience has focused on the detailed implementation of computation, studying\nneu", "ne43ch01_fishell\n\narjats.cls\n\njune 19, 2020\n\n7:39\n\nannu. rev. neurosci. 2020. 43:1\u201330\n\nfirst published as a review in advance on\njuly 12, 2019\n\nthe annual review of neuroscience is online at\nneuro.annualreviews.org\n\nhttps://doi.org/10.1146/annurev-neuro-070918-\n050421\n\ncopyright \u00a9 2020 by annual reviews.\nall rights reserved\n\nannual review of neuroscience\ninterneuron types as attractors\nand controllers\n\ngord fishell1,2,3 and adam kepecs4,5\n1department of neurobiology, blavatnik institute, harvard", "inferring single-trial neural population dynamics \nusing sequential auto-encoders\n\n\u200a1,2,3,4,5*, daniel\u00a0j.\u00a0o\u2019shea\u200a\n\nchethan\u00a0pandarinath\u200a\nsergey\u00a0d.\u00a0stavisky3,4,5,6, jonathan\u00a0c.\u00a0kao4,8, eric\u00a0m.\u00a0trautmann6, matthew\u00a0t.\u00a0kaufman6,22, \nstephen\u00a0i.\u00a0ryu4,9, leigh\u00a0r.\u00a0hochberg10,11,12, jaimie\u00a0m.\u00a0henderson3,5, krishna\u00a0v.\u00a0shenoy4,5,13,14,15,16, \nl.\u00a0f.\u00a0abbott17,18,19 and david\u00a0sussillo\u200a\n\n\u200a4,6, jasmine\u00a0collins7,20, rafal\u00a0jozefowicz7,21, \n\n\u200a4,5,7*\n\nneuroscience is experiencing a revolution in which simultaneous r", "the tradeoffs of large scale learning\n\nl\u00b4eon bottou\n\nnec laboratories of america\nprinceton, nj 08540, usa\n\nolivier bousquet\n\ngoogle z\u00a8urich\n\n8002 zurich, switzerland\n\nleon@bottou.org\n\nolivier.bousquet@m4x.org\n\nabstract\n\nthis contribution develops a theoretical framework that takes into account the\neffect of approximate optimization on learning algorithms. the analysis shows\ndistinct tradeoffs for the case of small-scale and large-scale learning problems.\nsmall-scale learning problems are subject", "reports\n\nidly, had immediate, major, and visible impacts\n_\non the island\ns biota and physical landscape, and\nbegan investing in monumental architecture and\nstatuary within the first century or two of set-\ntlement. although still poorly dated, monumen-\ntal architecture and statuary are known from\nislands, such as the societies, marquesas, and\naustral islands, from perhaps as early as 1200\na.d. nearly immediate building of monuments,\ncarving giant statues, and transporting them to\nevery corner of ", "9\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n9\n2\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n4\nv\n8\n3\n4\n5\n\n.\n\n0\n1\n3\n1\n:\nv\ni\nx\nr\na\n\nvariational bayesian inference\n\nfor linear and logistic regression\n\njan drugowitsch\n\nabstract. the article describe the model, derivation, and implementation\nof variational bayesian inference for linear and logistic regression, both with\nand without automatic relevance determination. it has the dual function of\nacting as a tutorial for the derivation of variational bayesian inference for\nsimple models, as ", "how recurrent networks implement contextual processing in sentiment analysis\n\nniru maheswaranathan * 1 david sussillo * 1\n\n0\n2\n0\n2\n\n \nr\np\na\n7\n1\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n1\nv\n3\n1\n0\n8\n0\n\n.\n\n4\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nabstract\n\nneural networks have a remarkable capacity for\ncontextual processing\u2014using recent or nearby in-\nputs to modify processing of current input. for\nexample, in natural language, contextual process-\ning is necessary to correctly interpret negation\n(e.g. phrases such as \u201cnot bad\u201d). how", "neuroimage 106 (2015) 222\u2013237\n\ncontents lists available at sciencedirect\n\nneuroimage\n\nj ou r n a l h o m e p a ge : w ww . e l s e v i e r . c o m / l oc a te / yn i mg\n\na computational analysis of the neural bases of bayesian inference\nantonio kolossa a, bruno kopp b,\u204e, tim fingscheidt a\na institute for communications technology, technische universit\u00e4t braunschweig, schleinitzstr. 22, 38106 braunschweig, germany\nb department of neurology, hannover medical school, carl-neuberg-str. 1, 30625 hann", "journal of computational physics 378 (2019) 686\u2013707\n\ncontents lists available at sciencedirect\n\njournal  of  computational  physics\n\nwww.elsevier.com/locate/jcp\n\nphysics-informed  neural  networks:  a  deep  learning \nframework  for  solving  forward  and  inverse  problems  involving \nnonlinear  partial  differential  equations\nm. raissi a, p. perdikaris b,\u2217, g.e. karniadakis a\n\na division of applied mathematics, brown university, providence, ri, 02912, usa\nb department of mechanical engineerin", "neuroimage 80 (2013) 105\u2013124\n\ncontents lists available at sciverse sciencedirect\n\nneuroimage\n\nj o u r n a l h o m e p a ge : w ww . e l s e v i e r . c o m/ l o c a t e / y n i m g\n\nthe minimal preprocessing pipelines for the human connectome project\nmatthew f. glasser a,\u204e, stamatios n. sotiropoulos b, j. anthony wilson c, timothy s. coalson a,\nbruce fischl d,e, jesper l. andersson b, junqian xu f,g, saad jbabdi b, matthew webster b,\njonathan r. polimeni d, david c. van essen a, mark jenkinson b", "original research\npublished: 08 october 2015\ndoi: 10.3389/fncom.2015.00120\n\nan algorithm to predict the\nconnectome of neural microcircuits\n\nmichael w. reimann, james g. king, eilif b. muller, srikanth ramaswamy and\nhenry markram *\n\nblue brain project, \u00e9cole polytechnique f\u00e9d\u00e9rale de lausanne (epfl) biotech campus, geneva, switzerland\n\nexperimentally mapping synaptic connections, in terms of the numbers and locations of\ntheir synapses and estimating connection probabilities, is still not a tracta", "article\ntransparency of arti\ufb01cial intelligence in healthcare: insights\nfrom professionals in computing and healthcare worldwide\njose bernal 1,2,3\n\nand claudia mazo 4,*,\u2020\n\n1 german center for neurodegenerative diseases (dzne), 39120 magdeburg, germany\n2\n\ninstitute of cognitive neurology and dementia research, otto-von-guericke university magdeburg,\n39120 magdeburg, germany\n\n3 centre for clinical brain sciences, the university of edinburgh, edinburgh eh16 4sb, uk\n4 dcu school of computing, dublin ", "the  journal \n\nof  neuroscience, \n\nfebruary \n\n1990, \n\n70(2): \n\n436-447 \n\nhead-direction \nmoving  rats. \n\ncells  recorded \n\nthe  postsubiculum \nii.  effects  of  environmental  manipulations \n\nfrom \n\nin  freely \n\njeffrey  s.  taube,  robert  u.  muller,  and  james  b.  ranck, \n\njr. \n\ndepartment \n\nof  physiology,  suny  health  sciences  center  at  brooklyn,  brooklyn,  new  york  11203 \n\ncharacteristics \n\nof  postsubicular \n\nhead-direc- \n\n(taube \n\nfiring \n\nthe  discharge \ntion  cells \nvious  pa", "i an update to this article is included at the end\n\nneuron\n\narticle\n\nneuromodulators control the polarity of\nspike-timing-dependent synaptic plasticity\n\ngeun hee seol,1,2,6 jokubas ziburkus,1,3,6 shiyong huang,1 lihua song,4 in tae kim,4 kogo takamiya,5\nrichard l. huganir,5 hey-kyoung lee,4 and alfredo kirkwood1,5,*\n1the mind/brain institute, johns hopkins university, baltimore, md 21218, usa\n2department of basic nursing science, korea university, seoul, korea\n3department of biology and biochemi", "article\n\nhttps://doi.org/10.1038/s41467-021-23103-1\n\nopen\n\nspectral bias and task-model alignment explain\ngeneralization in kernel regression and in\ufb01nitely\nwide neural networks\n\nabdulkadir canatar\n\n1,2, blake bordelon2,3 & cengiz pehlevan\n\n2,3\u2709\n\n;\n,\n:\n)\n(\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\na theoretical understanding of generalization remains an open problem for many machine\nlearning models, including deep networks where overparameterization leads to better per-\nformance, contradicting the conventional wisdom", "letter\n\ncommunicated by joshua b. tenenbaum\n\nlaplacian eigenmaps for dimensionality reduction and data\nrepresentation\n\nmikhail belkin\nmisha@math.uchicago.edu\ndepartment of mathematics, university of chicago, chicago, il 60637, u.s.a.\n\npartha niyogi\nniyogi@cs.uchicago.edu\ndepartment of computer science and statistics, university of chicago,\nchicago, il 60637 u.s.a.\n\none of the central problems in machine learning and pattern recognition\nis to develop appropriate representations for complex data. ", "a r t i c l e s\n\na category-free neural population supports evolving \ndemands during decision-making\ndavid raposo1\u20133, matthew t kaufman1,3 & anne k churchland1\nthe posterior parietal cortex (ppc) receives diverse inputs and is involved in a dizzying array of behaviors. these many behaviors \ncould rely on distinct categories of neurons specialized to represent particular variables or could rely on a single population of \nppc neurons that is leveraged in different ways. to distinguish these possib", "rsif.royalsocietypublishing.org\n\nresearch\n\ncite this article: friston k. 2013 life as we\nknow it. j r soc interface 10: 20130475.\nhttp://dx.doi.org/10.1098/rsif.2013.0475\n\nreceived: 27 may 2013\naccepted: 12 june 2013\n\nsubject areas:\nbiomathematics\n\nkeywords:\nautopoiesis, self-organization, active inference,\nfree energy, ergodicity, random attractor\n\nauthor for correspondence:\nkarl friston\ne-mail: k.friston@ucl.ac.uk\n\nlife as we know it\n\nkarl friston\n\nthe wellcome trust centre for neuroimaging, i", "machine learning, 37, 183\u2013233 (1999)\nc(cid:176) 1999 kluwer academic publishers. manufactured in the netherlands.\n\nan introduction to variational methods\nfor graphical models\n\nmichael i. jordan\ndepartment of electrical engineering and computer sciences and department of statistics,\nuniversity of california, berkeley, ca 94720, usa\n\njordan@cs.berkeley.edu\n\nzoubin ghahramani\ngatsby computational neuroscience unit, university college london wc1n 3ar, uk\n\nzoubin@gatsby.ucl.ac.uk\n\ntommi s. jaakkola\na", "adversarial score matching and improved\nsampling for image generation\n\nanonymous authors\npaper under double-blind review\n\nabstract\n\ndenoising score matching with annealed langevin sampling (dsm-als) has\nrecently found success in generative modeling. the approach works by \ufb01rst\ntraining a neural network to estimate the score of a distribution, and then using\nlangevin dynamics to sample from the data distribution assumed by the score\nnetwork. despite the convincing visual quality of samples, this m", "the forget-me-not process\n\nkieran milan\u2020, joel veness\u2020, james kirkpatrick, demis hassabis\n\ngoogle deepmind\n\n{kmilan,aixi,kirkpatrick,demishassabis}@google.com\n\nanna koop, michael bowling\n\nuniversity of alberta\n\n{anna,bowling}@cs.ualberta.ca\n\nabstract\n\nwe introduce the forget-me-not process, an ef\ufb01cient, non-parametric meta-\nalgorithm for online probabilistic sequence prediction for piecewise stationary,\nrepeating sources. our method works by taking a bayesian approach to partition-\ning a stream ", "the journal of neuroscience, may 15, 2001, 21(10):3646\u20133655\n\ncoding speci\ufb01city in cortical microcircuits: a multiple-electrode\nanalysis of primate prefrontal cortex\n\nchristos constantinidis, matthew n. franowicz, and patricia s. goldman-rakic\nsection of neurobiology, yale school of medicine, new haven, connecticut 06510\n\nneurons with directional speci\ufb01cities are active in the prefrontal\ncortex (pfc) during tasks that require spatial working memory.\nalthough the coordination of neuronal activity ", "letter\n\ncommunicated by michael shadlen\n\nthe effect of correlated variability on the accuracy of a\npopulation code\n\nl. f. abbott\nvolen center and department of biology, brandeis university, waltham, ma 02454-\n9110, u.s.a.\n\n\u2217\npeter dayan\ndepartment of brain and cognitive sciences, massachusetts institute of technology,\ncambridge, ma 02139, u.s.a.\n\nwe study the impact of correlated neuronal \ufb01ring rate variability on the\naccuracy with which an encoded quantity can be extracted from a pop-\nulation o", "journal of machine learning research 11 (2010) 625-660\n\nsubmitted 8/09; published 2/10\n\nwhy does unsupervised pre-training help deep learning?\n\ndumitru erhan\u2217\nyoshua bengio\naaron courville\npierre-antoine manzagol\npascal vincent\nd\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle\nuniversit\u00b4e de montr\u00b4eal\n2920, chemin de la tour\nmontr\u00b4eal, qu\u00b4ebec, h3t 1j8, canada\n\ndumitru.erhan@umontreal.ca\nyoshua.bengio@umontreal.ca\naaron.courville@umontreal.ca\npierre-antoine.manzagol@umontreal.ca\npascal", "neuron\n\narticle\n\nmodel-based in\ufb02uences on humans\u2019 choices\nand striatal prediction errors\n\nnathaniel d. daw,1,* samuel j. gershman,2 ben seymour,3 peter dayan,4 and raymond j. dolan3\n1center for neural science and department of psychology, new york university, new york, ny 10012, usa\n2department of psychology and neuroscience institute, princeton university, princeton, nj 08540, usa\n3wellcome trust centre for neuroimaging, institute of neurology, university college london, wc1n 3bg london, uk\n4ga", "0\n2\n0\n2\n\n \n\ng\nu\na\n3\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n4\n3\n9\n8\n0\n\n.\n\n3\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nnerf: representing scenes as\n\nneural radiance fields for view synthesis\n\nben mildenhall1(cid:63)\n\npratul p. srinivasan1(cid:63) matthew tancik1(cid:63)\n\njonathan t. barron2 ravi ramamoorthi3 ren ng1\n\n1uc berkeley\n\n2google research\n\n3uc san diego\n\nabstract. we present a method that achieves state-of-the-art results\nfor synthesizing novel views of complex scenes by optimizing an under-\nlying continuous volumetri", "journal of the american statistical association\n\nissn: 0162-1459 (print) 1537-274x (online) journal homepage: https://www.tandfonline.com/loi/uasa20\n\nvariational inference: a review for statisticians\n\ndavid m. blei, alp kucukelbir & jon d. mcauliffe\n\nto cite this article: david m. blei, alp kucukelbir & jon d. mcauliffe (2017) variational inference:\na review for statisticians, journal of the american statistical association, 112:518, 859-877,\ndoi: 10.1080/01621459.2017.1285773\n\nto link to this a", "6028 \u2022 the journal of neuroscience, july 4, 2018 \u2022 38(27):6028 \u2013 6044\n\nbehavioral/cognitive\n\ndeep neural networks for modeling visual perceptual\nlearning\n\nx li k. wenliang1 and aaron r. seitz2\n1gatsby computational neuroscience unit, university college london, london w1t 4jg, united kingdom and 2department of psychology, university of\ncalifornia\u2013riverside, riverside, california 92521\n\nunderstanding visual perceptual learning (vpl) has become increasingly more challenging as new phenomena are dis", "ne41ch19_josselyn\n\nari\n\n24 may 2018\n\n10:32\n\nannual review of neuroscience\nmemory allocation:\nmechanisms and function\nsheena a. josselyn1,2,3,4,5 and paul w. frankland1,2,3,4,6\n1department of psychology, university of toronto, ontario m5s 3g3, canada;\nemail: sheena.josselyn@sickkids.ca, paul.frankland@sickkids.ca\n2program in neurosciences & mental health, hospital for sick children, toronto,\nontario m5g 1x8, canada\n3department of physiology, university of toronto, ontario m5s 1a8, canada\n4institu", "neuroimage 93 (2014) 165\u2013175\n\ncontents lists available at sciencedirect\n\nneuroimage\n\nj o u r n a l h o m e p a ge : w ww . e l s e v i e r . c o m/ l o c a t e / y n i m g\n\nreview\ntrends and properties of human cerebral cortex:\ncorrelations with cortical myelin content\nmatthew f. glasser a,1, manu s. goyal b, todd m. preuss e,f,g, marcus e. raichle a,b,c,d, david c. van essen a,\u204e\na department of anatomy and neurobiology, washington university school of medicine, 660 s. euclid avenue, st. louis, ", "cl concentrations in the sajama ice core, and to\na number of other pedological and geomorpho-\nlogical features indicative of long-term dry cli-\nmates (8, 11\u201314, 18). this decline in human\nactivity around the altiplano paleolakes is seen\nin most caves, with early and late occupations\nseparated by largely sterile mid-holocene sed-\niments. however, a few sites, including the\ncaves of tulan-67 and tulan-68, show that\npeople did not completely disappear from the\narea. all of the sites of sporadic occ", "2\n2\n0\n2\n\n \n\np\ne\ns\n9\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n8\n8\n9\n4\n1\n\n.\n\n9\n0\n2\n2\n:\nv\ni\nx\nr\na\n\ndreamfusion: text-to-3d using 2d diffusion\n\nben poole1, ajay jain2, jonathan t. barron1, ben mildenhall1\n1google research, 2uc berkeley\n{pooleb, barron, bmild}@google.com, ajayj@berkeley.edu\n\nabstract\n\nrecent breakthroughs in text-to-image synthesis have been driven by diffusion\nmodels trained on billions of image-text pairs. adapting this approach to 3d synthe-\nsis would require large-scale datasets of labe", "biorxiv preprint \nthe copyright holder for this preprint\n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is \n\nhttps://doi.org/10.1101/2022.03.17.484712\n; \n\nthis version posted march 19, 2022. \n\ndoi: \n\nmade available under a\n\ncc-by 4.0 international license\n\n.\n\nthe combination of hebbian and predictive plasticity learns\n\ninvariant object representations in deep sensory networks\n\nmanu srinath halvagal1,2 and", "\f", "6\n1\n0\n2\n\n \n\np\ne\ns\n3\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n2\nv\n8\n5\n1\n5\n0\n\n.\n\n9\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nreal-time single image and video super-resolution using an ef\ufb01cient\n\nsub-pixel convolutional neural network\n\nwenzhe shi1, jose caballero1, ferenc husz\u00b4ar1, johannes totz1, andrew p. aitken1,\n\nrob bishop1, daniel rueckert1, zehan wang1\n\n1twitter\n\n1{wshi,jcaballero,fhuszar,jtotz,aitken,rbishop,zehanw}@twitter.com\n\nabstract\n\nrecently, several models based on deep neural networks\nhave achieved great success in t", "8\n1\n0\n2\n\n \n\ng\nu\na\n9\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n7\n5\n3\n3\n0\n\n.\n\n8\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nerror forward-propagation: reusing feedforward connec-\ntions to propagate errors in deep learning\n\nadam a. kohan\n\nuniversity of massachusetts amherst\n\nedward a. rietman\n\nuniversity of massachusetts amherst\n\nhava t. siegelmann\n\nuniversity of massachusetts amherst\n\nakohan@cs.umass.edu\n\nerietman@umass.edu\n\nhava@cs.umass.edu\n\nabstract\n\nwe introduce error forward-propagation, a biologically plausible mechanism to pr", "structure discovery in nonparametric regression through\n\ncompositional kernel search\n\ndavid duvenaud\u2217\u2020\njames robert lloyd\u2217\u2020\nroger grosse\u2021\njoshua b. tenenbaum\u2021\nzoubin ghahramani\u2020\n\nabstract\n\ndespite its importance, choosing the struc-\ntural form of the kernel\nin nonparametric\nregression remains a black art. we de\ufb01ne\na space of kernel structures which are built\ncompositionally by adding and multiplying a\nsmall number of base kernels. we present a\nmethod for searching over this space of struc-\ntures", "2\n2\n0\n2\n\n \n\nb\ne\nf\n7\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n7\n8\n6\n0\n0\n\n.\n\n4\n0\n9\n1\n:\nv\ni\nx\nr\na\n\non the power and limitations of random features\n\nfor understanding neural networks\n\nohad shamir\ngilad yehudai\nweizmann institute of science\n\n{gilad.yehudai,ohad.shamir}@weizmann.ac.il\n\nabstract\n\nrecently, a spate of papers have provided positive theoretical results for training over-parameterized\nneural networks (where the network size is larger than what is needed to achieve low error). the key\ninsight is t", "dimensionality reduction of calcium-imaged \nneuronal population activity\n\nhttps://doi.org/10.1038/s43588-022-00390-2\n\nreceived: 10 march 2022\n\naccepted: 5 december 2022\n\npublished online: 29 december 2022\n\n check for updates\n\ntze hui koh1,2, william e. bishop2,3,4, takashi kawashima4,5, brian b. jeon1,2, \nranjani srinivasan1,6, yu mu\u2009\nmisha b. ahrens\u2009\n\n \u20097, ziqiang wei4, sandra j. kuhlman\u2009\n\n, steven m. chase\u2009\n\n & byron m. yu\u2009\n\n \u20098,9, \n \u20091,8,10,11 \n\n \u20091,8,11 \n\n \u20094 \n\ncalcium imaging has been widel", "the heavily connected brain\n\nreview summary\n\ncortical high-density counterstream \narchitectures\n\nnikola t. markov, m\u00e1ria ercsey-ravasz, david c. van essen, kenneth knoblauch, \nzolt\u00e1n toroczkai,* henry kennedy*\n\nread the full article online\nhttp://dx.doi.org/10.1126/science.1238406\n\ncite this article as n. t. markov et al., \nscience 342, 1238406 (2013). \ndoi: 10.1126/science.1238406\n\nbackground: the cerebral cortex is divisible into many individual areas, each exhibiting distinct \nconnectivity pr", "the role of acetylcholine in learning and memory\nmichael e hasselmo\n\npharmacological data clearly indicate that both muscarinic and\nnicotinic acetylcholine receptors have a role in the encoding of\nnew memories. localized lesions and antagonist infusions\ndemonstrate the anatomical locus of these cholinergic effects,\nand computational modeling links the function of cholinergic\nmodulation to speci\ufb01c cellular effects within these regions.\nacetylcholine has been shown to increase the strength of\naffe", "deep boltzmann machines\n\nruslan salakhutdinov\n\ndepartment of computer science\n\nuniversity of toronto\n\nrsalakhu@cs.toronto.edu\n\ngeoffrey hinton\n\ndepartment of computer science\n\nuniversity of toronto\nhinton@cs.toronto.edu\n\nabstract\n\nwe present a new learning algorithm for boltz-\nmann machines that contain many layers of hid-\nden variables. data-dependent expectations are\nestimated using a variational approximation that\ntends to focus on a single mode, and data-\nindependent expectations are approxi", "pattern recognition 46 (2013) 2175\u20132186\n\ncontents lists available at sciverse sciencedirect\n\npattern recognition\n\njournal homepage: www.elsevier.com/locate/pr\n\npassage method for nonlinear dimensionality reduction of data\non multi-cluster manifolds\n\ndeyu meng a,n, yee leung b, zongben xu a\na institute for information and system sciences and ministry of education key lab for intelligent networks and network security, xi\u2019an jiaotong university, xi\u2019an 710049, pr china\nb department of geography and ", "t i m e l i n e\n\nfrom the neuron doctrine to  \nneural networks\n\nbe a useful paradigm, or act as guideposts, to \nunderstand many brain computations. this \narticle does not provide an exhaustive review \nbut instead illustrates with a small number \nof examples the transition between these two \nparadigms of neuroscience.\n\nrafael yuste\n\nabstract | for over a century, the neuron doctrine \u2014 which states that the neuron \nis the structural and functional unit of the nervous system \u2014 has provided a \nconce", "neuron, vol. 46, 681\u2013692, may 19, 2005, copyright \u00a92005 by elsevier inc. doi 10.1016/j.neuron.2005.04.026\n\nuncertainty, neuromodulation, and attention\n\nangela j. yu* and peter dayan\ngatsby computational neuroscience unit\n17 queen square\nlondon wc1n 3ar\nunited kingdom\n\nsummary\n\nuncertainty in various forms plagues our interactions\nwith the environment. in a bayesian statistical frame-\nwork, optimal inference and prediction, based on un-\nreliable observations in changing contexts, require\nthe repr", "8\n1\n0\n2\n\n \nr\np\na\n0\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n3\n9\n8\n6\n0\n\n.\n\n4\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nastudyonoverfittingindeepreinforcementlearningchiyuanzhangchiyuan@google.comoriolvinyalsvinyals@google.comremimunosmunos@google.comsamybengiobengio@google.comabstractrecentyearshavewitnessedsignificantprogressesindeepreinforcementlearning(rl).empoweredwithlargescaleneuralnetworks,carefullydesignedarchitectures,noveltrainingalgorithmsandmassivelyparallelcomputingdevices,researchersareabletoattackmanychallengin", "training deep spiking neural networks using\n\nbackpropagation\n\njun haeng lee\u2217\u2020, tobi delbruck\u2020, michael pfeiffer\u2020\n\n\u2217samsung advanced institute of technology, samsung electronics\n\u2020institute of neuroinformatics, university of zurich and eth zurich\n\njunhaeng2.lee@samsung.com\n\n{tobi, pfeiffer}@ini.uzh.ch\n\n6\n1\n0\n2\n\n \n\ng\nu\na\n1\n3\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n1\nv\n2\n8\n7\n8\n0\n\n.\n\n8\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\ndeep spiking neural networks (snns) hold great potential for improving the latency and energy\nef\ufb01cie", "nmda receptor-dependent long-term\npotentiation and long-term depression\n(ltp/ltd)\n\nchristian lu\u00a8 scher1 and robert c. malenka2\n\n1department of basic neurosciences and clinic of neurology, university of geneva and geneva university\nhospital, 1211 geneva, switzerland\n2nancy pritzker laboratory, department of psychiatry and behavioral sciences, stanford university school of\nmedicine, palo alto, california 94305-5453\n\ncorrespondence: christian.luscher@unige.ch and malenka@stanford.edu\n\nlong-term pot", "neuron\n\nperspective\n\nhow does the brain solve\nvisual object recognition?\n\njames j. dicarlo,1,* davide zoccolan,2 and nicole c. rust3\n1department of brain and cognitive sciences and mcgovern institute for brain research, massachusetts institute of technology,\ncambridge, ma 02139, usa\n2cognitive neuroscience and neurobiology sectors, international school for advanced studies (sissa), trieste, 34136, italy\n3department of psychology, university of pennsylvania, philadelphia, pa 19104, usa\n*correspon", "0\n2\n0\n2\n\n \nr\na\n\nm\n3\n\n \n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n4\nv\n4\n6\n1\n2\n0\n\n.\n\n2\n1\n9\n1\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2020\n\nplug and play language models: a simple\napproach to controlled text generation\n\nsumanth dathathri \u2217\u2217\ncms, caltech\n\nandrea madotto \u2217\nhkust\n\njanice lan\nuber ai\n\neric frank\nuber ai\n\npiero molino\nuber ai\n\njason yosinski \u2020\u2020\nuber ai\n\njane hung\nuber ai\n\nrosanne liu \u2020\nuber ai\n\ndathathris@gmail.com, amadotto@connect.ust.hk\n{janlan, jane.hung, mysterefrank, piero, yosinski, r", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nbrain\u2013computer  interfaces  for  dissecting  cognitive\nprocesses  underlying  sensorimotor  control\nmatthew  d  golub1,3,  steven  m  chase2,3,  aaron  p  batista3,4,5\nand  byron  m  yu1,2,3\n\nabstract\n\nsensorimotor  control  engages  cognitive  processes  such  as\nprediction,  learning,  and  multisensory  integration.\nunderstanding  the  neural  mechanisms  underlying  these\ncognitive  processes  with  arm  reaching  is  challenging  ", "letter\nfully integrated silicon probes for high-density \nrecording of neural activity\n\ndoi:10.1038/nature24636\n\njames j. jun1*, nicholas a. steinmetz2,3,4*, joshua h. siegle5*, daniel j. denman5*, marius bauza6,7*, brian barbarits1*, \nalbert k. lee1*, costas a. anastassiou5,8, alexandru andrei9, \u00e7a\u011fatay ayd\u0131n10,11, mladen barbic1, timothy j. blanche5,12, \nvincent bonin9,10,11,13, jo\u00e3o couto10,11, barundeb dutta9, sergey l. gratiy5, diego a. gutnisky1, michael h\u00e4usser3,14, bill karsh1, \npeter led", "communication dynamics in complex \nbrain networks\n\nandrea avena-koenigsberger1, bratislav misic2 and olaf sporns1,3\nabstract | neuronal signalling and communication underpin virtually all aspects of brain activity \nand function. network science approaches to modelling and analysing the dynamics of \ncommunication on networks have proved useful for simulating functional brain connectivity and \npredicting emergent network states. this review surveys important aspects of communication \ndynamics in b", "a r t i c l e s\n\ncardinal rules: visual orientation perception reflects \nknowledge of environmental statistics\nahna r girshick1,2, michael s landy1,2 & eero p simoncelli1\u20134\nhumans are good at performing visual tasks, but experimental measurements have revealed substantial biases in the perception \nof basic visual attributes. an appealing hypothesis is that these biases arise through a process of statistical inference, in which \ninformation from noisy measurements is fused with a probabilistic mo", "pruning of memories by context-based prediction error\n\nghootae kima,b, jarrod a. lewis-peacockc,d, kenneth a. normana,b, and nicholas b. turk-brownea,b,1\n\nadepartment of psychology and bprinceton neuroscience institute, princeton university, princeton, nj 08544; and cdepartment of psychology and dinstitute\nfor neuroscience, university of texas at austin, austin, tx 78712\n\nedited by daniel l. schacter, harvard university, cambridge, ma, and approved may 8, 2014 (received for review october 16, 20", "gradient descent converges to minimizers\n\njason d. lee\u266f\n\n, max simchowitz\u266f, michael i. jordan\u266f\u2020, and benjamin recht\u266f\u2020\n\n\u266fdepartment of electrical engineering and computer sciences\n\n6\n1\n0\n2\n\n \nr\na\n\nm\n4\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n5\n1\n9\n4\n0\n\n.\n\n2\n0\n6\n1\n:\nv\ni\nx\nr\na\n\n\u2020department of statistcs\n\nuniversity of california, berkeley\n\nmarch 7, 2016\n\nabstract\n\nwe show that gradient descent converges to a local minimizer, almost surely with random initializa-\n\ntion. this is proved by applying the stab", "0\n0\n0\n2\n\n \nr\np\na\n4\n2\n\n \n\n \n \n]\nn\na\n-\na\nt\na\nd\n\n.\ns\nc\ni\ns\ny\nh\np\n[\n \n \n\n1\nv\n7\n5\n0\n4\n0\n0\n0\n/\ns\nc\ni\ns\ny\nh\np\n:\nv\ni\nx\nr\na\n\nthe information bottleneck method\n\nnaftali tishby,1,2 fernando c. pereira,3 and william bialek1\n\n1nec research institute, 4 independence way\nprinceton, new jersey 08540\n2institute for computer science, and\ncenter for neural computation\nhebrew university\njerusalem 91904, israel\n3at&t shannon laboratory\n180 park avenue\nflorham park, new jersey 07932\n\n30 september 1999\n\nwe de\ufb01ne the r", "survey of spiking in the mouse visual system \nreveals functional hierarchy\n\nhttps://doi.org/10.1038/s41586-020-03171-x\nreceived: 23 december 2019\naccepted: 9 december 2020\npublished online: 20 january 2021\n\n check for updates\n\njoshua h. siegle1,6\u2009\u2709, xiaoxuan jia1,6\u2009\u2709, s\u00e9verine durand1, sam gale1, corbett bennett1, \nnile graddis1, greggory heller1, tamina k. ramirez1, hannah choi1,2, jennifer a. luviano1, \npeter a. groblewski1, ruweida ahmed1, anton arkhipov1, amy bernard1, yazan n. billeh1, \ndil", "research\n\nthe graphical brain: belief propagation\n\nand active inference\n\nkarl j. friston1, thomas parr1, and bert de vries2,3\n\n1wellcome trust centre for neuroimaging, institute of neurology, university college london, united kingdom\n\n2eindhoven university of technology, department of electrical engineering, eindhoven, the netherlands\n\n3gn hearing, eindhoven, the netherlands\n\nkeywords: bayesian, neuronal, connectivity, factor graphs, free energy, belief propagation,\nmessage passing\n\na n o p e n ", "scalarized multi-objective reinforcement learning:\n\nnovel design techniques\n\nkristof van moffaert\n\nmadalina m. drugan\n\nann now\u00b4e\n\ndepartment of computer science\n\ndepartment of computer science\n\ndepartment of computer science\n\nvrije universiteit brussel\n\nvrije universiteit brussel\n\nvrije universiteit brussel\n\npleinlaan 2, 1050 brussels, belgium\n\npleinlaan 2, 1050 brussels, belgium\n\npleinlaan 2, 1050 brussels, belgium\n\nemail: kvmoffae@vub.ac.be\n\nemail: mdrugan@vub.ac.be\n\nemail: anowe@vub.ac.be\n\nab", "decoding cognition from spontaneous \nneural activity\n\n 1,3,4,7\n\n 3,5, timothy\u00a0e.\u00a0j.\u00a0behrens4,6 \n\n 1,2,3\u2009\u2709, matthew\u00a0m.\u00a0nour3,4, nicolas\u00a0w.\u00a0schuck \n\nyunzhe\u00a0liu \nand raymond\u00a0j.\u00a0dolan \nabstract | in human neuroscience, studies of cognition are rarely grounded in non- task- evoked, \n\u2018spontaneous\u2019 neural activity. indeed, studies of spontaneous activity tend to focus predominantly \non intrinsic neural patterns (for example, resting- state networks). taking a \u2018representation- rich\u2019 \napproach bridges th", "2\n2\n0\n2\n\n \n\ny\na\nm\n0\n1\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n8\n9\n1\n5\n0\n\n.\n\n5\n0\n2\n2\n:\nv\ni\nx\nr\na\n\nreducing activation recomputation\n\nin large transformer models\n\nvijay korthikanti, jared casper, sangkug lym, lawrence mcafee, michael andersch,\n\nmohammad shoeybi, and bryan catanzaro\n\nnvidia\n\nabstract\n\ntraining large transformer models is one of the most important computational challenges of\nmodern ai. in this paper, we show how to signi\ufb01cantly accelerate training of large transformer\nmodels by reducing act", "8\n1\n0\n2\n\n \n\nn\na\nj\n \n\n0\n3\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n5\n7\n4\n8\n0\n\n.\n\n3\n0\n7\n1\n:\nv\ni\nx\nr\na\n\novercoming catastrophic forgetting by\n\nincremental moment matching\n\nsang-woo lee1, jin-hwa kim1, jaehyun jun1, jung-woo ha2, and byoung-tak zhang1,3\n\nseoul national university1\n\nclova ai research, naver corp2\n\nsurromind robotics3\n\n{slee,jhkim,jhjun}@bi.snu.ac.kr jungwoo.ha@navercorp.com\n\nbtzhang@bi.snu.ac.kr\n\nabstract\n\ncatastrophic forgetting is a problem of neural networks that loses the information\nof the ", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/40042593\n\nresponse acquisition by humans with delayed reinforcement\n\narticle\u00a0\u00a0in\u00a0\u00a0journal of the experimental analysis of behavior \u00b7 may 2009\n\ndoi: 10.1901/jeab.2009.91-377\u00a0\u00b7\u00a0source: pubmed\n\nreads\n71\n\ncitations\n20\n\n1 author:\n\nhiroto okouchi\nosaka kyoiku university\n\n51 publications\u00a0\u00a0\u00a0359 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nall content following this page was uploaded by hiroto okouchi on 30 octobe", "psychological review\n1990, vot97, no. 3, 332-361\n\ncopyright 1990 by the american psychological association, inc.\n0033-295x/90/m0.75\n\non the control of automatic processes: a parallel  distributed\n\nprocessing account of the stroop  effect\n\njonathan d. cohen\n\ncarnegie mellon university\n\nuniversity of pittsburgh\n\nstanford university\n\nkevin dunbar\nmcgill university\n\njames l. mcclelland\ncarnegie mellon university\n\ntraditional views of automatic!ty are in need of revision. for example, automaticity of", "dueling network architectures for deep reinforcement learning\n\nziyu wang\ntom schaul\nmatteo hessel\nhado van hasselt\nmarc lanctot\nnando de freitas\ngoogle deepmind, london, uk\n\nziyu@google.com\nschaul@google.com\nmtthss@google.com\nhado@google.com\nlanctot@google.com\nnandodefreitas@google.com\n\nabstract\n\nin recent years there have been many successes\nof using deep representations in reinforcement\nlearning. still, many of these applications use\nconventional architectures, such as convolutional\nnetworks, ", "9\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n1\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n8\n8\n6\n4\n0\n\n.\n\n6\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nan improved analysis of training over-parameterized\n\ndeep neural networks\n\ndifan zou\u2217 and quanquan gu\u2020\n\nabstract\n\na recent line of research has shown that gradient-based algorithms with random initialization\ncan converge to the global minima of the training loss for over-parameterized (i.e., su\ufb03ciently\nwide) deep neural networks. however, the condition on the width of the neural network to\nensure the global ", "biorxiv preprint \n(which was not certified by peer review) is the author/funder, who has granted biorxiv a license to display the preprint in perpetuity. it is made \n\nhttps://doi.org/10.1101/2022.12.07.519455\n; \n\nthis version posted december 7, 2022. \n\nthe copyright holder for this preprint\n\ndoi: \n\navailable under a\n\ncc-by 4.0 international license\n.\n\n 1  mega-scale movie-fields in the mouse visuo-hippocampal network chinmay s. purandare1-3\u2020\u2709 and mayank r. mehta2-4\u2709  1 department of bioengineeri", "available  online  at  www.sciencedirect.com\n\nsciencedirect\n\nneural  circuits  as  computational  dynamical  systems\ndavid  sussillo\n\nmany  recent  studies  of  neurons  recorded  from  cortex  reveal\ncomplex  temporal  dynamics.  how  such  dynamics  embody  the\ncomputations  that  ultimately  lead  to  behavior  remains  a\nmystery.  approaching  this  issue  requires  developing  plausible\nhypotheses  couched  in  terms  of  neural  dynamics.  a  tool  ideally\nsuited  to  aid  in  this  questi", "neural tangent kernel:\n\nconvergence and generalization in neural networks\n\narthur jacot\n\n\u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\narthur.jacot@netopera.net\n\nimperial college london and \u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\nfranck gabriel\n\nfranckrgabriel@gmail.com\n\ncl\u00b4ement hongler\n\n\u00b4ecole polytechnique f\u00b4ed\u00b4erale de lausanne\n\nclement.hongler@gmail.com\n\nabstract\n\nat initialization, arti\ufb01cial neural networks (anns) are equivalent to gaussian\nprocesses in the in\ufb01nite-width limit (12; 9),", "report\n\nreal-time readout of large-scale unsorted neural\nensemble place codes\n\ngraphical abstract\n\nauthors\n\nsile hu, davide ciliberti,\nandres d. grosmark, ...,\nmatthew a. wilson, fabian kloosterman,\nzhe chen\n\ncorrespondence\nfabian.kloosterman@nerf.be (f.k.),\nzhe.chen@nyulangone.org (z.c.)\n\nin brief\nthe hippocampal and neocortical\nneuronal ensembles encode rich spatial\ninformation in navigation. hu et al.\ndevelop computational techniques that\naccommodate real-time decoding and\nassessment of large", "9\n1\n0\n2\n\n \nl\nu\nj\n \n\n5\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n9\n4\n6\n2\n0\n\n.\n\n7\n0\n9\n1\n:\nv\ni\nx\nr\na\n\njournal of machine learning research vv (20yy) pp\n\nsubmitted 06/2019; published mm/20yy\n\na uni\ufb01ed framework of online learning algorithms for\n\ntraining recurrent neural networks\n\nowen marschall\ncenter for neural science\nnew york university\nnew york, ny 10006, usa\n\nkyunghyun cho\nnew york university\nfacebook ai research\ncifar azrieli global scholar\n\ncristina savin\ncenter for neural science\ncenter for data science", "usa\n\n(cid:88)\n\nj\n\nsupplementary materials: cell-type-speci\ufb01c neuromodulation\nguides synaptic credit assignment in a spiking neural network\n\nyuhan helena liu1,2,3,*, stephen smith2,4, stefan mihalas1,2,3, eric shea-brown1,2,3, and\n\nuygar s\u00fcmb\u00fcl2,*\n\n1department of applied mathematics, university of washington, seattle, wa, usa\n\n2allen institute, 615 westlake ave n, seattle wa, usa\n\n3computational neuroscience center, university of washington, seattle, wa, usa\n4department of molecular and cellular ", "published as a conference paper at iclr 2020\n\nunbiased contrastive divergence algorithm\nfor training energy-based latent variable\nmodels\n\nyixuan qiu\ndepartment of statistics and data science\ncarnegie mellon university\npittsburgh, pa 15213, usa\nyixuanq@andrew.cmu.edu\n\nlingsong zhang & xiao wang\ndepartment of statistics\npurdue university\nwest lafayette, in 47907, usa\n{lingsong, wangxiao}@purdue.edu\n\nabstract\n\nthe contrastive divergence algorithm is a popular approach to training energy-\nbased late", "beyond backprop: online alternating minimization with auxiliary variables\n\n9\n1\n0\n2\n\n \n\nn\nu\nj\n \n\n5\n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n4\nv\n7\n7\n0\n9\n0\n\n.\n\n6\n0\n8\n1\n:\nv\ni\nx\nr\na\n\nanna choromanska* 1 benjamin cowen* 1 sadhana kumaravel* 2 ronny luss* 2 mattia rigotti* 2\n\nirina rish* 2 brian kingsbury 2 paolo diachille 2 viatcheslav gurev 2 ravi tejwani 3 djallel bouneffouf 2\n\nabstract\n\ndespite signi\ufb01cant recent advances in deep neural\nnetworks, training them remains a challenge due\nto the highly non-convex na", "research article\n\nsomato-dendritic synaptic plasticity and\nerror-backpropagation in active dendrites\nmathieu schiess1*, robert urbanczik1*, walter senn1,2*\n\n1 department of physiology, university of bern, bern, switzerland, 2 center for cognition, learning and\nmemory, university of bern, bern, switzerland\n\n* mathieu.schiess@hesge.ch (ms); urbanczik@pyl.unibe.ch (ru); senn@pyl.unibe.ch (ws)\n\na11111\n\nabstract\n\nin the last decade dendrites of cortical neurons have been shown to nonlinearly combine\n", "a transcriptomic axis predicts state \nmodulation of cortical interneurons\n\nhttps://doi.org/10.1038/s41586-022-04915-7\nreceived: 26 october 2021\naccepted: 27 may 2022\npublished online: 6 july 2022\nopen access\n\n check for updates\n\nst\u00e9phane bugeon1\u2009\u2709, joshua duffield1, mario dipoppa1,2, anne ritoux1, isabelle prankerd1, \ndimitris nicoloutsopoulos1, david orme1, maxwell shinn1, han peng3, hamish forrest1, \naiste viduolyte1, charu bai reddy1,4, yoh isogai5, matteo carandini4 & kenneth d. harris1\u2009\u2709\n\nt", "7\n1\n0\n2\n\n \nl\nu\nj\n \n\n5\n2\n\n \n \n]\nl\nc\n.\ns\nc\n[\n \n \n\n3\nv\n2\n2\n1\n3\n0\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\nconvolutionalsequencetosequencelearningjonasgehringmichaelaulidavidgrangierdenisyaratsyannn.dauphinfacebookairesearchabstracttheprevalentapproachtosequencetosequencelearningmapsaninputsequencetoavariablelengthoutputsequenceviarecurrentneuralnet-works.weintroduceanarchitecturebaseden-tirelyonconvolutionalneuralnetworks.1com-paredtorecurrentmodels,computationsoverallelementscanbefullyparallelizedduringtrainingto", "journal of vision (2008) 8(8):11, 1\u201318\n\nhttp://journalofvision.org/8/8/11/\n\n1\n\ntopological analysis of population activity\nin visual cortex\n\ngurjeet singh\n\nfacundo memoli\n\ntigran ishkhanov\n\nguillermo sapiro\n\ngunnar carlsson\n\ndario l. ringach\n\ninstitute for computational and mathematical engineering,\nstanford university, stanford, ca, usa\n\ndepartment of mathematics, stanford university,\nstanford, ca, usa\n\ndepartment of mathematics, stanford university,\nstanford, ca, usa\n\ndepartment of electrical ", "9\n1\n0\n2\n\n \nl\nu\nj\n \n\n5\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n1\nv\n4\n7\n3\n6\n0\n\n.\n\n7\n0\n9\n1\n:\nv\ni\nx\nr\na\n\nwhat does it mean to understand a neural network?\n\ntimothy p. lillicrap & konrad p. kording\n\njuly 2019\n\nabstract\n\nwe can de\ufb01ne a neural network that can learn to recognize objects in less than 100 lines\nof code. however, after training, it is characterized by millions of weights that contain the\nknowledge about many object types across visual scenes. such networks are thus dramatically\neasier to understand in", "7\n1\n0\n2\n\n \n\ng\nu\na\n6\n1\n\n \n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n2\nv\n5\n6\n1\n8\n0\n\n.\n\n6\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nsupervised learning based on temporal coding in\n\nspiking neural networks\n\ndepartment of bioengineering, jacobs school of engineering\n\nhesham mostafa\n\ninstitute of neural computation\n\nuc san diego, la jolla, ca 92093 usa\n\nemail: hmmostafa@ucsd.edu\n\nabstract\n\ngradient descent training techniques are remarkably successful in training analog-\nvalued arti\ufb01cial neural networks (anns). such training techniques, howe", "\u2029\n\ninteractions\u2029between\u2029intrinsic\u2029and\u2029stimulus\u00adevoked\u2029\u2029\n\nactivity\u2029in\u2029recurrent\u2029neural\u2029networks\u2029\n\ndepartment\u2029of\u2029physiology\u2029and\u2029cellular\u2029biophysics\u2029\ncolumbia\u2029university\u2029college\u2029of\u2029physicians\u2029and\u2029surgeons\u2029\n\n\u2029\nl.f.\u2029abbott\u2029and\u2029kanaka\u2029rajan\u2029\n\u2029\ndepartment\u2029of\u2029neuroscience\u2029\nnew\u2029york,\u2029ny\u202910032\u20102695\u2029usa\u2029\n\u2029and\u2029\n\u2029\nhaim\u2029sompolinsky\u2029\n\u2029\nracah\u2029institute\u2029of\u2029physics\u2029\nhebrew\u2029university\u2029\njerusalem,\u2029israel\u2029\n\ninterdisciplinary\u2029center\u2029for\u2029neural\u2029computation\u2029\n\nintroduction\u2029\n\n\u2029\u2029\n\u2029trial\u2010to\u2010trial\u2029variability\u2029is\u2029an\u2029essentia", "research | reports\n\nand the bosonic field, v is the quartic coupling\namong the bosonic fields, l is the linear coupling\nbetween the bosonic field and the orthorhombic\nlattice distortion d, and q is the momentum trans-\nfer within one brillouin zone. minimizing the ac-\ntion with respect to d, we arrive at d \u00bc l dh i=cs,\nwhere cs is the shear modulus. in other words,\nthe orthorhombic lattice distortion is proportional\nto the nematic order parameter dh i; and both are\nexpected to develop nonzero exp", "towards biologically plausible\n\nconvolutional networks\n\nroman pogodin\ngatsby unit, ucl\n\nroman.pogodin.17@ucl.ac.uk\n\nyash mehta\n\ngatsby unit, ucl\n\ny.mehta@ucl.ac.uk\n\n2\n2\n0\n2\n\n \n\nn\na\nj\n \n\n5\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n1\n3\n0\n3\n1\n\n.\n\n6\n0\n1\n2\n:\nv\ni\nx\nr\na\n\ntimothy p. lillicrap\n\ndeepmind; complex, ucl\n\ncountzero@google.com\n\npeter e. latham\ngatsby unit, ucl\n\npel@gatsby.ucl.ac.uk\n\nabstract\n\nconvolutional networks are ubiquitous in deep learning. they are particularly\nuseful for images, as they reduce ", "6\n1\n0\n2\n\n \n\ny\na\nm\n1\n3\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n7\n5\n0\n6\n0\n\n.\n\n4\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nhierarchical deep reinforcement learning:\n\nintegrating temporal abstraction and\n\nintrinsic motivation\n\ntejas d. kulkarni\u2217\n\nkarthik r. narasimhan\u2217\n\nbcs, mit\n\ntejask@mit.edu\n\ncsail, mit\n\nkarthikn@mit.edu\n\nardavan saeedi\n\ncsail, mit\n\nardavans@mit.edu\n\njoshua b. tenenbaum\n\nbcs, mit\n\njbt@mit.edu\n\nabstract\n\nlearning goal-directed behavior in environments with sparse feedback is\na major challenge for reinforcement ", "appendix for credit assignment through\n\nbroadcasting a global error vector\n\ndavid g. clark, l.f. abbott, sueyeon chung\n\ncontents\na supplementary \ufb01gures\nb formulation of vnns using vector input units\nc assumption in gevb sign match proof\nd alternative derivation of gevb\ne gradient alignment angle and relative standard deviation\nf concentration of relative standard deviation in wide networks\ng architectures\nh global error-vector broadcasting in convolutional networks\ni training\nj direct feedback a", "7476 \u2022 the journal of neuroscience, july 11, 2007 \u2022 27(28):7476 \u20137481\n\nbehavioral/systems/cognitive\n\ninduction of long-term memory by exposure to novelty\nrequires protein synthesis: evidence for a behavioral\ntagging\n\ndiego moncada1 and hayde\u00b4e viola1,2\n1instituto de biolog\u0131\u00b4a celular y neurociencias, facultad de medicina, and 2departamento de fisiolog\u0131\u00b4a, biolog\u0131\u00b4a molecular y celular, facultad de ciencias\nexactas y naturales, universidad de buenos aires, 1121 buenos aires, argentina\n\na behavior", "self-training with noisy student improves imagenet classi\ufb01cation\n\nqizhe xie\u2217 1, minh-thang luong1, eduard hovy2, quoc v. le1\n1google research, brain team, 2carnegie mellon university\n\n{qizhex, thangluong, qvl}@google.com, hovy@cmu.edu\n\n0\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n9\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n2\n5\n2\n4\n0\n\n.\n\n1\n1\n9\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\nwe present noisy student training, a semi-supervised\nlearning approach that works well even when labeled data\nis abundant. noisy student training achieves 88.4% top-", "review\n\nrepresentational  geometry:\nintegrating  cognition,  computation,\nand  the  brain\nnikolaus  kriegeskorte1 and  rogier  a.  kievit1,2\n\n1 medical  research  council,  cognition  and  brain  sciences  unit,  cambridge,  uk\n2 department  of  psychological  methods,  university  of  amsterdam,  amsterdam,  the  netherlands\n\nthe  cognitive  concept  of  representation  plays  a  key  role\nin  theories  of  brain  information  processing.  however,\nlinking  neuronal  activity  to  representatio", "1\n2\n0\n2\n\n \n\nb\ne\nf\n5\n2\n\n \n\n \n \n]\n\nv\nc\n.\ns\nc\n[\n \n \n\n1\nv\n7\n2\n6\n2\n1\n\n.\n\n2\n0\n1\n2\n:\nv\ni\nx\nr\na\n\nhow to represent part-whole hierarchies\n\nin a neural network\n\ngeo\ufb00rey hinton\ngoogle research\n\n&\n\nthe vector institute\n\n&\n\ndepartment of computer science\n\nuniversity of toronto\n\nfebruary 22, 2021\n\nabstract\n\nthis paper does not describe a working system. instead, it presents a\nsingle idea about representation which allows advances made by several\ndi\ufb00erent groups to be combined into an imaginary system called g", "physiological reviews\nvol. 81, no. 3, july 2001\n\nprinted in u.s.a.\n\ncerebellar long-term depression: characterization,\n\nsignal transduction, and functional roles\n\nmasao ito\n\nbrain science institute, riken, wako, saitama, japan\n\ni. introduction (historical background)\n\na. dissection of neuronal network in 1960s\nb. exploration of synaptic plasticity in 1970s to 1980s\nc. system approach in 1970s to 1980s\nd. discovery of signal transduction and cognitive function in 1990s\n\nii. characterization of ce", "gradient-based learning applied\nto document recognition\n\nyann lecun, member, ieee, l \u00b4eon bottou, yoshua bengio, and patrick haffner\n\ninvited paper\n\nmultilayer neural networks trained with the back-propagation\nalgorithm constitute the best example of a successful gradient-\nbased learning technique. given an appropriate network\narchitecture, gradient-based learning algorithms can be used\nto synthesize a complex decision surface that can classify\nhigh-dimensional patterns, such as handwritten char", "how does batch normalization help optimization?\n\nshibani santurkar\u2217\n\ndimitris tsipras\u2217\n\nandrew ilyas\u2217\n\naleksander m \u02dbadry\n\nmit\n\nmit\n\nmit\n\nmit\n\nshibani@mit.edu\n\ntsipras@mit.edu\n\nailyas@mit.edu\n\nmadry@mit.edu\n\nabstract\n\nbatch normalization (batchnorm) is a widely adopted technique that enables\nfaster and more stable training of deep neural networks (dnns). despite its\npervasiveness, the exact reasons for batchnorm\u2019s effectiveness are still poorly\nunderstood. the popular belief is that this effecti", "a biologically plausible learning rule for\n\ndeep learning in the brain\n\n9\n1\n0\n2\n\n \nl\nu\nj\n \n\n2\n\n \n \n]\ne\nn\n.\ns\nc\n[\n \n \n\n3\nv\n8\n6\n7\n1\n0\n\n.\n\n1\n1\n8\n1\n:\nv\ni\nx\nr\na\n\nisabella pozzi\n\nvision & cognition group\n\nnetherlands institute for neuroscience\n\namsterdam, the netherlands\n\ni.pozzi@nin.knaw.nl\n\nsander m. boht\u00e9\n\nmachine learning group\n\ncentrum wiskunde & informatica\n\namsterdam, the netherlands\n\ns.m.bohte@cwi.nl\n\npieter r. roelfsema\n\nvision & cognition group\n\nnetherlands institute for neuroscience\n\namster", "b\ni\no\nl\n.\n \nc\ny\nb\ne\nr\nn\ne\nt\ni\nc\ns\n \n2\n7\n,\n \n7\n7\n \n8\n7\n \n(\n1\n9\n7\n7\n)\n \nb\ni\no\nl\no\ng\ni\nc\na\nl\n \nc\ny\nb\ne\nr\nn\ne\nt\ni\nc\ns\n \n(cid:14)\n9\n \nb\ny\n \ns\np\nr\ni\nn\ng\ne\nr\n-\nv\ne\nr\nl\na\ng\n \n1\n9\n7\n7\n \nd\ny\nn\na\nm\ni\nc\ns\n \no\nf\n \np\na\nt\nt\ne\nr\nn\n \nf\no\nr\nm\na\nt\ni\no\nn\n \ni\nn\n \nl\na\nt\ne\nr\na\nl\n-\ni\nn\nh\ni\nb\ni\nt\ni\no\nn\n \nt\ny\np\ne\n \nn\ne\nu\nr\na\nl\n \nf\ni\ne\nl\nd\ns\n*\n \ns\nh\nu\nn\n-\ni\nc\nh\ni\n \na\nm\na\nr\ni\n*\n*\n \nt\nh\ne\n \nc\ne\nn\nt\ne\nr\n \nf\no\nr\n \ns\ny\ns\nt\ne\nm\ns\n \nn\ne\nu\nr\no\ns\nc\ni\ne\nn\nc\ne\n,\n \nu\nn\ni\nv\ne\nr\ns\ni\nt\ny\n \no\nf\n \nm\na\ns\ns\na\nc\nh\nu\ns\ne\nt\nt\ns\n,\n \na\nm\nh\ne\nr\ns", "8360 \u2022 the journal of neuroscience, august 9, 2006 \u2022 26(32):8360 \u2013 8367\n\nbehavioral/systems/cognitive\n\nthe role of the ventromedial prefrontal cortex in abstract\nstate-based inference during decision making in humans\n\nalan n. hampton,1 peter bossaerts,1,2 and john p. o\u2019doherty1,2\n1computation and neural systems program and 2division of humanities and social sciences, california institute of technology, pasadena, california 91125\n\nmany real-life decision-making problems incorporate higher-order s", "forum\n\nplanning  as  inference\n\nmatthew  botvinick1 and  marc  toussaint2\n\n1 princeton  neuroscience  institute  and  department  of  psychology,  princeton  university,  princeton,  nj,  usa\n2 department  of  mathematics  and  computer  science,  freie  universita\u00a8 t  berlin,  berlin,  germany\n\nrecent  developments  in  decision-making  research  are\nbringing  the  topic  of  planning  back  to  center  stage  in\ncognitive  science.  this  renewed  interest  reopens  an  old,\nbut  still  unansw", "article\n\ndoi:10.1038/nature11129\n\nneural population dynamics during\nreaching\n\nmark m. churchland1,2,3*, john p. cunningham4,5*, matthew t. kaufman2,3, justin d. foster2, paul nuyujukian6,7,\nstephen i. ryu2,8 & krishna v. shenoy2,3,6,9\n\nmost theories of motor cortex have assumed that neural activity represents movement parameters. this view derives\nfrom what is known about primary visual cortex, where neural activity represents patterns of light. yet it is unclear how\nwell the analogy between mot", "0\n2\n0\n2\n\n \n\nv\no\nn\n6\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n0\n5\n3\n0\n1\n\n.\n\n6\n0\n0\n2\n:\nv\ni\nx\nr\na\n\nkernel methods through the roof:\nhandling billions of points e\ufb03ciently\n\ngiacomo meanti1, luigi carratino1, lorenzo rosasco1,2,3, and alessandro rudi4\n\n1malga, dibris, universit\u00e0 degli studi di genova, genova, italy\n2center for brains, minds and machines, mit, cambridge, ma, usa\n\n3istituto italiano di tecnologia, genova, italy\n\n4inria - d\u00e9partement d\u2019informatique de l\u2019\u00e9cole normale sup\u00e9rieure - psl research u", "synaptic neuroscience\nvoltage and spike timing interact in stdp \u2013 a unified model\n\noriginal research article\npublished: 21 july 2010\ndoi: 10.3389/fnsyn.2010.00025\n\nclaudia clopath1,2* and wulfram gerstner1\n\n1  laboratory of computational neuroscience, brain-mind institute, ecole polytechnique f\u00e9d\u00e9rale de lausanne, lausanne, switzerland\n2  laboratory of neurophysics and physiology, unite mixte de recherche 8119, centre national de la recherche scientifique, universit\u00e9 paris descartes, paris, fran", "neuron\n\narticle\n\nmatching categorical object representations\nin inferior temporal cortex of man and monkey\n\nnikolaus kriegeskorte,1,* marieke mur,1,2 douglas a. ruff,1 roozbeh kiani,3 jerzy bodurka,1,4 hossein esteky,5,6\nkeiji tanaka,7 and peter a. bandettini1,4\n1section on functional imaging methods, laboratory of brain and cognition, national institute of mental health, national institutes of health,\nbethesda, md 20892-1148, usa\n2department of cognitive neuroscience, faculty of psychology, maa", "letter\n\ncommunicated by yoshua bengio\n\nan approximation of the error backpropagation\nalgorithm in a predictive coding network\nwith local hebbian synaptic plasticity\n\njames c. r. whittington\njames.whittington@ndcn.ox.ac.uk\nmrc brain network dynamics unit, university of oxford, oxford, ox1 3th, u.k.,\nand fmrib centre, nuf\ufb01eld department of clinical neurosciences, university\nof oxford, john radcliffe hospital, oxford, ox3 9du, u.k.\n\nrafal bogacz\nrafal.bogacz@ndcn.ox.ac.uk\nmrc brain network dynamics", "7\n1\n0\n2\n\n \n\nv\no\nn\n \n5\n\n \n \n]\n\n.\n\nc\no\nh\nt\na\nm\n\n[\n \n \n\n2\nv\n2\n1\n4\n0\n1\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\ngradient descent can take exponential time to\n\nescape saddle points\n\nsimon s. du\n\ncarnegie mellon university\n\nssdu@cs.cmu.edu\n\nchi jin\n\nuniversity of california, berkeley\n\nchijin@berkeley.edu\n\njason d. lee\n\nuniversity of southern california\n\njasonlee@marshall.usc.edu\n\nmichael i. jordan\n\nuniversity of california, berkeley\njordan@cs.berkeley.edu\n\nbarnab\u00e1s p\u00f3czos\n\ncarnegie mellon university\nbapoczos@cs.cmu.e", "neuron\n\narticle\n\nmodeling the spatial reach of the lfp\n\nhenrik linde\u00b4 n,1,2 tom tetzlaff,1,3 tobias c. potjans,3,4,5 klas h. pettersen,1,6 sonja gru\u00a8 n,3,7,8 markus diesmann,3,4,8,9\nand gaute t. einevoll1,6,*\n1department of mathematical sciences and technology, norwegian university of life sciences, n-1432 a\u02da s, norway\n2department of computational biology, school of computer science and communication, royal institute of technology (kth),\n10044 stockholm, sweden\n3institute of neuroscience and med", "a simple framework for contrastive learning of visual representations\n\nting chen 1 simon kornblith 1 mohammad norouzi 1 geoffrey hinton 1\n\nabstract\n\nthis paper presents simclr: a simple framework\nfor contrastive learning of visual representations.\nwe simplify recently proposed contrastive self-\nsupervised learning algorithms without requiring\nspecialized architectures or a memory bank. in\norder to understand what enables the contrastive\nprediction tasks to learn useful representations,\nwe system", "provably faster gradient descent via long steps\n\nbenjamin grimmer\u2217\n\nabstract\n\nthis work establishes provably faster convergence rates for gradient descent in smooth convex\noptimization via a computer-assisted analysis technique. our theory allows nonconstant stepsize\npolicies with frequent long steps potentially violating descent by analyzing the overall effect of\nmany iterations at once rather than the typical one-iteration inductions used in most first-order\nmethod analyses. we show that long ", "decoupled neural interfaces using synthetic gradients\n\nmax jaderberg 1 wojciech marian czarnecki 1 simon osindero 1 oriol vinyals 1 alex graves 1 david silver 1\n\nkoray kavukcuoglu 1\n\n7\n1\n0\n2\n\n \nl\nu\nj\n \n\n3\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n3\n4\n3\n5\n0\n\n.\n\n8\n0\n6\n1\n:\nv\ni\nx\nr\na\n\nabstract\n\ntraining directed neural networks typically re-\nquires forward-propagating data through a com-\nputation graph, followed by backpropagating er-\nror signal, to produce weight updates. all lay-\ners, or more generally, module", "training-free uncertainty estimation for\ndense regression: sensitivity as a surrogate\n\nlu mi1, hao wang2, yonglong tian1, hao he1, nir n shavit1\n\n1 mit csail\n\n2 rutgers university\n\nlumi@mit.edu\n\nabstract\n\nuncertainty estimation is an essential step in the evaluation\nof the robustness for deep learning models in computer vi-\nsion, especially when applied in risk-sensitive areas. how-\never, most state-of-the-art deep learning models either fail\nto obtain uncertainty estimation or need significant ", "1\n2\n0\n2\n\n \n\nn\nu\nj\n \n\n8\n1\n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n4\nv\n9\n0\n4\n3\n0\n\n.\n\n0\n1\n0\n2\n:\nv\ni\nx\nr\na\n\npublished as a conference paper at iclr 2021\n\nlearning mesh-based simulation\nwith graph networks\n\ntobias pfaff\u2217, meire fortunato\u2217, alvaro sanchez-gonzalez\u2217, peter w. battaglia\ndeepmind, london, uk\n{tpfaff,meirefortunato,alvarosg,peterbattaglia}@google.com\n\nabstract\n\nmesh-based simulations are central to modeling complex physical systems in\nmany disciplines across science and engineering. mesh representations", "journal of machine learning research 18 (2017) 1-52\n\nsubmitted 10/16; revised 6/17; published 7/17\n\nconvolutional neural networks analyzed via\n\nconvolutional sparse coding\n\nvardan papyan*\ndepartment of computer science\ntechnion - israel institute of technology\ntechnion city, haifa 32000, israel\n\nyaniv romano*\ndepartment of electrical engineering\ntechnion - israel institute of technology\ntechnion city, haifa 32000, israel\n\nmichael elad\ndepartment of computer science\ntechnion - israel institute of", "23 jan 2002 14:1\n\nar\n\nar148-13.tex ar148-13.sgm\n\nlatex2e(2001/05/10)\n\np1: gjc\n\n10.1146/annurev.physiol.64.092501.114547\n\nannu. rev. physiol. 2002. 64:355\u2013405\ndoi: 10.1146/annurev.physiol.64.092501.114547\ncopyright c(cid:176) 2002 by annual reviews. all rights reserved\n\nshort-term synaptic plasticity\n\nrobert s. zucker\ndepartment of molecular and cell biology, university of california, berkeley,\ncalifornia 94720; e-mail: zucker@socrates.berkeley.edu\n\nwade g. regehr\ndepartment of neurobiology, harv", "serotonin modulation of cortical neurons and networks\npau celada 1,2, m. victoria puig 3 and francesc artigas 1,2*\n\n1 department of neurochemistry and neuropharmacology, institut d\u2019 investigacions biom\u00e8diques de barcelona (csic), idibaps, barcelona, spain\n2 centro de investigaci\u00f3n biom\u00e9dica en red de salud mental (cibersam), madrid, spain\n3 the picower institute for learning and memory, massachusetts institute of technology, cambridge, ma, usa\n\nreview article\npublished: 19 april 2013\ndoi: 10.338", "journal of machine learning research 11 (2010) 3371-3408\n\nsubmitted 5/10; published 12/10\n\nstacked denoising autoencoders: learning useful representations in\n\na deep network with a local denoising criterion\n\npascal vincent\nd\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle\nuniversit\u00b4e de montr\u00b4eal\n2920, chemin de la tour\nmontr\u00b4eal, qu\u00b4ebec, h3t 1j8, canada\nhugo larochelle\ndepartment of computer science\nuniversity of toronto\n10 king\u2019s college road\ntoronto, ontario, m5s 3g4, canada\n\npasca", "article\n\nlinking connectivity, dynamics, and computations\nin low-rank recurrent neural networks\n\nhighlights\nd we study network models characterized by minimal\n\nconnectivity structures\n\nauthors\n\nfrancesca mastrogiuseppe,\nsrdjan ostojic\n\nd for such models, low-dimensional dynamics can be directly\n\ninferred from connectivity\n\ncorrespondence\nsrdjan.ostojic@ens.fr\n\nd computations emerge from distributed and mixed\n\nrepresentations\n\nd implementing speci\ufb01c tasks yields predictions linking\n\nconnectivity ", "j neurophysiol 100: 3445\u20133457, 2008.\nfirst published october 1, 2008; doi:10.1152/jn.90833.2008.\n\ninnovative methodology\n\ntoward optimal target placement for neural prosthetic devices\n\njohn p. cunningham,1 byron m. yu,1,4,5 vikash gilja,2 stephen i. ryu,3 and krishna v. shenoy1,4\n1departments of electrical engineering, 2computer science, and 3neurosurgery, and 4neurosciences program, stanford university,\nstanford, california; and 5gatsby computational neuroscience unit university college london,", " \n\n \n\nwww.sciencemag.org/content/357/6355/1033/suppl/dc1 \n\n \n \n \n \n\nsupplementary materials for \n\nbehavioral time scale synaptic plasticity underlies ca1 place fields \n\nkatie c. bittner, aaron d. milstein, christine grienberger, sandro romani, jeffrey c. magee* \n\n*corresponding author. email: mageej@janelia.hhmi.org \n\npublished 8 september 2017, science 357, 1033 (2017) \n\ndoi: 10.1126/science.aan3846 \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n \n\n \n \nthis pdf file includes: \n \n\nmaterials and methods \nfigs. s1 to s9 \n", "reports\n\npronounced than in drbp nulls (5, 14). function-\nally, drbp and bruchpilot phenotypes appear\nsimilar: both demonstrate decreased and desyn-\nchronized evoked sv release with atypical short-\nterm facilitation. however, the deficits in evoked\nsv release are much more severe in drbp nulls\nthan in bruchpilot nulls [i.e., release occurs at 5%\nversus 30% (5) of the respective wild-type level].\ndrbp levels were clearly reduced in bruchpilot\nmutants (fig. s7), whereas gross bruchpilot lev-\nels w", "1\n2\n0\n2\n\n \n\ng\nu\na\n2\n\n \n\n \n \n]\n\n.\n\nc\nn\no\ni\nb\n-\nq\n[\n \n \n\n1\nv\n0\n1\n2\n1\n0\n\n.\n\n8\n0\n1\n2\n:\nv\ni\nx\nr\na\n\n| chethanpandarinath2\n\noriginal article\nneural data science / analysis\nrepresentationlearningforneuralpopulation\nactivitywithneuraldatatransformers\njoelye1\n1schoolofinteractivecomputing,georgia\ninstituteoftechnology,atlanta,ga,usa\n2wallaceh.coulterdepartmentof\nbiomedicalengineeringanddepartmentof\nneurosurgery,emoryuniversityand\ngeorgiainstituteoftechnology,atlanta,\nga,usa\ncorrespondence\nemail: joelye9@g", "a r t i c l e s\n\ngaba promotes the competitive selection of dendritic \nspines by controlling local ca2+ signaling\ntatsuya hayama1,2,6, jun noguchi1,2,6, satoshi watanabe1,2, noriko takahashi1,2, akiko hayashi-takagi1\u20133, \ngraham c r ellis-davies4, masanori matsuzaki2,3,5 & haruo kasai1,2\n\nactivity-dependent competition of synapses plays a key role in neural organization and is often promoted by gaba; however, its \ncellular bases are poorly understood. excitatory synapses of cortical pyramidal neu", "a\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\n\nt\n\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\na\nu\nt\nh\no\nr\n \n\nm\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nhhs public access\nauthor manuscript\nj math psychol. author manuscript; available in pmc 2019 november 19.\n\npublished in final edited form as:\nj math psychol. 2017 february ; 76(b): 65\u201379. doi:10.1016/j.jmp.2016.01.001.\n\napproaches to analysis in model-based cognitive neuroscience\n\nbrandon m. turnera,*, birte u. forstmannb, bradley c. lovec, tho", "text categorization with support vector\nmachines: learning with many relevant\n\nfeatures\n\nthorsten joachims\n\nuniversit\u0007at dortmund\n\ninformatik ls\b, baroper str. \u0003\u0000\u0001\n\n\u0004\u0004\u0002\u0002\u0001 dortmund, germany\n\nabstract. this paper explores the use of support vector machines\n\u001csvms\u001d for learning text classi\u0000ers from examples. it analyzes the par-\nticular properties of learning with text data and identi\u0000es why svms\nare appropriate for this task. empirical results support the theoretical\n\u0000ndings. svms achieve substanti", "7\n1\n0\n2\n\n \n\nv\no\nn\n4\n\n \n\n \n \n]\nl\nm\n\n.\nt\na\nt\ns\n[\n \n \n\n2\nv\n4\n2\n2\n7\n0\n\n.\n\n5\n0\n7\n1\n:\nv\ni\nx\nr\na\n\naide: an algorithm for measuring the accuracy of\n\nprobabilistic inference algorithms\n\nmarco f. cusumano-towner\nprobabilistic computing project\n\nmassachusetts institute of technology\n\nmarcoct@mit.edu\n\nvikash k. mansinghka\n\nprobabilistic computing project\n\nmassachusetts institute of technology\n\nvkm@mit.edu\n\nabstract\n\napproximate probabilistic inference algorithms are central to many \ufb01elds. exam-\nples include", "letter\n\ncommunicated by terrence j. sejnowski\n\nrelating stdp to bcm\n\neugene m. izhikevich\neugene.izhikevich@nsi.edu,www.nsi.edu/users/izhikevich\nniraj s. desai\ndesai@nsi.edu\nthe neurosciences institute, san diego, ca, 92121, u.s.a.\n\nwe demonstrate that the bcm learning rule follows directly from stdp\nwhen pre- and postsynaptic neurons \u0005re uncorrelated or weakly corre-\nlated poisson spike trains, and only nearest-neighbor spike interactions\nare taken into account.\n\n1 introduction\n\nover the past s", "a r t i c l e s\n\ncortical activity in the null space: permitting \npreparation without movement\nmatthew t kaufman1\u20133, mark m churchland4\u20137, stephen i ryu2,8 & krishna v shenoy1,2,9,10\nneural circuits must perform computations and then selectively output the results to other circuits. yet synapses do not change \nradically at millisecond timescales. a key question then is: how is communication between neural circuits controlled? in motor \ncontrol, brain areas directly involved in driving movement a", "supplementary materials for: a solution to the learning\n\ndilemma for recurrent networks of spiking neurons\n\nguillaume bellec1,\u25e6, franz scherr1,\u25e6, anand subramoney1,\n\nelias hajek1, darjan salaj1, robert legenstein1, wolfgang maass1,\u2217\n\n1institute of theoretical computer science, graz university of technology,\n\nin\ufb00eldgasse 16b, graz, austria\n\n\u2217 to whom correspondence should be addressed; e-mail: maass@igi.tugraz.at.\n\n\u25e6 equal contributions.\n\ncontents\n\nsupplementary figures\n\nfigure 1 performance comp", "published as a conference paper at iclr 2020\n\nimplementing inductive bias for different\nnavigation tasks through diverse rnn attr-\nractors\n\ntie xu, omri barak\nrappaport faculty of medicine and network biology research laboratory\ntechnion, israel institute of technology\nhaifa, 320003, israel\nfexutie@gmail.com, omri.barak@gmail.com\n\nabstract\n\nnavigation is crucial for animal behavior and is assumed to require an internal rep-\nresentation of the external environment, termed a cognitive map. the pre", "see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/266656356\n\nevolving deep unsupervised convolutional networks for vision-based\nreinforcement learning\n\narticle \u00b7 july 2014\n\ndoi: 10.1145/2576768.2598358\n\ncitations\n91\n\n3 authors, including:\n\nfaustino gomez\nnnaisense sa\n\n71 publications\u00a0\u00a0\u00a09,850 citations\u00a0\u00a0\u00a0\n\nsee profile\n\nreads\n3,379\n\nall content following this page was uploaded by faustino gomez on 29 march 2016.\n\nthe user has requested en", "supplementary information\n\ndoi:10.1038/nature12160\n\nsupplementary methods\n\nm.1 de\ufb01nition and properties of dimensionality\n\nconsider an experiment with a discrete set of c di\ufb00erent experimental conditions, correspond-\ning to all distinct combinations of task-relevant variables. for example, in the experiment\nthat we analyzed, the animal has to remember a sequence of two visual cues. the \ufb01rst and\nthe second cue can be any of four visual objects, and the second cue cannot be the same\nas the \ufb01rst on", "on the importance of initialization and momentum in deep learning\n\nilya sutskever1\njames martens\ngeorge dahl\ngeo\u21b5rey hinton\n\nabstract\n\ndeep and recurrent neural networks (dnns\nand rnns respectively) are powerful mod-\nels that were considered to be almost impos-\nsible to train using stochastic gradient de-\nscent with momentum.\nin this paper, we\nshow that when stochastic gradient descent\nwith momentum uses a well-designed random\ninitialization and a particular type of slowly\nincreasing schedule fo", "5\n1\n0\n2\n\n \nr\na\n\nm\n2\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n3\nv\n7\n6\n1\n3\n0\n\n.\n\n2\n0\n5\n1\n:\nv\ni\nx\nr\na\n\nbatch normalization: accelerating deep network training by\n\nreducing internal covariate shift\n\nsergey ioffe\n\nchristian szegedy\n\ngoogle inc., sioffe@google.com\n\ngoogle inc., szegedy@google.com\n\nabstract\n\ntraining deep neural networks is complicated by the fact\nthat the distribution of each layer\u2019s inputs changes during\ntraining, as the parameters of the previous layers change.\nthis slows down the training by re", "further\nannual\nreviews\nclick here for quick links to \nannual reviews content online, \nincluding:\n\u2022 other articles in this volume\n\u2022 top cited articles\n\u2022 top downloaded articles\n\u2022 our comprehensive search\n\ncerebellum-like structures\nand their implications for\ncerebellar function\ncurtis c. bell,1 victor han,2\nand nathaniel b. sawtell1\n1neurological sciences institute, oregon health and science university,\nbeaverton, oregon 97006; email: bellc@ohsu.edu, sawtelln@ohsu.edu\n2oregon regional primate cen", "letter\n\ndoi:10.1038/nature19818\n\ncortico-fugal output from visual cortex promotes \nplasticity of innate motor behaviour\n\nbao-hua liu1,2, andrew d. huberman3 & massimo scanziani1,2,4\n\nthe mammalian visual cortex massively innervates the brainstem, \na phylogenetically older structure, via cortico-fugal axonal \nprojections1. many cortico-fugal projections target brainstem \nnuclei that mediate innate motor behaviours, but the function \nof these projections remains poorly understood1\u20134. a prime \nexam", "a r t i c l e s\n\nlearning the value of information in an uncertain world\n\ntimothy e j behrens1,2, mark w woolrich1, mark e walton2 & matthew f s rushworth1,2\n\nour decisions are guided by outcomes that are associated with decisions made in the past. however, the amount of in\ufb02uence\neach past outcome has on our next decision remains unclear. to ensure optimal decision-making, the weight given to decision\noutcomes should re\ufb02ect their salience in predicting future outcomes, and this salience should b", "2017 ieee/rsj international conference on intelligent robots and systems (iros)\nseptember 24\u201328, 2017, vancouver, bc, canada\n\n978-1-5386-2682-5/17/$31.00 \u00a92017 ieee\n\n2371\n\nauthorized licensed use limited to: university of washington libraries. downloaded on september 29,2023 at 07:03:10 utc from ieee xplore.  restrictions apply. \n\ndeepreinforcementlearningwithsuccessorfeaturesfornavigationacrosssimilarenvironmentsjingweizhangjosttobiasspringenbergjoschkaboedeckerwolframburgardabstract\u2014inthispape", "richards, b. a., lillicrap, t. p., beaudoin, p., bengio, y., bogacz, r.,\nchristensen, a., clopath, c., costa, r. p., de berker, a., ganguli, s.,\ngillon, c. j., hafner, d., kepecs, a., kriegeskorte, n., latham, p.,\nlindsay, g. w., miller, k. d., naud, r., pack, c. c., ... kording, k. p.\n(2019). a deep learning framework for neuroscience. nature\nneuroscience, 22(11), 1761-1770. https://doi.org/10.1038/s41593-\n019-0520-2\n\npeer reviewed version\n\nlink to published version (if available):\n10.1038/s415", "j neurophysiol 101: 1813\u20131822, 2009.\nfirst published january 28, 2009; doi:10.1152/jn.91050.2008.\n\ntime course of attentional modulation in the frontal eye field during\ncurve tracing\n\np. s. khayat,1,3 a. pooresmaeili,1 and p. r. roelfsema1,2\n1department of vision and cognition, netherlands institute for neuroscience, institute of the royal netherlands academy of arts\nand sciences; 2department of integrative neurophysiology, centre for neurogenomics and cognitive research, vrije universiteit,\nams", "8\n1\n0\n2\n\n \nc\ne\nd\n5\n\n \n\n \n \n]\n\ng\nl\n.\ns\nc\n[\n \n \n\n2\nv\n2\n4\n8\n0\n1\n\n.\n\n5\n0\n8\n1\n:\nv\ni\nx\nr\na\n\napproximating real-time recurrent learning with\n\nrandom kronecker factors\n\nasier mujika \u2217\n\ndepartment of computer science\n\neth z\u00fcrich, switzerland\nasierm@inf.ethz.ch\n\nflorian meier\n\ndepartment of computer science\n\neth z\u00fcrich, switzerland\nmeierflo@inf.ethz.ch\n\nangelika steger\n\ndepartment of computer science\n\neth z\u00fcrich, switzerland\nsteger@inf.ethz.ch\n\nabstract\n\ndespite all the impressive advances of recurrent ne", "letters\n\nvol 453 | 19 june 2008 | doi:10.1038/nature06910\n\nneural substrates of vocalization feedback\nmonitoring in primate auditory cortex\nsteven j. eliades1 & xiaoqin wang1\n\nvocal communication involves both speaking and hearing, often\ntaking place concurrently. vocal production, including human\nspeech and animal vocalization, poses a number of unique chal-\nlenges for the auditory system. it is important for the auditory\nsystem to monitor external sounds continuously from the acoustic\nenvironm", "axiomatic attribution for deep networks\n\nmukund sundararajan * 1 ankur taly * 1 qiqi yan * 1\n\nabstract\n\nwe study the problem of attributing the pre-\ndiction of a deep network to its input features,\na problem previously studied by several other\nworks. we identify two fundamental axioms\u2014\nsensitivity and implementation invariance that\nattribution methods ought to satisfy. we show\nthat they are not satis\ufb01ed by most known attri-\nbution methods, which we consider to be a fun-\ndamental weakness of thos", "h i g h l i g h t s\n\nc e l l b i o lo g y   o f  t h e   n e u r o n\n\ndirecting neurofilament traffic\n\nneurofilaments are components of the\nneuronal cytoskeleton that are important\nboth for the maturation of axons and the\nmaintenance of axonal integrity. the\nindividual subunits that make up the\nneurofilaments are transported from the cell\nbody to the axon, where they bundle together\nto form filaments. so how does the neuron\nensure that the neurofilaments are assembled\nin the correct compartment "]}